<!DOCTYPE html>
<!-- saved from url=(0046)https://www.lesswrong.com/users/scottalexander -->
<html lang="en" class=" iiafsv idc0_345"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="preload" as="style" href="./scott_files/allStyles"><link rel="stylesheet" type="text/css" href="./scott_files/icon"><link rel="stylesheet" type="text/css" href="./scott_files/reset-min.css"><link rel="stylesheet" type="text/css" href="./scott_files/css"><link rel="stylesheet" type="text/css" href="./scott_files/jvr1gjm.css"><script type="text/javascript" async="" src="./scott_files/linkid.js"></script><script type="text/javascript" async="" src="./scott_files/js"></script><script type="text/javascript" async="" src="./scott_files/analytics.js"></script><script type="text/javascript" async="" src="./scott_files/recaptcha__en.js" crossorigin="anonymous" integrity="sha384-0UmWi2drUOFtiIl9tXA3B9fWn6Oz5K3vecq0zC8vAzesBQkPzPdlJKXnimDbxieW"></script><script async="" src="./scott_files/gtm.js"></script><script>window.publicInstanceSettings = {"forumType":"LessWrong","title":"LessWrong","siteNameWithArticle":"LessWrong","sentry":{"url":"https://1ab1949fc8d04608b43132f37bb2a1b0@sentry.io/1301611","environment":"production","release":"69f0f3c5d57b596e8249571383f8a280eff9bb23"},"debug":false,"aboutPostId":"bJ2haLkcGeLtTWaD5","faqPostId":"2rWKkWuPrgTMpLRbp","expectedDatabaseId":"production","tagline":"A community blog devoted to refining the art of rationality","faviconUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico","forumSettings":{"headerTitle":"LESSWRONG","shortForumTitle":"LW","tabTitle":"LessWrong"},"analytics":{"environment":"lesswrong.com"},"testServer":false,"fmCrosspost":{"siteName":"the EA Forum","baseUrl":"https://forum.effectivealtruism.org/"}}</script><script defer="" src="./scott_files/bundle.js"></script><title>Scott Alexander - LessWrong</title><meta data-react-helmet="true" name="twitter:image:src" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"><meta data-react-helmet="true" property="og:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"><meta data-react-helmet="true" http-equiv="Accept-CH" content="DPR, Viewport-Width, Width"><meta data-react-helmet="true" property="og:title" content="Scott Alexander - LessWrong"><meta data-react-helmet="true" name="description" content="Scott Alexander&#39;s profile on LessWrong — A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" name="twitter:description" content="Scott Alexander&#39;s profile on LessWrong — A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:url" content="https://www.lesswrong.com/users/scottalexander"><meta data-react-helmet="true" property="og:description" content="Scott Alexander&#39;s profile on LessWrong — A community blog devoted to refining the art of rationality"><link data-react-helmet="true" rel="canonical" href="https://www.lesswrong.com/users/scottalexander"><link data-react-helmet="true" rel="shortcut icon" href="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico"><link data-react-helmet="true" rel="alternate" type="application/rss+xml" href="https://www.lesswrong.com/feed.xml">
<script>var tabId = "RWjpH3oyCcXqWeF6B"</script>
<script> var publicSettings = {"siteImage":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg","intercomAppId":"wtb8z7sj","googleTagManager":{"apiKey":"GTM-TRC765W"},"reCaptcha":{"apiKey":"6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc"},"googleMaps":{"apiKey":"AIzaSyA3C48rl26gynG3qIuNuS-3Bh_Zz9jFXkY"},"algolia":{"appId":"Z0GR6EXQHD","searchKey":"0b1d20b957917dbb5e1c2f3ad1d04ee2","autoSyncIndexes":false,"indexPrefix":"test_"},"ckEditor":{"uploadUrl":"https://39669.cke-cs.com/easyimage/upload/","webSocketUrl":"39669.cke-cs.com/ws"},"logRocket":{"apiKey":"mtnxzn/lesswrong","sampleDensity":5},"hasEvents":true,"hideUnreviewedAuthorComments":false,"cloudinary":{"cloudName":"lesswrong-2-0","uploadPresetGridImage":"tz0mgw2s","uploadPresetBanner":"navcjwf7"},"forum":{"numberOfDays":10,"numberOfWeeks":4,"numberOfMonths":4,"numberOfYears":4,"postInterval":30,"maxPostsPerDay":5},"locale":"en-US","legacyRouteAcronym":"lw","logoUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1498011194/LessWrong_Logo_skglnw.svg","maxDocumentsPerRequest":5000,"commentInterval":15,"timeDecayFactor":1.15,"mapbox":{"apiKey":"pk.eyJ1IjoiaGFicnlrYSIsImEiOiJjaWxvcnhidzgwOGlodHJrbmJ2bmVmdjRtIn0.inr-_5rWOOslGQxY8iDFOA"},"petrov":{"afterTime":1664247600000,"beforeTime":1664161200000,"petrovServerUrl":"https://forum.effectivealtruism.org/graphql","petrovPostId":"KTEciTeFwL2tTujZk","petrovGamePostId":"KTEciTeFwL2tTujZk"},"gatherTownMessage":"Schelling social hours on Tues 1pm and Thurs 6pm PT","gardenOpenToPublic":false,"frontpageScoreBonus":0,"stripe":{"publicKey":"pk_live_51HtKAwA2QvoATZCZiy9f2nc6hA52YS1BE81cFu9FEV1IKar0Bwx6hIpxxxYHnhaxO9KM7kRYofZId3sUUI7Q0NeO00tGni3Wza"},"defaultModeratorComments":[{"label":"Option A","id":"FfMok764BCY6ScqWm"},{"label":"Option B","id":"yMHoNoYZdk5cKa3wQ"}],"gatherTownUserTrackingIsBroken":true,"annualReview":{"announcementPostPath":"/posts/qCc7tm29Guhz6mtf7/the-lesswrong-2021-review-intellectual-circle-expansion","votingResultsPostId":"TSaJ9Zcvc3KWh3bjX","start":"2022-12-01T18:00:00Z","nominationPhaseEnd":"2022-12-15T08:00:00Z","reviewPhaseEnd":"2023-01-15T08:00:00Z","end":"2023-02-01T12:00:00Z"},"enableGoodHeartProject":false,"moderationEmail":"team@lesswrong.com"}</script><script>window.themeOptions = {"name":"dark"}</script><style id="jss-insertion-point"></style><style data-jss="" data-meta="MuiSvgIcon">
.MuiSvgIcon-root {
  fill: currentColor;
  width: 1em;
  height: 1em;
  display: inline-block;
  font-size: 24px;
  transition: fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  user-select: none;
  flex-shrink: 0;
}
.MuiSvgIcon-colorPrimary {
  color: #5f9b65;
}
.MuiSvgIcon-colorSecondary {
  color: #5f9b65;
}
.MuiSvgIcon-colorAction {
  color: #fff;
}
.MuiSvgIcon-colorError {
  color: #bf360c;
}
.MuiSvgIcon-colorDisabled {
  color: rgba(255, 255, 255, 0.3);
}
.MuiSvgIcon-fontSizeInherit {
  font-size: inherit;
}
.MuiSvgIcon-fontSizeSmall {
  font-size: 20px;
}
.MuiSvgIcon-fontSizeLarge {
  font-size: 36px;
}
</style><style data-jss="" data-meta="MuiTooltip">
.MuiTooltip-popper {
  z-index: 1500;
  opacity: 0.9;
}
.MuiTooltip-tooltip {
  color: #fff;
  padding: .7rem;
  z-index: 10000000;
  font-size: 1rem;
  max-width: 300px;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  background-color: rgba(75,75,75,.94);
}
.MuiTooltip-touch {
  padding: 8px 16px;
  font-size: 0.875rem;
  line-height: 1.14286em;
}
.MuiTooltip-tooltipPlacementLeft {
  margin: 0 24px ;
  transform-origin: right center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementLeft {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementRight {
  margin: 0 24px;
  transform-origin: left center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementRight {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementTop {
  margin: 24px 0;
  transform-origin: center bottom;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementTop {
    margin: 14px 0;
  }
}
.MuiTooltip-tooltipPlacementBottom {
  margin: 24px 0;
  transform-origin: center top;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementBottom {
    margin: 14px 0;
  }
}
</style><style data-jss="" data-meta="MuiTouchRipple">
.MuiTouchRipple-root {
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: block;
  z-index: 0;
  position: absolute;
  overflow: hidden;
  border-radius: inherit;
  pointer-events: none;
}
.MuiTouchRipple-ripple {
  top: 0;
  left: 0;
  width: 50px;
  height: 50px;
  opacity: 0;
  position: absolute;
}
.MuiTouchRipple-rippleVisible {
  opacity: 0.3;
  transform: scale(1);
  animation: mui-ripple-enter 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-ripplePulsate {
  animation-duration: 200ms;
}
.MuiTouchRipple-child {
  width: 100%;
  height: 100%;
  opacity: 1;
  display: block;
  border-radius: 50%;
  background-color: currentColor;
}
.MuiTouchRipple-childLeaving {
  opacity: 0;
  animation: mui-ripple-exit 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-childPulsate {
  top: 0;
  left: 0;
  position: absolute;
  animation: mui-ripple-pulsate 2500ms cubic-bezier(0.4, 0, 0.2, 1) 200ms infinite;
}
@-webkit-keyframes mui-ripple-enter {
  0% {
    opacity: 0.1;
    transform: scale(0);
  }
  100% {
    opacity: 0.3;
    transform: scale(1);
  }
}
@-webkit-keyframes mui-ripple-exit {
  0% {
    opacity: 1;
  }
  100% {
    opacity: 0;
  }
}
@-webkit-keyframes mui-ripple-pulsate {
  0% {
    transform: scale(1);
  }
  50% {
    transform: scale(0.92);
  }
  100% {
    transform: scale(1);
  }
}
</style><style data-jss="" data-meta="MuiButtonBase">
.MuiButtonBase-root {
  color: inherit;
  border: 0;
  margin: 0;
  cursor: pointer;
  display: inline-flex;
  outline: none;
  padding: 0;
  position: relative;
  align-items: center;
  user-select: none;
  border-radius: 0;
  vertical-align: middle;
  justify-content: center;
  -moz-appearance: none;
  text-decoration: none;
  background-color: transparent;
  -webkit-appearance: none;
  -webkit-tap-highlight-color: transparent;
}
.MuiButtonBase-root::-moz-focus-inner {
  border-style: none;
}
.MuiButtonBase-root.MuiButtonBase-disabled {
  cursor: default;
  pointer-events: none;
}
</style><style data-jss="" data-meta="MuiToolbar">
.MuiToolbar-root {
  display: flex;
  position: relative;
  align-items: center;
}
.MuiToolbar-gutters {
  padding-left: 16px;
  padding-right: 16px;
}
@media (min-width:600px) {
  .MuiToolbar-gutters {
    padding-left: 24px;
    padding-right: 24px;
  }
}
.MuiToolbar-regular {
  min-height: 56px;
}
@media (min-width:0px) and (orientation: landscape) {
  .MuiToolbar-regular {
    min-height: 48px;
  }
}
@media (min-width:600px) {
  .MuiToolbar-regular {
    min-height: 64px;
  }
}
.MuiToolbar-dense {
  min-height: 48px;
}
</style><style data-jss="" data-meta="MuiIconButton">
.MuiIconButton-root {
  flex: 0 0 auto;
  color: #fff;
  padding: 12px;
  overflow: visible;
  font-size: 1.5rem;
  text-align: center;
  transition: background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  border-radius: 50%;
}
.MuiIconButton-root:hover {
  background-color: rgba(255, 255, 255, 0.1);
}
.MuiIconButton-root.MuiIconButton-disabled {
  color: rgba(255, 255, 255, 0.3);
}
@media (hover: none) {
  .MuiIconButton-root:hover {
    background-color: transparent;
  }
}
.MuiIconButton-root:hover.MuiIconButton-disabled {
  background-color: transparent;
}
.MuiIconButton-colorInherit {
  color: inherit;
}
.MuiIconButton-colorPrimary {
  color: #5f9b65;
}
.MuiIconButton-colorPrimary:hover {
  background-color: rgba(95, 155, 101, 0.1);
}
@media (hover: none) {
  .MuiIconButton-colorPrimary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-colorSecondary {
  color: #5f9b65;
}
.MuiIconButton-colorSecondary:hover {
  background-color: rgba(95, 155, 101, 0.1);
}
@media (hover: none) {
  .MuiIconButton-colorSecondary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-label {
  width: 100%;
  display: flex;
  align-items: inherit;
  justify-content: inherit;
}
</style><style data-jss="" data-meta="MuiSnackbar">
.MuiSnackbar-root {
  left: 0;
  right: 0;
  z-index: 1400;
  display: flex;
  position: fixed;
  align-items: center;
  justify-content: center;
}
.MuiSnackbar-anchorOriginTopCenter {
  top: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginBottomCenter {
  bottom: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginTopRight {
  top: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopRight {
    top: 24px;
    left: auto;
    right: 24px;
  }
}
.MuiSnackbar-anchorOriginBottomRight {
  bottom: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomRight {
    left: auto;
    right: 24px;
    bottom: 24px;
  }
}
.MuiSnackbar-anchorOriginTopLeft {
  top: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopLeft {
    top: 24px;
    left: 24px;
    right: auto;
  }
}
.MuiSnackbar-anchorOriginBottomLeft {
  bottom: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomLeft {
    left: 24px;
    right: auto;
    bottom: 24px;
  }
}
</style><style data-jss="" data-meta="MuiButton">
.MuiButton-root {
  color: rgba(255,255,255,0.87);
  padding: 8px 16px;
  font-size: 0.875rem;
  min-width: 64px;
  box-sizing: border-box;
  min-height: 36px;
  transition: background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  font-weight: 500;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  text-transform: uppercase;
}
.MuiButton-root:hover {
  text-decoration: none;
  background-color: rgba(255, 255, 255, 0.1);
}
.MuiButton-root.MuiButton-disabled {
  color: rgba(255, 255, 255, 0.3);
}
@media (hover: none) {
  .MuiButton-root:hover {
    background-color: transparent;
  }
}
.MuiButton-root:hover.MuiButton-disabled {
  background-color: transparent;
}
.MuiButton-label {
  width: 100%;
  display: inherit;
  align-items: inherit;
  justify-content: inherit;
}
.MuiButton-textPrimary {
  color: #5f9b65;
}
.MuiButton-textPrimary:hover {
  background-color: rgba(95, 155, 101, 0.1);
}
@media (hover: none) {
  .MuiButton-textPrimary:hover {
    background-color: transparent;
  }
}
.MuiButton-textSecondary {
  color: #5f9b65;
}
.MuiButton-textSecondary:hover {
  background-color: rgba(95, 155, 101, 0.1);
}
@media (hover: none) {
  .MuiButton-textSecondary:hover {
    background-color: transparent;
  }
}
.MuiButton-outlined {
  border: 1px solid rgba(255, 255, 255, 0.23);
}
.MuiButton-outlinedPrimary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedPrimary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedPrimary.MuiButton-disabled {
  border: 1px solid rgba(255, 255, 255, 0.3);
}
.MuiButton-outlinedSecondary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedSecondary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedSecondary.MuiButton-disabled {
  border: 1px solid rgba(255, 255, 255, 0.3);
}
.MuiButton-contained {
  color: #fff;
  box-shadow: 0px 1px 5px 0px rgba(255,255,255,0.2),0px 2px 2px 0px rgba(255,255,255,0.14),0px 3px 1px -2px rgba(255,255,255,0.12);
  background-color: #505050;
}
.MuiButton-contained.MuiButton-focusVisible {
  box-shadow: 0px 3px 5px -1px rgba(255,255,255,0.2),0px 6px 10px 0px rgba(255,255,255,0.14),0px 1px 18px 0px rgba(255,255,255,0.12);
}
.MuiButton-contained:active {
  box-shadow: 0px 5px 5px -3px rgba(255,255,255,0.2),0px 8px 10px 1px rgba(255,255,255,0.14),0px 3px 14px 2px rgba(255,255,255,0.12);
}
.MuiButton-contained.MuiButton-disabled {
  color: rgba(255, 255, 255, 0.3);
  box-shadow: none;
  background-color: rgba(255, 255, 255, 0.12);
}
.MuiButton-contained:hover {
  background-color: #616161;
}
@media (hover: none) {
  .MuiButton-contained:hover {
    background-color: #505050;
  }
}
.MuiButton-contained:hover.MuiButton-disabled {
  background-color: rgba(255, 255, 255, 0.12);
}
.MuiButton-containedPrimary {
  color: #000000;
  background-color: #5f9b65;
}
.MuiButton-containedPrimary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedPrimary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-containedSecondary {
  color: #000000;
  background-color: #5f9b65;
}
.MuiButton-containedSecondary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedSecondary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-fab {
  width: 56px;
  height: 56px;
  padding: 0;
  min-width: 0;
  box-shadow: 0px 3px 5px -1px rgba(255,255,255,0.2),0px 6px 10px 0px rgba(255,255,255,0.14),0px 1px 18px 0px rgba(255,255,255,0.12);
  border-radius: 50%;
}
.MuiButton-fab:active {
  box-shadow: 0px 7px 8px -4px rgba(255,255,255,0.2),0px 12px 17px 2px rgba(255,255,255,0.14),0px 5px 22px 4px rgba(255,255,255,0.12);
}
.MuiButton-extendedFab {
  width: auto;
  height: 48px;
  padding: 0 16px;
  min-width: 48px;
  border-radius: 24px;
}
.MuiButton-colorInherit {
  color: inherit;
}
.MuiButton-mini {
  width: 40px;
  height: 40px;
}
.MuiButton-sizeSmall {
  padding: 7px 8px;
  min-width: 64px;
  font-size: 0.8125rem;
  min-height: 32px;
}
.MuiButton-sizeLarge {
  padding: 8px 24px;
  min-width: 112px;
  font-size: 0.9375rem;
  min-height: 40px;
}
.MuiButton-fullWidth {
  width: 100%;
}
</style><style data-jss="" data-meta="MuiModal">
.MuiModal-root {
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 1300;
  position: fixed;
}
.MuiModal-hidden {
  visibility: hidden;
}
</style><style data-jss="" data-meta="MuiPopover">
.MuiPopover-paper {
  outline: none;
  position: absolute;
  min-width: 16px;
  max-width: calc(100% - 32px);
  overflow-y: auto;
  overflow-x: hidden;
  min-height: 16px;
  max-height: calc(100% - 32px);
}
</style><style data-jss="" data-meta="MuiMenu">
.MuiMenu-paper {
  max-height: calc(100% - 96px);
  -webkit-overflow-scrolling: touch;
}
</style><style data-jss="" data-meta="MuiBadge">
.MuiBadge-root {
  display: inline-flex;
  position: relative;
  vertical-align: middle;
}
.MuiBadge-badge {
  top: -11px;
  right: -11px;
  width: 22px;
  height: 22px;
  display: flex;
  z-index: 1;
  position: absolute;
  flex-wrap: wrap;
  font-size: 0.75rem;
  align-items: center;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  align-content: center;
  border-radius: 50%;
  flex-direction: row;
  justify-content: center;
}
.MuiBadge-colorPrimary {
  color: #000000;
  background-color: #5f9b65;
}
.MuiBadge-colorSecondary {
  color: #000000;
  background-color: #5f9b65;
}
.MuiBadge-colorError {
  color: #000000;
  background-color: #bf360c;
}
</style><style data-jss="" data-meta="MuiDrawer">
.MuiDrawer-docked {
  flex: 0 0 auto;
}
.MuiDrawer-paper {
  top: 0;
  flex: 1 0 auto;
  height: 100%;
  display: flex;
  z-index: 1200;
  outline: none;
  position: fixed;
  overflow-y: auto;
  flex-direction: column;
  -webkit-overflow-scrolling: touch;
}
.MuiDrawer-paperAnchorLeft {
  left: 0;
  right: auto;
}
.MuiDrawer-paperAnchorRight {
  left: auto;
  right: 0;
}
.MuiDrawer-paperAnchorTop {
  top: 0;
  left: 0;
  right: 0;
  bottom: auto;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorBottom {
  top: auto;
  left: 0;
  right: 0;
  bottom: 0;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorDockedLeft {
  border-right: 1px solid rgba(255, 255, 255, 0.12);
}
.MuiDrawer-paperAnchorDockedTop {
  border-bottom: 1px solid rgba(255, 255, 255, 0.12);
}
.MuiDrawer-paperAnchorDockedRight {
  border-left: 1px solid rgba(255, 255, 255, 0.12);
}
.MuiDrawer-paperAnchorDockedBottom {
  border-top: 1px solid rgba(255, 255, 255, 0.12);
}
</style><style data-jss="">
.jss90 {
  top: 0;
  left: 0;
  bottom: 0;
  z-index: 1199;
  position: fixed;
}
.jss91 {
  right: auto;
}
.jss92 {
  left: auto;
  right: 0;
}
.jss93 {
  right: 0;
  bottom: auto;
}
.jss94 {
  top: auto;
  right: 0;
  bottom: 0;
}
</style><link id="main-styles" rel="stylesheet" type="text/css" onerror="window.missingMainStylesheet=true" href="./scott_files/allStyles"><script async="" src="./scott_files/wtb8z7sj"></script><style>@font-face {
	font-family: "wticons";
	src: url("data:font/woff2;charset=utf-8;base64,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") format("woff2");
}

.wticons {
	line-height: 1;
}

.wticons:before {
	font-family: wticons !important;
	font-style: normal;
	font-weight: normal !important;
	vertical-align: top;
}

.wticon-account:before {
	content: "\f101";
}
.wticon-add:before {
	content: "\f102";
}
.wticon-cardResizeDrag:before {
	content: "\f103";
}
.wticon-casual:before {
	content: "\f104";
}
.wticon-check:before {
	content: "\f105";
}
.wticon-checkSmall:before {
	content: "\f106";
}
.wticon-chevron:before {
	content: "\f107";
}
.wticon-copy:before {
	content: "\f108";
}
.wticon-copySmall:before {
	content: "\f109";
}
.wticon-dismiss:before {
	content: "\f10a";
}
.wticon-downChevron:before {
	content: "\f10b";
}
.wticon-error:before {
	content: "\f10c";
}
.wticon-expand:before {
	content: "\f10d";
}
.wticon-feedback:before {
	content: "\f10e";
}
.wticon-filledDownArrow:before {
	content: "\f10f";
}
.wticon-find:before {
	content: "\f110";
}
.wticon-formal:before {
	content: "\f111";
}
.wticon-gift:before {
	content: "\f112";
}
.wticon-grayLogo:before {
	content: "\f113";
}
.wticon-ignore:before {
	content: "\f114";
}
.wticon-info:before {
	content: "\f115";
}
.wticon-leftChevron:before {
	content: "\f116";
}
.wticon-logo:before {
	content: "\f117";
}
.wticon-love:before {
	content: "\f118";
}
.wticon-noRecommendations:before {
	content: "\f119";
}
.wticon-paragraphRewrite:before {
	content: "\f11a";
}
.wticon-paste:before {
	content: "\f11b";
}
.wticon-pin:before {
	content: "\f11c";
}
.wticon-premium:before {
	content: "\f11d";
}
.wticon-premiumDetail:before {
	content: "\f11e";
}
.wticon-premiumFull:before {
	content: "\f11f";
}
.wticon-recommendationLight:before {
	content: "\f120";
}
.wticon-recommendationLightCard:before {
	content: "\f121";
}
.wticon-recommendationLightNoSuggestions:before {
	content: "\f122";
}
.wticon-refine:before {
	content: "\f123";
}
.wticon-rewrite:before {
	content: "\f124";
}
.wticon-rightChevron:before {
	content: "\f125";
}
.wticon-rocket:before {
	content: "\f126";
}
.wticon-sentenceExamples:before {
	content: "\f127";
}
.wticon-settings:before {
	content: "\f128";
}
.wticon-shorten:before {
	content: "\f129";
}
.wticon-tutorial:before {
	content: "\f12a";
}
.wticon-unlock:before {
	content: "\f12b";
}
.wticon-warn:before {
	content: "\f12c";
}
.wticon-WordtuneButton:before {
	content: "\f12d";
}
.wticon-x:before {
	content: "\f12e";
}

/*# sourceMappingURL=data:application/json;base64,{"version":3,"sources":["webpack://src/shared/Icons.font.js"],"names":[],"mappings":"AAAA;CACC,sBAAsB;CACtB,63SAA63S;AAC93S;;AAEA;CACC,cAAc;AACf;;AAEA;CACC,+BAA+B;CAC/B,kBAAkB;CAClB,8BAA8B;CAC9B,mBAAmB;AACpB;;AAEA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB;AACA;CACC,gBAAgB;AACjB","sourcesContent":["@font-face {\n\tfont-family: \"wticons\";\n\tsrc: url(\"data:font/woff2;charset=utf-8;base64,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\") format(\"woff2\");\n}\n\n.wticons {\n\tline-height: 1;\n}\n\n.wticons:before {\n\tfont-family: wticons !important;\n\tfont-style: normal;\n\tfont-weight: normal !important;\n\tvertical-align: top;\n}\n\n.wticon-account:before {\n\tcontent: \"\\f101\";\n}\n.wticon-add:before {\n\tcontent: \"\\f102\";\n}\n.wticon-cardResizeDrag:before {\n\tcontent: \"\\f103\";\n}\n.wticon-casual:before {\n\tcontent: \"\\f104\";\n}\n.wticon-check:before {\n\tcontent: \"\\f105\";\n}\n.wticon-checkSmall:before {\n\tcontent: \"\\f106\";\n}\n.wticon-chevron:before {\n\tcontent: \"\\f107\";\n}\n.wticon-copy:before {\n\tcontent: \"\\f108\";\n}\n.wticon-copySmall:before {\n\tcontent: \"\\f109\";\n}\n.wticon-dismiss:before {\n\tcontent: \"\\f10a\";\n}\n.wticon-downChevron:before {\n\tcontent: \"\\f10b\";\n}\n.wticon-error:before {\n\tcontent: \"\\f10c\";\n}\n.wticon-expand:before {\n\tcontent: \"\\f10d\";\n}\n.wticon-feedback:before {\n\tcontent: \"\\f10e\";\n}\n.wticon-filledDownArrow:before {\n\tcontent: \"\\f10f\";\n}\n.wticon-find:before {\n\tcontent: \"\\f110\";\n}\n.wticon-formal:before {\n\tcontent: \"\\f111\";\n}\n.wticon-gift:before {\n\tcontent: \"\\f112\";\n}\n.wticon-grayLogo:before {\n\tcontent: \"\\f113\";\n}\n.wticon-ignore:before {\n\tcontent: \"\\f114\";\n}\n.wticon-info:before {\n\tcontent: \"\\f115\";\n}\n.wticon-leftChevron:before {\n\tcontent: \"\\f116\";\n}\n.wticon-logo:before {\n\tcontent: \"\\f117\";\n}\n.wticon-love:before {\n\tcontent: \"\\f118\";\n}\n.wticon-noRecommendations:before {\n\tcontent: \"\\f119\";\n}\n.wticon-paragraphRewrite:before {\n\tcontent: \"\\f11a\";\n}\n.wticon-paste:before {\n\tcontent: \"\\f11b\";\n}\n.wticon-pin:before {\n\tcontent: \"\\f11c\";\n}\n.wticon-premium:before {\n\tcontent: \"\\f11d\";\n}\n.wticon-premiumDetail:before {\n\tcontent: \"\\f11e\";\n}\n.wticon-premiumFull:before {\n\tcontent: \"\\f11f\";\n}\n.wticon-recommendationLight:before {\n\tcontent: \"\\f120\";\n}\n.wticon-recommendationLightCard:before {\n\tcontent: \"\\f121\";\n}\n.wticon-recommendationLightNoSuggestions:before {\n\tcontent: \"\\f122\";\n}\n.wticon-refine:before {\n\tcontent: \"\\f123\";\n}\n.wticon-rewrite:before {\n\tcontent: \"\\f124\";\n}\n.wticon-rightChevron:before {\n\tcontent: \"\\f125\";\n}\n.wticon-rocket:before {\n\tcontent: \"\\f126\";\n}\n.wticon-sentenceExamples:before {\n\tcontent: \"\\f127\";\n}\n.wticon-settings:before {\n\tcontent: \"\\f128\";\n}\n.wticon-shorten:before {\n\tcontent: \"\\f129\";\n}\n.wticon-tutorial:before {\n\tcontent: \"\\f12a\";\n}\n.wticon-unlock:before {\n\tcontent: \"\\f12b\";\n}\n.wticon-warn:before {\n\tcontent: \"\\f12c\";\n}\n.wticon-WordtuneButton:before {\n\tcontent: \"\\f12d\";\n}\n.wticon-x:before {\n\tcontent: \"\\f12e\";\n}\n"],"sourceRoot":""} */</style></head>
<body class="abTestNoEffect_group1 collectionsPageABTest_originalLayoutGroup booksProgressBarABTest_control welcomeBoxABTest_welcomeBox twoLineEventsSidebar_expanded vsc-initialized" data-new-gr-c-s-check-loaded="14.1089.0" data-gr-ext-installed=""><div id="StayFocusd-infobar" style="display: none; top: 193.636px;">
    <img src="chrome-extension://laankejkbhbdhmipfmgcngdelahlfoji/common/img/eye_19x19_red.png">
    <span id="StayFocusd-infobar-msg"></span>
    <span id="StayFocusd-infobar-links">
        <a id="StayFocusd-infobar-never-show">hide forever</a>&nbsp;&nbsp;|&nbsp;&nbsp;
        <a id="StayFocusd-infobar-hide">hide once</a>
    </span>
</div>
<div id="react-app"><div class="wrapper" id="wrapper"><div></div><span></span><div class="IntercomWrapper-intercomFrame" id="intercom-outer-frame"></div><noscript class="noscript-warning"> This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. </noscript><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TRC765W" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div class="Header-root"><div style="height:64px" class="Header-headroom headroom-wrapper"><div class="headroom headroom--pinned headroom--scrolled"><header class="Header-appBar"><div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><h2 class="Typography-root Typography-title Header-title"><div class="Header-hideSmDown"><div class="Header-titleSubtitleContainer"><a class="Header-titleLink" href="https://www.lesswrong.com/">LESSWRONG</a></div></div><div class="Header-hideMdUp"><a class="Header-titleLink" href="https://www.lesswrong.com/">LW</a></div></h2><div class="Header-rightHeaderItems"><div class="SearchBar-root"><div class="SearchBar-rootChild"><div class="SearchBar-searchInputArea"><div><svg class="MuiSvgIcon-root SearchBar-searchIcon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></div><div></div></div></div></div><div><div class="UsersMenu-root"><a href="https://www.lesswrong.com/users/stuckwork"><button tabindex="0" class="MuiButtonBase-root MuiButton-root UsersMenu-userButtonRoot MuiButton-text MuiButton-flat" type="button"><span class="MuiButton-label"><span class="UsersMenu-userButtonContents">Stuckwork</span></span><span class="MuiTouchRipple-root"></span></button></a></div></div><div class="KarmaChangeNotifier-root"><div><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root KarmaChangeNotifier-karmaNotifierButton" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root KarmaChangeNotifier-starIcon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M22 9.24l-7.19-.62L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27 18.18 21l-1.63-7.03L22 9.24zM12 15.4l-3.76 2.27 1-4.28-3.32-2.88 4.38-.38L12 6.1l1.71 4.04 4.38.38-3.32 2.88 1 4.28L12 15.4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button></div></div><span class="MuiBadge-root NotificationsMenuButton-badgeContainer"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root NotificationsMenuButton-buttonClosed" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 22c1.1 0 2-.9 2-2h-4c0 1.1.9 2 2 2zm6-6v-5c0-3.07-1.63-5.64-4.5-6.32V4c0-.83-.67-1.5-1.5-1.5s-1.5.67-1.5 1.5v.68C7.64 5.36 6 7.92 6 11v5l-2 2v1h16v-1l-2-2zm-2 1H8v-6c0-2.48 1.51-4.5 4-4.5s4 2.02 4 4.5v6z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><span class="MuiBadge-badge NotificationsMenuButton-badge"></span></span></div></div></header><div class="jss90 jss91" style="width: 20px;"></div></div></div><div class="NotificationsMenu-root"></div></div><div class="Layout-standaloneNavFlex"><div class="Layout-searchResultsArea"></div><div class="Layout-main"><div class="flash-messages FlashMessages-root"></div><div class="page users-profile UsersProfile-profilePage"><div class="SingleColumnSection-root"><div class="UsersProfile-usernameTitle">Scott Alexander</div><aside class="Typography-root Typography-body2 UsersProfile-userInfo"><div class="UsersProfile-meta"><span class="UsersProfile-userMetaInfo" title="39559 karma"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">39559</span></span><span class="UsersProfile-userMetaInfo" title="15 karma on alignmentforum.org"><span class="OmegaIcon-root UsersProfile-icon UsersProfile-specificalz">Ω</span><span class="Typography-root Typography-body2 MetaInfo-root">15</span></span><span class="UsersProfile-userMetaInfo" title="212 posts"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M14 2H6c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6zm2 16H8v-2h8v2zm0-4H8v-2h8v2zm-3-5V3.5L18.5 9H13z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">212</span></span><span class="UsersProfile-userMetaInfo" title="1564 comments"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M20 2H4c-1.1 0-1.99.9-1.99 2L2 22l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-2 12H6v-2h12v2zm0-3H6V9h12v2zm0-3H6V6h12v2z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">1564</span></span><span class="UsersProfile-userMetaInfo" title="19 wiki edits"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M3 17.25V21h3.75L17.81 9.94l-3.75-3.75L3 17.25zM20.71 7.04c.39-.39.39-1.02 0-1.41l-2.34-2.34a.9959.9959 0 0 0-1.41 0l-1.83 1.83 3.75 3.75 1.83-1.83z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">19</span></span></div><div><a data-cy="message">Message</a></div><a class="NotifyMeButton-root"><span class="">Subscribe to posts</span></a></aside></div><div class="SingleColumnSection-root"></div><div class="SingleColumnSection-root"><div class="SectionTitle-root"><h1 id="sequences" class="Typography-root Typography-display1 SectionTitle-title">Sequences</h1><div class="SectionTitle-children"></div></div><div class="SequencesGridWrapper-gridWrapper"><div class="SequencesGrid-grid"><div class="SequencesGrid-gridContent"><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZNNi2uNx9E6iwGKKG"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/vitugifyyh2upm9ucjzh"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Introduction to Game Theory</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/G2GDw3m4MJ5ixSM92"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/djfksyoldrjt4ef5jts3"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">The Blue-Minimizing Robot</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#k5MPpr72eiGknaS7F"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/byzxi4zdrlvodk0ph46r"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Hypotheses and Hunches</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#TQW9brvXJ5Fajorr4"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/dyq1iu03mw0qo54n6byk"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Probability and Predictions</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#WnTvZdXz2q9ySfr4o"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/opwbi6lh0ud7r7dlyghc"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Parables and Prayers</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#TKDT2Mt6dDMH8AsZW"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/lel3jdh48of1dhtwfo4i"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Futurism and Forecasting</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#xmDeR64CivZiTAcLx"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/u0ackeoho1tquuiozpt4"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Community and Cooperation</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#zfXAcwLnGocsCsriG"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/hxgrnxobgf692eqpd8mz"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Economics and Efficiency</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#B384FrQNrxSq4hZoS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/ggdn92agzidnk0voif2z"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Research and Reviews</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#BQBqPowfxjvoee8jw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/zukiyrvljrwfe5bql7ek"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Studies and Statistics</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#NHXY86jBahi968uW4"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/bgpjay2m1labmbqdtjmi"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Categorisation and Concepts</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#XsMTxdQ6fprAQMoKi"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/rfpef83ejiwbsi1pmroz"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Argument and Analysis</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#rNuPrZvabXe2MaZv8"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="./scott_files/acfvxltz0mnyd7jqdq76"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Politics and Pragmatics</div></div></div></span></div></div></div></div></div><div class="SingleColumnSection-root"><div class="UsersProfile-title"><div class="SectionTitle-root"><h1 id="posts" class="Typography-root Typography-display1 SectionTitle-title">Posts</h1><div class="SectionTitle-children"><span class="SettingsButton-iconWithLabelGroup"><svg class="MuiSvgIcon-root SettingsButton-icon SettingsButton-iconWithLabel" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path transform="scale(1.2, 1.2)" fill="none" d="M0 0h20v20H0V0z"></path><path transform="scale(1.2, 1.2)" d="M15.95 10.78c.03-.25.05-.51.05-.78s-.02-.53-.06-.78l1.69-1.32c.15-.12.19-.34.1-.51l-1.6-2.77c-.1-.18-.31-.24-.49-.18l-1.99.8c-.42-.32-.86-.58-1.35-.78L12 2.34c-.03-.2-.2-.34-.4-.34H8.4c-.2 0-.36.14-.39.34l-.3 2.12c-.49.2-.94.47-1.35.78l-1.99-.8c-.18-.07-.39 0-.49.18l-1.6 2.77c-.1.18-.06.39.1.51l1.69 1.32c-.04.25-.07.52-.07.78s.02.53.06.78L2.37 12.1c-.15.12-.19.34-.1.51l1.6 2.77c.1.18.31.24.49.18l1.99-.8c.42.32.86.58 1.35.78l.3 2.12c.04.2.2.34.4.34h3.2c.2 0 .37-.14.39-.34l.3-2.12c.49-.2.94-.47 1.35-.78l1.99.8c.18.07.39 0 .49-.18l1.6-2.77c.1-.18.06-.39-.1-.51l-1.67-1.32zM10 13c-1.65 0-3-1.35-3-3s1.35-3 3-3 3 1.35 3 3-1.35 3-3 3z"></path></svg><span class="SettingsButton-label">Sorted by New</span></span></div></div></div><div class=""><div class="PostsList2-posts"><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">43</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/KCcdhZK7omEMwBdju/bay-solstice-2022-call-for-volunteers"><span><span>Bay Solstice 2022 Call For Volunteers</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">3mo</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">2</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">62</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/fLdADsBLAMuGvky2M/acx-meetups-everywhere-list"><span><span>ACX Meetups Everywhere List</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">4mo</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">0</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">95</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/gBpYo7mt2zNBmtBJd/crosspost-on-hreha-on-behavioral-economics"><span><span>[Crosspost] On Hreha On Behavioral Economics</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">1y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">6</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder PostsItem2-isRead"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">108</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root PostsTitle-read"><a href="https://www.lesswrong.com/posts/kxW6q5YdTGWh5sWby/eight-hundred-slightly-poisoned-word-games"><span><span>Eight Hundred Slightly Poisoned Word Games</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">1y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-unreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">5</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">100</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/wZGpoZgDANdkwTrwt/toward-a-bayesian-theory-of-willpower"><span><span>Toward A Bayesian Theory Of Willpower</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">2y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">28</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder PostsItem2-isRead"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">123</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root PostsTitle-read"><a href="https://www.lesswrong.com/posts/hNqte2p48nqKux3wS/trapped-priors-as-a-basic-problem-of-rationality"><span><span>Trapped Priors As A Basic Problem Of Rationality</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"><span class="CuratedIcon-postIcon"><span class="LWTooltip-root"><a href="https://www.lesswrong.com/recommendations"><svg class="MuiSvgIcon-root CuratedIcon-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></a></span></span></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">2y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"><span class="CuratedIcon-postIcon"><span class="LWTooltip-root"><a href="https://www.lesswrong.com/recommendations"><svg class="MuiSvgIcon-root CuratedIcon-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></a></span></span></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-unreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">29</div></div><span class="ReviewPostButton-root"><span class="LWTooltip-root">Review</span></span><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">144</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/GZSzMqr8hAB2dR8pk/studies-on-slack"><span><span>Studies On Slack</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">3y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">34</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">42</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/mbCccXJuuRBZdXdpH/confirmation-bias-as-misfire-of-normal-bayesian-reasoning"><span><span>Confirmation Bias As Misfire Of Normal Bayesian Reasoning</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">3y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">9</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background PostsItem2-bottomBorder"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">17</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/4Y2J7NtuweW2B8JvB/map-of-effective-altruism"><span><span>Map Of Effective Altruism</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">3y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">1</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div><div class="PostsItem2-row"><div class="PostsItem2-root PostsItem2-background"><div class="PostsItem2-postsItem PostsItem2-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItem2-karma"><span class="LWTooltip-root">77</span></span><span class="PostsItem2-title"><span><span class="PostsTitle-root"><a href="https://www.lesswrong.com/posts/irbREZtZzPi7WEYex/book-review-human-compatible-1"><span><span>Book Review: Human Compatible</span></span></a><span class="PostsTitle-hideSmDown"><span class="PostsItemIcons-iconSet"></span></span></span></span></span><span class="PostsItem2-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt">3y</span></span><div class="PostsItem2-mobileSecondRowSpacer"></div><div class="PostsItem2-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-nonMobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">6</div></div><div class="PostsItem2-mobileDismissButton"></div></div><div class="PostsItem2-actions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="PostsItem2-archiveButton"></div></div></div></div><aside class="Typography-root Typography-body2 SectionFooter-root"><a class="LoadMore-root LoadMore-sectionFooterStyles" href="https://www.lesswrong.com/users/scottalexander#">Load More</a></aside></div></div><div class="SingleColumnSection-root"><div class="SectionTitle-root"><h1 id="wiki-contributions" class="Typography-root Typography-display1 SectionTitle-title">Wiki Contributions</h1><div class="SectionTitle-children"></div></div><div class="TagEditsByUser-root"><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->9y<!-- --> </span></span></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->9y<!-- --> </span></span></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+<!-- -->52</span>/<span class="ChangeMetricsDisplay-charsRemoved">-<!-- -->52</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+<!-- -->11</span>/<span class="ChangeMetricsDisplay-charsRemoved">-<!-- -->11</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+<!-- -->13</span>/<span class="ChangeMetricsDisplay-charsRemoved">-<!-- -->9</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+<!-- -->8</span>/<span class="ChangeMetricsDisplay-charsRemoved">-<!-- -->7</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+<!-- -->8</span>/<span class="ChangeMetricsDisplay-charsRemoved">-<!-- -->46</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/list-of-blogs">List of Blogs</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> <!-- -->10y<!-- --> </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root"><span class="ChangeMetricsDisplay-charsAdded">(+<!-- -->2537<!-- -->)</span></span></div></div></div><a class="LoadMore-root" href="https://www.lesswrong.com/users/scottalexander#">Load More</a></div></div><div class="SingleColumnSection-root"><a href="https://www.lesswrong.com/users/scottalexander/replies"><div class="SectionTitle-root"><h1 id="comments" class="Typography-root Typography-display1 SectionTitle-title">Comments</h1><div class="SectionTitle-children"></div></div></a><div class="RecentComments-root"><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="5WW38E6reJptZxbcL"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/z9Syf3pGffpvHwfr4/?commentId=5WW38E6reJptZxbcL">I’m mildly skeptical that blindness prevents schizophrenia</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/z9Syf3pGffpvHwfr4/i-m-mildly-skeptical-that-blindness-prevents-schizophrenia?commentId=5WW38E6reJptZxbcL"><span class="LWTooltip-root">4mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">13</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">0</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Thanks, this had always kind of bothered me, and it's good to see someone put work into thinking about it.</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="c2sKkBsEwDr5wwGWk"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/?commentId=c2sKkBsEwDr5wwGWk">chinchilla's wild implications</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications?commentId=c2sKkBsEwDr5wwGWk"><span class="LWTooltip-root">5mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">53</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">0</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Thanks for posting this, it was really interesting. Some very dumb questions from someone who doesn't understand ML at all:</p><p>1. All of the loss numbers in this post "feel" very close together, and close to the minimum loss of 1.69. Does loss only make sense on a very small scale (like from 1.69 to 2.2), or is this telling us that language models are very close to optimal and there are only minimal remaining possible gains? What was the loss of GPT-1?</p><p>2. Humans "feel" better than even SOTA language models, but need less training data than those models, even though right now the only way to improve the models is through more training data. What am I supposed to conclude from this? Are humans running on such a different paradigm that none of this matters? Or is it just that humans are better at common-sense language tasks, but worse at token-prediction language tasks, in some way where the tails come apart once language models get good enough?</p><p>3. Does this disprove claims that "scale is all you need" for AI, since we've already maxed out scale, or are those claims talking about something different?</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="Z93ArrtDjJnBtkfGb"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/7iAABhWpcGeP5e6SB/?commentId=Z93ArrtDjJnBtkfGb">It’s Probably Not Lithium</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/7iAABhWpcGeP5e6SB/it-s-probably-not-lithium?commentId=Z93ArrtDjJnBtkfGb"><span class="LWTooltip-root">5mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">5</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">0</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>For the first part of the experiment, mostly nuts, bananas, olives, and eggs. Later I added vegan sausages + condiments.&nbsp;</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="6tm2qkbM4ecWfBzyF"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/7iAABhWpcGeP5e6SB/?commentId=6tm2qkbM4ecWfBzyF">It’s Probably Not Lithium</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/7iAABhWpcGeP5e6SB/it-s-probably-not-lithium?commentId=6tm2qkbM4ecWfBzyF"><span class="LWTooltip-root">5mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">27</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">7</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Adding my anecdote to everyone else's: after learning about the palatability hypothesis, I resolved to eat only non-tasty food for a while, and lost 30 pounds over about four months (200 -&gt; 170). I've since relaxed my diet a little to include a little tasty food, and now (8 months after the start) have maintained that loss (even going down a little further).</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="RGKkmyvyoeWe2LB7d"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/MnFqyPLqbiKL8nSR7/?commentId=RGKkmyvyoeWe2LB7d">My experience at and around MIRI and CFAR (inspired by Zoe Curzi's writeup of experiences at Leverage)</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/MnFqyPLqbiKL8nSR7/my-experience-at-and-around-miri-and-cfar-inspired-by-zoe?commentId=RGKkmyvyoeWe2LB7d"><span class="LWTooltip-root">5mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">92</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Update: I interviewed many of the people involved and feel like I understand the situation better.</p><p>My main conclusion is that I was wrong about Michael making people psychotic. Everyone I talked to had some other risk factor, like a preexisting family or personal history, or took recreational drugs at doses that would explain their psychotic episodes.</p><p>Michael has a tendency to befriend people with high trait psychoticism and heavy drug use, and often has strong opinions on their treatment, which explains why he is often very close to people and very noticeable at the moment they become psychotic. But aside from one case where he recommended someone take a drug that made a bad situation slightly worse, and the general Berkeley rationalist scene that he (and I and everyone else here) is a part of having lots of crazy ideas that are psychologically stressful, I no longer think he is a major cause.</p><p>While interviewing the people involved, I did get some additional reasons to worry that he uses cult-y high-pressure recruitment tactics on people he wants things from, in ways that make me continue to be nervous about the effect he *could* have on people. But the original claim I made that I knew of specific cases of psychosis which he substantially helped precipitate turned out to be wrong, and I apologize to him and to Jessica. Jessica's later post <span><span><a class="PostLinkPreviewWithPost-link" href="https://www.lesswrong.com/posts/pQGFeKvjydztpgnsY/occupational-infohazards">https://www.lesswrong.com/posts/pQGFeKvjydztpgnsY/occupational-infohazards</a></span></span> explained in more detail what happened to her, including the role of MIRI and of Michael and his friends, and everything she said there matches what I found too. Insofar as anything I wrote above produces impressions that differs from her explanation, assume that she is right and I am wrong.</p><p>Since the interviews involve a lot of private people's private details, I won't be posting anything more substantial than this publicly without a lot of thoughts and discussion. If for some reason this is important to you, let me know and I can send you a more detailed summary of my thoughts.</p><p>I'm deliberately leaving this comment in this obscure place for now while I talk to Michael and Jessica about whether they would prefer a more public apology that also brings all of this back to people's attention again.</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="sYyrcRgcFLYdgPRLd"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/?commentId=sYyrcRgcFLYdgPRLd">“Pivotal Act” Intentions: Negative Consequences and Fallacious Arguments</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/Jo89KvfAs9z7owoZp/pivotal-act-intentions-negative-consequences-and-fallacious?commentId=sYyrcRgcFLYdgPRLd"><span class="LWTooltip-root">8mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="TwoAxisVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">26</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">64</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">9</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>I agree it's not necessarily a good idea to go around founding the Let's Commit A Pivotal Act AI Company.</p><p>But I think there's room for subtlety somewhere like "Conditional on you being in a situation where you <i>could</i> take a pivotal act, which is a small and unusual fraction of world-branches, maybe you should take a pivotal act."</p><p>That is, <i>if</i> you are in a position where you have the option to build an AI capable of destroying all competing AI projects, the moment you notice this you should update heavily in favor of short timelines (zero in your case, but everyone else should be close behind) and fast takeoff speeds (since your AI has these impressive capabilities). You should also update on existing AI regulation being insufficient (since it was insufficient to prevent <i>you</i>)</p><p>Somewhere halfway between "found the Let's Commit A Pivotal Act Company" and "if you happen to stumble into a pivotal act, take it", there's an intervention to spread a norm of "if a good person who cares about the world happens to stumble into a pivotal-act-capable AI, take the opportunity". I don't think this norm would necessarily accelerate a race. After all, bad people who want to seize power can take pivotal acts whether we want them to or not. The only people who are bound by norms are good people who care about the future of humanity. I, as someone with no loyalty to any individual AI team, would prefer that (good, norm-following) teams take pivotal acts if they happen to end up with the first superintelligence, rather than not doing that.</p><p>Another way to think about this is that all good people should be equally happy with any other good person creating a pivotal AGI, so they won't need to race among themselves. They might be less happy with a bad person creating a pivotal AGI, but in that case you <i>should</i> race and you have no other option. I realize "good" and "bad" are very simplistic but I don't think adding real moral complexity changes the calculation much.</p><p>I am more concerned about your point where someone rushes into a pivotal act without being sure their own AI is aligned. I agree this would be very dangerous, but it seems like a job for normal cost-benefit calculation: what's the risk of your AI being unaligned if you act now, vs. someone else creating an unaligned AI if you wait X amount of time? Do we have any reason to think teams would be systematically biased when making this calculation?</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="uLevmvu75v6KTagyT"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/?commentId=uLevmvu75v6KTagyT">Call For Distillers</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/zo9zKcz47JxDErFzQ/call-for-distillers?commentId=uLevmvu75v6KTagyT"><span class="LWTooltip-root">8mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">20</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>My current plan is to go through most of the MIRI dialogues and anything else lying around that I think would be of interest to my readers, at some slow rate where I don't scare off people who don't want to read too much AI stuff. If anyone here feels like something else would be a better use of my time, let me know.</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="2FnGoaCjqgm6HxXE7"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/vxeBgsbeQ29zvrijo/?commentId=2FnGoaCjqgm6HxXE7">How to Interpret Vitamin D Dosage Using Numbers</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/vxeBgsbeQ29zvrijo/how-to-interpret-vitamin-d-dosage-using-numbers?commentId=2FnGoaCjqgm6HxXE7"><span class="LWTooltip-root">8mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">28</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>I don't think hunter-gatherers get 16000 to 32000 IU of Vitamin D daily. <span><span><span><a href="https://sci-hub.st/https://www.sciencedirect.com/science/article/abs/pii/S1011134415301561">This study</a></span></span></span> suggests Hadza hunter-gatherers get more like 2000. I think the difference between their calculation and yours is that they find that hunter-gatherers avoid the sun during the hottest part of the day. It might also have to do with them being black, I'm not sure.</p><p>Hadza hunter gatherers have serum D levels of about 44 ng/ml. Based on <span><span><span><a href="https://www.researchgate.net/profile/Reinhold-Vieth/publication/7959166_Estimates_of_Optimal_Vitamin_D_Status/links/00b7d52026357671c5000000/Estimates-of-Optimal-Vitamin-D-Status.pdf">this paper</a></span></span></span>, I think you would need total vitamin D (diet + sunlight + supplements) of about 4400 IU/day to get that amount. If you start off as a mildly deficient American (15 ng/ml), you'd need an extra 2900 IU/day; if you start out as an average white American (30 ng/ml), you'd need an extra 1400 IU/day. The Hadza are probably an overestimate of what you need since they're right on the equator - hunter-gatherers in eg Europe probably did fine too. I think this justifies the doses of 400 - 2000 IU/day in studies as reasonably evolutionarily-informed.</p><p>Please don't actually take 16000 IU/day of vitamin D daily, if taken long-term this would put you at risk for <span><span><span><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6158375/">vitamin D toxicity.</a></span></span></span></p><p>I also agree with the issues about the individual studies which other people have brought up.</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="Z63qjNcNey9cuKsKQ"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/MPhc7q48Cmo7As2th/?commentId=Z63qjNcNey9cuKsKQ">Is Metaculus Slow to Update?</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/MPhc7q48Cmo7As2th/is-metaculus-slow-to-update?commentId=Z63qjNcNey9cuKsKQ"><span class="LWTooltip-root">9mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">8</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Thanks for looking into this.</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="aDjjbW2aWKfFCf757"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-titleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/LbyxFk8JmPKPAQBvL/?commentId=aDjjbW2aWKfFCf757">We're already in AI takeoff</a></span></div><div class="CommentsItem-body"><div class="CommentsItem-meta"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItem-username CommentUserName-author"><span class=""><a class="UsersNameDisplay-userName" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/LbyxFk8JmPKPAQBvL/we-re-already-in-ai-takeoff?commentId=aDjjbW2aWKfFCf757"><span class="LWTooltip-root">9mo</span><svg class="MuiSvgIcon-root CommentsItemDate-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">32</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="CommentsItem-menu"><svg class="MuiSvgIcon-root CommentsMenu-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 8c1.1 0 2-.9 2-2s-.9-2-2-2-2 .9-2 2 .9 2 2 2zm0 2c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm0 6c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></span></div><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Maybe. It might be that if you described what you wanted more clearly, it would be the same thing that I want, and possibly I was incorrectly associating this with the things at CFAR you say you're against, in which case sorry.</p><p>But I still don't feel like I quite understand your suggestion. You talk of "stupefying egregores" as problematic insofar as they distract from the object-level problem. But I don't understand how pivoting to egregore-fighting isn't also a distraction from the object-level problem. Maybe this is because I don't understand what fighting egregores consists of, and if I knew, then I would agree it was some sort of reasonable problem-solving step.</p><p>I agree that the Sequences contain a lot of useful deconfusion, but I interpret them as useful primarily because they provide a template for good thinking, and not because clearing up your thinking about those things is itself necessary for doing good work. I think of the cryonics discussion the same way I think of the Many Worlds discussion - following the motions of someone as they get the right answer to a hard question trains you to do this thing yourself.</p><p>I'm sorry if "cultivate your will" has the wrong connotations, but you did say "The problem that's upstream of this is <i>the lack of will</i>", and I interpreted a lot of your discussion of de-numbing and so on as dealing with this.</p><blockquote><p>Part of what inspired me to write this piece at all was seeing a kind of blindness to these memetic forces in how people talk about AI risk and alignment research. Making bizarre assertions about what things need to happen on the god scale of "AI researchers" or "governments" or whatever, roughly on par with people loudly asserting opinions about what POTUS should do. It strikes me as immensely obvious that memetic forces precede AGI. If the memetic landscape slants down mercilessly toward existential oblivion here, then the thing to do isn't to prepare to swim upward against a future avalanche. It's to orient to the landscape.</p></blockquote><p>The claim "memetic forces precede AGI" seems meaningless to me, except insofar as memetic forces precede everything (eg the personal computer was invented because people wanted personal computers and there was a culture of inventing things). Do you mean it in a stronger sense? If so, what sense?</p><p>I also don't understand why it's wrong to talk about what "AI researchers" or "governments" should do. Sure, it's more virtuous to act than to chat randomly about stuff, but many Less Wrongers are in positions to change what AI researchers do, and if they have opinions about that, they should voice them. This post of yours right now seems to be about what "the rationalist community" should do, and I don't think it's a category error for you to write it.&nbsp;</p><p>Maybe this would easier if you described what actions we should take conditional on everything you wrote being right.</p></div></div><div class="CommentsItem-bottom"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a></div></div></div></div></div></div></div><a class="LoadMore-root" href="https://www.lesswrong.com/users/scottalexander#">Load More</a></div></div><div class="SingleColumnSection-root ReportUserButton-reportUserSection"><button class="ReportUserButton-reportUserBtn">Report user</button></div></div><div class="Footer-root"></div></div></div></div></div>

<script>window.ssrRenderedAt = "2022-12-18T15:20:57.771Z"</script>
<script>window.__APOLLO_STATE__ = {"Revision:P4zDcpuj7sXjWMSwh_biography":{"_id":"P4zDcpuj7sXjWMSwh_biography","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2022-09-26T19:17:47.925Z","userId":"P4zDcpuj7sXjWMSwh","originalContents":{"__typename":"ContentType","type":"ckEditorMarkup","data":""},"html":"","markdown":"","draftJS":{"blocks":[{"key":"d41d8","text":"","type":"unstyled","depth":0,"inlineStyleRanges":[],"entityRanges":[],"data":{}}],"entityMap":{}},"ckEditorMarkup":"","wordCount":1,"htmlHighlight":"","plaintextDescription":null},"Revision:P4zDcpuj7sXjWMSwh_moderationGuidelines":{"_id":"P4zDcpuj7sXjWMSwh_moderationGuidelines","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2022-09-26T19:17:47.805Z","userId":"P4zDcpuj7sXjWMSwh","originalContents":{"__typename":"ContentType","type":"ckEditorMarkup","data":""},"html":"","markdown":"","draftJS":{"blocks":[{"key":"d41d8","text":"","type":"unstyled","depth":0,"inlineStyleRanges":[],"entityRanges":[],"data":{}}],"entityMap":{}},"ckEditorMarkup":"","wordCount":1,"htmlHighlight":"","plaintextDescription":null},"User:P4zDcpuj7sXjWMSwh":{"_id":"P4zDcpuj7sXjWMSwh","__typename":"User","beta":null,"email":"bart.bussmann@uantwerpen.be","services":{"password":{},"email":{},"resume":{}},"acceptedTos":null,"pageUrl":"https://www.lesswrong.com/users/stuckwork","voteBanned":null,"banned":null,"isReviewed":true,"nullifyVotes":null,"hideIntercom":null,"hideNavigationSidebar":null,"currentFrontpageFilter":null,"frontpageFilterSettings":{"personalBlog":0,"tags":[{"tagId":"Ng8Gice9KNkncxqcj","tagName":"Rationality","filterMode":10},{"tagId":"sYm3HiWcfZvrGu3ui","tagName":"AI","filterMode":-25},{"tagId":"3uE2pXvbcnS9nnZRE","tagName":"World Modeling","filterMode":10},{"tagId":"xexCWMyds6QLWognu","tagName":"World Optimization","filterMode":25},{"tagId":"fkABsGCJZ6y9qConW","tagName":"Practical","filterMode":25},{"tagId":"izp6eeJJEg9v5zcur","tagName":"Community","filterMode":"Default"},{"tagId":"dqx5k65wjFfaiJ9sQ","tagName":"Procrastination","filterMode":25},{"tagId":"udPbn9RthmgTtHMiG","tagName":"Productivity","filterMode":25},{"tagId":"tNsqhzTibgGJKPEWB","tagName":"Coronavirus","filterMode":-25},{"tagId":"sSNtcEQsqHgN8ZmRF","tagName":"Fun Theory","filterMode":25},{"tagId":"irYLXtT9hkPXoZqhH","tagName":"Growth Stories","filterMode":25},{"tagId":"AodfCFefLAuwDyj7Z","tagName":"Self Experimentation","filterMode":25},{"tagId":"WqLn4pAWi5hn6McHQ","tagName":"Self Improvement","filterMode":25}]},"allPostsTimeframe":"yearly","allPostsSorting":"top","allPostsFilter":"curated","allPostsShowLowKarma":null,"allPostsIncludeEvents":null,"allPostsOpenSettings":true,"draftsListSorting":null,"draftsListShowArchived":null,"draftsListShowShared":null,"lastNotificationsCheck":"2022-09-26T19:17:15.699Z","bannedUserIds":null,"bannedPersonalUserIds":null,"biography":{"__ref":"Revision:P4zDcpuj7sXjWMSwh_biography"},"moderationStyle":null,"moderationGuidelines":{"__ref":"Revision:P4zDcpuj7sXjWMSwh_moderationGuidelines"},"noKibitz":null,"showHideKarmaOption":null,"markDownPostEditor":null,"hideElicitPredictions":null,"hideAFNonMemberInitialWarning":null,"commentSorting":"postCommentsTop","location":"","googleLocation":null,"mongoLocation":null,"mapLocation":null,"mapLocationSet":false,"mapMarkerText":null,"htmlMapMarkerText":"","nearbyEventsNotifications":true,"nearbyEventsNotificationsLocation":{"address_components":[{"long_name":"Antwerp","short_name":"Antwerp","types":["locality","political"]},{"long_name":"Antwerp","short_name":"AN","types":["administrative_area_level_2","political"]},{"long_name":"Flanders","short_name":"Flanders","types":["administrative_area_level_1","political"]},{"long_name":"Belgium","short_name":"BE","types":["country","political"]}],"adr_address":"<span class=\"locality\">Antwerp<\/span>, <span class=\"country-name\">Belgium<\/span>","formatted_address":"Antwerp, Belgium","geometry":{"location":{"lat":51.2194475,"lng":4.4024643},"viewport":{"south":51.14334002058517,"west":4.217600064800268,"north":51.37743006638301,"east":4.497840011267312}},"icon":"https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/geocode-71.png","icon_background_color":"#7B9EB0","icon_mask_base_uri":"https://maps.gstatic.com/mapfiles/place_api/icons/v2/generic_pinlet","name":"Antwerp","photos":[{"height":3468,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/115378283856592738578\">Nanny Bierkens Brouwers<\/a>"],"width":4624},{"height":3648,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/112420603678656129455\">Koen Vermast<\/a>"],"width":2736},{"height":3472,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/107785888654238559604\">Gabriel Kowalski<\/a>"],"width":4624},{"height":3000,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/111509551464512526264\">Wendell Lins<\/a>"],"width":4000},{"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/106379315498608992869\">Florian Truestedt<\/a>"],"width":4032},{"height":4032,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/100586229028946490622\">Herwig Aertbelien<\/a>"],"width":2268},{"height":4000,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/111603820554165793043\">Gabriel Lambert<\/a>"],"width":2252},{"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/100226050364319902366\">Sige Nagels<\/a>"],"width":4032},{"height":2268,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/112748523251514087962\">Pavel Stulir<\/a>"],"width":4032},{"height":4000,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/103590333584434233469\">Panagiotis Chatzimichail<\/a>"],"width":3000}],"place_id":"ChIJfYjDv472w0cRuIqogoRErz4","reference":"ChIJfYjDv472w0cRuIqogoRErz4","types":["locality","political"],"url":"https://maps.google.com/?q=Antwerp,+Belgium&ftid=0x47c3f68ebfc3887d:0x3eaf448482a88ab8","utc_offset":60,"vicinity":"Antwerp","website":"http://www.antwerpen.be/","html_attributions":[],"utc_offset_minutes":60},"nearbyEventsNotificationsRadius":99,"nearbyPeopleNotificationThreshold":null,"hideFrontpageMap":null,"emailSubscribedToCurated":null,"subscribedToDigest":null,"unsubscribeFromAll":null,"emails":[{"address":"bart.bussmann@uantwerpen.be","verified":true}],"whenConfirmationEmailSent":null,"hideSubscribePoke":null,"hideMeetupsPoke":null,"noCollapseCommentsFrontpage":null,"noCollapseCommentsPosts":null,"noSingleLineComments":null,"karmaChangeNotifierSettings":{"updateFrequency":"daily","timeOfDayGMT":11,"dayOfWeekGMT":"Saturday","showNegativeKarma":false},"karmaChangeLastOpened":null,"shortformFeedId":null,"viewUnreviewedComments":null,"recommendationSettings":{"frontpage":{"method":"sample","count":3,"scoreOffset":0,"scoreExponent":3,"personalBlogpostModifier":0,"includePersonal":true,"includeMeta":false,"frontpageModifier":10,"curatedModifier":50,"onlyUnread":true,"hideContinueReading":true,"hideBookmarks":true}},"theme":null,"bookmarkedPostsMetadata":null,"hiddenPostsMetadata":null,"auto_subscribe_to_my_posts":true,"auto_subscribe_to_my_comments":true,"autoSubscribeAsOrganizer":true,"noExpandUnreadCommentsReview":null,"reviewVotesQuadratic":null,"reviewVotesQuadratic2019":null,"reviewVotesQuadratic2020":null,"hideTaggingProgressBar":null,"hideFrontpageBookAd":null,"hideFrontpageBook2019Ad":null,"abTestKey":"W7oqEuwqPe7HFXZqc","abTestOverrides":null,"sortDraftsBy":null,"petrovPressedButtonDate":null,"petrovLaunchCodeDate":null,"petrovOptOut":true,"lastUsedTimezone":"Europe/Amsterdam","acknowledgedNewUserGuidelines":null,"notificationSubforumUnread":null,"experiencedIn":null,"interestedIn":null,"oldSlugs":null,"groups":["canModeratePersonal"],"jobTitle":null,"organization":null,"careerStage":null,"howOthersCanHelpMe":null,"howICanHelpOthers":null,"profileTagIds":null,"profileTags":[],"organizerOfGroupIds":null,"organizerOfGroups":[],"programParticipation":null,"website":null,"linkedinProfileURL":null,"facebookProfileURL":null,"twitterProfileURL":null,"githubProfileURL":null,"frontpagePostCount":2,"afSequenceCount":null,"afSequenceDraftCount":null,"sequenceDraftCount":null,"profileImageId":null,"noindex":null,"paymentEmail":null,"paymentInfo":null,"goodHeartTokens":16,"postingDisabled":null,"allCommentingDisabled":null,"commentingOnOtherUsersDisabled":null,"conversationsDisabled":null,"walledGardenInvite":null,"hideWalledGardenUI":null,"walledGardenPortalOnboarded":null,"taggingDashboardCollapsed":null,"usernameUnset":null,"slug":"stuckwork","createdAt":"2020-10-10T16:33:19.165Z","username":"Stuckwork","displayName":"Stuckwork","previousDisplayName":null,"fullName":null,"karma":97,"afKarma":0,"deleted":null,"isAdmin":false,"htmlBio":"","postCount":2,"commentCount":14,"sequenceCount":null,"afPostCount":null,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"maxCommentCount":14,"maxPostCount":2,"voteCount":60,"smallUpvoteCount":21,"bigUpvoteCount":25,"smallDownvoteCount":6,"bigDownvoteCount":4,"reviewedByUserId":"XtphY3uYHwruKqDyG","reviewedAt":"2020-10-31T20:02:50.027Z","signUpReCaptchaRating":null,"needsReview":false,"sunshineNotes":null,"sunshineFlagged":false,"snoozedUntilContentCount":null,"moderatorActions":null,"associatedClientId":null,"karmaChanges":{"__typename":"KarmaChanges","totalChange":0,"updateFrequency":"daily","startDate":"2022-12-17T11:00:00.000Z","endDate":"2022-12-18T11:00:00.000Z","nextBatchDate":"2022-12-19T11:00:00.000Z","posts":[],"comments":[],"tagRevisions":[]}},"ROOT_QUERY":{"__typename":"Query","currentUser":{"__ref":"User:P4zDcpuj7sXjWMSwh"},"users({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":10,\"slug\":\"scottalexander\",\"view\":\"usersProfile\"}}})":{"__typename":"MultiUserOutput","results":[{"__ref":"User:XgYW5s8njaYrtyP7q"}],"totalCount":null},"notifications({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":20,\"userId\":\"P4zDcpuj7sXjWMSwh\",\"view\":\"userNotifications\"}}})":{"__typename":"MultiNotificationOutput","results":[{"__ref":"Notification:fRospfegkFaK6t2bn"},{"__ref":"Notification:Bcj9kdJqYu9pqXM4Y"},{"__ref":"Notification:dzyspHv2MXQFdGktb"},{"__ref":"Notification:oruCmFkh9eNEjqwxC"},{"__ref":"Notification:Qhiz5xCagSxsE2StA"},{"__ref":"Notification:hj4wrbrZLKgx4GAqX"},{"__ref":"Notification:5PzoZNgyDxLsuzhCa"},{"__ref":"Notification:tvEQ25xW2Kw9ckty9"},{"__ref":"Notification:9HDCtMETNsadckTWm"},{"__ref":"Notification:NX865rwsvfWra5Krk"},{"__ref":"Notification:jGF3iyXyiQxXoTDKN"},{"__ref":"Notification:mQycwuMfMLjdGiEZD"},{"__ref":"Notification:LKo5ipftkjZKCWTcm"},{"__ref":"Notification:p7bWsaj9ppWzW7ZAJ"},{"__ref":"Notification:LndcjC6GT4SiCkNad"},{"__ref":"Notification:AFWCuvKEfsBQSpntg"},{"__ref":"Notification:PMEhyN4kqeoagnAHF"},{"__ref":"Notification:PM2XHtriZtucDdwBE"},{"__ref":"Notification:ueyRmcxN7WZnd9Gfk"},{"__ref":"Notification:RwtCqY5bXvdfvq6Gj"}],"totalCount":null},"notifications({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":20,\"type\":\"newMessage\",\"userId\":\"P4zDcpuj7sXjWMSwh\",\"view\":\"userNotifications\"}}})":{"__typename":"MultiNotificationOutput","results":[],"totalCount":null},"user({\"input\":{\"selector\":{\"documentId\":\"P4zDcpuj7sXjWMSwh\"}}})":{"__typename":"SingleUserOutput","result":{"__ref":"User:P4zDcpuj7sXjWMSwh"}},"user({\"input\":{\"selector\":{\"documentId\":\"XgYW5s8njaYrtyP7q\"}}})":{"__typename":"SingleUserOutput","result":{"__ref":"User:XgYW5s8njaYrtyP7q"}},"conversations({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":1,\"participantIds\":[\"P4zDcpuj7sXjWMSwh\",\"XgYW5s8njaYrtyP7q\"],\"userId\":\"P4zDcpuj7sXjWMSwh\",\"view\":\"userGroupUntitledConversations\"}}})":{"__typename":"MultiConversationOutput","results":[],"totalCount":null},"subscriptions({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"collectionName\":\"Users\",\"documentId\":\"XgYW5s8njaYrtyP7q\",\"limit\":1,\"type\":\"newPosts\",\"userId\":\"P4zDcpuj7sXjWMSwh\",\"view\":\"subscriptionState\"}}})":{"__typename":"MultiSubscriptionOutput","results":[],"totalCount":null},"revisions({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":10,\"userId\":\"XgYW5s8njaYrtyP7q\",\"view\":\"revisionsByUser\"}}})":{"__typename":"MultiRevisionOutput","results":[{"__ref":"Revision:5f5c37ee1b5cdee568cfd5f1"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5f0"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5e9"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5d1"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5d0"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5cf"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5ce"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5cd"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5cc"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd5cb"}],"totalCount":null},"sequences({\"input\":{\"enableCache\":false,\"enableTotal\":true,\"terms\":{\"limit\":9,\"userId\":\"XgYW5s8njaYrtyP7q\",\"view\":\"userProfile\"}}})":{"__typename":"MultiSequenceOutput","results":[{"__ref":"Sequence:ZNNi2uNx9E6iwGKKG"},{"__ref":"Sequence:G2GDw3m4MJ5ixSM92"},{"__ref":"Sequence:k5MPpr72eiGknaS7F"},{"__ref":"Sequence:TQW9brvXJ5Fajorr4"},{"__ref":"Sequence:WnTvZdXz2q9ySfr4o"},{"__ref":"Sequence:TKDT2Mt6dDMH8AsZW"},{"__ref":"Sequence:xmDeR64CivZiTAcLx"},{"__ref":"Sequence:zfXAcwLnGocsCsriG"},{"__ref":"Sequence:B384FrQNrxSq4hZoS"}],"totalCount":13},"comments({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"authorIsUnreviewed\":null,\"limit\":10,\"userId\":\"XgYW5s8njaYrtyP7q\",\"view\":\"profileRecentComments\"}}})":{"__typename":"MultiCommentOutput","results":[{"__ref":"Comment:5WW38E6reJptZxbcL"},{"__ref":"Comment:c2sKkBsEwDr5wwGWk"},{"__ref":"Comment:Z93ArrtDjJnBtkfGb"},{"__ref":"Comment:6tm2qkbM4ecWfBzyF"},{"__ref":"Comment:RGKkmyvyoeWe2LB7d"},{"__ref":"Comment:sYyrcRgcFLYdgPRLd"},{"__ref":"Comment:uLevmvu75v6KTagyT"},{"__ref":"Comment:2FnGoaCjqgm6HxXE7"},{"__ref":"Comment:Z63qjNcNey9cuKsKQ"},{"__ref":"Comment:aDjjbW2aWKfFCf757"}],"totalCount":null},"posts({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"authorIsUnreviewed\":null,\"excludeEvents\":true,\"limit\":10,\"userId\":\"XgYW5s8njaYrtyP7q\",\"view\":\"userPosts\"}}})":{"__typename":"MultiPostOutput","results":[{"__ref":"Post:KCcdhZK7omEMwBdju"},{"__ref":"Post:fLdADsBLAMuGvky2M"},{"__ref":"Post:gBpYo7mt2zNBmtBJd"},{"__ref":"Post:kxW6q5YdTGWh5sWby"},{"__ref":"Post:wZGpoZgDANdkwTrwt"},{"__ref":"Post:hNqte2p48nqKux3wS"},{"__ref":"Post:GZSzMqr8hAB2dR8pk"},{"__ref":"Post:mbCccXJuuRBZdXdpH"},{"__ref":"Post:4Y2J7NtuweW2B8JvB"},{"__ref":"Post:irbREZtZzPi7WEYex"}],"totalCount":null}},"User:XgYW5s8njaYrtyP7q":{"_id":"XgYW5s8njaYrtyP7q","__typename":"User","slug":"scottalexander","createdAt":"2009-02-28T15:53:46.032Z","username":"Yvain","displayName":"Scott Alexander","previousDisplayName":null,"fullName":null,"karma":39559,"afKarma":15,"deleted":false,"isAdmin":false,"htmlBio":"","postCount":212,"commentCount":1564,"sequenceCount":13,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":19,"oldSlugs":["yvain"],"groups":["trustLevel1","canModeratePersonal","alignmentVoters","alignmentForum"],"jobTitle":null,"organization":null,"careerStage":null,"biography":null,"howOthersCanHelpMe":null,"howICanHelpOthers":null,"profileTagIds":null,"profileTags":[],"organizerOfGroupIds":null,"organizerOfGroups":[],"programParticipation":null,"website":null,"linkedinProfileURL":null,"facebookProfileURL":null,"twitterProfileURL":null,"githubProfileURL":null,"frontpagePostCount":116,"afSequenceCount":0,"afSequenceDraftCount":0,"sequenceDraftCount":1,"moderationStyle":null,"moderationGuidelines":null,"profileImageId":null,"bannedUserIds":null,"location":null,"googleLocation":null,"mapLocation":null,"mapLocationSet":false,"mapMarkerText":null,"htmlMapMarkerText":"","mongoLocation":null,"shortformFeedId":null,"viewUnreviewedComments":null,"auto_subscribe_to_my_posts":true,"auto_subscribe_to_my_comments":true,"autoSubscribeAsOrganizer":true,"petrovPressedButtonDate":null,"petrovOptOut":null,"sortDraftsBy":null,"noindex":null,"paymentEmail":null,"paymentInfo":null,"goodHeartTokens":null,"postingDisabled":null,"allCommentingDisabled":null,"commentingOnOtherUsersDisabled":null,"conversationsDisabled":null,"email":null,"emails":null,"maxCommentCount":1564,"maxPostCount":992,"voteCount":null,"smallUpvoteCount":null,"bigUpvoteCount":null,"smallDownvoteCount":null,"bigDownvoteCount":null,"banned":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","reviewedAt":null,"signUpReCaptchaRating":null,"needsReview":null,"sunshineNotes":null,"sunshineFlagged":null,"snoozedUntilContentCount":null,"moderatorActions":null,"associatedClientId":null,"walledGardenInvite":true,"hideWalledGardenUI":null,"walledGardenPortalOnboarded":null,"taggingDashboardCollapsed":null,"usernameUnset":null,"moderatorAssistance":null},"Notification:fRospfegkFaK6t2bn":{"_id":"fRospfegkFaK6t2bn","__typename":"Notification","documentId":"ZFYzYQbEEAXKBCski","documentType":"post","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-09-07T18:12:46.534Z","link":"/events/ZFYzYQbEEAXKBCski/introduction-to-effective-altruism-how-to-do-good-with-your","message":"New event in your area: Introduction to Effective Altruism: How to do good with your career","type":"newEventInRadius","viewed":false,"extraData":null},"Notification:Bcj9kdJqYu9pqXM4Y":{"_id":"Bcj9kdJqYu9pqXM4Y","__typename":"Notification","documentId":"ZFYzYQbEEAXKBCski","documentType":"post","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-09-07T18:12:46.532Z","link":"/events/ZFYzYQbEEAXKBCski/introduction-to-effective-altruism-how-to-do-good-with-your","message":"New event in your area: Introduction to Effective Altruism: How to do good with your career","type":"newEventInRadius","viewed":false,"extraData":null},"Notification:dzyspHv2MXQFdGktb":{"_id":"dzyspHv2MXQFdGktb","__typename":"Notification","documentId":"Qh3AeTyREkrJFwGH4","documentType":"post","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-08-30T12:00:55.278Z","link":"/events/Qh3AeTyREkrJFwGH4/the-hague-netherlands-acx-meetups-everywhere-2022","message":"Event in your area updated: The Hague, Netherlands – ACX Meetups Everywhere 2022","type":"editedEventInRadius","viewed":false,"extraData":null},"Notification:oruCmFkh9eNEjqwxC":{"_id":"oruCmFkh9eNEjqwxC","__typename":"Notification","documentId":"Qh3AeTyREkrJFwGH4","documentType":"post","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-08-24T23:06:57.256Z","link":"/events/Qh3AeTyREkrJFwGH4/the-hague-netherlands-acx-meetups-everywhere-2022","message":"New event in your area: The Hague, Netherlands – ACX Meetups Everywhere 2022","type":"newEventInRadius","viewed":false,"extraData":null},"Notification:Qhiz5xCagSxsE2StA":{"_id":"Qhiz5xCagSxsE2StA","__typename":"Notification","documentId":"unrrGibxptCgid4mr","documentType":"post","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-08-24T23:06:49.378Z","link":"/events/unrrGibxptCgid4mr/helmond-netherlands-acx-meetups-everywhere-2022","message":"New event in your area: Helmond, Netherlands – ACX Meetups Everywhere 2022","type":"newEventInRadius","viewed":false,"extraData":null},"Notification:hj4wrbrZLKgx4GAqX":{"_id":"hj4wrbrZLKgx4GAqX","__typename":"Notification","documentId":"7hmoCG5un5qhpyJZc","documentType":"post","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-08-24T23:06:38.438Z","link":"/events/7hmoCG5un5qhpyJZc/delft-netherlands-acx-meetups-everywhere-2022","message":"New event in your area: Delft, Netherlands – ACX Meetups Everywhere 2022","type":"newEventInRadius","viewed":false,"extraData":null},"Notification:5PzoZNgyDxLsuzhCa":{"_id":"5PzoZNgyDxLsuzhCa","__typename":"Notification","documentId":"ko2N3qbWxfcmAaP6w","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-04-04T15:18:43.319Z","link":"/posts/pi4owuC7Rdab7uWWR/book-review-why-greatness-cannot-be-planned-the-myth-of-the?commentId=ko2N3qbWxfcmAaP6w","message":"Devansh Pandey left a new comment on \"[Book Review] Why Greatness Cannot Be Planned: The Myth of the Objective\"","type":"newComment","viewed":false,"extraData":null},"Notification:tvEQ25xW2Kw9ckty9":{"_id":"tvEQ25xW2Kw9ckty9","__typename":"Notification","documentId":"us8Xj3hsaKkLypGpa","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-04-03T19:45:04.572Z","link":"/posts/yzmDgP2hueHRsCP7i/working-out-in-vr-really-works?commentId=us8Xj3hsaKkLypGpa","message":"Yonatan Cale replied to your comment on \"Working Out in VR Really Works\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:9HDCtMETNsadckTWm":{"_id":"9HDCtMETNsadckTWm","__typename":"Notification","documentId":"Em79MsoK7N4ZmaLwD","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-04-03T19:41:21.691Z","link":"/posts/yzmDgP2hueHRsCP7i/working-out-in-vr-really-works?commentId=Em79MsoK7N4ZmaLwD","message":"Dustin replied to your comment on \"Working Out in VR Really Works\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:NX865rwsvfWra5Krk":{"_id":"NX865rwsvfWra5Krk","__typename":"Notification","documentId":"9G7uuis6HQxFfwGGr","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2022-04-03T19:40:04.045Z","link":"/posts/LkEpSNvehbKgne5Ln/a-simple-guide-to-life?commentId=9G7uuis6HQxFfwGGr","message":"jasoncrawford replied to your comment on \"A simple guide to life\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:jGF3iyXyiQxXoTDKN":{"_id":"jGF3iyXyiQxXoTDKN","__typename":"Notification","documentId":"4orNGHnFYh7rQ6FAx","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-10-03T16:41:04.260Z","link":"/posts/zHS4FJhByRjqsuH4o/the-best-software-for-every-need?commentId=4orNGHnFYh7rQ6FAx","message":"milo replied to your comment on \"The Best Software For Every Need\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:mQycwuMfMLjdGiEZD":{"_id":"mQycwuMfMLjdGiEZD","__typename":"Notification","documentId":"nRBAvvRE2pQeSghh2","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-09-20T02:08:50.753Z","link":"/posts/zHS4FJhByRjqsuH4o/the-best-software-for-every-need?commentId=nRBAvvRE2pQeSghh2","message":"charlesoblack replied to your comment on \"The Best Software For Every Need\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:LKo5ipftkjZKCWTcm":{"_id":"LKo5ipftkjZKCWTcm","__typename":"Notification","documentId":"nAPrHTWZrSSrbJNi4","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-09-12T17:14:51.185Z","link":"/posts/zHS4FJhByRjqsuH4o/the-best-software-for-every-need?commentId=nAPrHTWZrSSrbJNi4","message":"mb99 replied to your comment on \"The Best Software For Every Need\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:p7bWsaj9ppWzW7ZAJ":{"_id":"p7bWsaj9ppWzW7ZAJ","__typename":"Notification","documentId":"utSEyebNsDphzBumx","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-09-12T11:59:52.657Z","link":"/posts/zHS4FJhByRjqsuH4o/the-best-software-for-every-need?commentId=utSEyebNsDphzBumx","message":"supposedlyfun replied to your comment on \"The Best Software For Every Need\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:LndcjC6GT4SiCkNad":{"_id":"LndcjC6GT4SiCkNad","__typename":"Notification","documentId":"TB6tvMJBAnjEHXb2n","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-09-11T20:51:58.929Z","link":"/posts/zHS4FJhByRjqsuH4o/the-best-software-for-every-need?commentId=TB6tvMJBAnjEHXb2n","message":"Gunnar_Zarncke replied to your comment on \"The Best Software For Every Need\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:AFWCuvKEfsBQSpntg":{"_id":"AFWCuvKEfsBQSpntg","__typename":"Notification","documentId":"MmhmFNembXimGdmuX","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-09-10T20:37:45.612Z","link":"/posts/zHS4FJhByRjqsuH4o/the-best-software-for-every-need?commentId=MmhmFNembXimGdmuX","message":"TheTrueSquidward replied to your comment on \"The Best Software For Every Need\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:PMEhyN4kqeoagnAHF":{"_id":"PMEhyN4kqeoagnAHF","__typename":"Notification","documentId":"kxhSSsFnNCqsxSyMh","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2021-09-07T20:18:43.949Z","link":"/posts/wBxEJdZomzzuMjDcN/tasks-apps-w-time-estimates-to-gauge-how-much-you-ll?commentId=kxhSSsFnNCqsxSyMh","message":"IrenicTruth replied to your comment on \"Tasks apps w/ time estimates to gauge how much you'll overshoot?\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:PM2XHtriZtucDdwBE":{"_id":"PM2XHtriZtucDdwBE","__typename":"Notification","documentId":"ahYNY76tnGRn3Zhst","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2020-10-25T16:06:08.094Z","link":"/posts/cQF7Gkh2an66t9Xhv/prediction-and-explanation-are-not-causation?commentId=ahYNY76tnGRn3Zhst","message":"jasoncrawford replied to your comment on \"“Prediction” and “explanation” are not causation\"","type":"newReplyToYou","viewed":false,"extraData":null},"Notification:ueyRmcxN7WZnd9Gfk":{"_id":"ueyRmcxN7WZnd9Gfk","__typename":"Notification","documentId":"Ws3ixaao8bDnZ9sNv","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2020-10-12T11:33:00.961Z","link":"/posts/mD53vNoGXAfGwCqWu/how-to-reach-80-of-your-goals-exactly-80-1?commentId=Ws3ixaao8bDnZ9sNv","message":"Richard Horvath left a new comment on \"How to reach 80% of your goals. Exactly 80%.\"","type":"newComment","viewed":false,"extraData":null},"Notification:RwtCqY5bXvdfvq6Gj":{"_id":"RwtCqY5bXvdfvq6Gj","__typename":"Notification","documentId":"8H2fNtwf8WxvX3whR","documentType":"comment","deleted":false,"userId":"P4zDcpuj7sXjWMSwh","createdAt":"2020-10-11T21:10:24.690Z","link":"/posts/mD53vNoGXAfGwCqWu/how-to-reach-80-of-your-goals-exactly-80-1?commentId=8H2fNtwf8WxvX3whR","message":"Alexei replied to your comment on \"How to reach 80% of your goals. Exactly 80%.\"","type":"newReplyToYou","viewed":false,"extraData":null},"Tag:5f5c37ee1b5cdee568cfb2f2":{"_id":"5f5c37ee1b5cdee568cfb2f2","__typename":"Tag","userId":"XgYW5s8njaYrtyP7q","name":"List of Blogs","slug":"list-of-blogs","core":null,"postCount":0,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.728Z","wikiOnly":true,"deleted":false},"Revision:5f5c37ee1b5cdee568cfd5f1":{"_id":"5f5c37ee1b5cdee568cfd5f1","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":0,"removed":0},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.38.0","editedAt":"2013-08-02T03:53:32.000Z","commitMessage":"/* Gone But Not Forgotten */","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5f0":{"_id":"5f5c37ee1b5cdee568cfd5f0","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":0,"removed":0},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.37.0","editedAt":"2013-08-02T03:53:11.000Z","commitMessage":"","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5e9":{"_id":"5f5c37ee1b5cdee568cfd5e9","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":52,"removed":52},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.30.0","editedAt":"2013-06-03T04:01:21.000Z","commitMessage":"/* Generic Rationality and Philosophy */","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5d1":{"_id":"5f5c37ee1b5cdee568cfd5d1","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":11,"removed":11},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.6.0","editedAt":"2013-05-10T09:27:01.000Z","commitMessage":"/* Gender and Politics */","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5d0":{"_id":"5f5c37ee1b5cdee568cfd5d0","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":13,"removed":9},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.5.0","editedAt":"2013-05-10T08:36:57.000Z","commitMessage":"/* Economics and Social Science */","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5cf":{"_id":"5f5c37ee1b5cdee568cfd5cf","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":0,"removed":0},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.4.0","editedAt":"2013-05-10T08:02:07.000Z","commitMessage":"","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5ce":{"_id":"5f5c37ee1b5cdee568cfd5ce","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":8,"removed":7},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.3.0","editedAt":"2013-05-10T07:56:47.000Z","commitMessage":"","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5cd":{"_id":"5f5c37ee1b5cdee568cfd5cd","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":8,"removed":46},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.2.0","editedAt":"2013-05-10T07:56:05.000Z","commitMessage":"","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5cc":{"_id":"5f5c37ee1b5cdee568cfd5cc","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":0,"removed":0},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.1.0","editedAt":"2013-05-10T07:55:18.000Z","commitMessage":"","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd5cb":{"_id":"5f5c37ee1b5cdee568cfd5cb","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb2f2"},"documentId":"5f5c37ee1b5cdee568cfb2f2","changeMetrics":{"added":2537,"removed":0},"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"version":"1.0.0","editedAt":"2013-05-10T07:54:18.000Z","commitMessage":"Created page with \"==Generic Rationality and Philosophy==  ciphergoth: [http://mindsarentmagic.wordpress.com/ Minds Aren't Magic] ''(focus on cryonics)''  DavidGerard: [http://rationalblogs.org/...\"","userId":"XgYW5s8njaYrtyP7q","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:ZNNi2uNx9E6iwGKKG_":{"_id":"ZNNi2uNx9E6iwGKKG_","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2018-04-08T21:50:10.329Z","userId":"XgYW5s8njaYrtyP7q","html":"<p>This sequence of posts is a primer on game theory intended at an introductory level. Because it is introductory, Less Wrong veterans may find some parts boring, obvious, or simplistic - although hopefully nothing is so simplistic as to be outright wrong.<\/p><p>Parts of this sequence draw heavily upon material from <em><u><a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\">The Art of Strategy<\/a><\/u><\/em> by Avinash Dixit and Barry Nalebuff, and it may in part be considered a (very favorable) review of the book accompanied by an exploration of its content. I have tried to include enough material to be useful, but not so much material that it becomes a plagiarism rather than a review (it&#x27;s probably a bad idea to pick a legal fight with people who write books called <em>The Art of Strategy<\/em>.) Therefore, for the most complete and engaging presentation of this material, I highly recommend the original book.<\/p><p>Special thanks to Luke for his book recommendation and his strong encouragement to write this.<\/p>","wordCount":156,"htmlHighlight":"<p>This sequence of posts is a primer on game theory intended at an introductory level. Because it is introductory, Less Wrong veterans may find some parts boring, obvious, or simplistic - although hopefully nothing is so simplistic as to be outright wrong.<\/p><p>Parts of this sequence draw heavily upon material from <em><u><a href=\"http://www.amazon.com/The-Art-Strategy-Theorists-Business/dp/0393062430\">The Art of Strategy<\/a><\/u><\/em> by Avinash Dixit and Barry Nalebuff, and it may in part be considered a (very favorable) review of the book accompanied by an exploration of its content. I have tried to include enough material to be useful, but not so much material that it becomes a plagiarism rather than a review (it&#x27;s probably a bad idea to pick a legal fight with people who write books called <em>The Art of Strategy<\/em>.) Therefore, for the most complete and engaging presentation of this material, I highly recommend the original book.<\/p><p>Special thanks to Luke for his book recommendation and his strong encouragement to write this.<\/p>","plaintextDescription":"This sequence of posts is a primer on game theory intended at an introductory level. Because it is introductory, Less Wrong veterans may find some parts boring, obvious, or simplistic - although hopefully nothing is so simplistic as to be outright wrong.\n\nParts of this sequence draw heavily upon material from The Art of Strategy by Avinash Dixit and Barry Nalebuff, and it may in part be considered a (very favorable) review of the book accompanied by an exploration of its content. I have tried to include enough material to be useful, but not so much material that it becomes a plagiarism rather than a review (it's probably a bad idea to pick a legal fight with people who write books called The Art of Strategy.) Therefore, for the most complete and engaging presentation of this material, I highly recommend the original book.\n\nSpecial thanks to Luke for his book recommendation and his strong encouragement to write this."},"Sequence:ZNNi2uNx9E6iwGKKG":{"_id":"ZNNi2uNx9E6iwGKKG","__typename":"Sequence","createdAt":"2018-04-08T21:50:10.329Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:ZNNi2uNx9E6iwGKKG_"},"gridImageId":"sequencesgrid/vitugifyyh2upm9ucjzh","bannerImageId":"sequences/zvybkycf2vyasr4zwptr","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":1,"userProfileOrder":null,"af":null,"title":"Introduction to Game Theory","canonicalCollection":null},"Revision:G2GDw3m4MJ5ixSM92_":{"_id":"G2GDw3m4MJ5ixSM92_","__typename":"Revision","version":"1.2.0","updateType":"minor","editedAt":"2022-09-21T21:34:27.910Z","userId":"r38pkCm7wF4M44MDQ","html":"<p>A sequence on models of the human mind, drawing on psychology and neuroscience to discuss motivation, learning and behaviour.<\/p>","wordCount":19,"htmlHighlight":"<p>A sequence on models of the human mind, drawing on psychology and neuroscience to discuss motivation, learning and behaviour.<\/p>","plaintextDescription":"A sequence on models of the human mind, drawing on psychology and neuroscience to discuss motivation, learning and behaviour."},"Sequence:G2GDw3m4MJ5ixSM92":{"_id":"G2GDw3m4MJ5ixSM92","__typename":"Sequence","createdAt":"2018-02-22T18:10:39.949Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:G2GDw3m4MJ5ixSM92_"},"gridImageId":"sequencesgrid/djfksyoldrjt4ef5jts3","bannerImageId":"sequences/ad7shnab6qq5v6cqxpme","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":1,"userProfileOrder":null,"af":null,"title":"The Blue-Minimizing Robot","canonicalCollection":null},"Revision:k5MPpr72eiGknaS7F_":{"_id":"k5MPpr72eiGknaS7F_","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-09-05T02:19:15.940Z","userId":"XgYW5s8njaYrtyP7q","html":"<div class=\"ory-row\"><div class=\"ory-cell ory-cell-sm-12 ory-cell-xs-12\"><div class=\"ory-cell-inner ory-cell-leaf\"><div><p><\/p><\/div><\/div><\/div><\/div>","wordCount":1,"htmlHighlight":"<div class=\"ory-row\"><div class=\"ory-cell ory-cell-sm-12 ory-cell-xs-12\"><div class=\"ory-cell-inner ory-cell-leaf\"><div><p><\/p><\/div><\/div><\/div><\/div>","plaintextDescription":""},"Sequence:k5MPpr72eiGknaS7F":{"_id":"k5MPpr72eiGknaS7F","__typename":"Sequence","createdAt":"2017-09-05T02:19:15.940Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:k5MPpr72eiGknaS7F_"},"gridImageId":"sequencesgrid/byzxi4zdrlvodk0ph46r","bannerImageId":"sequences/ysr4l3sqb6vfszn49p1u","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Hypotheses and Hunches","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Revision:TQW9brvXJ5Fajorr4_":{"_id":"TQW9brvXJ5Fajorr4_","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2022-06-23T22:40:56.142Z","userId":"r38pkCm7wF4M44MDQ","html":"<p>Nearly everyone is very very very overconfident. We know this from <a href=\"http://www.researchgate.net/profile/Baruch_Fischhoff/publication/230726569_Knowing_with_certainty_the_appropriateness_of_extreme_confidence/links/00b4952b854b29281c000000.pdf\">experiments<\/a> where people answer true/false trivia questions, then are asked to state how confident they are in their answer. If people’s confidence was well-calibrated, someone who said they were 99% confident (ie only 1% chance they’re wrong) would get the question wrong only 1% of the time. In fact, people who say they are 99% confident get the question wrong about 20% of the time.<\/p><p>It gets worse. People who say there’s only a 1 in 100,000 chance they’re wrong? Wrong 15% of the time. One in a million? Wrong 5% of the time. They’re not just overconfident, they are <i>fifty thousand times<\/i> as confident as they should be.<\/p>","wordCount":119,"htmlHighlight":"<p>Nearly everyone is very very very overconfident. We know this from <a href=\"http://www.researchgate.net/profile/Baruch_Fischhoff/publication/230726569_Knowing_with_certainty_the_appropriateness_of_extreme_confidence/links/00b4952b854b29281c000000.pdf\">experiments<\/a> where people answer true/false trivia questions, then are asked to state how confident they are in their answer. If people’s confidence was well-calibrated, someone who said they were 99% confident (ie only 1% chance they’re wrong) would get the question wrong only 1% of the time. In fact, people who say they are 99% confident get the question wrong about 20% of the time.<\/p><p>It gets worse. People who say there’s only a 1 in 100,000 chance they’re wrong? Wrong 15% of the time. One in a million? Wrong 5% of the time. They’re not just overconfident, they are <i>fifty thousand times<\/i> as confident as they should be.<\/p>","plaintextDescription":"Nearly everyone is very very very overconfident. We know this from experiments where people answer true/false trivia questions, then are asked to state how confident they are in their answer. If people’s confidence was well-calibrated, someone who said they were 99% confident (ie only 1% chance they’re wrong) would get the question wrong only 1% of the time. In fact, people who say they are 99% confident get the question wrong about 20% of the time.\n\nIt gets worse. People who say there’s only a 1 in 100,000 chance they’re wrong? Wrong 15% of the time. One in a million? Wrong 5% of the time. They’re not just overconfident, they are fifty thousand times as confident as they should be."},"Sequence:TQW9brvXJ5Fajorr4":{"_id":"TQW9brvXJ5Fajorr4","__typename":"Sequence","createdAt":"2017-09-05T01:01:05.221Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:TQW9brvXJ5Fajorr4_"},"gridImageId":"sequencesgrid/dyq1iu03mw0qo54n6byk","bannerImageId":"sequences/s7io2gbfmdhk7lyn0flk","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Probability and Predictions","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Revision:WnTvZdXz2q9ySfr4o_":{"_id":"WnTvZdXz2q9ySfr4o_","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-08-24T01:44:23.722Z","userId":"XgYW5s8njaYrtyP7q","html":"<div class=\"ory-row\"><div class=\"ory-cell ory-cell-sm-12 ory-cell-xs-12\"><div class=\"ory-cell-inner ory-cell-leaf\"><div><p><\/p><\/div><\/div><\/div><\/div>","wordCount":1,"htmlHighlight":"<div class=\"ory-row\"><div class=\"ory-cell ory-cell-sm-12 ory-cell-xs-12\"><div class=\"ory-cell-inner ory-cell-leaf\"><div><p><\/p><\/div><\/div><\/div><\/div>","plaintextDescription":""},"Sequence:WnTvZdXz2q9ySfr4o":{"_id":"WnTvZdXz2q9ySfr4o","__typename":"Sequence","createdAt":"2017-08-24T01:44:23.722Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:WnTvZdXz2q9ySfr4o_"},"gridImageId":"sequencesgrid/opwbi6lh0ud7r7dlyghc","bannerImageId":"sequences/r6u4ghtv3smv1zeh6rvv","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Parables and Prayers","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Revision:TKDT2Mt6dDMH8AsZW_":{"_id":"TKDT2Mt6dDMH8AsZW_","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-08-24T01:41:22.191Z","userId":"XgYW5s8njaYrtyP7q","html":"<p>A sequence of futurism discussion that includes AGI, brain emulations and the Fermi paradox.<\/p>","wordCount":14,"htmlHighlight":"<p>A sequence of futurism discussion that includes AGI, brain emulations and the Fermi paradox.<\/p>","plaintextDescription":"A sequence of futurism discussion that includes AGI, brain emulations and the Fermi paradox."},"Sequence:TKDT2Mt6dDMH8AsZW":{"_id":"TKDT2Mt6dDMH8AsZW","__typename":"Sequence","createdAt":"2017-08-24T01:41:22.191Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:TKDT2Mt6dDMH8AsZW_"},"gridImageId":"sequencesgrid/lel3jdh48of1dhtwfo4i","bannerImageId":"sequences/bj5eolzptpnu9fi9gi1a","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Futurism and Forecasting","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Sequence:xmDeR64CivZiTAcLx":{"_id":"xmDeR64CivZiTAcLx","__typename":"Sequence","createdAt":"2017-08-24T01:39:54.799Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":null,"gridImageId":"sequencesgrid/u0ackeoho1tquuiozpt4","bannerImageId":"sequences/c1h4gtqbcw3v04ikuprj","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Community and Cooperation","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Revision:zfXAcwLnGocsCsriG_":{"_id":"zfXAcwLnGocsCsriG_","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-08-24T01:37:08.113Z","userId":"XgYW5s8njaYrtyP7q","html":"<div class=\"ory-row\"><div class=\"ory-cell ory-cell-sm-12 ory-cell-xs-12\"><div class=\"ory-cell-inner ory-cell-leaf\"><div><p><\/p><\/div><\/div><\/div><\/div>","wordCount":1,"htmlHighlight":"<div class=\"ory-row\"><div class=\"ory-cell ory-cell-sm-12 ory-cell-xs-12\"><div class=\"ory-cell-inner ory-cell-leaf\"><div><p><\/p><\/div><\/div><\/div><\/div>","plaintextDescription":""},"Sequence:zfXAcwLnGocsCsriG":{"_id":"zfXAcwLnGocsCsriG","__typename":"Sequence","createdAt":"2017-08-24T01:37:08.113Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:zfXAcwLnGocsCsriG_"},"gridImageId":"sequencesgrid/hxgrnxobgf692eqpd8mz","bannerImageId":"sequences/z8ya8jyaxlbsfztpyfjo","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Economics and Efficiency","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Revision:B384FrQNrxSq4hZoS_":{"_id":"B384FrQNrxSq4hZoS_","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-08-24T01:32:25.576Z","userId":"XgYW5s8njaYrtyP7q","html":"<p>Synthesising scientific knowledge to answer a policy question is difficult. This sequence is a series of attempts to do just that, with intricate and winding literature reviews.<\/p>","wordCount":27,"htmlHighlight":"<p>Synthesising scientific knowledge to answer a policy question is difficult. This sequence is a series of attempts to do just that, with intricate and winding literature reviews.<\/p>","plaintextDescription":"Synthesising scientific knowledge to answer a policy question is difficult. This sequence is a series of attempts to do just that, with intricate and winding literature reviews."},"Sequence:B384FrQNrxSq4hZoS":{"_id":"B384FrQNrxSq4hZoS","__typename":"Sequence","createdAt":"2017-08-24T01:32:25.576Z","userId":"XgYW5s8njaYrtyP7q","user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"contents":{"__ref":"Revision:B384FrQNrxSq4hZoS_"},"gridImageId":"sequencesgrid/ggdn92agzidnk0voif2z","bannerImageId":"sequences/p6oizab58tj8y9gulhz6","canonicalCollectionSlug":"codex","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"curatedOrder":null,"userProfileOrder":null,"af":null,"title":"Research and Reviews","canonicalCollection":{"__typename":"Collection","title":"The Codex"}},"Post:z9Syf3pGffpvHwfr4":{"_id":"z9Syf3pGffpvHwfr4","__typename":"Post","slug":"i-m-mildly-skeptical-that-blindness-prevents-schizophrenia","title":"I’m mildly skeptical that blindness prevents schizophrenia","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"vRcer5FTqagjMkcDz"},"Revision:5WW38E6reJptZxbcL_":{"_id":"5WW38E6reJptZxbcL_","__typename":"Revision","html":"<p>Thanks, this had always kind of bothered me, and it's good to see someone put work into thinking about it.<\/p>","plaintextMainText":"Thanks, this had always kind of bothered me, and it's good to see someone put\nwork into thinking about it.","wordCount":20},"Comment:5WW38E6reJptZxbcL":{"_id":"5WW38E6reJptZxbcL","__typename":"Comment","post":{"__ref":"Post:z9Syf3pGffpvHwfr4"},"tag":null,"postId":"z9Syf3pGffpvHwfr4","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"contents":{"__ref":"Revision:5WW38E6reJptZxbcL_"},"postedAt":"2022-08-16T07:19:41.283Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":13,"extendedScore":{"agreement":0,"agreementVoteCount":0},"score":0.0019839438348115256,"voteCount":10,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":5,"afExtendedScore":{"agreement":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-08-16T07:19:41.293Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"twoAxis","isPinnedOnProfile":null},"Post:6Fpvch8RR29qLEWNH":{"_id":"6Fpvch8RR29qLEWNH","__typename":"Post","slug":"chinchilla-s-wild-implications","title":"chinchilla's wild implications","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"k26FNEL3tvGuDC5JD"},"Revision:c2sKkBsEwDr5wwGWk_":{"_id":"c2sKkBsEwDr5wwGWk_","__typename":"Revision","html":"<p>Thanks for posting this, it was really interesting. Some very dumb questions from someone who doesn't understand ML at all:<\/p><p>1. All of the loss numbers in this post \"feel\" very close together, and close to the minimum loss of 1.69. Does loss only make sense on a very small scale (like from 1.69 to 2.2), or is this telling us that language models are very close to optimal and there are only minimal remaining possible gains? What was the loss of GPT-1?<\/p><p>2. Humans \"feel\" better than even SOTA language models, but need less training data than those models, even though right now the only way to improve the models is through more training data. What am I supposed to conclude from this? Are humans running on such a different paradigm that none of this matters? Or is it just that humans are better at common-sense language tasks, but worse at token-prediction language tasks, in some way where the tails come apart once language models get good enough?<\/p><p>3. Does this disprove claims that \"scale is all you need\" for AI, since we've already maxed out scale, or are those claims talking about something different?<\/p>","plaintextMainText":"Thanks for posting this, it was really interesting. Some very dumb questions\nfrom someone who doesn't understand ML at all:\n\n1. All of the loss numbers in this post \"feel\" very close together, and close to\nthe minimum loss of 1.69. Does loss only make sense on a very small scale (like\nfrom 1.69 to 2.2), or is this telling us that language models are very close to\noptimal and there are only minimal remaining possible gains? What was the loss\nof GPT-1?\n\n2. Humans \"feel\" better than even SOTA language models, but need less training\ndata than those models, even though right now the only way to improve the models\nis through more training data. What am I supposed to conclude from this? Are\nhumans running on such a different paradigm that none of this matters? Or is it\njust that humans are better at common-sense language tasks, but worse at\ntoken-prediction language tasks, in some way where the tails come apart once\nlanguage models get good enough?\n\n3. Does this disprove claims that \"scale is all you need\" for AI, since we've\nalready maxed out scale, or are those claims talking about something different?","wordCount":193},"Comment:c2sKkBsEwDr5wwGWk":{"_id":"c2sKkBsEwDr5wwGWk","__typename":"Comment","post":{"__ref":"Post:6Fpvch8RR29qLEWNH"},"tag":null,"postId":"6Fpvch8RR29qLEWNH","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":6,"contents":{"__ref":"Revision:c2sKkBsEwDr5wwGWk_"},"postedAt":"2022-08-01T23:19:32.367Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":53,"extendedScore":{"agreement":0,"agreementVoteCount":0},"score":0.005671,"voteCount":29,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":12,"afExtendedScore":{"agreement":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-08-03T16:04:33.331Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":2,"votingSystem":"twoAxis","isPinnedOnProfile":null},"Post:7iAABhWpcGeP5e6SB":{"_id":"7iAABhWpcGeP5e6SB","__typename":"Post","slug":"it-s-probably-not-lithium","title":"It’s Probably Not Lithium","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"QEixL4himR6veBCej"},"Revision:Z93ArrtDjJnBtkfGb_":{"_id":"Z93ArrtDjJnBtkfGb_","__typename":"Revision","html":"<p>For the first part of the experiment, mostly nuts, bananas, olives, and eggs. Later I added vegan sausages + condiments.&nbsp;<\/p>","plaintextMainText":"For the first part of the experiment, mostly nuts, bananas, olives, and eggs.\nLater I added vegan sausages + condiments.","wordCount":20},"Comment:Z93ArrtDjJnBtkfGb":{"_id":"Z93ArrtDjJnBtkfGb","__typename":"Comment","post":{"__ref":"Post:7iAABhWpcGeP5e6SB"},"tag":null,"postId":"7iAABhWpcGeP5e6SB","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":"k4efZt4zTQAR6rxEX","topLevelCommentId":"DXARj7atbeqoo77kx","descendentCount":1,"contents":{"__ref":"Revision:Z93ArrtDjJnBtkfGb_"},"postedAt":"2022-07-05T18:45:53.263Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":5,"extendedScore":{"agreement":0,"agreementVoteCount":2},"score":0.0004866992876482415,"voteCount":5,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"agreement":0,"agreementVoteCount":2},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.6.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-09-25T16:29:44.506Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":2,"votingSystem":"twoAxis","isPinnedOnProfile":null},"Revision:6tm2qkbM4ecWfBzyF_":{"_id":"6tm2qkbM4ecWfBzyF_","__typename":"Revision","html":"<p>Adding my anecdote to everyone else's: after learning about the palatability hypothesis, I resolved to eat only non-tasty food for a while, and lost 30 pounds over about four months (200 -&gt; 170). I've since relaxed my diet a little to include a little tasty food, and now (8 months after the start) have maintained that loss (even going down a little further).<\/p>","plaintextMainText":"Adding my anecdote to everyone else's: after learning about the palatability\nhypothesis, I resolved to eat only non-tasty food for a while, and lost 30\npounds over about four months (200 -> 170). I've since relaxed my diet a little\nto include a little tasty food, and now (8 months after the start) have\nmaintained that loss (even going down a little further).","wordCount":63},"Comment:6tm2qkbM4ecWfBzyF":{"_id":"6tm2qkbM4ecWfBzyF","__typename":"Comment","post":{"__ref":"Post:7iAABhWpcGeP5e6SB"},"tag":null,"postId":"7iAABhWpcGeP5e6SB","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":"tLnrajhE7AhpzJi5m","topLevelCommentId":"DXARj7atbeqoo77kx","descendentCount":6,"contents":{"__ref":"Revision:6tm2qkbM4ecWfBzyF_"},"postedAt":"2022-07-04T22:29:45.646Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":27,"extendedScore":{"approvalVoteCount":12,"agreement":7,"agreementVoteCount":4},"score":0.001953,"voteCount":12,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":10,"afExtendedScore":{"approvalVoteCount":6,"agreement":4,"agreementVoteCount":2},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.6.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-09-25T16:29:44.506Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":2,"votingSystem":"twoAxis","isPinnedOnProfile":null},"Post:MnFqyPLqbiKL8nSR7":{"_id":"MnFqyPLqbiKL8nSR7","__typename":"Post","slug":"my-experience-at-and-around-miri-and-cfar-inspired-by-zoe","title":"My experience at and around MIRI and CFAR (inspired by Zoe Curzi's writeup of experiences at Leverage)","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"gSKzrqGFdS7DkXhuE"},"Revision:RGKkmyvyoeWe2LB7d_":{"_id":"RGKkmyvyoeWe2LB7d_","__typename":"Revision","html":"<p>Update: I interviewed many of the people involved and feel like I understand the situation better.<\/p><p>My main conclusion is that I was wrong about Michael making people psychotic. Everyone I talked to had some other risk factor, like a preexisting family or personal history, or took recreational drugs at doses that would explain their psychotic episodes.<\/p><p>Michael has a tendency to befriend people with high trait psychoticism and heavy drug use, and often has strong opinions on their treatment, which explains why he is often very close to people and very noticeable at the moment they become psychotic. But aside from one case where he recommended someone take a drug that made a bad situation slightly worse, and the general Berkeley rationalist scene that he (and I and everyone else here) is a part of having lots of crazy ideas that are psychologically stressful, I no longer think he is a major cause.<\/p><p>While interviewing the people involved, I did get some additional reasons to worry that he uses cult-y high-pressure recruitment tactics on people he wants things from, in ways that make me continue to be nervous about the effect he *could* have on people. But the original claim I made that I knew of specific cases of psychosis which he substantially helped precipitate turned out to be wrong, and I apologize to him and to Jessica. Jessica's later post <a href=\"https://www.lesswrong.com/posts/pQGFeKvjydztpgnsY/occupational-infohazards\">https://www.lesswrong.com/posts/pQGFeKvjydztpgnsY/occupational-infohazards<\/a> explained in more detail what happened to her, including the role of MIRI and of Michael and his friends, and everything she said there matches what I found too. Insofar as anything I wrote above produces impressions that differs from her explanation, assume that she is right and I am wrong.<\/p><p>Since the interviews involve a lot of private people's private details, I won't be posting anything more substantial than this publicly without a lot of thoughts and discussion. If for some reason this is important to you, let me know and I can send you a more detailed summary of my thoughts.<\/p><p>I'm deliberately leaving this comment in this obscure place for now while I talk to Michael and Jessica about whether they would prefer a more public apology that also brings all of this back to people's attention again.<\/p>","plaintextMainText":"Update: I interviewed many of the people involved and feel like I understand the\nsituation better.\n\nMy main conclusion is that I was wrong about Michael making people psychotic.\nEveryone I talked to had some other risk factor, like a preexisting family or\npersonal history, or took recreational drugs at doses that would explain their\npsychotic episodes.\n\nMichael has a tendency to befriend people with high trait psychoticism and heavy\ndrug use, and often has strong opinions on their treatment, which explains why\nhe is often very close to people and very noticeable at the moment they become\npsychotic. But aside from one case where he recommended someone take a drug that\nmade a bad situation slightly worse, and the general Berkeley rationalist scene\nthat he (and I and everyone else here) is a part of having lots of crazy ideas\nthat are psychologically stressful, I no longer think he is a major cause.\n\nWhile interviewing the people involved, I did get some additional reasons to\nworry that he uses cult-y high-pressure recruitment tactics on people he wants\nthings from, in ways that make me continue to be nervous about the effect he\n*could* have on people. But the original claim I made that I knew of specific\ncases of psychosis which he substantially helped precipitate turned out to be\nwrong, and I apologize to him and to Jessica. Jessica's later post \nhttps://www.lesswrong.com/posts/pQGFeKvjydztpgnsY/occupational-infohazards\n[https://www.lesswrong.com/posts/pQGFeKvjydztpgnsY/occupational-infohazards] \nexplained in more detail what happened to her, including the role of MIRI and of\nMichael and his friends, and everything she said there matches what I found too.\nInsofar as anything I wrote above produces impressions that differs from her\nexplanation, assume that she is right and I am wrong.\n\nSince the interviews involve a lot of private people's private details, I won't\nbe posting anything more substantial than this publicly without a lot of\nthoughts and discussion. If for ","wordCount":369},"Comment:RGKkmyvyoeWe2LB7d":{"_id":"RGKkmyvyoeWe2LB7d","__typename":"Comment","post":{"__ref":"Post:MnFqyPLqbiKL8nSR7"},"tag":null,"postId":"MnFqyPLqbiKL8nSR7","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":"FoSXdfJand9y5nNkQ","topLevelCommentId":"dLyEcki7dBdxFkvJd","descendentCount":0,"contents":{"__ref":"Revision:RGKkmyvyoeWe2LB7d_"},"postedAt":"2022-07-03T23:17:34.821Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":92,"extendedScore":null,"score":0.007015607245475425,"voteCount":28,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":38,"afExtendedScore":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"0.25.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-07-03T23:17:34.826Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":3,"votingSystem":"default","isPinnedOnProfile":null},"Post:Jo89KvfAs9z7owoZp":{"_id":"Jo89KvfAs9z7owoZp","__typename":"Post","slug":"pivotal-act-intentions-negative-consequences-and-fallacious","title":"“Pivotal Act” Intentions: Negative Consequences and Fallacious Arguments","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"f7Mag7bDZKv59Bsaw"},"Revision:sYyrcRgcFLYdgPRLd_":{"_id":"sYyrcRgcFLYdgPRLd_","__typename":"Revision","html":"<p>I agree it's not necessarily a good idea to go around founding the Let's Commit A Pivotal Act AI Company.<\/p><p>But I think there's room for subtlety somewhere like \"Conditional on you being in a situation where you <i>could<\/i> take a pivotal act, which is a small and unusual fraction of world-branches, maybe you should take a pivotal act.\"<\/p><p>That is, <i>if<\/i> you are in a position where you have the option to build an AI capable of destroying all competing AI projects, the moment you notice this you should update heavily in favor of short timelines (zero in your case, but everyone else should be close behind) and fast takeoff speeds (since your AI has these impressive capabilities). You should also update on existing AI regulation being insufficient (since it was insufficient to prevent <i>you<\/i>)<\/p><p>Somewhere halfway between \"found the Let's Commit A Pivotal Act Company\" and \"if you happen to stumble into a pivotal act, take it\", there's an intervention to spread a norm of \"if a good person who cares about the world happens to stumble into a pivotal-act-capable AI, take the opportunity\". I don't think this norm would necessarily accelerate a race. After all, bad people who want to seize power can take pivotal acts whether we want them to or not. The only people who are bound by norms are good people who care about the future of humanity. I, as someone with no loyalty to any individual AI team, would prefer that (good, norm-following) teams take pivotal acts if they happen to end up with the first superintelligence, rather than not doing that.<\/p><p>Another way to think about this is that all good people should be equally happy with any other good person creating a pivotal AGI, so they won't need to race among themselves. They might be less happy with a bad person creating a pivotal AGI, but in that case you <i>should<\/i> race and you have no other option. I realize \"good\" and \"bad\" are very simplistic but I don't think adding real moral complexity changes the calculation much.<\/p><p>I am more concerned about your point where someone rushes into a pivotal act without being sure their own AI is aligned. I agree this would be very dangerous, but it seems like a job for normal cost-benefit calculation: what's the risk of your AI being unaligned if you act now, vs. someone else creating an unaligned AI if you wait X amount of time? Do we have any reason to think teams would be systematically biased when making this calculation?<\/p>","plaintextMainText":"I agree it's not necessarily a good idea to go around founding the Let's Commit\nA Pivotal Act AI Company.\n\nBut I think there's room for subtlety somewhere like \"Conditional on you being\nin a situation where you could take a pivotal act, which is a small and unusual\nfraction of world-branches, maybe you should take a pivotal act.\"\n\nThat is, if you are in a position where you have the option to build an AI\ncapable of destroying all competing AI projects, the moment you notice this you\nshould update heavily in favor of short timelines (zero in your case, but\neveryone else should be close behind) and fast takeoff speeds (since your AI has\nthese impressive capabilities). You should also update on existing AI regulation\nbeing insufficient (since it was insufficient to prevent you)\n\nSomewhere halfway between \"found the Let's Commit A Pivotal Act Company\" and \"if\nyou happen to stumble into a pivotal act, take it\", there's an intervention to\nspread a norm of \"if a good person who cares about the world happens to stumble\ninto a pivotal-act-capable AI, take the opportunity\". I don't think this norm\nwould necessarily accelerate a race. After all, bad people who want to seize\npower can take pivotal acts whether we want them to or not. The only people who\nare bound by norms are good people who care about the future of humanity. I, as\nsomeone with no loyalty to any individual AI team, would prefer that (good,\nnorm-following) teams take pivotal acts if they happen to end up with the first\nsuperintelligence, rather than not doing that.\n\nAnother way to think about this is that all good people should be equally happy\nwith any other good person creating a pivotal AGI, so they won't need to race\namong themselves. They might be less happy with a bad person creating a pivotal\nAGI, but in that case you should race and you have no other option. I realize\n\"good\" and \"bad\" are very simplistic but I don't think adding real moral\ncomplexity changes the calculation much.\n\nI am more concerned abo","wordCount":423},"Comment:sYyrcRgcFLYdgPRLd":{"_id":"sYyrcRgcFLYdgPRLd","__typename":"Comment","post":{"__ref":"Post:Jo89KvfAs9z7owoZp"},"tag":null,"postId":"Jo89KvfAs9z7owoZp","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":1,"contents":{"__ref":"Revision:sYyrcRgcFLYdgPRLd_"},"postedAt":"2022-04-20T08:02:06.245Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":64,"extendedScore":{"approvalVoteCount":31,"agreement":9,"agreementVoteCount":4},"score":0.003181,"voteCount":33,"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":26,"afExtendedScore":{"approvalVoteCount":19,"agreement":8,"agreementVoteCount":3},"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-04-20T19:23:27.800Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"twoAxis","isPinnedOnProfile":null},"Post:zo9zKcz47JxDErFzQ":{"_id":"zo9zKcz47JxDErFzQ","__typename":"Post","slug":"call-for-distillers","title":"Call For Distillers","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"MEu8MdhruX5jfGsFQ"},"Revision:uLevmvu75v6KTagyT_":{"_id":"uLevmvu75v6KTagyT_","__typename":"Revision","html":"<p>My current plan is to go through most of the MIRI dialogues and anything else lying around that I think would be of interest to my readers, at some slow rate where I don't scare off people who don't want to read too much AI stuff. If anyone here feels like something else would be a better use of my time, let me know.<\/p>","plaintextMainText":"My current plan is to go through most of the MIRI dialogues and anything else\nlying around that I think would be of interest to my readers, at some slow rate\nwhere I don't scare off people who don't want to read too much AI stuff. If\nanyone here feels like something else would be a better use of my time, let me\nknow.","wordCount":64},"Comment:uLevmvu75v6KTagyT":{"_id":"uLevmvu75v6KTagyT","__typename":"Comment","post":{"__ref":"Post:zo9zKcz47JxDErFzQ"},"tag":null,"postId":"zo9zKcz47JxDErFzQ","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":"bK5aXkhKxgmYxPgSA","topLevelCommentId":"bK5aXkhKxgmYxPgSA","descendentCount":0,"contents":{"__ref":"Revision:uLevmvu75v6KTagyT_"},"postedAt":"2022-04-09T11:52:42.798Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":20,"extendedScore":null,"score":0.0013660587955413137,"voteCount":12,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":4,"afExtendedScore":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-04-09T11:52:42.801Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"default","isPinnedOnProfile":null},"Post:vxeBgsbeQ29zvrijo":{"_id":"vxeBgsbeQ29zvrijo","__typename":"Post","slug":"how-to-interpret-vitamin-d-dosage-using-numbers","title":"How to Interpret Vitamin D Dosage Using Numbers","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"nt2XsHkdksqZ3snNr"},"Revision:2FnGoaCjqgm6HxXE7_":{"_id":"2FnGoaCjqgm6HxXE7_","__typename":"Revision","html":"<p>I don't think hunter-gatherers get 16000 to 32000 IU of Vitamin D daily. <a href=\"https://sci-hub.st/https://www.sciencedirect.com/science/article/abs/pii/S1011134415301561\">This study<\/a> suggests Hadza hunter-gatherers get more like 2000. I think the difference between their calculation and yours is that they find that hunter-gatherers avoid the sun during the hottest part of the day. It might also have to do with them being black, I'm not sure.<\/p><p>Hadza hunter gatherers have serum D levels of about 44 ng/ml. Based on <a href=\"https://www.researchgate.net/profile/Reinhold-Vieth/publication/7959166_Estimates_of_Optimal_Vitamin_D_Status/links/00b7d52026357671c5000000/Estimates-of-Optimal-Vitamin-D-Status.pdf\">this paper<\/a>, I think you would need total vitamin D (diet + sunlight + supplements) of about 4400 IU/day to get that amount. If you start off as a mildly deficient American (15 ng/ml), you'd need an extra 2900 IU/day; if you start out as an average white American (30 ng/ml), you'd need an extra 1400 IU/day. The Hadza are probably an overestimate of what you need since they're right on the equator - hunter-gatherers in eg Europe probably did fine too. I think this justifies the doses of 400 - 2000 IU/day in studies as reasonably evolutionarily-informed.<\/p><p>Please don't actually take 16000 IU/day of vitamin D daily, if taken long-term this would put you at risk for <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6158375/\">vitamin D toxicity.<\/a><\/p><p>I also agree with the issues about the individual studies which other people have brought up.<\/p>","plaintextMainText":"I don't think hunter-gatherers get 16000 to 32000 IU of Vitamin D daily. This\nstudy\n[https://sci-hub.st/https://www.sciencedirect.com/science/article/abs/pii/S1011134415301561] \nsuggests Hadza hunter-gatherers get more like 2000. I think the difference\nbetween their calculation and yours is that they find that hunter-gatherers\navoid the sun during the hottest part of the day. It might also have to do with\nthem being black, I'm not sure.\n\nHadza hunter gatherers have serum D levels of about 44 ng/ml. Based on this\npaper\n[https://www.researchgate.net/profile/Reinhold-Vieth/publication/7959166_Estimates_of_Optimal_Vitamin_D_Status/links/00b7d52026357671c5000000/Estimates-of-Optimal-Vitamin-D-Status.pdf]\n, I think you would need total vitamin D (diet + sunlight + supplements) of\nabout 4400 IU/day to get that amount. If you start off as a mildly deficient\nAmerican (15 ng/ml), you'd need an extra 2900 IU/day; if you start out as an\naverage white American (30 ng/ml), you'd need an extra 1400 IU/day. The Hadza\nare probably an overestimate of what you need since they're right on the equator\n- hunter-gatherers in eg Europe probably did fine too. I think this justifies\nthe doses of 400 - 2000 IU/day in studies as reasonably evolutionarily-informed.\n\nPlease don't actually take 16000 IU/day of vitamin D daily, if taken long-term\nthis would put you at risk for vitamin D toxicity.\n[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6158375/]\n\nI also agree with the issues about the individual studies which other people\nhave brought up.","wordCount":207},"Comment:2FnGoaCjqgm6HxXE7":{"_id":"2FnGoaCjqgm6HxXE7","__typename":"Comment","post":{"__ref":"Post:vxeBgsbeQ29zvrijo"},"tag":null,"postId":"vxeBgsbeQ29zvrijo","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":1,"contents":{"__ref":"Revision:2FnGoaCjqgm6HxXE7_"},"postedAt":"2022-04-08T07:58:16.109Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":28,"extendedScore":null,"score":0.0016287642341438389,"voteCount":12,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-04-09T19:13:45.536Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"default","isPinnedOnProfile":null},"Post:MPhc7q48Cmo7As2th":{"_id":"MPhc7q48Cmo7As2th","__typename":"Post","slug":"is-metaculus-slow-to-update","title":"Is Metaculus Slow to Update?","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"Fn7m44CAhjRP9AquS"},"Revision:Z63qjNcNey9cuKsKQ_":{"_id":"Z63qjNcNey9cuKsKQ_","__typename":"Revision","html":"<p>Thanks for looking into this.<\/p>","plaintextMainText":"Thanks for looking into this.","wordCount":5},"Comment:Z63qjNcNey9cuKsKQ":{"_id":"Z63qjNcNey9cuKsKQ","__typename":"Comment","post":{"__ref":"Post:MPhc7q48Cmo7As2th"},"tag":null,"postId":"MPhc7q48Cmo7As2th","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"contents":{"__ref":"Revision:Z63qjNcNey9cuKsKQ_"},"postedAt":"2022-03-27T07:37:28.828Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":8,"extendedScore":null,"score":0.0005605140445636923,"voteCount":5,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-03-27T07:37:28.832Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":null,"votingSystem":"default","isPinnedOnProfile":null},"Post:LbyxFk8JmPKPAQBvL":{"_id":"LbyxFk8JmPKPAQBvL","__typename":"Post","slug":"we-re-already-in-ai-takeoff","title":"We're already in AI takeoff","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"d3372gCBr7DkDvdWC"},"Revision:aDjjbW2aWKfFCf757_":{"_id":"aDjjbW2aWKfFCf757_","__typename":"Revision","html":"<p>Maybe. It might be that if you described what you wanted more clearly, it would be the same thing that I want, and possibly I was incorrectly associating this with the things at CFAR you say you're against, in which case sorry.<\/p><p>But I still don't feel like I quite understand your suggestion. You talk of \"stupefying egregores\" as problematic insofar as they distract from the object-level problem. But I don't understand how pivoting to egregore-fighting isn't also a distraction from the object-level problem. Maybe this is because I don't understand what fighting egregores consists of, and if I knew, then I would agree it was some sort of reasonable problem-solving step.<\/p><p>I agree that the Sequences contain a lot of useful deconfusion, but I interpret them as useful primarily because they provide a template for good thinking, and not because clearing up your thinking about those things is itself necessary for doing good work. I think of the cryonics discussion the same way I think of the Many Worlds discussion - following the motions of someone as they get the right answer to a hard question trains you to do this thing yourself.<\/p><p>I'm sorry if \"cultivate your will\" has the wrong connotations, but you did say \"The problem that's upstream of this is <i>the lack of will<\/i>\", and I interpreted a lot of your discussion of de-numbing and so on as dealing with this.<\/p><blockquote><p>Part of what inspired me to write this piece at all was seeing a kind of blindness to these memetic forces in how people talk about AI risk and alignment research. Making bizarre assertions about what things need to happen on the god scale of \"AI researchers\" or \"governments\" or whatever, roughly on par with people loudly asserting opinions about what POTUS should do. It strikes me as immensely obvious that memetic forces precede AGI. If the memetic landscape slants down mercilessly toward existential oblivion here, then the thing to do isn't to prepare to swim upward against a future avalanche. It's to orient to the landscape.<\/p><\/blockquote><p>The claim \"memetic forces precede AGI\" seems meaningless to me, except insofar as memetic forces precede everything (eg the personal computer was invented because people wanted personal computers and there was a culture of inventing things). Do you mean it in a stronger sense? If so, what sense?<\/p><p>I also don't understand why it's wrong to talk about what \"AI researchers\" or \"governments\" should do. Sure, it's more virtuous to act than to chat randomly about stuff, but many Less Wrongers are in positions to change what AI researchers do, and if they have opinions about that, they should voice them. This post of yours right now seems to be about what \"the rationalist community\" should do, and I don't think it's a category error for you to write it.&nbsp;<\/p><p>Maybe this would easier if you described what actions we should take conditional on everything you wrote being right.<\/p>","plaintextMainText":"Maybe. It might be that if you described what you wanted more clearly, it would\nbe the same thing that I want, and possibly I was incorrectly associating this\nwith the things at CFAR you say you're against, in which case sorry.\n\nBut I still don't feel like I quite understand your suggestion. You talk of\n\"stupefying egregores\" as problematic insofar as they distract from the\nobject-level problem. But I don't understand how pivoting to egregore-fighting\nisn't also a distraction from the object-level problem. Maybe this is because I\ndon't understand what fighting egregores consists of, and if I knew, then I\nwould agree it was some sort of reasonable problem-solving step.\n\nI agree that the Sequences contain a lot of useful deconfusion, but I interpret\nthem as useful primarily because they provide a template for good thinking, and\nnot because clearing up your thinking about those things is itself necessary for\ndoing good work. I think of the cryonics discussion the same way I think of the\nMany Worlds discussion - following the motions of someone as they get the right\nanswer to a hard question trains you to do this thing yourself.\n\nI'm sorry if \"cultivate your will\" has the wrong connotations, but you did say\n\"The problem that's upstream of this is the lack of will\", and I interpreted a\nlot of your discussion of de-numbing and so on as dealing with this.\n\nThe claim \"memetic forces precede AGI\" seems meaningless to me, except insofar\nas memetic forces precede everything (eg the personal computer was invented\nbecause people wanted personal computers and there was a culture of inventing\nthings). Do you mean it in a stronger sense? If so, what sense?\n\nI also don't understand why it's wrong to talk about what \"AI researchers\" or\n\"governments\" should do. Sure, it's more virtuous to act than to chat randomly\nabout stuff, but many Less Wrongers are in positions to change what AI\nresearchers do, and if they have opinions about that, they should voice them.\nThis post of yours right","wordCount":486},"Comment:aDjjbW2aWKfFCf757":{"_id":"aDjjbW2aWKfFCf757","__typename":"Comment","post":{"__ref":"Post:LbyxFk8JmPKPAQBvL"},"tag":null,"postId":"LbyxFk8JmPKPAQBvL","tagId":null,"tagCommentType":"DISCUSSION","parentCommentId":"2vZxCaSoYH4uL4yKn","topLevelCommentId":"ekjBvP8BfKzegwCAW","descendentCount":0,"contents":{"__ref":"Revision:aDjjbW2aWKfFCf757_"},"postedAt":"2022-03-13T04:34:55.445Z","repliesBlockedUntil":null,"userId":"XgYW5s8njaYrtyP7q","deleted":false,"deletedPublic":false,"deletedReason":null,"hideAuthor":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":32,"extendedScore":null,"score":0.0018285036117970228,"voteCount":9,"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"lastSubthreadActivity":"2022-03-13T04:34:55.449Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":2,"votingSystem":"default","isPinnedOnProfile":null},"Revision:KCcdhZK7omEMwBdju_":{"_id":"KCcdhZK7omEMwBdju_","__typename":"Revision","htmlHighlight":"<p>It's September, which means going to beaches, holding barbecues, and . . . preparing for this year's Bay Area Winter Solstice. This year's team will include me, Shauna, Nova, and Mingyuan. In the meantime, we're looking for volunteers.<\/p><p>First, we have logistics volunteers. Some of these, especially AV and lighting, will require a commitment to be kind of agenty in figuring out what you need to do, attending dress rehearsals to help test things, etc. The simple tasks will just require you to show up day of. &nbsp;If you want to help with any of the following logistics tasks, please email <a href=\"mailto:smkravec@celest.ai\">smkravec@celest.ai<\/a>:<\/p><ol><li>Someone to monitor the Twitch stream/simulcast and give updates to AV on that.<\/li><li>Someone to work closely with Nova on the video side of AV; filming the event and running the live stream (possibly that's worth a separate role, they'd be responsible for the audio submix to the stream)<\/li><li>Someone to help setup and run lighting<\/li><li>5-10 people who can help with simple tasks on the day of, like ushering, taking tickets, and moving large objects<\/li><\/ol><p>Second, we have creative tasks. We seem pretty good for people willing to do creative tasks this year, so you don't need to volunteer unless you're really enthusiastic. If you do volunteer, expect to have to audition. If you're interested in any of the following creative tasks, please email <a href=\"mailto:scott@slatestarcodex.com\">scott@slatestarcodex.com<\/a>:<\/p><ol><li>Giving a speech (either writing your own, or giving a reading selected by the team)<\/li><li>Singing a song (either your own, or performing one selected by the team)<\/li><\/ol><p>We'll give volunteers more information by email, and we'll try to give everyone more information as we get closer to the event.<\/p><p>Thanks!<\/p>","wordCount":281,"version":"1.0.0"},"Revision:vtozKm5BZ8gf6zd45_description":{"_id":"vtozKm5BZ8gf6zd45_description","__typename":"Revision","htmlHighlight":"<p>The <strong>Secular Solstice<\/strong> is a holiday tradition invented by Less Wrong user Raymond Arnold. It is celebrated on-or-near December 21st, the winter solstice in the northern hemisphere. It is typically celebrated in large groups with a ceremony involving singing and storytelling.<\/p>"},"Tag:vtozKm5BZ8gf6zd45":{"_id":"vtozKm5BZ8gf6zd45","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:vtozKm5BZ8gf6zd45_description"},"userId":"kmiXJjx2GS4txx3yj","name":"Secular Solstice","slug":"secular-solstice","core":null,"postCount":57,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-05T22:37:23.988Z","wikiOnly":false,"deleted":false},"Revision:izp6eeJJEg9v5zcur_description":{"_id":"izp6eeJJEg9v5zcur_description","__typename":"Revision","htmlHighlight":"<p>The <strong>LessWrong<\/strong> <strong>Community<\/strong> consists of the people who write on LessWrong and who contribute to its mission of refining the art of human rationality. This tag includes community events, analysis of the health, norms and direction of the community, and space to understand communities in general.<\/p><p>LessWrong also has many brothers and sisters like the Berkeley Rationality Community, <a href=\"https://www.reddit.com/r/slatestarcodex/\">SlateStarCodex<\/a>, <a href=\"https://www.reddit.com/r/rational/\">Rational Fiction<\/a>, <a href=\"https://forum.effectivealtruism.org/\">Effective Altruism<\/a>, <a href=\"https://www.alignmentforum.org/\">AI Alignment<\/a>, and more, who participate here. To see upcoming LessWrong events, go to the <a href=\"https://www.lesswrong.com/community\">community section<\/a>.<\/p><hr><h2><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Community Sub-Topics<\/strong><\/h2><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:50%\"><p><strong>All<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/bounties-active?showPostCount=true&amp;useTagName=true\">Bounties (active)<\/a><br><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities?showPostCount=true\">Grants &amp; Fundraising<\/a><br><a href=\"http://www.lesswrong.com/tag/growth-stories?showPostCount=true&amp;useTagName=true\">Growth Stories<\/a><br><a href=\"https://www.lesswrong.com/tag/online-socialization?showPostCount=true&amp;useTagName=true\">Online Socialization<\/a><br><a href=\"https://www.lesswrong.com/tag/petrov-day?showPostCount=true&amp;useTagName=true\">Petrov Day<\/a><br><a href=\"https://www.lesswrong.com/tag/public-discourse?showPostCount=true&amp;useTagName=true\">Public Discourse<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/ritual?showPostCount=true&amp;useTagName=true\">Ritual<\/a><br><a href=\"https://www.lesswrong.com/tag/solstice-celebration?showPostCount=true&amp;useTagName=true\">Solstice Celebration<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%\"><p><strong>LessWrong<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/events-community?showPostCount=true&amp;useTagName=true\">Events (Community)<\/a><br><a href=\"https://www.lesswrong.com/tag/site-meta?showPostCount=true&amp;useTagName=true\">Site Meta<\/a><br><a href=\"https://www.lesswrong.com/tag/greaterwrong-meta?showPostCount=true&amp;useTagName=true\">GreaterWrong Meta<\/a><br><a href=\"https://www.lesswrong.com/tag/lesswrong-events?showPostCount=true&amp;useTagName=true\">LessWrong Events<\/a><br><a href=\"http://www.lesswrong.com/tag/lw-moderation?showPostCount=true&amp;useTagName=true\">LW Moderation<\/a><br><a href=\"http://www.lesswrong.com/tag/meetups-topic?showPostCount=true&amp;useTagName=true\">Meetups (topic)<\/a><br><a href=\"http://www.lesswrong.com/tag/moderation-topic?showPostCount=true&amp;useTagName=true\">Moderation (topic)<\/a><br><a href=\"http://www.lesswrong.com/tag/the-sf-bay-area?showPostCount=true&amp;useTagName=true\">The SF Bay Area<\/a><br><a href=\"http://www.lesswrong.com/tag/tagging?showPostCount=true\">Tagging<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure><p><i>Not all Community posts are tagged with subtopics.<\/i><\/p><hr><p>This tag applies to any post about:<\/p><ul><li>Specific projects, orgs, and prizes [e.g. <a href=\"http://www.lesswrong.com/posts/xFGQdgJndLcthgWoE\"><u>1<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH\"><u>2<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN\"><u>3<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc\"><u>4<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg\"><u>5<\/u><\/a>]<\/li><li>Requests and offers for help [<a href=\"http://www.lesswrong.com/posts/bSWavBThj6ebB62gD\"><u>1<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/LuL7LLqcdmM7TTYvW\"><u>2<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/x72ta8C3dKu2QRfPv\"><u>3<\/u><\/a>]<\/li><li>Announcements, retrospectives, funding requests, and AMAs from orgs [<a href=\"http://www.lesswrong.com/posts/XJiNtvxoiLCpBn6FH\"><u>1<\/u><\/a> <a href=\"https://www.lesswrong.com/posts/96N8BT9tJvybLbn5z/we-run-the-center-for-applied-rationality-ama\"><u>2<\/u><\/a> <a href=\"http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH\"><u>3<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN\"><u>4<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/tCHsm5ZyAca8HfJSG\"><u>5<\/u><\/a>]<\/li><li>Discussions of the orgs in the LessWrong, Rationalist cluster [<a href=\"http://www.lesswrong.com/posts/KpnyCT7CZy4Qe6kx6\"><u>1<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si\"><u>2<\/u><\/a>]<\/li><li>Discussions about the LessWrong, Rationalist, and related communities [<a href=\"http://www.lesswrong.com/posts/2Ee5DPBxowTTXZ6zf\"><u>1<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/yGycR8tFA3JJbvApp\"><u>2<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/zAqoj79A7QuhJKKvi\"><u>3<\/u><\/a>]<\/li><\/ul><p>While the <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a><i> <\/i>core tag is for posts discussing how to do good in general, the Community tag is for the specific, concrete efforts of our community to execute plans.<\/p>"},"Tag:izp6eeJJEg9v5zcur":{"_id":"izp6eeJJEg9v5zcur","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:izp6eeJJEg9v5zcur_description"},"userId":"XtphY3uYHwruKqDyG","name":"Community","slug":"community","core":true,"postCount":1204,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":15,"createdAt":"2020-06-14T03:38:34.631Z","wikiOnly":false,"deleted":false},"Revision:T57Qd9J3AfxmwhQtY_description":{"_id":"T57Qd9J3AfxmwhQtY_description","__typename":"Revision","htmlHighlight":"<p>The rationalist community has chapters all over the world, the oldest being the NYC community, which has been around since 2009. Many of these groups are centered around regular meetups, or call themselves 'meetups'.<\/p><p>These are posts about meetups and local communities in general, not about specific communities (unless they also provide general insight).<br><br>For example - <a href=\"https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-are-meetups-actually-trying-to-accomplish\">What are meetups actually trying to accomplish?<\/a><\/p><p>For specific meetups see <a href=\"https://www.lesswrong.com/tag/events-community\">Events(community)<\/a><\/p><p>See also - <a href=\"https://www.lesswrong.com/tag/community\">Community<\/a><\/p>"},"Tag:T57Qd9J3AfxmwhQtY":{"_id":"T57Qd9J3AfxmwhQtY","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:T57Qd9J3AfxmwhQtY_description"},"userId":"sKAL2jzfkYkDbQmx9","name":"Meetups & Local Communities (topic)","slug":"meetups-and-local-communities-topic","core":false,"postCount":81,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-31T06:45:58.891Z","wikiOnly":false,"deleted":false},"Post:KCcdhZK7omEMwBdju":{"_id":"KCcdhZK7omEMwBdju","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:KCcdhZK7omEMwBdju_"},"fmCrosspost":null,"readTimeMinutes":1,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:vtozKm5BZ8gf6zd45"},{"__ref":"Tag:izp6eeJJEg9v5zcur"},{"__ref":"Tag:T57Qd9J3AfxmwhQtY"}],"url":null,"postedAt":"2022-09-04T06:44:19.043Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"commentCount":2,"voteCount":15,"baseScore":43,"extendedScore":null,"unlisted":false,"score":0.005400338919174546,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2022-10-19T06:50:59.793Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":9,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2022-09-04T06:44:19.048Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"reviewCount":null,"reviewVoteCount":null,"positiveReviewVoteCount":null,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":null,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"bay-solstice-2022-call-for-volunteers","title":"Bay Solstice 2022 Call For Volunteers","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:fLdADsBLAMuGvky2M_":{"_id":"fLdADsBLAMuGvky2M_","__typename":"Revision","htmlHighlight":"<p>Here's the <a href=\"https://astralcodexten.substack.com/\">Astral Codex Ten<\/a> worldwide meetups list, crossposted at LW's request in case people here are interested in attending. Some cities have an ACX but not an LW meetup group, or vice versa; others combine their groups. You can find the list below, in the following order:<\/p><ol><li>Africa &amp; Middle East<\/li><li>Asia-Pacific (including Australia)<\/li><li>Canada<\/li><li>Europe (including UK)<\/li><li>Latin America<\/li><li>United States<\/li><\/ol><p>You can see a map of all the events on <a href=\"https://www.lesswrong.com/community\">the LessWrong community page<\/a>.<\/p><p>Within each section, it’s alphabetized first by country/state, then by city - so the first entry in Europe is Vienna, <strong>A<\/strong>ustria. Sorry if this is confusing.<\/p><p>I'll provisionally be attending the meetups in Berkeley, Los Angeles, and San Diego. ACX meetups coordinator Mingyuan will provisionally be attending Paris and London. I’ll be announcing some of the biggest ones on the blog, regardless of whether or not I attend.<\/p><p><strong>Extra Info For Potential Attendees<\/strong><\/p><p><strong>1. <\/strong>If you’re reading this, you’re invited. Please don’t feel like you “won’t be welcome” just because you’re new to the blog, demographically different from the average reader, or hate ACX and everything it stands for. You’ll be fine!&nbsp;<br><strong>2<\/strong>. You don’t have to RSVP or contact the organizer to be able to attend (unless the event description says otherwise); RSVPs are mostly to give organizers a better sense of how many people might show up, and let them tell you if there are last-second changes. I’ve also given email addresses for all organizers in case you have a question.<\/p><p><strong>Extra Info For Meetup Organizers:<\/strong><br><br><strong>1.<\/strong> If you’re the host, bring a sign that says “ACX MEETUP” and prop it up somewhere (or otherwise be identifiable).<br><strong>2. <\/strong>Bring blank labels and pens for nametags.<br><strong>3. <\/strong>Have people type their name and email address in a spreadsheet or in a Google Form (accessed via a bit.ly link or QR code), so you can start a mailing list to make organizing future meetups easier.<br><strong>4.<\/strong> If it’s the first meetup, people are probably just going to want to talk, and if you try to organize some kind of “fun” “event” it’ll probably just be annoying.<br><strong>5.<\/strong> It’s easier to schedule a followup meetup while you’re having the first, compared to trying to do it later on by email.<br><strong>6.<\/strong> In case people want to get to know each other better outside the meetup, you might want to mention <a href=\"https://www.reciprocity.io/\">reciprocity.io<\/a>, the rationalist friend-finder/dating site.<br><strong>7.<\/strong> If you didn’t make a LessWrong event for your meetup, the LessWrong team d... <\/p>","wordCount":12304,"version":"1.3.0"},"Revision:fLdADsBLAMuGvky2M_moderationGuidelines":{"_id":"fLdADsBLAMuGvky2M_moderationGuidelines","__typename":"Revision","html":""},"Revision:fLdADsBLAMuGvky2M_customHighlight":{"_id":"fLdADsBLAMuGvky2M_customHighlight","__typename":"Revision","html":""},"Post:fLdADsBLAMuGvky2M":{"_id":"fLdADsBLAMuGvky2M","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:fLdADsBLAMuGvky2M_"},"fmCrosspost":null,"readTimeMinutes":49,"moderationGuidelines":{"__ref":"Revision:fLdADsBLAMuGvky2M_moderationGuidelines"},"customHighlight":{"__ref":"Revision:fLdADsBLAMuGvky2M_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:izp6eeJJEg9v5zcur"}],"url":null,"postedAt":"2022-08-26T18:12:04.083Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2022-08-26T18:43:32.708Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"commentCount":null,"voteCount":25,"baseScore":62,"extendedScore":null,"unlisted":false,"score":0.007254735243869941,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2022-08-26T18:12:04.083Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":12,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2022-08-26T18:12:04.092Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"reviewCount":null,"reviewVoteCount":null,"positiveReviewVoteCount":null,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":null,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"acx-meetups-everywhere-list","title":"ACX Meetups Everywhere List","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:gBpYo7mt2zNBmtBJd_":{"_id":"gBpYo7mt2zNBmtBJd_","__typename":"Revision","htmlHighlight":"<p>[crossposted from <a href=\"https://astralcodexten.substack.com/p/on-hreha-on-behavioral-economics\">Astral Codex Ten<\/a>]<\/p><p>Jason Hreha’s article on <a href=\"https://www.thebehavioralscientist.com/articles/the-death-of-behavioral-economics\">The Death Of Behavioral Economics<\/a> has been going around lately, after an experiment by behavioral econ guru Dan Ariely was discovered to be fraudulent. The article argues that this is the tip of the iceberg - looking back on the last few years of replication crisis, behavioral economics has been undermined almost to the point of irrelevance.<\/p><p>The article itself mostly just urges behavioral economists to do better, which is always good advice for everyone. But as usual, it’s the inflammatory title that’s gone viral. I think a strong interpretation of behavioral economics as dead or debunked is unjustified.<\/p><p><strong>I.<\/strong><\/p><p>My medical school had final exams made of true-false questions, with an option to answer “don’t know”. They were scored like so: if you got it right, +1 point; wrong, -0.5 points; don’t know, 0. You can easily confirm that it’s always worth guessing even if you genuinely don’t know the answer (+0.25 points on average instead of 0). On average people probably had to guess on ~30% of questions (don’t ask; it’s an Irish education system thing), so you could increase your test score 7.5% with the right strategy here.<\/p><p>I knew all this, but it was still really hard to guess. I did it, but I had to fight my natural inclinations. And when I talked about this with friends - smart people, the sort of people who got into medical school! - none of them guessed, and no matter how much I argued with them <a href=\"https://www.lesswrong.com/posts/znBJwbuT3f5eWgM4E/shut-up-and-guess\">they refused to start. <\/a>The average medical student would sell their soul for 7.5% higher grades on standardized tests - but <i>this <\/i>was a step too far.<\/p><p>This is Behavioral Econ 101 stuff - <a href=\"https://en.wikipedia.org/wiki/Risk_aversion\">risk aversion<\/a>, <a href=\"https://en.wikipedia.org/wiki/Loss_aversion\">loss aversion<\/a>, and <a href=\"https://en.wikipedia.org/wiki/Prospect_theory\">prospect theory<\/a>. If it’s true, the core of behavioral economics is salvageable. There might be some bad studies built on top of that core, but the basic insights are right.<\/p><p>One more example: every time I order food from GrubHub, I get a menu like this:<\/p><figure class=\"image\"><img src=\"https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F9ffa9ca4-fe51-4da5-a049-2be13c2c3489_704x118.png\"><\/figure><p>And every time I order food from UberEats, I get a menu like this:<\/p><figure class=\"image\"><img src=\"https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F43182725-42d6-4943-b4df-559d1d337e75_457x315.png\"><\/figure><p>I find I usually click the third box on both. I want to tip generously, but giving the <i>maximum possible<\/i> tip seems profligate. Surely the third box is the right compromise.<\/p><p>I recently noticed that this is insane. For a $35 meal, I’m giving GrubHub drivers $3 and UberEats drivers $7 for the same service (or maybe there’s some difference between their services which makes UberEats suggest the h... <\/p>","wordCount":6519,"version":"1.0.0"},"Revision:Ng8Gice9KNkncxqcj_description":{"_id":"Ng8Gice9KNkncxqcj_description","__typename":"Revision","htmlHighlight":"<p><strong>Rationality<\/strong> is the art of thinking in ways that result in <a href=\"https://www.lesswrong.com/tag/world-modeling\">accurate beliefs<\/a> and <a href=\"https://www.lesswrong.com/tag/decision-theory\">good decisions<\/a>. It is the primary topic of LessWrong.<\/p><p>Rationality is not only about avoiding the vices of <a href=\"https://www.lesswrong.com/tag/self-deception\">self-deception<\/a> and obfuscation (the failure to <a href=\"https://www.lesswrong.com/tag/conversation-topic\">communicate clearly<\/a>), but also about the virtue of <a href=\"https://www.lesswrong.com/tag/curiosity\">curiosity<\/a>, seeing the world more clearly than before, and <a href=\"https://www.lesswrong.com/tag/ambition\">achieving things<\/a> <a href=\"https://www.lesswrong.com/tag/skill-building\">previously unreachable<\/a> <a href=\"https://www.lesswrong.com/tag/coordination-cooperation\">to you<\/a>. The study of rationality on LessWrong includes a theoretical understanding of ideal cognitive algorithms, as well as building a practice that uses these idealized algorithms to inform <a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\">heuristics<\/a>, <a href=\"https://www.lesswrong.com/tag/habits\">habits<\/a>, and <a href=\"https://www.lesswrong.com/tag/techniques\">techniques<\/a>, to successfully reason and make decisions in the real world.<\/p><p>Topics covered in rationality include (but are not limited to): normative and theoretical explorations of <a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">ideal<\/a> <a href=\"https://www.lesswrong.com/tag/probability-and-statistics\">reasoning<\/a>; the <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\">capabilities and limitations<\/a> <a href=\"https://www.lesswrong.com/tag/neuroscience\">of our brain<\/a>, <a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\">mind and psychology<\/a>; applied advice such as <a href=\"https://www.lesswrong.com/tag/introspection\">introspection<\/a> techniques and <a href=\"https://www.lesswrong.com/tag/group-rationality\">how to achieve truth collaboratively<\/a>; practical techniques and methodologies for figuring out what’s true ranging from rough quantitative modeling to full research guides.<\/p><p>Note that content about <i>how the world is <\/i>can be found under <a href=\"https://www.lesswrong.com/tag/world-modeling\">World Modeling<\/a>, and practical advice about <i>how to change the world<\/i> is categorized under <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a> or <a href=\"/tag/practical\">Practical<\/a>.<\/p><hr><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Theory / Concepts<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><u>Anticipated Experiences<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/aumann-s-agreement-theorem?showPostCount=true&amp;useTagName=true\">Aumann's Agreement Theorem<\/a><br><a href=\"http://www.lesswrong.com/tag/bayes-theorem?showPostCount=true&amp;useTagName=true\"><u>Bayes Theorem<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/bounded-rationality?showPostCount=true&amp;useTagName=true\">Bounded Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence?showPostCount=true&amp;useTagName=true\">Conservation of Expected<\/a><br><a href=\"http://www.lesswrong.com/tag/contrarianism?showPostCount=true&amp;useTagName=true\">Contrarianism<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/epistemology?showPostCount=true&amp;useTagName=true\"><u>Epistemology<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><u>Gears-Level<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/hansonian-pre-rationality?useTagName=true&amp;showPostCount=true\">Hansonian Pre-Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/law-thinking?showPostCount=true&amp;useTagName=true\">Law-Thinking<\/a><br><a href=\"http://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"http://www.lesswrong.com/tag/occam-s-razor?showPostCount=true&amp;useTagName=true\">Occam's razor<\/a><br><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/truth-semantics-and-meaning?showPostCount=true&amp;useTagName=true\">Truth, Semantics, &amp; Meaning<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Applied Topics<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/alief?showPostCount=true&amp;useTagName=true\"><u>Alief<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\">Betting<\/a><br><a href=\"http://www.lesswrong.com/tag/cached-thoughts?showPostCount=true&amp;useTagName=true\">Cached Thoughts<\/a><br><a href=\"http://www.lesswrong.com/tag/calibration?showPostCount=true&amp;useTagName=true\">Calibration<\/a><br><a href=\"https://www.lesswrong.com/tag/dark-arts?showPostCount=true&amp;useTagName=true\">Dark Arts<\/a><br><a href=\"http://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism<\/a><br><a href=\"http://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\">Epistemic Modesty<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/identity?showPostCount=true&amp;useTagName=true\">Identity<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/introspection?showPostCount=true&amp;useTagName=true\"><u>Introspection<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/intuition?showPostCount=true&amp;useTagName=true\">Intuition<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><u>Practice &amp; Philosophy of Science<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><br><a href=\"http://www.lesswrong.com/tag/taking-ideas-seriously?showPostCount=true&amp;useTagName=true\">Taking Ideas Seriously<\/a><br><a href=\"https://www.lesswrong.com/tag/value-of-information?showPostCount=true&amp;useTagName=true\">Value of Information<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Failure Modes<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic<\/a><br><a href=\"https://www.lesswrong.com/tag/bucket-errors?showPostCount=true&amp;useTagName=true\">Bucket Errors<\/a><br><a href=\"https://www.lesswrong.com/tag/compartmentalization?showPostCount=true&amp;useTagName=true\">Compartmentalization<\/a><br><a href=\"https://www.lesswrong.com/tag/confirmation-bias?showPostCount=true&amp;useTagName=true\"><u>Confirmation Bias<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/logical-fallacies?showPostCount=true&amp;useTagName=true\">Fallacies<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart’s Law<\/a><br><a href=\"http://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><u>Groupthink<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\">Heuristics and Biases<\/a><br><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy?showPostCount=true&amp;useTagName=true\">Mind Projection Fallacy<\/a><br><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><u>Motivated Reasoning<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true&amp;useTagName=true\">Pica<\/a><br><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality?showPostCount=true&amp;useTagName=true\">Pitfalls of Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalization<\/a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\">Self-Deception<\/a><br><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy?showPostCount=true&amp;useTagName=true\">Sunk-Cost Fallacy<\/a><\/p><\/td><\/tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Communication<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true\"><u>Conversation<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing?showPostCount=true&amp;useTagName=true\"><u>Decoupling vs Contextualizing<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/disagreement?showPostCount=true&amp;useTagName=true\"><u>Disagreement<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/good-explanations-advice?showPostCount=true&amp;useTagName=true\">Good Explanations (Advice)<\/a><br><a href=\"http://www.lesswrong.com/tag/ideological-turing-tests?showPostCount=true&amp;useTagName=true\">Ideological Turing Tests<\/a><br><a href=\"https://www.lesswrong.com/tag/inferential-distance?showPostCount=true&amp;useTagName=true\">Inferential Distance<\/a><br><a href=\"https://www.lesswrong.com/tag/information-cascades?showPostCount=true&amp;useTagName=true\">Information Cascades<\/a><br><a href=\"https://www.lesswrong.com/tag/memetic-immune-system?showPostCount=true&amp;useTagName=true\">Memetic Immune System<\/a><br><a href=\"https://www.lesswrong.com/tag/philosophy-of-language?showPostCount=true&amp;useTagName=true\"><u>Philos<\/u><\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>..."},"Tag:Ng8Gice9KNkncxqcj":{"_id":"Ng8Gice9KNkncxqcj","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:Ng8Gice9KNkncxqcj_description"},"userId":"r38pkCm7wF4M44MDQ","name":"Rationality","slug":"rationality","core":true,"postCount":2449,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":40,"createdAt":"2020-06-14T22:24:17.072Z","wikiOnly":false,"deleted":false},"Revision:3uE2pXvbcnS9nnZRE_description":{"_id":"3uE2pXvbcnS9nnZRE_description","__typename":"Revision","htmlHighlight":"<p><strong>World Modeling<\/strong> is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, <i>finding out how the damn thing works,<\/i> and writing it down for the rest of us.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.<\/i><\/p><p>-- <a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality\"><u>Twelve Virtues of Rationality<\/u><\/a><\/p><\/blockquote><hr><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Modeling Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics<\/a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics<\/a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology<\/a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics<\/a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming<\/a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers<\/a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews<\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History<\/a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics<\/a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies<\/a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease<\/a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis<\/a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution<\/a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology<\/a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness<\/a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution<\/a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology<\/a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine<\/a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience<\/a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus<\/a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence<\/a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review<\/a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models<\/a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science<\/a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets<\/a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism<\/a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><p>&nbsp;<\/p><h2>A definition by elimination<\/h2><p>Properly considered, the overwhelming majority of content LessWrong is about <i>modeling how the world is<\/i>, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality<\/a>, <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a>... <\/p>"},"Tag:3uE2pXvbcnS9nnZRE":{"_id":"3uE2pXvbcnS9nnZRE","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:3uE2pXvbcnS9nnZRE_description"},"userId":"r38pkCm7wF4M44MDQ","name":"World Modeling","slug":"world-modeling","core":true,"postCount":2631,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":27,"createdAt":"2020-06-14T22:24:50.898Z","wikiOnly":false,"deleted":false},"Revision:PDJ6KqJBRzvKPfuS3_description":{"_id":"PDJ6KqJBRzvKPfuS3_description","__typename":"Revision","htmlHighlight":"<p><strong>Economics<\/strong> is the social science that studies how humans and other agents interact in a universe with scarce resources. It deals with topics such as trade, specialization of labor, accumulation of capital, technology, and resource consumption. Agents in economics are generally assumed to have utility functions, which they try to maximize under various constraints.<\/p><p>Economics is usually separated into microeconomics and macroeconomics. Microeconomics concerns the behavior of agents as they interact in a market. More narrowly, it studies the price mechanism, a decentralized system of allocating goods and services based on an evolving system of prices and trade, which all actors in a market economy contribute towards. The price mechanism is closely related to the concept of the <a href=\"https://en.wikipedia.org/wiki/Invisible_hand\">invisible hand<\/a>, first introduced by <a href=\"https://en.wikipedia.org/wiki/Adam_Smith\">Adam Smith<\/a>. <a href=\"https://www.lesswrong.com/tag/game-theory\">Game theory<\/a> is the mathematical study of rational agency, which formalizes many standard results in microeconomics.<\/p><p>Macroeconomics concerns the aggregate behavior of entire economies. For example, it studies economic growth, inflation, international trade and unemployment. An ongoing debate concerns to what extent the <a href=\"https://www.lesswrong.com/tag/economic-consequences-of-agi\">impacts of artificial intelligence<\/a> should be viewed through the lens of economics.<\/p>"},"Tag:PDJ6KqJBRzvKPfuS3":{"_id":"PDJ6KqJBRzvKPfuS3","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:PDJ6KqJBRzvKPfuS3_description"},"userId":"r38pkCm7wF4M44MDQ","name":"Economics","slug":"economics","core":null,"postCount":260,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-14T22:24:48.135Z","wikiOnly":false,"deleted":false},"Revision:4R8JYu4QF2FqzJxE5_description":{"_id":"4R8JYu4QF2FqzJxE5_description","__typename":"Revision","htmlHighlight":"<p><strong>Heuristics <\/strong>and<strong> Biases<\/strong> are the ways human reasoning differs from a theoretical ideal agent, due to reasoning shortcuts that don't always work (heuristics) and systematic errors (biases).<\/p><p><i>See also<\/i>: <a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic<\/a>, <a href=\"https://www.lesswrong.com/tag/confirmation-bias\">Confirmation Bias<\/a>, <a href=\"https://www.lesswrong.com/tag/fallacies\">Fallacies<\/a>, <a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\">Predictably Wrong<\/a>, <a href=\"https://www.lesswrong.com/tag/rationality?showPostCount=true\">Rationality<\/a>, <a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\">Your Intuitions Are Not Magic<\/a>, <a href=\"https://lessestwrong.com/tag/bias\">Bias<\/a>, <a href=\"https://lessestwrong.com/tag/heuristic\">Heuristic<\/a><\/p><h1>Basics<\/h1><p><a href=\"https://www.lesswrong.com/posts/jnZbHi873v9vcpGpZ/what-s-a-bias-again\">“Cognitive biases”<\/a> are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by <i>the shape of our own mental machinery<\/i>. For example, our mental processes might be evolutionarily adapted to specifically believe some things that arent true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.<\/p><p>Our brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a “(cognitive) bias.” Not to be confused with the colloquial “that person is biased,” which just means “that person has a skewed or prejudiced attitude toward something.”<\/p><p>A bias is an obstacle to our goal of obtaining truth, and thus <i>in our way<\/i>.<\/p><p>We are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, we're curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them “biases” or not.<\/p><p><a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\">It's also useful to know the kinds of faults human brains are prone to, in the same way it's useful to know that your car's brakes are a little gummy (so you don't sail through a red light and into an 18-wheeler).<\/a><\/p><p>The Sequence, <a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\">Predictably Wrong<\/a>, offers an excellent introduction to the topic for those who are not familiar.<\/p><h1>Wait a minute... fallacies, biases, heuristics... what's the difference??<\/h1><p>While a <strong>bias<\/strong> is always wrong, a <strong>heuristic <\/strong>is just a shortcut which may or may not give you an accurate answer. Just because you know complex mathematical methods for precisely... <\/p>"},"Tag:4R8JYu4QF2FqzJxE5":{"_id":"4R8JYu4QF2FqzJxE5","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:4R8JYu4QF2FqzJxE5_description"},"userId":"BpBzKEueak7J8vHNi","name":"Heuristics & Biases","slug":"heuristics-and-biases","core":null,"postCount":195,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-13T15:40:30.194Z","wikiOnly":false,"deleted":false},"Revision:vg4LDxjdwHLotCm8w_description":{"_id":"vg4LDxjdwHLotCm8w_description","__typename":"Revision","htmlHighlight":"<p>The <strong>Replication Crisis<\/strong> was the discovery that many fields of so-called science were producing experimental results that could not be replicated, because they were illusions resulting from bad statistical and experimental practices.<\/p><p>The replication crisis began in the early 2010s when several high-profile irreproducible results inspired mass replication attempts, revealing that the majority of papers checked in psychology and a number of other fields were not replicable. Some of the irreproducible results, like <a href=\"https://www.lesswrong.com/tag/priming\">Priming<\/a>, appeared to bear on rationality and were referenced in early LessWrong posts.<\/p><p><strong>External Links:<\/strong><br><a href=\"https://retractionwatch.com/\">Retraction Watch<\/a><br><a href=\"https://www.gwern.net/Replication\">Replication<\/a> on gwern.net<br><a href=\"https://en.wikipedia.org/wiki/Replication_crisis\">Wikipedia<\/a><\/p><p><strong>Related Pages:<\/strong> <a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science\">Practice &amp; Philosophy of Science<\/a>, <a href=\"https://www.lesswrong.com/tag/psychology\">Psychology<\/a>, <a href=\"https://www.lesswrong.com/tag/information-cascades\">Information Cascades<\/a>, <a href=\"https://www.lesswrong.com/tag/falsifiability\">Falsifiability<\/a><\/p>"},"Tag:vg4LDxjdwHLotCm8w":{"_id":"vg4LDxjdwHLotCm8w","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:vg4LDxjdwHLotCm8w_description"},"userId":"nLbwLhBaQeG6tCNDN","name":"Replication Crisis","slug":"replication-crisis","core":null,"postCount":55,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-02T20:55:24.286Z","wikiOnly":false,"deleted":false},"Post:gBpYo7mt2zNBmtBJd":{"_id":"gBpYo7mt2zNBmtBJd","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:gBpYo7mt2zNBmtBJd_"},"fmCrosspost":null,"readTimeMinutes":26,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"},{"__ref":"Tag:PDJ6KqJBRzvKPfuS3"},{"__ref":"Tag:4R8JYu4QF2FqzJxE5"},{"__ref":"Tag:vg4LDxjdwHLotCm8w"}],"url":null,"postedAt":"2021-08-31T18:14:39.075Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2021-08-31T19:00:09.564Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":6,"voteCount":32,"baseScore":95,"extendedScore":null,"unlisted":false,"score":0.0025350444637986876,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2021-11-10T23:05:27.967Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2021-08-31T18:14:39.079Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":null,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"crosspost-on-hreha-on-behavioral-economics","title":"[Crosspost] On Hreha On Behavioral Economics","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:kxW6q5YdTGWh5sWby_":{"_id":"kxW6q5YdTGWh5sWby_","__typename":"Revision","htmlHighlight":"<p><i>[cross-posted from my blog <\/i><a href=\"https://astralcodexten.substack.com/p/eight-hundred-slightly-poisoned-word\"><i>Astral Codex Ten<\/i><\/a><i>]<\/i><\/p><p>In 2012, a Berkeley team found that indoor carbon dioxide had dramatic negative effects on cognition (<a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3548274/\">paper<\/a>, <a href=\"https://alumni.berkeley.edu/california-magazine/summer-2016-welcome-there/your-brain-carbon-dioxide-research-finds-even-low\">popular article<\/a>). Subjects in poorly ventilated environments did up to 50% worse on a test of reasoning and decision-making. This is potentially pretty important, because lots of office buildings (and private houses) count as poorly-ventilated environments, so a lot of decision-making might be happening while severely impaired.<\/p><p>Since then people have debated this on and off, with <a href=\"https://dash.harvard.edu/bitstream/handle/1/27662232/4892924.pdf?sequence=1\">some studies<\/a> confirming the effect and <a href=\"https://onlinelibrary.wiley.com/doi/abs/10.1111/ina.12284\">others<\/a> failing to find it. I personally am skeptical, partly because the effect is so big I would expect someone to have noticed, but also because submarines, spaceships, etc have orders of magnitude more carbon dioxide than any civilian environment, but people still seem to do pretty hard work in them pretty effectively.<\/p><p>As part of my continuing effort to test this theory in my own life, I played a word game eight hundred times under varying ventilation conditions.<\/p><p>…okay, fine, no, I admit it, I played a word game eight hundred times because I’m addicted to it. But since I was playing the word game eight hundred times anyway, I varied the ventilation conditions to see what would happen.<\/p><p>The game was WordTwist, which you can find <a href=\"https://wordtwist.puzzlebaron.com/init5.php\">here<\/a> (warning: potentially addictive). You get a 5x5 square of letters and you have to find as many words as possible (of four letters or more) within three minutes. You can move up, down, right, left, or diagonal, and get more points for harder words. A typical board looks like this:<\/p><figure class=\"image\"><img src=\"https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F1902078a-2b4f-4f8d-8b6c-7a8122700660_298x299.png\"><figcaption>Did you spot “lace”? What about “intrapsychically”?<\/figcaption><\/figure><p>I played this game about 5-10x/day over three months. During this time, the carbon dioxide monitor in my room recorded levels between 445 ppm (with all windows open and the fan on) and 3208 ppm (with all windows closed and several people crammed into the room for several hours). I discounted a stray reading of 285 as an outlier, since this is climatologically impossible (I’m not claiming my monitor is perfectly calibrated, just that it clearly shows higher levels when my room is less well ventilated). CO2 445 is basically the same as outdoors; 3208 is considered extremely poor air quality, likely to cause headaches, nausea, and other minor ailments. The Berkeley study looked at levels between 600 and 2500, so my range was comparable to theirs.... <\/p>","wordCount":1042,"version":"1.1.0"},"Revision:fkABsGCJZ6y9qConW_description":{"_id":"fkABsGCJZ6y9qConW_description","__typename":"Revision","htmlHighlight":"<p><strong>Practical<\/strong> posts give direct, actionable advice on how to achieve goals and generally succeed. The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many places on the internet will give you advice; here, we value survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.<\/p><p>Material that is directly about <i>how to think better<\/i> can be found at <a href=\"https://www.lessestwrong.com/tag/rationality\">Rationality<\/a>.<\/p><p>&nbsp;<\/p><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Practical Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:2px solid hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Domains of Well-being<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\">Careers<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\">Exercise (Physical)<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\">Gratitude<\/a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\">Happiness<\/a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\">Human Bodies<\/a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\">Nutrition<\/a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\">Parenting<\/a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\">Slack<\/a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\">Sleep<\/a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\">Well-being<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Skills, Tools, Techniques<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\">Cryonics<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring<\/a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\">Habits<\/a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\">Hamming Questions<\/a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\">Life Improvements<\/a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\">Meditation<\/a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\">More Dakka<\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u>Pica<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\">Planning &amp; Decision-Making<\/a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\">Self Experimentation<\/a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\">Skill Building<\/a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\">Software Tools<\/a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\">Spaced Repetition<\/a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\">Virtues (Instrumental)<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Productivity<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\">Akrasia<\/a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\">Motivations<\/a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\">Prioritization<\/a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\">Procrastination<\/a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\">Productivity<\/a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\">Willpower<\/a><\/p><\/td><\/tr><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><strong>Interpersonal<\/strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u>Circling<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\">Conversation (topic)<\/a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\">Communication Cultures<\/a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u>Relationship<\/u><\/a><\/td><\/tr><\/tbody><\/table><\/figure>"},"Tag:fkABsGCJZ6y9qConW":{"_id":"fkABsGCJZ6y9qConW","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:fkABsGCJZ6y9qConW_description"},"userId":"oBSWiHjgproTiThmY","name":"Practical","slug":"practical","core":true,"postCount":1827,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":2000,"createdAt":"2020-06-14T06:06:46.947Z","wikiOnly":false,"deleted":false},"Revision:xexCWMyds6QLWognu_description":{"_id":"xexCWMyds6QLWognu_description","__typename":"Revision","htmlHighlight":"<p><strong>World Optimization<\/strong> is the full use of our agency. It is extending the reach of human civilization. It is building cities and democracies and economic systems and computers and flight and science and space rockets and the internet. World optimization is about adding to that list.&nbsp;<br><br>But it’s not just about growth, it’s also about preservation. We are still in the dawn of civilization, with most of civilization in the billions of years ahead. We mustn’t let this light go out.<\/p><hr><h1>World Optimization Sub-Topics<\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Moral Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/altruism?showPostCount=true&amp;useTagName=true\">Altruism<\/a><br><a href=\"https://www.lesswrong.com/tag/consequentialism?showPostCount=true&amp;useTagName=true\">Consequentialism<\/a><br><a href=\"https://www.lesswrong.com/tag/deontology?showPostCount=true&amp;useTagName=true\">Deontology<\/a><br><a href=\"http://www.lesswrong.com/tag/ethics-and-morality?showPostCount=true&amp;useTagName=true\"><u>Ethics &amp; Morality<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/metaethics?showPostCount=true&amp;useTagName=true\">Metaethics<\/a><br><a href=\"http://www.lesswrong.com/tag/moral-uncertainty?showPostCount=true&amp;useTagName=true\"><u>Moral Uncertainty<\/u><\/a><\/p><p>&nbsp;<\/p><p>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\"><p><strong>Causes / Interventions<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/animal-welfare?showPostCount=true&amp;useTagName=true\">Animal Welfare<\/a><br><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\">Existential Risk<\/a><br><a href=\"http://www.lesswrong.com/tag/futurism?showPostCount=true&amp;useTagName=true\">Futurism<\/a><br><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=true&amp;useTagName=true\">Mind Uploading<\/a><br><a href=\"https://www.lesswrong.com/tag/life-extension?showPostCount=true&amp;useTagName=true\">Life Extension<\/a><br><a href=\"http://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&amp;useTagName=false\"><u>S-risks<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/transhumanism?showPostCount=true&amp;useTagName=true\"><u>Transhumanism<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/voting-theory?showPostCount=true&amp;useTagName=true\">Voting Theory<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\"><p><strong>Working with Humans<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><u>Coalitional Instincts<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coordination / Cooperation<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/institution-design?showPostCount=true&amp;useTagName=true\">Institution Design<\/a><br><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/simulacrum-levels?showPostCount=true&amp;useTagName=true\">Simulacrum Levels<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Applied Topics<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/blackmail?showPostCount=true&amp;useTagName=true\">Blackmail<\/a><br><a href=\"http://www.lesswrong.com/tag/censorship?showPostCount=true&amp;useTagName=true\">Censorship<\/a><br><a href=\"http://www.lesswrong.com/tag/chesterton-s-fence?showPostCount=true&amp;useTagName=true\">Chesterton's Fence<\/a><br><a href=\"http://www.lesswrong.com/tag/death?showPostCount=true&amp;useTagName=true\">Death<\/a><br><a href=\"https://www.lesswrong.com/tag/deception?showPostCount=true&amp;useTagName=true\">Deception<\/a><br><a href=\"https://www.lesswrong.com/tag/honesty?showPostCount=true&amp;useTagName=true\">Honesty<\/a><br><a href=\"https://www.lesswrong.com/tag/hypocrisy?showPostCount=true&amp;useTagName=true\">Hypocrisy<\/a><br><a href=\"https://www.lesswrong.com/tag/information-hazards?showPostCount=true&amp;useTagName=true\">Information Hazards<\/a><br><a href=\"https://www.lesswrong.com/tag/meta-honesty?showPostCount=true&amp;useTagName=true\">Meta-Honesty<\/a><br><a href=\"http://www.lesswrong.com/tag/pascal-s-mugging?showPostCount=true&amp;useTagName=true\">Pascal's Mugging<\/a><br><a href=\"http://www.lesswrong.com/tag/war?showPostCount=true&amp;useTagName=true\">War<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Value &amp; Virtue<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/ambition?showPostCount=true&amp;useTagName=true\">Ambition<\/a><br><a href=\"https://www.lesswrong.com/tag/art?showPostCount=true&amp;useTagName=true\">Art<\/a><br><a href=\"https://www.lesswrong.com/tag/aesthetics?showPostCount=true&amp;useTagName=true\">Aesthetics<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"http://www.lesswrong.com/tag/courage?showPostCount=true&amp;useTagName=true\">Courage<\/a><br><a href=\"http://www.lesswrong.com/tag/fun-theory?showPostCount=true&amp;useTagName=true\">Fun Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/principles?showPostCount=true&amp;useTagName=true\">Principles<\/a><br><a href=\"http://www.lesswrong.com/tag/suffering?showPostCount=true&amp;useTagName=true\"><u>Suffering<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/superstimuli?showPostCount=true&amp;useTagName=true\">Superstimuli<\/a><br><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\">Wireheading<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Meta<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cause-prioritization?showPostCount=true&amp;useTagName=true\">Cause Prioritization<\/a><br><a href=\"http://www.lesswrong.com/tag/center-on-long-term-risk-clr?useTagName=true&amp;showPostCount=true\">Center for Long-term Risk<\/a><br><a href=\"https://www.lesswrong.com/tag/effective-altruism?showPostCount=true&amp;useTagName=true\">Effective Altruism<\/a><br><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\">Heroic Responsibility<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><hr><p>Content which describes <i>how the world is <\/i>that directly bears upon choices one makes to optimize the world fall under this tag. Examples include discussion of the moral patienthood of different animals, the potential of human civilization, and the most effective interventions against a global health threat.<\/p><p>Some material has both immediate relevance to world optimization decisions but also can inform broader world models. This material might be included under both <a href=\"https://www.lesswrong.com/tag/world-modeling\">World Modeling<\/a> tag and this tag.<\/p>"},"Tag:xexCWMyds6QLWognu":{"_id":"xexCWMyds6QLWognu","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:xexCWMyds6QLWognu_description"},"userId":"XtphY3uYHwruKqDyG","name":"World Optimization","slug":"world-optimization","core":true,"postCount":1672,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":20,"createdAt":"2020-06-14T03:38:23.532Z","wikiOnly":false,"deleted":false},"Revision:dNNwmxdmvtxMGMFgX_description":{"_id":"dNNwmxdmvtxMGMFgX_description","__typename":"Revision","htmlHighlight":"<p>Poor <strong>Air Quality<\/strong> can reduce cognitive functioning<sup class=\"footnote-ref\"><a href=\"#fn-uWmzz52bAWHLsBzHd-1\" id=\"fnref-uWmzz52bAWHLsBzHd-1\">[1]<\/a><\/sup>, lifespans<sup class=\"footnote-ref\"><a href=\"#fn-uWmzz52bAWHLsBzHd-2\" id=\"fnref-uWmzz52bAWHLsBzHd-2\">[2]<\/a><\/sup> and the techniques to improve air quality are also useful for getting rid of aerosolized respiratory pathogens. Improving air quality can be an impactful global health intervention.<sup class=\"footnote-ref\"><a href=\"#fn-uWmzz52bAWHLsBzHd-3\" id=\"fnref-uWmzz52bAWHLsBzHd-3\">[3]<\/a><\/sup> Many members of the LessWrong community have also put effort into improving the air quality of their own homes or offices, as an implication of instrumental rationality.<\/p><p>Newer Green buildings are infamous among those who care about this topic for being excellently sealed, meaning that they have less interchange with outside air. This is good for energy efficiency, but bad for indoor air quality.<\/p>\n<h2>The Carbon Dioxide Debate<\/h2>\n<p>Mostly when people talk about air quality, they're talking about particulates and Volatile Organic Compounds (VOCs). However, some studies have tried to look at <a href=\"https://www.lesswrong.com/posts/pPZ27eZdBXtGuLqZC/what-is-up-with-carbon-dioxide-and-cognition-an-offer\">carbon dioxide alone<\/a>, and have found large effects on cognition. It is this wiki author's belief that better studies have failed to find anything close to the size of the original effect, if anything.<sup class=\"footnote-ref\"><a href=\"#fn-uWmzz52bAWHLsBzHd-2\" id=\"fnref-uWmzz52bAWHLsBzHd-2:1\">[2:1]<\/a><\/sup><sup class=\"footnote-ref\"><a href=\"#fn-uWmzz52bAWHLsBzHd-4\" id=\"fnref-uWmzz52bAWHLsBzHd-4\">[4]<\/a><\/sup><\/p>\n<h2>External Links<\/h2>\n<ul>\n<li><a href=\"https://forum.effectivealtruism.org/tag/air-pollution\">Air Pollution<\/a> on the Effective Altruism Forum<\/li>\n<li><a href=\"https://patrickcollison.com/pollution\">Collection of studies<\/a> by Patrick Collison on air pollution and cognition<\/li>\n<\/ul>\n<hr class=\"footnotes-sep\">\n<section class=\"footnotes\">\n<ol class=\"footnotes-list\">\n<li id=\"fn-uWmzz52bAWHLsBzHd-1\" class=\"footnote-item\"><p><a href=\"https://www.iza.org/publications/dp/12632/indoor-air-quality-and-cognitive-performance\">Künn et. al<\/a> <a href=\"#fnref-uWmzz52bAWHLsBzHd-1\" class=\"footnote-backref\">↩︎<\/a><\/p>\n<\/li>\n<li id=\"fn-uWmzz52bAWHLsBzHd-2\" class=\"footnote-item\"><p><a href=\"https://www.nature.com/articles/s41598-021-01802-5\">Juginovic et. al<\/a> <a href=\"#fnref-uWmzz52bAWHLsBzHd-2\" class=\"footnote-backref\">↩︎<\/a> <a href=\"#fnref-uWmzz52bAWHLsBzHd-2:1\" class=\"footnote-backref\">↩︎<\/a><\/p>\n<\/li>\n<li id=\"fn-uWmzz52bAWHLsBzHd-3\" class=\"footnote-item\"><p><a href=\"https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/#south-asian-air-quality-021112\">Alexander Berger on the 80,000 Hours Podcast<\/a> (link goes to transcript or audio) <a href=\"#fnref-uWmzz52bAWHLsBzHd-3\" class=\"footnote-backref\">↩︎<\/a><\/p>\n<\/li>\n<li id=\"fn-uWmzz52bAWHLsBzHd-4\" class=\"footnote-item\"><p><a href=\"https://www.lesswrong.com/posts/kxW6q5YdTGWh5sWby/eight-hundred-slightly-poisoned-word-games\">Eight Hundred Slight Poisoned Word Games<\/a> by Scott Alexander <a href=\"#fnref-uWmzz52bAWHLsBzHd-4\" class=\"footnote-backref\">↩︎<\/a><\/p>\n<\/li>\n<\/ol>\n<\/section>"},"Tag:dNNwmxdmvtxMGMFgX":{"_id":"dNNwmxdmvtxMGMFgX","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:dNNwmxdmvtxMGMFgX_description"},"userId":"ezbRa3dntKWQ5995r","name":"Air Quality","slug":"air-quality","core":false,"postCount":14,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-05-17T09:57:18.577Z","wikiOnly":false,"deleted":false},"Post:kxW6q5YdTGWh5sWby":{"_id":"kxW6q5YdTGWh5sWby","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:kxW6q5YdTGWh5sWby_"},"fmCrosspost":null,"readTimeMinutes":4,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fkABsGCJZ6y9qConW"},{"__ref":"Tag:xexCWMyds6QLWognu"},{"__ref":"Tag:dNNwmxdmvtxMGMFgX"}],"url":null,"postedAt":"2021-08-09T20:17:17.814Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2021-08-09T20:37:39.890Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":5,"voteCount":57,"baseScore":108,"extendedScore":null,"unlisted":false,"score":0.002604,"lastVisitedAt":"2021-08-18T18:31:12.965Z","isFuture":false,"isRead":true,"lastCommentedAt":"2021-08-21T13:17:00.467Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":20,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2021-08-09T20:17:17.817Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":null,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"eight-hundred-slightly-poisoned-word-games","title":"Eight Hundred Slightly Poisoned Word Games","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:wZGpoZgDANdkwTrwt_":{"_id":"wZGpoZgDANdkwTrwt_","__typename":"Revision","htmlHighlight":"<p>(crossposted from <a href=\"https://astralcodexten.substack.com/p/towards-a-bayesian-theory-of-willpower\">Astral Codex Ten<\/a>)<\/p><p><strong>I.<\/strong><\/p><p>What is willpower?<\/p><p>Five years ago, I reviewed <a href=\"https://slatestarcodex.com/2015/03/12/book-review-willpower/\">Baumeister and Tierney's book<\/a> on the subject. They tentatively concluded it's a way of rationing brain glucose. But their key results have <a href=\"https://www.discovermagazine.com/mind/the-end-of-ego-depletion-theory\">failed to replicate<\/a>, and people who know more about glucose physiology say it <a href=\"http://www.robkurzban.com/blog/2014/1/16/no-sugar-coating-problems-for-the-glucose-model\">makes no theoretical sense<\/a>.<\/p><p>Robert Kurzban, one of the most on-point critics of the glucose theory, gives his own model of willpower: it's a way of <a href=\"https://repository.upenn.edu/cgi/viewcontent.cgi?referer=https://duckduckgo.com/&amp;httpsredir=1&amp;article=1020&amp;context=psychology_papers\">minimizing opportunity costs<\/a>. But how come my brain is convinced that playing Civilization for ten hours has no opportunity cost, but spending five seconds putting away dishes has such immense opportunity costs that it will probably leave me permanently destitute? I can't find any correlation between the subjective phenomenon of willpower or effort-needingness and real opportunity costs at all.<\/p><p>A tradition originating in psychotherapy, and ably represented eg <a href=\"https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip/p/M4w2rdYgCKctbADMn\">here by Kaj Sotala<\/a>, interprets willpower as conflict between mental agents. One \"subagent\" might want to sit down and study for a test. But maybe one subagent represents the pressure your parents are putting on you to do well in school so you can become a doctor and have a stable career, and another subagent represents your own desire to drop out and become a musician, and even though the \"do well in school\" subagent is on top now, the \"become a musician\" subagent is strong enough to sabotage you by making you feel mysteriously unable to study. This usually ends with something about how enough therapy can help you reconcile these subagents and have lots of willpower again. But this works a lot better in <a href=\"https://slatestarcodex.com/2019/11/20/book-review-all-therapy-books/\">therapy books<\/a> than it does in real life. Also, what childhood trauma made my subagents so averse to doing dishes?<\/p><p>I've come to disagree with all of these perspectives. I think willpower is best thought of as a Bayesian process, ie an attempt to add up different kinds of evidence.<\/p><p><strong>II.<\/strong><\/p><p>My model has several different competing mental processes trying to determine your actions. One is a prior on motionlessness; if you have no reason at all to do anything, stay where you are. A second is a pure reinforcement learner - \"do whatever has brought you the most reward in the past\". And the third is your high-level conscious calculations about what the right thing to do is.<\/p><p>These all submit \"evidence\" to your basal ganglia, the brain structure that chooses actions. Using the same evi... <\/p>","wordCount":1798,"version":"1.0.0"},"Revision:YrLoz567b553YouZ2_description":{"_id":"YrLoz567b553YouZ2_description","__typename":"Revision","htmlHighlight":"<p><strong>Willpower<\/strong> is the ability to overcome urges to do or not some activity– to overcome temptation. Typically there is a sense of coercing oneself to do things despite inner resistance.<\/p>\n<p>Willpower is of interest those who wish to increase their productivity or otherwise do more thing that they wish to be done. The question then is \"how does one increase willpower?\"<\/p>\n<p>There is an argument that the use of willpower is undesirable. The use of willpower my constitute a form of <em>inner violence<\/em> which is in tension with <em>inner<\/em> <em>alignment<\/em> of <a href=\"https://www.lessestwrong.com/tag/subagents\">one's parts<\/a>– a better path to productivity and wellbeing.<\/p>\n<p><strong>Related:<\/strong> <a href=\"https://www.lessestwrong.com/tag/akrasia\">Akrasia<\/a><\/p>\n<h2>Resources<\/h2>\n<ul>\n<li>The writings on <a href=\"http://mindingourway.com/\">Minding Our Way<\/a> concerning productivity.<\/li>\n<\/ul>"},"Tag:YrLoz567b553YouZ2":{"_id":"YrLoz567b553YouZ2","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:YrLoz567b553YouZ2_description"},"userId":"XchweonPm2TC7EJES","name":"Willpower","slug":"willpower","core":null,"postCount":30,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-01T08:21:48.698Z","wikiOnly":false,"deleted":false},"Revision:YAotJ9Le3S2rCJgf8_description":{"_id":"YAotJ9Le3S2rCJgf8_description","__typename":"Revision","htmlHighlight":"<p>(From <a href=\"https://en.wikipedia.org/wiki/Predictive_coding\">Wikipedia<\/a>) <strong>predictive processing<\/strong> is a theory of brain function in which the brain is constantly generating and updating a mental model of the environment. The model is used to generate predictions of sensory input that are compared to actual sensory input. This comparison results in prediction errors that are then used to update and revise the mental model.<\/p><p><strong>External Links:<\/strong><br><a href=\"https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/\">Book Review: Surfing Uncertainty<\/a> - Introduction to predictive processing by Scott Alexander<br><a href=\"https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/\">Predictive Processing And Perceptual Control<\/a> by Scott Alexander<\/p><p><strong>Related Pages: <\/strong><a href=\"https://www.lesswrong.com/tag/perceptual-control-theory\">Perceptual Control Theory<\/a>, <a href=\"https://www.lesswrong.com/tag/neuroscience\">Neuroscience<\/a><\/p>"},"Tag:YAotJ9Le3S2rCJgf8":{"_id":"YAotJ9Le3S2rCJgf8","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:YAotJ9Le3S2rCJgf8_description"},"userId":"qxJ28GN72aiJu96iF","name":"Predictive Processing","slug":"predictive-processing","core":false,"postCount":28,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-09T09:52:22.567Z","wikiOnly":false,"deleted":false},"Post:wZGpoZgDANdkwTrwt":{"_id":"wZGpoZgDANdkwTrwt","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:wZGpoZgDANdkwTrwt_"},"fmCrosspost":null,"readTimeMinutes":7,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:YrLoz567b553YouZ2"},{"__ref":"Tag:YAotJ9Le3S2rCJgf8"}],"url":null,"postedAt":"2021-03-26T02:33:55.056Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2021-03-26T02:42:37.435Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":28,"voteCount":44,"baseScore":100,"extendedScore":null,"unlisted":false,"score":0.001839,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2021-03-29T22:50:16.069Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":21,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2021-03-26T02:33:55.056Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":null,"positiveReviewVoteCount":0,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":null,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"toward-a-bayesian-theory-of-willpower","title":"Toward A Bayesian Theory Of Willpower","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:hNqte2p48nqKux3wS_":{"_id":"hNqte2p48nqKux3wS_","__typename":"Revision","htmlHighlight":"<p><i>Crossposted from <\/i><a href=\"http://astralcodexten.substack.com\"><i>Astral Codex Ten<\/i><\/a><\/p><p><strong>Introduction and review<\/strong><\/p><p>Last month I talked about<a href=\"https://osf.io/8ev4u/\"> <\/a>van der Bergh et al’s work on <a href=\"https://astralcodexten.substack.com/p/the-precision-of-sensory-evidence\">the precision of sensory evidence<\/a>, which introduced the idea of a <i>trapped prior<\/i>. I think this concept has far-reaching implications for the rationalist project as a whole. I want to re-derive it, explain it more intuitively, then talk about why it might be relevant for things like intellectual, political and religious biases.<\/p><p>To review: the brain combines raw experience (eg sensations, memories) with context (eg priors, expectations, other related sensations and memories) to produce perceptions. You don’t notice this process; you are only able to consciously register the final perception, which feels exactly like raw experience.<\/p><figure class=\"image\"><img src=\"https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8035d190-aad0-4303-be08-396481371c20_500x497.png\"><\/figure><p><i>A typical optical illusion. The top chess set and the bottom chess set are the same color (grayish). But the top appears white and the bottom black because of the context (darker vs. lighter background). You perceive not the raw experience (grayish color) but the final perception modulated by context; to your conscious mind, it just seems like a brute fact that the top is white and the bottom black, and it is hard to convince yourself otherwise.<\/i><\/p><p>Or: maybe you feel like you are using a particular context independent channel (eg hearing). Unbeknownst to you, the information in that channel is being context-modulated by the inputs of a different channel (eg vision). You don’t feel like “this is what I’m hearing, but my vision tells me differently, so I’ll compromise”. You feel like “this is exactly what I heard, with my ears, in a way vision didn’t affect at all”.<\/p><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=2k8fHR9jKVM\"><div><iframe src=\"https://www.youtube.com/embed/2k8fHR9jKVM\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"><\/iframe><\/div><\/div><\/figure><p><i>This is called the McGurk Effect. The man is saying the same syllable each time, but depending on what picture of his mouth moving you see, you hear it differently. Your vision is context-modulating your hearing, but it just sounds like hearing something.<\/i><\/p><p>The most basic illusion I know of is the Wine Illusion; dye a white wine red, and lots of people will say it tastes like red wine. The raw experience - the taste of the wine itself - is that of a white wine. But the context is that you're drinking a red liquid. Result: it tastes like a red wine.<\/p><figure class=\"image image_resized\" style=\"width:60.2%\"><img src=\"https://cdn.substack.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F458e226f-0f24-4814-bfa0-fa05e7fa2af4_259x217.png\"><\/figure><p>The placebo effect is almost equally simple. You're in pain, so your doctor gives you a “painkiller” (unbeknownst to you, it’s really a sugar pill). The raw experience is the nerve sending out just as many pain impulses as before.... <\/p>","wordCount":4314,"version":"1.3.0"},"Revision:5hpGj9nDLgokfghvR_description":{"_id":"5hpGj9nDLgokfghvR_description","__typename":"Revision","htmlHighlight":"<p><strong>Confirmation bias<\/strong> (also known as positive bias) is the tendency to search for, interpret, favor, and recall information in a way that confirms or strengthens one&apos;s prior personal beliefs or hypotheses [<a href=\"https://en.wikipedia.org/wiki/Confirmation_bias\">1<\/a>]. &#xA0;For example, one might test hypotheses with positive rather than negative examples, thus missing obvious disconfirming tests.<\/p><p><em>See also: <\/em><a href=\"https://www.lesswrong.com/tag/motivated-skepticism\">Motivated skepticism<\/a>, <a href=\"https://lesswrong.com/tag/availability-heuristic\">Availability heuristic<\/a>, <a href=\"https://www.lesswrong.com/tag/surprise\">Surprise<\/a>, <a href=\"https://www.lesswrong.com/tag/narrative-fallacy\">Narrative fallacy<\/a>, <a href=\"https://www.lesswrong.com/tag/privileging-the-hypothesis\">Privileging the hypothesis<\/a>, <a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\">Heuristics and Biases<\/a><\/p><h2>External Links<\/h2><ul><li><a href=\"https://www.edge.org/conversation/kevin_kelly-speculations-on-the-future-of-science\">Speculations on the Future of Science <\/a>by Kevin Kelly<\/li><li><a href=\"http://psy2.ucsd.edu/~mckenzie/Wason1960QJEP.pdf\">On the Failure to Eliminate Hypotheses in a Conceptual Task<\/a> by P.C. Wason<\/li><li><a href=\"https://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\">Write Your Hypothetical Apostasy<\/a> by <a href=\"https://www.lesswrong.com/tag/nick-bostrom\">Nick Bostrom<\/a><\/li><li><a href=\"https://en.wikipedia.org/wiki/Confirmation_bias\">Confirmation Bias<\/a>, Wikipedia<\/li><\/ul>"},"Tag:5hpGj9nDLgokfghvR":{"_id":"5hpGj9nDLgokfghvR","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:5hpGj9nDLgokfghvR_description"},"userId":"nLbwLhBaQeG6tCNDN","name":"Confirmation Bias","slug":"confirmation-bias","core":null,"postCount":26,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-02-20T02:56:23.333Z","wikiOnly":false,"deleted":false},"Revision:5f5c37ee1b5cdee568cfb108_description":{"_id":"5f5c37ee1b5cdee568cfb108_description","__typename":"Revision","htmlHighlight":"<p>In the context of <a href=\"https://wiki.lesswrong.com/wiki/Bayes's_Theorem\">Bayes's Theorem<\/a>, <strong>priors<\/strong> refer generically to the beliefs an agent holds regarding a fact, hypothesis or consequence, before being presented with evidence. Upon being presented with new evidence, the agent can multiply their prior with a <a href=\"https://wiki.lesswrong.com/wiki/likelihood_distribution\">likelihood distribution<\/a> to calculate a new (posterior) probability for their belief.<\/p><h2>Examples<\/h2><p>Suppose you had a barrel containing some number of red and white balls. You start with the belief that each ball was independently assigned red color (vs. white color) at some fixed probability. Furthermore, you start out ignorant of this fixed probability (the parameter could be anywhere between 0 and 1). Each red ball you see then makes it <i>more<\/i> likely that the next ball will be red, following a <a href=\"http://en.wikipedia.org/wiki/Rule_of_succession\">Laplacian Rule of Succession<\/a>. For example, seeing 6 red balls out of 10 suggests that the initial probability used for assigning the balls a red color was .6, and that there's also a probability of .6 for the next ball being red.<\/p><p>On the other hand, if you start out with the prior belief that the barrel contains exactly 10 red balls and 10 white balls, then each red ball you see makes it <i>less<\/i> likely that the next ball will be red (because there are fewer red balls remaining).<\/p><p>Thus our prior affects how we interpret the evidence. The first prior is an inductive prior - things that happened before are predicted to happen again with greater probability. The second prior is anti-inductive - the more red balls we see, the fewer we expect to see in the future.<\/p><p>As a real life example, consider two leaders from different political parties. Each one has his own beliefs - priors - about social organization and the roles of people and government in society. These differences in priors can be attributed to a wide range of factors, ranging from their educational backgrounds to hereditary differences in personality. However, neither can show that his beliefs are better than those of the other, unless he can show that his priors were generated by sources which track reality better<a href=\"#fn1\"><sup>1<\/sup><\/a>.<\/p><p>Because carrying out any reasoning at all seems to require a prior of some kind, ideal Bayesians would need some sort of priors from the moment that they were born. The question of where an ideal Bayesian would get this prior from has occasionally been a matter of considerable controversy in the philosophy of probability.<\/p><h2>Updating prior probabilities<\/h2><p>In informal discus... <\/p>"},"Tag:5f5c37ee1b5cdee568cfb108":{"_id":"5f5c37ee1b5cdee568cfb108","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:5f5c37ee1b5cdee568cfb108_description"},"userId":"e8voDq2aJLGcdWuw6","name":"Priors","slug":"priors","core":null,"postCount":20,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:51.862Z","wikiOnly":false,"deleted":false},"Revision:EuDw6uxQW2ZBRFhMo_description":{"_id":"EuDw6uxQW2ZBRFhMo_description","__typename":"Revision","htmlHighlight":"<p>CFAR's <a href=\"https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all\">2019 workshop participant handbook<\/a> defines an <strong>aversion<\/strong> as<\/p><blockquote><p>any sort of mental mechanism that causes us to be less likely to engage in a particular activity, or to do so only with pain, displeasure, or discomfort. Aversions can be conscious or unconscious, reasoned or felt, verbal or visceral, and they can range anywhere from a slight tinge of antipathy to outright phobias.<\/p><\/blockquote><p><a href=\"https://acritch.com/aversions/\"><strong>Aversion factoring<\/strong><\/a> is <a href=\"https://www.lesswrong.com/tag/goal-factoring\">goal factoring<\/a> for aversions: trying to understand why you're averse to a particular thing by decomposing it into underlying preferences and experiences.<\/p><p>&nbsp;<\/p><h2>Ugh fields<\/h2><p>Aversions make us less likely to engage in \"activities,\" and as Scott Alexander's (2011) <a href=\"https://www.lesswrong.com/posts/5dhWhjfxn4tPfFQdi/physical-and-mental-behavior\">Physical and Mental Behavior<\/a> notes, this includes mental as well as physical activities. Pavlovian conditioning can cause humans to unconsciously flinch from even thinking about a serious personal problem they have.<\/p><p>Writing in 2010, Roko introduced the term <a href=\"https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields\"><strong>ugh field<\/strong><\/a> to refer to this problem:&nbsp;<\/p><blockquote><p>If you fail or are punished sufficiently many times in some problem area, and acting in that area is always [preceded] by thinking about it, your brain will propagate the psychological pain right back to the moment you first begin to entertain a thought about the problem, and hence cut your conscious optimizing ability right out of the loop.<\/p><p>[...] The subtlety with the Ugh Field is that the flinch occurs <i><strong>before you start to consciously think<\/strong><\/i> about how to deal with the Unhappy Thing, meaning that you never deal with it, and you don't even have the option of dealing with it in the normal run of things. I find it frightening that my lizard brain could implicitly be making life decisions for me, without even asking my permission!<\/p><\/blockquote><p>The ugh field forms a self-shadowing blind spot covering an area desperately in need of optimization.<\/p><p>&nbsp;<\/p><h2>Blog posts and external links<\/h2><ul><li><a href=\"https://www.lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/\">Avoiding Your Belief's Real Weak Points<\/a><\/li><li><a href=\"https://www.lesswrong.com/lw/2cv/defeating_ugh_fields_in_practice/\">Defeating Ugh Fields in Practice<\/a> by Psychohistorian<\/li><li><a href=\"https://www.lesswrong.com/lw/5a9/learned_blankness/\">Learned Blankness<\/a> by <a href=\"https://www.lesswrong.com/tag/anna-salamon\">Anna Salamon<\/a><\/li><li><a href=\"https://www.lesswrong.com/lw/4up/dont_fear_failure/\">Don't Fear Failure<\/a> by <a href=\"http://shugyoshayear.com/\">atucker<\/a><\/li><li>Wikipedia: <a href=\"https://en.wikipedia.org/wiki/Experiential_avoidance\">Experiential Avoidance<\/a><\/li><\/ul><p>&nbsp;<\/p><h2>Related pages<\/h2><ul><li>Non-tags: <a href=\"https://wiki.lesswrong.com/wiki/Curiosity_stopper\">Semantic stopsign<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/akrasia\">Akrasia<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/compartmentalization\">Compartmentalization<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/motivated-reasoning\">Motivated reasoning<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/motivations\">Motivations<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/priming\">Priming<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/trivial-inconvenience\">Trivial inconvenience<\/a><\/li><\/ul>"},"Tag:EuDw6uxQW2ZBRFhMo":{"_id":"EuDw6uxQW2ZBRFhMo","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:EuDw6uxQW2ZBRFhMo_description"},"userId":"SsduPgHwY2zeZpmKT","name":"Aversion","slug":"aversion","core":false,"postCount":19,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-20T04:12:21.069Z","wikiOnly":false,"deleted":false},"Revision:Wi3EopKJ2aNdtxSWg_description":{"_id":"Wi3EopKJ2aNdtxSWg_description","__typename":"Revision","htmlHighlight":"<p><strong>Neuroscience<\/strong> is a field of study dealing with the structure or function of the brain. It is of particular interest to LessWrong both because it can shed light on <a href=\"https://www.lesswrong.com/tag/ai?showPostCount=true\">AI<\/a>, and because it is often helpful for <a href=\"https://www.lesswrong.com/tag/rationality?showPostCount=true\">rationality<\/a>. For example, understanding how <a href=\"https://www.lesswrong.com/posts/rD57ysqawarsbry6v?lw_source=posts_sheet\">attentional control<\/a> works can inform possible solutions.<\/p>"},"Tag:Wi3EopKJ2aNdtxSWg":{"_id":"Wi3EopKJ2aNdtxSWg","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:Wi3EopKJ2aNdtxSWg_description"},"userId":"qxJ28GN72aiJu96iF","name":"Neuroscience","slug":"neuroscience","core":false,"postCount":128,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-09T09:57:06.243Z","wikiOnly":false,"deleted":false},"Revision:zQw5d37qwzdpgQs5P_description":{"_id":"zQw5d37qwzdpgQs5P_description","__typename":"Revision","htmlHighlight":"<p><strong>Cognitive science<\/strong> draws upon a variety of different disciplines to try to describe and explain the way humans think. It heavily involves <a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">neuroscience<\/a>, <a href=\"https://www.lesswrong.com/tag/psychology\">psychology<\/a>, and philosophy. It differs from neuroscience in that it focuses less on relating structure to function, and more on using many approaches to form higher-level models to predict behaviour.<\/p>"},"Tag:zQw5d37qwzdpgQs5P":{"_id":"zQw5d37qwzdpgQs5P","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:zQw5d37qwzdpgQs5P_description"},"userId":"qgdGA4ZEyW7zNdK84","name":"Cognitive Science","slug":"cognitive-science","core":false,"postCount":30,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-27T19:59:06.910Z","wikiOnly":false,"deleted":false},"Post:hNqte2p48nqKux3wS":{"_id":"hNqte2p48nqKux3wS","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:hNqte2p48nqKux3wS_"},"fmCrosspost":null,"readTimeMinutes":17,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:5hpGj9nDLgokfghvR"},{"__ref":"Tag:5f5c37ee1b5cdee568cfb108"},{"__ref":"Tag:EuDw6uxQW2ZBRFhMo"},{"__ref":"Tag:Wi3EopKJ2aNdtxSWg"},{"__ref":"Tag:zQw5d37qwzdpgQs5P"}],"url":null,"postedAt":"2021-03-12T20:02:28.639Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2021-03-12T20:12:44.633Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":29,"voteCount":60,"baseScore":123,"extendedScore":null,"unlisted":false,"score":0.002233,"lastVisitedAt":"2021-03-13T19:35:00.007Z","isFuture":false,"isRead":true,"lastCommentedAt":"2022-01-19T05:37:08.544Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2021-03-19T02:15:44.249Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":["EQNTWXLKMeWMp2FQS"],"suggestForCuratedUsernames":"Ben Pace","reviewForCuratedUserId":"r38pkCm7wF4M44MDQ","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":21,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2021-03-12T20:02:28.639Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":3,"positiveReviewVoteCount":3,"reviewVoteScoreAllKarma":5.5,"reviewVotesAllKarma":[4.5,1],"reviewVoteScoreHighKarma":4.5,"reviewVotesHighKarma":[4.5],"reviewVoteScoreAF":4.5,"reviewVotesAF":[4.5],"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":null,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"trapped-priors-as-a-basic-problem-of-rationality","title":"Trapped Priors As A Basic Problem Of Rationality","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:GZSzMqr8hAB2dR8pk_":{"_id":"GZSzMqr8hAB2dR8pk_","__typename":"Revision","htmlHighlight":"<html><head><\/head><body><h1><strong>I.<\/strong><\/h1><p>Imagine a distant planet full of eyeless animals. Evolving eyes is hard: they need to evolve Eye Part 1, then Eye Part 2, then Eye Part 3, in that order. Each of these requires a separate series of rare mutations.<\/p><p>Here on Earth, scientists believe each of these mutations must have had <a href=\"https://books.google.com/books?id=1bUoIpTQbLYC&amp;pg=PA153&amp;lpg=PA153&amp;dq=evolve+half+an+eye&amp;source=bl&amp;ots=dp1DlhtdUO&amp;sig=ACfU3U09G7Jq2xitoELZglQykIGJBzBBDw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwjsi5HngbDpAhUuGDQIHRRjCv44ChDoATAHegQICBAB#v=onepage&amp;q=evolve%20half%20an%20eye&amp;f=false\">its own benefits<\/a> – in the land of the blind, the man with only Eye Part 1 is king. But on this hypothetical alien planet, there is no such luck. You need all three Eye Parts or they’re useless. Worse, each Eye Part is metabolically costly; the animal needs to eat 1% more food per Eye Part it has. An animal with a full eye would be much more fit than anything else around, but an animal with only one or two Eye Parts will be at a small disadvantage.<\/p><p>So these animals will only evolve eyes in conditions of relatively weak evolutionary pressure. In a world of intense and perfect competition, where the fittest animal always survives to reproduce and the least fit always dies, the animal with Eye Part 1 will always die – it’s less fit than its fully-eyeless peers. The weaker the competition, and the more randomness dominates over survival-of-the-fittest, the more likely an animal with Eye Part 1 can survive and reproduce long enough to eventually produce a descendant with Eye Part 2, and so on.<\/p><p>There are lots of ways to decrease evolutionary pressure. Maybe natural disasters often decimate the population, dozens of generations are spend recolonizing empty land, and during this period there’s more than enough for everyone and nobody has to compete. Maybe there are frequent <a href=\"https://en.wikipedia.org/wiki/Whale_fall\">whalefalls<\/a>, and any animal nearby has hit the evolutionary jackpot and will have thousands of descendants. Maybe the population is isolated in little islands and mountain valleys, and one gene or another can reach fixation in a population totally by chance. It doesn’t matter exactly how it happens, it matters that evolutionary pressure is low.<\/p><p>The branch of evolutionary science that deals with this kind of situation is called “adaptive fitness landscapes”. Landscapes really are a great metaphor – consider somewhere like this:<\/p><figure style=\"width:100%;\"><img src=\"http://slatestarcodex.com/blog_images/landscape.png\"><\/figure><p>You pour out a bucket of water. Water “flows downhill”, so it’s tempting to say something like “water wants to be at the lowest point possible”. But that’s not quite right. The lowest point possible is the pit, and water won’t go there. It will just sit in the little puddle forever, because it woul<\/p><\/body><\/html>...","wordCount":7246,"version":"1.1.0"},"Revision:nZCb9BSnmXZXSNA2u_description":{"_id":"nZCb9BSnmXZXSNA2u_description","__typename":"Revision","htmlHighlight":"<p><strong>Evolution<\/strong> is \"<i>change in the heritable characteristics of biological populations over successive generations<\/i>\" (<a href=\"https://en.wikipedia.org/wiki/Evolution\">Wikipedia<\/a>). For posts about machine learning look <a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=false&amp;useTagName=false\">here<\/a>.<\/p><p><i>Related: <\/i><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology<\/a>, <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology<\/a>,<\/p><p>The sequence, <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8\">The Simple Math of Evolution<\/a> provides a good introduction to LessWrong thinking about evolution.<\/p><h1>Why be interested in evolution?<\/h1><p>Firstly, evolution is a useful case study of humans' ability (or inability) to model the real world. This is because it has a single clear criterion (\"relative reproductive fitness\") which is selected (optimized) for:<\/p><blockquote><p><i>\"If we can't see clearly the result of a single monotone optimization criterion—if we can't even train ourselves to hear a single pure note—then how will we listen to an orchestra? How will we see that \"Always be selfish\" or \"Always obey the government\" are poor guiding principles for human beings to adopt—if we think that even optimizing genes for inclusive fitness will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?<\/i><\/p><\/blockquote><blockquote><p><i>To train ourselves to see clearly, we need simple practice cases\" -- <\/i>Eliezer Yudkowsky<i>, <\/i><a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/i6fKszWY6gLZSX2Ey\"><i>Fake Optimisation Criteria<\/i><\/a><\/p><\/blockquote><p>Secondly, much of rationality necessarily revolves around the human brain (<a href=\"https://www.lesswrong.com/tag/transhumanism?usePostCount=false&amp;useTagName=false\">for<\/a> <a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=false&amp;useTagName=false\">now<\/a>). An understanding of how it came into being can be very helpful both for understanding 'bugs' in the system (like superstimuli), and for explaining <a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a>, among others.<\/p><blockquote><p><i>A candy bar is a superstimulus: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.&nbsp; &nbsp;A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment.&nbsp; The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs.&nbsp;<\/i><br><i>-- <\/i>Eliezer Yudkowsky, <a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/Jq73GozjsuhdwMLEG\">Superstimuli and the Collapse of Western Civilisation<\/a><\/p><\/blockquote><h2>See also<\/h2><ul><li><a href=\"https://lessestwrong.com/tag/evolution-as-alien-god\">Evolution as alien god<\/a><\/li><li><a href=\"https://lessestwrong.com/tag/slowness-of-evolution\">Slowness of evolution<\/a><\/li><li><a href=\"https://lessestwrong.com/tag/stupidity-of-evolution\">Stupidity of evolution<\/a><\/li><li><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\">Evolutionary psychology<\/a><\/li><\/ul><h2>External links<\/h2><ul><li><a href=\"http://dl.dropbox.com/u/33627365/Scholarship/Selfish%20Gene%20-%20Dawkins.pdf\">Richard Dawkins - The Selfish Gene<\/a> (PDF)<\/li><\/ul><h2>Summaries of Sequence's Posts on Evolution<\/h2><p><i>The following are summaries of posts concerning evolution in the Eliezer's sequences:<\/i><\/p><ul><li><a href=\"https://lessestwrong.com/lw/kr/an_alien_god/\">An Alien God<\/a> - Evolution is awesomely powerful, unb<\/li><\/ul>..."},"Tag:nZCb9BSnmXZXSNA2u":{"_id":"nZCb9BSnmXZXSNA2u","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:nZCb9BSnmXZXSNA2u_description"},"userId":"nLbwLhBaQeG6tCNDN","name":"Evolution","slug":"evolution","core":false,"postCount":117,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-14T23:18:24.369Z","wikiOnly":false,"deleted":false},"Revision:YSyvvi4uXvxAARX2D_description":{"_id":"YSyvvi4uXvxAARX2D_description","__typename":"Revision","htmlHighlight":"<p><strong>Slack<\/strong> is absence of binding constraints on behavior. The term is usually capitalized to distinguish it from the ordinary English meaning. Not to be confused with the communication app by the same name.<\/p><p>From the post which introduced this usage, <a href=\"https://www.lessestwrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\">Slack<\/a><strong>:<\/strong><\/p><blockquote><p><i>Poor is the person without Slack. Lack of Slack compounds and traps.<\/i><\/p><p><i>Slack means margin for error. You can relax.<\/i><\/p><p><i>Slack allows pursuing opportunities. You can explore. You can trade.<\/i><\/p><p><i>Slack prevents desperation. You can avoid bad trades and wait for better spots. You can be efficient.<\/i><\/p><p><i>Slack permits planning for the long term. You can invest.<\/i><\/p><p><i>Slack enables doing things for your own amusement. You can play games. You can have fun.<\/i><\/p><p><i>Slack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can have a code.<\/i><\/p><p><i>Slack presents things as they are without concern for how things look or what others think. You can be honest.<\/i><\/p><p><i>You can do some of these things, and choose not to do others. Because you don’t have to.<\/i><\/p><p><i>Only with slack can one be a righteous dude.<\/i><\/p><p><i>Slack is life.<\/i><\/p><\/blockquote><p><strong>Related Sequence:<\/strong> <a href=\"https://www.lesswrong.com/s/HXkpm9b8o964jbQ89\">Slack and the Sabbath<\/a><\/p>"},"Tag:YSyvvi4uXvxAARX2D":{"_id":"YSyvvi4uXvxAARX2D","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:YSyvvi4uXvxAARX2D_description"},"userId":"nLbwLhBaQeG6tCNDN","name":"Slack","slug":"slack","core":null,"postCount":34,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-07T20:41:59.282Z","wikiOnly":false,"deleted":false},"Revision:o9aQASibdsECTfYF6_description":{"_id":"o9aQASibdsECTfYF6_description","__typename":"Revision","htmlHighlight":"<p><strong>Moloch <\/strong>is the personification of the forces that coerce competing individuals to take actions which, although locally optimal, ultimately lead to situations where everyone is worse off. Moreover, no individual is able to unilaterally break out of the dynamic. The situation is a bad Nash equilibrium. A trap.<\/p><p>One example of a Molochian dynamic is a <a href=\"https://en.wikipedia.org/wiki/Red_Queen%27s_race\">Red Queen race<\/a> between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they have all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who still does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.<\/p><p>The topic of Moloch receives a formal treatment in the sequence <a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\">Inadequate Equilibria<\/a>, particularly in the chapter <a href=\"https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/moloch-s-toolbox-1-2\">Moloch's Toolbox<\/a>.<\/p><h2>Origin<\/h2><p><a href=\"https://www.lesswrong.com/users/yvain?sortedBy=top\">Scott Alexander<\/a> &nbsp;linked the name to the concept in his eponymous post, <a href=\"https://www.lessestwrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch\">Meditations on Moloch<\/a>. &nbsp;The post intersperses lines of Allan Ginsberg's poem, <a href=\"https://www.poetryfoundation.org/poems/49303/howl\">Howl<\/a>, with multiples examples of the dynamic including: the Prisoner's Dilemma, dollar auctions, <a href=\"https://web.archive.org/web/20160928190322/http://raikoth.net/libertarian.html\">fish farming story<\/a>, Malthusian trap, capitalism, two-income trap, agriculture, arms races, races to the bottom, education system, science, and government corruption and corporate welfare.&nbsp;<\/p><p>From Allan Ginsberg's <a href=\"https://www.poetryfoundation.org/poems/49303/howl\">Howl<\/a>:<\/p><blockquote><p><i>What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?<\/i><br><i>Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!<\/i><br><i>Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!<\/i><br><i>Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments!<\/i><br><i>Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!<\/i><br><i>Moloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in the long streets like endless Jehovahs! Moloch whose factories dream and croak in the fog! Moloch whose smoke-stacks and antennae crown<\/i><\/p><\/blockquote>..."},"Tag:o9aQASibdsECTfYF6":{"_id":"o9aQASibdsECTfYF6","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:o9aQASibdsECTfYF6_description"},"userId":"73yyrm8KF6GDK9sRy","name":"Moloch","slug":"moloch","core":null,"postCount":54,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-25T20:18:44.249Z","wikiOnly":false,"deleted":false},"Post:GZSzMqr8hAB2dR8pk":{"_id":"GZSzMqr8hAB2dR8pk","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:GZSzMqr8hAB2dR8pk_"},"fmCrosspost":null,"readTimeMinutes":29,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:nZCb9BSnmXZXSNA2u"},{"__ref":"Tag:YSyvvi4uXvxAARX2D"},{"__ref":"Tag:o9aQASibdsECTfYF6"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}],"url":null,"postedAt":"2020-05-13T05:00:02.772Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2020-05-13T05:43:55.144Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":34,"voteCount":64,"baseScore":144,"extendedScore":null,"unlisted":false,"score":0.001517,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2022-09-30T01:31:40.139Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":"bestoflesswrong","curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":null,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":30,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2020-05-13T05:00:02.918Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":1,"reviewVoteCount":20,"positiveReviewVoteCount":16,"reviewVoteScoreAllKarma":27,"reviewVotesAllKarma":[4,4,4,4,4,4,1,1,1],"reviewVoteScoreHighKarma":15,"reviewVotesHighKarma":[4,4,4,1,1,1],"reviewVoteScoreAF":13,"reviewVotesAF":[4,4,4,1,0],"finalReviewVoteScoreHighKarma":23.099999999999998,"finalReviewVotesHighKarma":[7.4,4,4,3.8,1,1,1,1,0.9,0,0,-1],"finalReviewVoteScoreAllKarma":42.699999999999996,"finalReviewVotesAllKarma":[9,7.4,4,4,4,4,4,3.8,1,1,1,1,1,1,0.9,0,0,-1,-3.4],"finalReviewVoteScoreAF":16.2,"finalReviewVotesAF":[7.4,4,3.8,1,0],"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":0,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"studies-on-slack","title":"Studies On Slack","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:mbCccXJuuRBZdXdpH_":{"_id":"mbCccXJuuRBZdXdpH_","__typename":"Revision","htmlHighlight":"<p>From the subreddit: <a href=\"https://www.fastcompany.com/90458795/humans-are-hardwired-to-dismiss-facts-that-dont-fit-their-worldview\">Humans Are Hardwired To Dismiss Facts That Don’t Fit Their Worldview<\/a>. Once you get through the preliminary Trump supporter and anti-vaxxer denunciations, it turns out to be an attempt at an evo psych explanation of confirmation bias:<\/p>\n<blockquote><p>Our ancestors evolved in small groups, where cooperation and persuasion had at least as much to do with reproductive success as holding accurate factual beliefs about the world. Assimilation into one’s tribe required assimilation into the group’s ideological belief system. An instinctive bias in favor of one’s in-group” and its worldview is deeply ingrained in human psychology.<\/p><\/blockquote>\n<p>I think the article as a whole makes good points, but I’m increasingly uncertain that confirmation bias can be separated from normal reasoning.<\/p><p>Suppose that one of my friends says she saw a coyote walk by her house in Berkeley. I know there are coyotes in the hills outside Berkeley, so I am not too surprised; I believe her.<\/p><p>Now suppose that same friend says she saw a polar bear walk by her house. I assume she is mistaken, lying, or hallucinating.<\/p><p>Is this confirmation bias? It sure sounds like it. When someone says something that confirms my preexisting beliefs (eg ‘coyotes live in this area, but not polar bears’), I believe it. If that same person provides the same evidence for something that challenges my preexisting beliefs, I reject it. What am I doing differently from an anti-vaxxer who rejects any information that challenges her preexisting beliefs (eg that vaccines cause autism)?<\/p><p>When new evidence challenges our established priors (eg a friend reports a polar bear, but I have a strong prior that there are no polar bears around), we ought to heavily discount the evidence <i>and<\/i> slightly shift our prior. So I should end up believing that my friend is probably wrong, but I should also be slightly less confident in my assertion that there are no polar bears loose in Berkeley today. This seems sufficient to explain confirmation bias, ie a tendency to stick to what we already believe and reject evidence against it.<\/p><p>The anti-vaxxer is still doing something wrong; she somehow managed to get a very strong prior on a false statement, and isn’t weighing the new evidence heavily enough. But I think it’s important to note that she’s attempting to carry out normal reasoning, and failing, rather than carrying out some special kind of reasoning called “... <\/p>","wordCount":494,"version":"1.0.0"},"Post:mbCccXJuuRBZdXdpH":{"_id":"mbCccXJuuRBZdXdpH","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:mbCccXJuuRBZdXdpH_"},"fmCrosspost":null,"readTimeMinutes":2,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:5hpGj9nDLgokfghvR"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:4R8JYu4QF2FqzJxE5"}],"url":null,"postedAt":"2020-02-13T07:20:02.085Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2020-02-13T07:56:02.230Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":9,"voteCount":16,"baseScore":42,"extendedScore":null,"unlisted":false,"score":0.000735,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2020-02-14T08:03:50.682Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":null,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"nLbwLhBaQeG6tCNDN","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2020-02-13T07:20:02.105Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":0,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"confirmation-bias-as-misfire-of-normal-bayesian-reasoning","title":"Confirmation Bias As Misfire Of Normal Bayesian Reasoning","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:4Y2J7NtuweW2B8JvB_":{"_id":"4Y2J7NtuweW2B8JvB_","__typename":"Revision","htmlHighlight":"<p>In the spirit of my old <a href=\"https://slatestarcodex.com/2014/09/05/mapmaker-mapmaker-make-me-a-map/\">map of the rationalist diaspora<\/a>, here’s a map of the effective altruist movement:<\/p>\n<p><a href=\"http://slatestarcodex.com/blog_images/ea_imagemap7.html\"><img src=\"http://slatestarcodex.com/blog_images/eamap_thumbnail.png\" /><\/a><\/p>\n<p>Continents are cause areas; cities are charities or organizations; mountains are individuals. Some things are clickable links with title-text explanations. Thanks to AG for helping me set up the imagemap. <\/p>","wordCount":48,"version":"1.0.0"},"Revision:qAvbtzdG2A2RBn7in_description":{"_id":"qAvbtzdG2A2RBn7in_description","__typename":"Revision","htmlHighlight":"<p><strong>Effective Altruism<\/strong> (EA) is a movement trying to invest time and money in causes that do the most possible good per unit investment. EA was at one point called <strong>optimal philanthropy<\/strong>.<\/p><p>The basic concept behind EA is that you would really struggle to donate 100 times more money or time to charity than you currently do but, spending a little time researching who to donate to <i>could<\/i> have an impact on roughly this order of magnitude. The same argument works for doing good with your career or volunteer hours.<\/p><p>The<strong> Effective Altruism movement <\/strong>also has its own forum,&nbsp;<strong> <\/strong><a href=\"https://forum.effectivealtruism.org/\"><strong>The EA Forum<\/strong><\/a>. It runs on the same software as LessWrong.<\/p><h1>Key Concepts<\/h1><h2>The Scale, Tractability, (Personal Fit) criteria<\/h2><p>Despite a broad diversity of ideas within the EA community on which areas are most pressing, there are a handful of criteria that are generally agreed make an area potentially impactful to work on (either directly or through donation). These are:<\/p><ul><li>The area has the potential for impact at <strong>scale, <\/strong>either in human lives saved, animal or human suffering alleviated,&nbsp;catastrophic crises averted, etc. Sometimes this is called \"importance\"<\/li><li>The area is generally <strong>neglected, <\/strong>that is, it has capacity for more support either financially or in terms of skills. &nbsp;An area with lots of resources should lead us to think we are less likely to be able to make improvements.<\/li><li>The area is <strong>tractable,<\/strong> it is a solvable problem, or is solvable with minimal resource investment (relative to other&nbsp;problem areas)<\/li><\/ul><p>A fourth semi-area is:<\/p><ul><li>Does the individual have good <strong>personal fit<\/strong>?<strong> <\/strong>Do they have unique skills which will make them more effective in an area.<\/li><\/ul><h2>Impartiality (geographic, species, time)&nbsp;<\/h2><h3>Global health and wellbeing (geographic impartiality)<\/h3><blockquote><p>One morning, I say to them, you notice a child has fallen in and appears to be drowning. To wade in and pull the child out would be easy but it will mean that you get your clothes wet and muddy, and by the time you go home and change you will have missed your first class.<\/p><p>I then ask the students: do you have any obligation to rescue the child? Unanimously, the students say they do. The importance of saving a child so far outweighs the cost of getting one’s clothes muddy and missing a class, that they refuse to consider it any kind of excuse for not saving the child. Does it make a difference, I ask, that there are other people walking past the pond who would equally be ab<\/p><\/blockquote>..."},"Tag:qAvbtzdG2A2RBn7in":{"_id":"qAvbtzdG2A2RBn7in","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:qAvbtzdG2A2RBn7in_description"},"userId":"qxJ28GN72aiJu96iF","name":"Effective Altruism","slug":"effective-altruism","core":false,"postCount":209,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-10T11:48:22.619Z","wikiOnly":false,"deleted":false},"Post:4Y2J7NtuweW2B8JvB":{"_id":"4Y2J7NtuweW2B8JvB","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:4Y2J7NtuweW2B8JvB_"},"fmCrosspost":null,"readTimeMinutes":1,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:qAvbtzdG2A2RBn7in"}],"url":null,"postedAt":"2020-02-03T06:20:02.200Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":1,"voteCount":9,"baseScore":17,"extendedScore":null,"unlisted":false,"score":0.000164,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2020-02-03T07:26:46.829Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":null,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":3,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2020-02-03T06:20:02.208Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":0,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"map-of-effective-altruism","title":"Map Of Effective Altruism","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null},"Revision:irbREZtZzPi7WEYex_":{"_id":"irbREZtZzPi7WEYex_","__typename":"Revision","htmlHighlight":"<p><b>I.<\/b><\/p><p><a href=\"https://en.wikipedia.org/wiki/Clarke%27s_three_laws\">Clarke’s First Law<\/a> goes: When a distinguished but elderly scientist states that something is possible, he is almost certainly right. When he states that something is impossible, he is very probably wrong.<\/p><p>Stuart Russell is only 58. But what he lacks in age, he makes up in distinction: he’s a computer science professor at Berkeley, neurosurgery professor at UCSF, DARPA advisor, and author of the leading textbook on AI. His new book <a href=\"https://www.amazon.com/Human-Compatible-Artificial-Intelligence-Problem/dp/0525558616/ref=as_li_ss_tl?keywords=human+compatible&amp;qid=1577149898&amp;sr=8-1&amp;linkCode=ll1&amp;tag=slatestarcode-20&amp;linkId=ea5e41f92ad5c6166f0b399c2b430671&amp;language=en_US\"><i>Human Compatible<\/i><\/a> states that superintelligent AI is possible; Clarke would recommend we listen.<\/p><p>I’m only half-joking: in addition to its contents, <i>Human Compatible<\/i> is important as an <i>artifact<\/i>, a crystallized proof that top scientists now think AI safety is worth writing books about. Nick Bostrom’s <i>Superintelligence: Paths, Dangers, Strategies<\/i> previously filled this role. But <i>Superintelligence<\/i> was in 2014, and by a philosophy professor. From the artifactual point of view, HC is just better – more recent, and by a more domain-relevant expert. <\/p><p>But if you also open up the books to see what’s inside (not recommended; that’s where the confusing stuff is), the two defy easy comparison. <\/p><p>S:PDS was unabashedly a weird book. It explored various outrageous scenarios (what if the AI destroyed humanity to prevent us from turning it off? what if it put us all in cryostasis so it didn’t count as destroying us? what if it converted the entire Earth into computronium?) with no excuse beyond that, outrageous or not, they might come true. Bostrom was going out on a very shaky limb to broadcast a crazy-sounding warning about what might be the most important problem humanity has ever faced, and the book made this absolutely clear.<\/p><p>HC somehow makes risk from superintelligence <i>not sound weird<\/i>. I can imagine my mother reading this book, nodding along, feeling better educated at the end of it, agreeing with most of what it says (it’s by a famous professor! I’m sure he knows his stuff!) and never having a moment where she sits bolt upright and goes <i>what?<\/i> It’s just a bizarrely normal, respectable book. It’s not that it’s dry and technical – HC is much more accessible than S:PDS, with funny anecdotes from Russell’s life, cute vignettes about hypothetical robots, and the occasional dad joke. It’s not hiding any of the weird superintelligence parts. Rereading it carefully, they’re all in there – when I leaf through it for examples, I come across a quote from Mora... <\/p>","wordCount":4757,"version":"1.0.0"},"Revision:4Kcm4etxAJjmeDkHP_description":{"_id":"4Kcm4etxAJjmeDkHP_description","__typename":"Revision","htmlHighlight":"<p><strong>Book Reviews<\/strong> on LessWrong are different from normal book reviews; they summarize and respond to a book's core ideas first, and judge whether you should read it second. A good book review sometimes distills the book's ideas so well that you no longer need to read the book.<\/p><p>Reviews engage with the perspective of the author, someone who has put in the effort to record their understanding of the world. Some of the best essays on LessWrong are reviews that teach us about <a href=\"https://www.lesswrong.com/tag/history\">history<\/a>, <a href=\"https://www.lesswrong.com/tag/replication-crisis\">psychology<\/a>, <a href=\"https://www.lesswrong.com/tag/biology\">biology<\/a>, or some other area where the author has developed a detailed understanding of a phenomena that few others have ever reached, and the essay writer engages with that perspective from our rationalist perspective.<\/p><p>Good book reviews embody the virtues of <a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><u>scholarship<\/u><\/a>, <a href=\"https://www.lesswrong.com/tag/curiosity\">curiosity<\/a> and perspective-taking.<\/p><p><strong>Related Pages:<\/strong> <a href=\"https://www.lesswrong.com/tag/epistemic-review\">Epistemic Review<\/a>, <a href=\"https://www.lesswrong.com/tag/summaries\">Summaries<\/a>, <a href=\"https://www.lesswrong.com/tag/literature-reviews\">Literature Reviews<\/a>, <a href=\"https://www.lesswrong.com/tag/lesswrong-review\">LessWrong Review<\/a><\/p>"},"Tag:4Kcm4etxAJjmeDkHP":{"_id":"4Kcm4etxAJjmeDkHP","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:4Kcm4etxAJjmeDkHP_description"},"userId":"qgdGA4ZEyW7zNdK84","name":"Book Reviews","slug":"book-reviews","core":null,"postCount":311,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":null,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-13T04:57:42.173Z","wikiOnly":false,"deleted":false},"Revision:sYm3HiWcfZvrGu3ui_description":{"_id":"sYm3HiWcfZvrGu3ui_description","__typename":"Revision","htmlHighlight":"<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:10px solid #f8f8f8\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\">Fixed Point Theorems<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\">Treacherous Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\">Conservatism (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\">Transparency / Interpretability<\/a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\">Tripwire<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><\/p><p>&nbsp;<\/p><p><strong>Strategy<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/ai-governance?showPostCount=true&amp;useTagName=true\">AI Governance<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true\">AI Risk<\/a><br><a href=\"http://www.lesswrong.com/tag/ai-services-cais?showPostCount=true&amp;useTagName=true\"><u>AI Services (CAIS)<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/ai-takeoff?showPostCount=true&amp;useTagName=true\">AI Takeoff<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-timelines?showPostCount=true&amp;useTagName=true\">AI Timelines<\/a><br><a href=\"https://www.lesswrong.com/tag/computing-overhang?showPostCount=true&amp;useTagName=true\">Computing Overhang<\/a><br><a href=\"https://www.lesswrong.com/tag/regulation-and-ai-risk?showPostCount=true&amp;useTagName=true\">Regulation and AI Risk<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&amp;useTagName=true\">Centre for Human-Compatible AI<\/a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\">DeepMind<\/a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&amp;useTagName=true\">Future of Humanity Institute<\/a><br><a href=\"https://www.lesswrong.com/tag/future-of-life-institute-fli?showPostCount=true&amp;useTagName=true\">Future of Life Institute<\/a><br><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true\">Machine Intelligence Research Institute<\/a><br><a href=\"https://www.lesswrong.com/tag/openai?showPostCount=true&amp;useTagName=true\">OpenAI<\/a><br><a href=\"https://www.lesswrong.com/tag/ought?showPostCount=true&amp;useTagName=true\">Ought<\/a><\/p><p>&nbsp;<\/p><p><strong>Other<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-capabilities?showPostCount=true&amp;useTagName=true\">AI Capabilities<\/a><br><a href=\"https://www.lesswrong.com/tag/gpt?showPostCount=true&amp;useTagName=true\">GPT<\/a><br><a href=\"https://www.lesswrong.com/tag/language-models?showPostCount=true&amp;useTagName=true\">L<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>..."},"Tag:sYm3HiWcfZvrGu3ui":{"_id":"sYm3HiWcfZvrGu3ui","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:sYm3HiWcfZvrGu3ui_description"},"userId":"r38pkCm7wF4M44MDQ","name":"AI","slug":"ai","core":true,"postCount":4039,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":2000,"createdAt":"2020-06-14T22:24:22.097Z","wikiOnly":false,"deleted":false},"Tag:cPFuhAE7PwoKF7yTj":{"_id":"cPFuhAE7PwoKF7yTj","__typename":"Tag","parentTag":null,"subTags":[],"description":null,"userId":"EQNTWXLKMeWMp2FQS","name":"Inverse Reinforcement Learning","slug":"inverse-reinforcement-learning","core":false,"postCount":30,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-29T23:00:48.538Z","wikiOnly":false,"deleted":false},"Post:irbREZtZzPi7WEYex":{"_id":"irbREZtZzPi7WEYex","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:irbREZtZzPi7WEYex_"},"fmCrosspost":null,"readTimeMinutes":19,"moderationGuidelines":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:4Kcm4etxAJjmeDkHP"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"},{"__ref":"Tag:cPFuhAE7PwoKF7yTj"}],"url":null,"postedAt":"2020-01-31T05:20:02.138Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2020-01-31T05:20:35.261Z","meta":false,"shareWithUsers":null,"sharingSettings":null,"coauthorStatuses":null,"hasCoauthorPermission":null,"commentCount":6,"voteCount":34,"baseScore":77,"extendedScore":null,"unlisted":false,"score":0.000685,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2020-02-03T10:21:59.111Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":null,"globalEvent":null,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":19,"afExtendedScore":null,"afCommentCount":null,"afLastCommentedAt":"2020-01-31T05:20:02.152Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":null,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"reviewVoteScoreAllKarma":null,"reviewVotesAllKarma":null,"reviewVoteScoreHighKarma":null,"reviewVotesHighKarma":null,"reviewVoteScoreAF":null,"reviewVotesAF":null,"finalReviewVoteScoreHighKarma":null,"finalReviewVotesHighKarma":null,"finalReviewVoteScoreAllKarma":null,"finalReviewVotesAllKarma":null,"finalReviewVoteScoreAF":null,"finalReviewVotesAF":null,"group":null,"nominationCount2018":null,"reviewCount2018":null,"nominationCount2019":0,"reviewCount2019":null,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[],"slug":"book-review-human-compatible-1","title":"Book Review: Human Compatible","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null}}</script>
<script>window.__APOLLO_FOREIGN_STATE__ = {}</script>

<script src="./scott_files/api.js"></script><iframe id="intercom-frame" style="position: absolute !important; opacity: 0 !important; width: 1px !important; height: 1px !important; top: 0 !important; left: 0 !important; border: none !important; display: block !important; z-index: -1 !important; pointer-events: none;" aria-hidden="true" tabindex="-1" title="Intercom" src="./scott_files/saved_resource.html"></iframe><div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s ease 0s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe title="reCAPTCHA" src="./scott_files/anchor.html" width="256" height="60" role="presentation" name="a-tx0vu8uxbsdn" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;" src="./scott_files/saved_resource(1).html"></iframe></div><div class="intercom-lightweight-app"><div class="intercom-lightweight-app-launcher intercom-launcher" role="button" tabindex="0" aria-label="Open Intercom Messenger" aria-live="polite"><div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 28 32"><path d="M28 32s-4.714-1.855-8.527-3.34H3.437C1.54 28.66 0 27.026 0 25.013V3.644C0 1.633 1.54 0 3.437 0h21.125c1.898 0 3.437 1.632 3.437 3.645v18.404H28V32zm-4.139-11.982a.88.88 0 00-1.292-.105c-.03.026-3.015 2.681-8.57 2.681-5.486 0-8.517-2.636-8.571-2.684a.88.88 0 00-1.29.107 1.01 1.01 0 00-.219.708.992.992 0 00.318.664c.142.128 3.537 3.15 9.762 3.15 6.226 0 9.621-3.022 9.763-3.15a.992.992 0 00.317-.664 1.01 1.01 0 00-.218-.707z"></path></svg></div><div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-minimize"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path fill-rule="evenodd" clip-rule="evenodd" d="M18.601 8.39897C18.269 8.06702 17.7309 8.06702 17.3989 8.39897L12 13.7979L6.60099 8.39897C6.26904 8.06702 5.73086 8.06702 5.39891 8.39897C5.06696 8.73091 5.06696 9.2691 5.39891 9.60105L11.3989 15.601C11.7309 15.933 12.269 15.933 12.601 15.601L18.601 9.60105C18.9329 9.2691 18.9329 8.73091 18.601 8.39897Z" fill="white"></path>
</svg>
</div></div><style id="intercom-lightweight-app-style" type="text/css">
  @keyframes intercom-lightweight-app-launcher {
    from {
      opacity: 0;
      transform: scale(0.5);
    }
    to {
      opacity: 1;
      transform: scale(1);
    }
  }

  @keyframes intercom-lightweight-app-gradient {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  @keyframes intercom-lightweight-app-messenger {
    
        from {
          opacity: 0;
          transform: translateY(20px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
        
  }

  .intercom-lightweight-app {
    position: fixed;
    z-index: 2147483001;
    width: 0;
    height: 0;
    font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif;
  }

  .intercom-lightweight-app-gradient {
    position: fixed;
    z-index: 2147483002;
    width: 500px;
    height: 500px;
    bottom: 0;
    right: 0;
    pointer-events: none;
    background: radial-gradient(
      ellipse at bottom right,
      rgba(29, 39, 54, 0.16) 0%,
      rgba(29, 39, 54, 0) 72%);
    animation: intercom-lightweight-app-gradient 200ms ease-out;
  }

  .intercom-lightweight-app-launcher {
    position: fixed;
    z-index: 2147483003;
    padding: 0 !important;
    margin: 0 !important;
    border: none;
    bottom: 20px;
    right: 20px;
    max-width: 60px;
    width: 60px;
    max-height: 60px;
    height: 60px;
    border-radius: 50%;
    background: #f5f5f5;
    cursor: pointer;
    box-shadow: 0 1px 6px 0 rgba(0, 0, 0, 0.06), 0 2px 32px 0 rgba(0, 0, 0, 0.16);
    transition: transform 167ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
    box-sizing: content-box;
  }

  


  .intercom-lightweight-app-launcher:focus {
    outline: none;

    
  }

  .intercom-lightweight-app-launcher-icon {
    display: flex;
    align-items: center;
    justify-content: center;
    position: absolute;
    top: 0;
    left: 0;
    width: 60px;
    height: 60px;
    transition: transform 100ms linear, opacity 80ms linear;
  }

  .intercom-lightweight-app-launcher-icon-open {
    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-open svg {
    width: 28px;
    height: 32px;
  }

  .intercom-lightweight-app-launcher-icon-open svg path {
    fill: rgba(0, 0, 0, 0.5);
  }

  .intercom-lightweight-app-launcher-icon-self-serve {
    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-self-serve svg {
    height: 56px;
  }

  .intercom-lightweight-app-launcher-icon-self-serve svg path {
    fill: rgba(0, 0, 0, 0.5);
  }

  .intercom-lightweight-app-launcher-custom-icon-open {
    max-height: 36px;
    max-width: 36px;

    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-minimize {
    
        opacity: 0;
        transform: rotate(-60deg) scale(0);
      
  }

  .intercom-lightweight-app-launcher-icon-minimize svg path {
    fill: rgba(0, 0, 0, 0.5);
  }

  .intercom-lightweight-app-messenger {
    position: fixed;
    z-index: 2147483003;
    overflow: hidden;
    background-color: white;
    animation: intercom-lightweight-app-messenger 250ms cubic-bezier(0, 1.2, 1, 1);
    transform-origin: bottom right;
    
        width: 376px;
        height: calc(100% - 120px);
        max-height: 704px;
        min-height: 250px;
        right: 20px;
        bottom: 100px;
        box-shadow: 0 5px 40px rgba(0,0,0,0.16);
      
    border-radius: 8px;
  }

  .intercom-lightweight-app-messenger-header {
    height: 75px;
    border-bottom: none;
    background: linear-gradient(
      135deg,
      rgb(245, 245, 245) 0%,
      rgb(194, 194, 194) 100%
    );
  }

  .intercom-lightweight-app-messenger-footer{
    position:absolute;
    bottom:0;
    width: 100%;
    min-height: 81px;
    background: #fff;
    font-size: 14px;
    line-height: 21px;
    border-top: 1px solid rgba(0, 0, 0, 0.05);
    box-shadow: 0px 0px 25px rgba(0, 0, 0, 0.05);
  }

  @media print {
    .intercom-lightweight-app {
      display: none;
    }
  }
</style></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>