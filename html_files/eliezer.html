<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link rel="preload" as="style" href="eliezer_files/allStyles.css"><link rel="stylesheet" type="text/css" href="eliezer_files/icon.css"><link rel="stylesheet" type="text/css" href="eliezer_files/reset-min.css"><link rel="stylesheet" type="text/css" href="eliezer_files/css.css"><link rel="stylesheet" type="text/css" href="eliezer_files/jvr1gjm.css"><link rel="stylesheet" type="text/css" href="eliezer_files/tqv5rhd.css"><script type="text/javascript" async="" charset="utf-8" src="eliezer_files/recaptcha__en.js" crossorigin="anonymous" integrity="sha384-C0eb2CrhokW3SgZMDSrT/ioPvOCBoj1s7JouJ8IrLFB+j5cW9qY3JDWtShxtCryz"></script><script async="" src="eliezer_files/google-analytics_analytics.js"></script><script>window.publicInstanceSettings = {"forumType":"LessWrong","title":"LessWrong","siteNameWithArticle":"LessWrong","sentry":{"url":"https://1ab1949fc8d04608b43132f37bb2a1b0@sentry.io/1301611","environment":"production","release":"69f0f3c5d57b596e8249571383f8a280eff9bb23"},"debug":false,"aboutPostId":"bJ2haLkcGeLtTWaD5","faqPostId":"2rWKkWuPrgTMpLRbp","contactPostId":"ehcYkvyz7dh9L7Wt8","expectedDatabaseId":"production","tagline":"A community blog devoted to refining the art of rationality","faviconUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico","faviconWithBadge":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_with_badge.ico","forumSettings":{"headerTitle":"LESSWRONG","shortForumTitle":"LW","tabTitle":"LessWrong"},"analytics":{"environment":"lesswrong.com"},"cluster":{"enabled":true,"numWorkers":2},"testServer":false,"fmCrosspost":{"siteName":"the EA Forum","baseUrl":"https://forum.effectivealtruism.org/"},"allowTypeIIIPlayer":true,"hasRejectedContentSection":true,"hasCuratedPosts":true,"performanceMetricLogging":{"enabled":true,"batchSize":100},"reviewBotId":"tBchiz3RM7rPwujrJ","recombee":{"databaseId":"lightcone-infrastructure-lesswrong-prod-2","publicApiToken":"sb95OJbQ7mKLQAm1abPog2m5vCPj7XqZlVYdHGyANcjzqaHT5fX6HEgB0vCfiLav"},"homepagePosts":{"feeds":[{"name":"forum-classic","label":"Latest","description":"The classic LessWrong frontpage algorithm that combines karma with time discounting, plus any tag-based weighting if applied.","showToLoggedOut":true},{"name":"recombee-hybrid","label":"Enriched","description":"An equal mix of Latest and Recommended.","showSparkleIcon":true,"defaultTab":true,"showToLoggedOut":true},{"name":"recombee-lesswrong-custom","label":"Recommended","description":"Personalized recommendations from the history of LessWrong, using a machine learning model that takes into account posts you've read and/or voted on.","showSparkleIcon":true,"showToLoggedOut":true},{"name":"forum-subscribed-authors","label":"Subscribed","description":"Posts and comments by people you've explicitly subscribed to.","isInfiniteScroll":true},{"name":"vertex-default","label":"Vertex","description":"Experimental feed for Google Vertex recommendations.","showLabsIcon":true,"adminOnly":true},{"name":"forum-bookmarks","label":"Bookmarks","description":"A list of posts you saved because you wanted to have them findable later."},{"name":"forum-continue-reading","label":"Resume Reading","description":"Further posts in post sequences that you started reading.","disabled":true}]}}</script><link rel="shortcut icon" href="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico"><script>window.publicSettings = {"forum":{"numberOfDays":10,"postInterval":30,"numberOfWeeks":4,"numberOfYears":4,"maxPostsPerDay":5,"numberOfMonths":4},"type3":{"cutoffDate":"2023-07-01","explicitlyAllowedPostIds":["SvKSwT6xYfYahH4XN","2weRdcvqANDq3zdPH","Zm7WAJMTaFvuh2Wc7","HcjL8ydHxPezj6wrt","pgGiqLQg2KWsaz5RE","jFzovY2CERF5bd2EW","sm6npdgZArSn4afeZ","CfX6pGepdjQYELSpK","NyFuuKQ8uCEDtd2du","LCjtqsQWapoSfDHqK","MxyRNd6qJsYAcXKuw","reG3g4wwzwJcKnFfh","zfeWGvTrS6wKQeeoF","oHsMeXehPy4jHcmwy","ofL22R6KZsfrvdmwg","655TmdcwAgryPGPWS","hhrv8aAcmkzJxvP58","iqQJiKcephtMgzJgN","mnpkM57R6ZbjnwrYw","6mRv7Cr57AJAtRFHv","ak9wY2t9K3K4GxCXv","Ay6GBGNcCgP55dRQ7","aH4mjhgqNPyYvJT85","JpoLCHytYiCm7fwNA","efMgZujzfjP9B9H4R","BjLxPLsev54LFCS3A","HTGCGASf9xfB6edAh","H6LnGwjKiGvDyR5yo","qL8Z9TBCNWQyN6yLq","F4xwRTrFQyazHufjD","LY7Nca846X8kcT8Jk","K9aLcuxAPyf5jGyFX","2AuvBPw6Rb7yxkvKc","muhtBvbh4etjkKXd9","ALEYMFAuFSCz8v5YE","CJxSgaqG6y7z6Rbij","k5TpDCEHeK4qwnJt8","4Y2J7NtuweW2B8JvB","BpYDqQNZ2NZNCqPp6","oMiogKLkK8L59WzDe","TyQSMmoJpRG3HBv5S","8KHR3tfa4SJjMSkXd","g4pi2jfQHFF6mPdjw","znEhB9hJtwXica5s3","Sd2r7H8bCmd9ChGbX","P2nYKqwmHdYKARTG8","FW3DEYbKPZJh5A8Bj","K3hFLRn7MvYacL466","ouSpHCCPgsXkwxAGb","w9SuQtRJLbDpeir6L","yPQGYn9rSme9RRpiQ","BD6WYC4GT6dnWaJRN","c8khnHoRTSGjmHLLf","TaPr4YSBbiakeKdwX","pyNPXST7feDX45ygt","ERPL3v2Y976W7XG3j","XpXQ4KNzLa9ZHYw8p","PBhrHw5X8sDmHDWkX","8KhThQXzsAEZ59iko","iYJo382hY28K7eCrP","KrEwDMN4YXp5YWD45","rNJ39yQmzTnseh8nL","hMQPyLDbg3bA7P6aN","3Jqz6JE8K6vyQ9hJ5","SQAfPKZBAAKYMjx25","Y345zuBetHqGnotwm","pZerSnxv6FPqvgoYu","3bPH2az479gzxDMbf","QXShCBvPydkwafekn","iLMkKDKmfbMkDuQBm","iNCg6mjw584r9BWZK","9oqF382ASmjaGBo7z","DdNB42JgBzbbvmAum","JP7eZYHB7aY6fA4TR","snwX7hXgLFikqDBr6","CsKrQdQJJCFPjfKjF","vhxywjnBH6ioRnnt3","A4MK9RQqSAJZjanQD","PCpzG9NJeviXM5YSq","KCcdhZK7omEMwBdju","kdmCm5NQTpqhJmGm6","2p8BWvcJvKkXGMsch","FLnDFnXyWrKr6eiT6","2gWs8SScqeDFidqyv","2HafkDSNdtMzptzcN","cTQRGJTQ2eGKm5G9g","qaHHJ3kkCQS4nsoGJ","gS8Jmcfoa9FAh92YK","eRhFaibbTeGbjdaaf","xij43oLTBRnEQv2bT","BZMc9Xzqw5WcCMHrr","2jZykdLg9fBGqKd46","gBChm3THPGFcrq5eH","9HSwh2mE3tX6xvZ2W","tEHJXNhw6t87foqJL","T5McDuWDeCvDZKeSj","PeTL97v92LxRJBsrM","Cq45AuedYnzekp3LX","pfmZ5cYQCahABGZzi","3wBj8BPquskZAbXu9","xPJKZyPCvap4Fven8","BPKvZuLRyiJBjfNbg","um7w5RogAHhxGy8Ti","CcyGR3pp3FCDuW6Pf","BfaAADSQ88cuxLQoD","ckuuDa8DmJ4pdFeD8","pczHfyxmnFhtKthqR","dymK5c7BkpgXH4acw","B4AyJXYPpGbBmxQzd","xNBRkPNHAGQ6EQaLS","88TN6y9M5xxAHHNwW","Lt8Rn4rkYwqiTXGPy","QdXrkWoK2Pp6XhNuQ","NjzBrtvDS4jXi5Krp","ZWC3n9c6v4s35rrZ3","Fy2b55mLtghd4fQpx","eaczwARbFnrisFx8E","KLjQedNYNEP4tW73W","DSnamjnW7Ad8vEEKd","7iDtkfyn322nPzTP4","eaSJtg8Kvc56bFBdt","AmaWMMWPzuQ62Ernf","jkf2YjuH8Z2E7hKBA","BroeiXGh9PrKZEkJ5","9Tw5RqnEzqEtaoEkq","EMJ3egz48BtZS8Pws","MkKcnPdTZ3pQ9F5yC","kjArXFinD3deRZNRu","Q8zqoBWBBHD2RjDuS","ePA4NDzZkunz98tLx","4xKeNKFXFB458f5N8","irbREZtZzPi7WEYex","QxZs5Za4qXBegXCgu","ZmQv4DFx6y4jFbhLy","M7rwT264CSYY6EdR3","z3cTkXbA7jgwGWPcv","9thqSN8HDLM3LTxK5","MtNnFg4uN32YPoKNa","Ep2Z42hYqj68QZz6w","ibk7q8msSYxZXmfCf","EgDpZS4HHeh5vqJPe","5dhWhjfxn4tPfFQdi","Wh8HAK6LR5CAoPCCC","Yy7mgec8tsbTAuTqb","azoP7WeKYYfgCozoh","Zh9AiXNjQaYXjmNaC","bJiyYJeCyh4HcKHub","aPrCzeFfbBmRsvzby","vXCK3kptLLggEfojX","M2LWXsJxKS626QNEA","LQp9cZPzJncFKh5c8","CZnBQtvDw33rmWpBD","miHttwTgajY2sjY3L","K2JBqDeETX2yEgyyZ","r8aAqSBeeeMNRtiYK","Gh2qQHrCg3teQen3c","mja6jZ6k9gAwki9Nu","qjSHfbjmSyMnGR9DS","Sx26Aj3xuMzmnKE4A","P3uavjFmZD5RopJKk","pJJdcZgB6mPNWoSWr","oGezscrQvPDgGvrbt","AYbhqi65SWzHzy7Xx","E4cKD9iTWHaE7f3AJ","x9FNKTEt68Rz6wQ6P","HAEPbGaMygJq8L59k","znBJwbuT3f5eWgM4E","yJfBzcDL9fBHJfZ6P","YAkpzvjC768Jm2TYb","LTtNXM9shNM9AC2mp","9hR2RmpJmxT8dyPo4","WQWhXzALcrzrJtqRh","p7WXmG6Fbo3eaSwm3","KheBaeW8Pi7LwewoF","A2Qam9Bd9xpbb2wLQ","asmZvCPHcB4SkSCMW","euJm4RwkAptZnP89i","r8stxYL29NF9w53am","6yTShbTdtATxKonY5","yDRX2fdkm3HqfTpav","EhEZoTFzys9EDmEXn","YSWa8rYeD3aDaofSP","rwkkcgSpnAyE8oNo3","HmfxSWnqnK265GEFM","Ltey8BS83qSkd9M3u","atcJqdhCxTZiJSxo2","pC47ZTsPNAkjavkXs","wJnm5cBiZGmKn595f","GrtbTAPfkJa4D6jjH","LgavAYtzFQZKg95WC","reitXJgJXFzKpdKyd","ZiQqsgGX6a42Sfpii","neQ7eXuaXpiYw7SBy","hQHuXuRGZxxWXaPgg","9kcTNWopvXFncXgPy","baTWMegR42PAsH9qJ","Kbm6QnJv9dgWsPHQP","gFMH3Cqw4XxwL69iy","R6M4vmShiowDn56of","6Fpvch8RR29qLEWNH","N6WM6hs7RQMKDhYjB","pdaGN6pQyQarFHXF4","SA9hDewwsYgnuscae","i9xyZBS3qzA8nFXNQ","bx3gkHJehRCYZAF3r","Jk9yMXpBLMWNTFLzh","JvZhhzycHu2Yd57RN","vzfz4AS6wbooaTeQk","gHefoxiznGfsbiAu9","sbcmACvB6DqYXYidL","kipMvuaK3NALvFHc9","xdwbX9pFEr7Pomaxv","XvN2QQpKTuEzgkZHY","uFNgRumrDTpBfQGrs","ii4xtogen7AyYmN6B","kpPnReyBC54KESiSn","FRv7ryoqtvSuqBxuT","u8GMcpEN9Z6aQiCvp","B2CfMNfay2P8f2yyc","JD7fwtRQ27yc8NoqS","mRwJce3npmzbKfxws","3rxMBRCYEmHCNDLhu","FWvzwCDRgcjb9sigb","KrJfoZzpSDpnrv9va","LpM3EAakwYdS6aRKf","Cf2xxC3Yx9g6w7yXN","qHCDysDnvhteW7kRd","mELQFMi9egPn5EAjK","qDmnyEMtJkE9Wrpau","4ZvJab25tDebB8FGE","4QemtxDFaGXyGSrGD","Psr9tnQFuEXiuqGcR","qmXqHKpgRfg83Nif9","ximou2kyQorm6MPjX","eccTPEonRe4BAvNpD","2cYebKxNp47PapHTL","pv7Qpu8WSge8NRbpB","PqMT9zGrNsGJNfiFR","B9kP6x5rpmuCzpfWb","zB4f7QqKhBHa5b37a","qc7P2NwfxQMC3hdgm","RcifQCKkRc9XTjxC2","YABJKJ3v97k9sbxwg","bNXdnRTpSXk9p4zmi","fRsjBseRuvRhMPPE5","MzKKi7niyEqkBPnyu","NQgWL7tvAPgN2LTLn","cujpciCqNbawBihhQ","wEebEiPpEwjYvnyqq","AqbWna2S85pFTsHH4","Nwgdq6kHke5LY692J","8xLtE3BwgegJ7WBbf","SWxnP5LZeJzuT3ccd","Tr7tAyt5zZpdTwTQK","ax695frGJEzGxFBK4","FkgsxrGf3QxhfLWHG","vJ7ggyjuP4u2yHNcP","X5RyaEDHNq5qutSHK","xhD6SHAAE9ghKZ9HS","AyNHoTWWAJ5eb99ji","F5ktR95qqpmGXXmLq","znfkdCoHMANwqc2WE","jbE85wCkRr9z7tqmD","4K5pJnKBGkqqTbyxx","yeADMcScw8EW9yxpH","9QxnfMYccz9QRgZ5z","X2i9dQQK3gETCyqh2","4XRjPocTprL4L8tmB","D6trAzh6DApKPhbv4","BcYfsi7vmhDvzQGiF","i42Dfoh4HtsCAfXxL","zp5AEENssb8ZDnoZR","KwdcMts8P8hacqwrX","RQpNHSiWaXTvDxt6R","nSjavaKcBrtNktzGa","hNqte2p48nqKux3wS","7im8at9PmhbT4JHsW","SwcyMEgLyd4C3Dern","AHhCrJ2KpTjsCSwbt","rz73eva3jv267Hy7B","E4zGWYzh6ZiG85b2z","hvGoYXi2kgnS3vxqb","D4hHASaZuLCW92gMy","v7c47vjta3mavY3QC","G5TwJ9BGxcgh5DsmQ","YRgMCXMbkKBZgMz4M","ham9i5wf4JCexXnkN","a4jRN9nbD79PAhWTB","xJyY5QkQvNJpZLJRo","ivpKSjM4D6FbqF4pZ","p7x32SEt43ZMC9r7r","f886riNJcArmpFahm","xhE4TriBSPywGuhqi","ThvvCE2HsLohJYd7b","diruo47z32eprenTg","JJFphYfMsdFMuprBy","ZDZmopKquzHYPRNxq","KkwtLtroaNToWs2H6","vKErZy7TFhjxtyBuG","3L46WGauGpr7nYubu","CSZnj2YNMKGfsMbZA","G2Lne2Fi7Qra5Lbuf","x6hpkYyzMG6Bf8T3W","aFaKhG86tTrKvtAnT","PrCmeuBPC4XLDQz8C","dYspinGtiba5oDCcv","9cbEPEuCa9E7uHMXT","N5Jm6Nj4HkNKySA5Z","asmZvCPHcB4SkSCMW","duxy4Hby5qMsv42i8","Djs38EWYZG8o7JMWY","A8iGaZ3uHNNGgJeaD","XYYyzgyuRH5rFN64K","2jfiMgKkh7qw9z8Do","JPan54R525D68NoEt","o4cgvYmNZnfS4xhxL","CeZXDmp8Z363XaM6b","DQKgYhEYP86PLW7tZ","niQ3heWwF6SydhS7R","gvK5QWRLk3H8iqcNy","fnkbdwckdfHS2H22Q","YicoiQurNBxSp7a65","JBFHzfPkXHB2XfDGj","tj8QP2EFdP8p54z6i","9fB4gvoooNYa4t56S","zTfSXQracE7TW8x4w","YcdArE79SDxwWAuyF","8xRSjC76HasLnMGSf","CvKnhXTu9BPcdKE4W","DtcbfwSrcewFubjxp","NxF5G6CJiof6cemTw","4ZwGqkMTyAvANYEDw","EF5M6CmKRd6qZk27Z","cCMihiwtZx7kdcKgt","Qz6w4GYZpgeDp6ATB","TPjbTXntR54XSZ3F2","x3fNwSe5aWZb5yXEG","bnY3L48TtDrKTzGRb","ZFtesgbY9XwtqqyZ5","S7csET9CgBtpi7sCh","tTWL6rkfEuQN9ivxj","L6Ktf952cwdMJnzWm","P6fSj3t4oApQQTB7E","4s2gbwMHSdh2SByyZ","sTwW3QLptTQKuyRXx","EYd63hYSzadcNnZTD","tF8z9HBoBn783Cirz","hyShz2ABiKX56j5tJ","YN6daWakNnkXEeznB","6DuJxY8X45Sco4bS2","TMFNQoRZxM4CuRCY6","q3JY4iRzjq56FyjGF","diutNaWF669WgEt3v","5okDRahtDewnWfFmz","r3NHPD3dLFNk9QE2Y","ALkH4o53ofm862vxc","N9oKuQKuf7yvCCtfq","WjsyEBHgSstgfXTvm","2G8j8D5auZKKAjSfY","rBkZvbGDQZhEymReM","nNqXfnjiezYukiMJi","36Dhz325MZNq3Cs6B","f2GF3q6fgyx8TqZcn","byewoxJiAfwE6zpep","nEBbw2Bc2CnN2RMxy","w4aeAFzSAguvqA5qu","xFotXGEotcKouifky","rzqACeBGycZtqCfaX","DoPo4PDjgSySquHX8","o3RLHYviTE4zMb9T9","5gfqG3Xcopscta3st","GNhMPAWcfBCASy8e6","uXH4r6MmKPedk8rMA","Gg9a4y8reWKtLe3Tn","bBdfbWfWxHN9Chjcq","sT6NxFxso6Z9xjS7o","k9dsbn8LZ6tTesDS3","exa5kmvopeRyfJgCy","YTJp5WBcktBimdxBG","X79Rc5cA5mSWBexnd","SvKpaPbZ2tibeDpgh","rQKstXH8ZMAdN5iqD","vQKbgEKjGZcpbCqDs","Z9cbwuevS9cqaR96h","pHHaNkG8xDcaq5DJF","sjRG35aq5fosJ6mdG","pPWiLGsWCtN92vLwu","D5BP9CxKHkcjA7gLv","57sq9qA3wurjres4K","t2LGSDwT7zSnAGybG","7Pq9KwZhG6vejmYpo","g3PwPgcdcWiP33pYn","zcriHTKgKNehSSdyG","kvLPC5YWgSujcHSkY","HnC29723hm6kJT7KP","CRiJuJxgArjBMJLvK","dyJfGeWo5GX2u6NGi","QLmSFeFexgTLsNeeA","kmT47aLQmqzcw329Y","givHhuPu6G43g8kWN","83DimRqppcaoyYAsy","vvzfFcbmKgEsDBRHh","FfNEt8mpi6qanNmXg","MrAfiomDNWCzxjei5","73kwTFKgi4AagxFHJ","iBBK4j6RWC7znEiDv","W8vSrHAM9qoWdzFoP","Rx9GLepCxctXDqCPc","4X9JLr2SpB6v68twG","yxTP9FckrwoMjxPc4","FuZ7MoR3dJEJuoRbN","xRyLxfytmLFZ6qz5s","mwGAyWmsSqzMz4WMd","xxC3Ka7axphW8kJ9E","KT8Mf3ey6uwQAkWek","GDT6tKH5ajphXHGny","ZXaRHHLsxaTTQQsZb","CHdsSaQGAvtkXBzmJ","HAEPbGaMygJq8L59k","SmDziGM9hBjW9DKmf","8NKu9WES7KeKRWEKK","NfdHG6oHBJ8Qxc26s","LTtNXM9shNM9AC2mp","uKp6tBFStnsvrot5t","baTWMegR42PAsH9qJ","Xqcorq5EyJBpZcCrN","7cAsBPGh98pGyrhz9","ZbgCx2ntD5eu8Cno9","9kcTNWopvXFncXgPy","HxWdXMqoQtjDhhNGA","xwBuoE9p8GE7RAuhd","inedT6KkbLSDwZvfd","sWLLdG6DWJEy3CH7n","dhj9dhiwhq3DX6W8z","yLLkWMDbC9ZNKbjDG","P3Yt66Wh5g7SbkKuT","brXr7PJ2W4Na2EW2q","45mNHCMaZgsvfDXbw","7izSBpNJSEXSAbaFh","pfoZSkZ389gnz5nZm","jfG6vdJZCwTQmG7kb","sGnPTfjE5JthAStqg","gvA4j8pGYG4xtaTkw","PZtsoaoSLpKjjbMqM","jnDibtfvWNHLucf4D","GrtbTAPfkJa4D6jjH","zEWJBFFMvQ835nq6h","64FdKLwmea8MCLWkE","Dx9LoqsEh3gHNJMDk","FMkQtPvzsriQAow5q","XuLG6M7sHuenYWbfC","PGv9THs68ArPur7yP","NcGBmDEe5qXB7dFBF","tEDXpFgsHsm5T8sWz","7gsehrZnvXo2YGiT7","x4n4jcoDP7xh5LWLq","boBZkTqPdboX5u7g9","CJw2tNHaEimx6nwNy","CcC8MocynqKPmMPwL","Rrt7uPJ8r3sYuLrXo","rwjv8bZfSuE9ZAigH","khYYedgupgrHonWNc","wrkEnGrTTrM2mnmGa","f9s7pHub6hbsX7YKT","YduZEfz8usGbJXN4x","55SHk8kh9dDvaDTCC","SFG9Cm7mf5eP4juKs","eLRSCC7r4KinuxqZX","oW6mbA3XHzcfJTwNq","kWMkDoy3izRTobZFe","LtsJLfnP4YwhGdaCf","w9kwayt5SWqBQe8Nx","h5CGM5qwivGk2f5T9","iPGpENE4ARKbzzQmt","PQ3nutgxfTgvq69Xt","3zZjF3YKJ257x79mu","9Qwignbzu4ddXLTsT","aiCtrN9EF2FjKz5sv","JcpwEKbmNHdwhpq5n","idipkijjz5PoxAwju","F7RgpHHDpZYBjZGia","xWTSHJASRaLABgHWc","Fg8dtE8HHkDoiGcwt","zPJE7MDtL25RpN7Cc","qqhdj3W3vSfB5E9ss","9SE67uz98kh6x2CxR","gR6H3egpRPNYnoTrA","qPoaA5ZSedivA4xJa","H6L7fuEN9qXDanQ6W","gfexKxsBDM6v2sCMo","7uJnA3XDpTgemRH2c","stb3Jjumzhv49zCEb","XjMkPyaPYTf7LrKiT","XuyRMxky6G8gq7a69","huRxRzwcvwTzvtEPY","8bWbNwiSGbGi9jXPS","sq3WkpyqGANT7hGRP","AyfDnnAdjG7HHeD3d","WmfapdnpFfHWzkdXY","8rYxw9xZfwy86jkpG","zFhhDCxz87yKwqYQf","doiMq8aH2yiZaCJsT","MQzbaHoiQutiHkx2M","ra9Pt2JkEDnKW4jsc","9YDk52NPrfq7nqLvd","KTEciTeFwL2tTujZk","6bSjRezJDxR2omHKE","r5H6YCmnn8DMtBtxt","JbcWQCxKWn3y49bNB","R4FX6wDmppvZ2JqpB","9vnWFwng8QzEnBT8z","XCtFBWoMeFwG8myYh","6uwLq8kofo4Tzxfe2","G993PFTwqqdQv4eTg","DWgWbXRfXLGHPgZJM","K7wtTqTEoKXC9Kb24","hmai5Lru5kWXpH7Ju","w4jjwDPa853m9P4ag","xvAkpCSdqgtYhEceo","6vMBpZtoRw4ia2JrK","Wzjjynmp8gMmdX6dt","CsN6WxwDnPzxAFhps","CLXkgEerPi9MpJCem","BKjJJH2cRpJcAnP7T","qXtbBAxmFkAQLQEJE","jES7mcPvKpfmzMTgC","D7epkkJb3CqDTYgX9","FpcgSoJDNNEZ4BQfj","mF8dkhZF9hAuLHXaD","camG6t6SxzfasF42i","HALKHS4pMbfghxsjD","HDXLTFnSndhpLj2XZ","fgYQjTktBmNZvMqce","fwNskn4dosKng9BCB","B5auLtDfQrvwEkw4Q","z7YvA5osMotdL5F4w","Hoh6umyMWSqzPGMJZ","vHSrtmr3EBohcw6t8","nsCwdYJEpmW5Hw5Xm","LKAXgTen4Xbqb8eZY","22GrdspteQc8EonMn","TSaJ9Zcvc3KWh3bjX","sJK6HN5vTPPnuuNgQ","mh3xapTix6fFtd3xM","JBnaLpsrYXLXjFocu","uR8c2NPp4bWHQ5u45","d4YGxMpzmvxknHfbe","wcNEXDHowiWkRxDNv","scNCmwaduCgJmCBYh","LsXtcLyzyfGg3gT5R","McN9BNtNcbYNfdCB5","4tzEAgdbNTwB6nKyL","sCFGEhwcB8MX3FQf5","G4uMdBzgDsxMsTNmr","34Gkqus9vusXRevR8","7MCqRnZzvszsxgtJi","HXxHcRCxR4oHrAsEr","cmrtpfG7hGEL9Zh9f","oHk9T3jbx2J5zJ39P","sYt3ZCrBq2QAf3rak","r8stxYL29NF9w53am","zymnWfGwf6BdDt64c","yyDrMYBfvYtKbmPmm","4gevjbK77NQS6hybY","jnjjzkH8Fdzg4D6EK","XKfQF73YnyMRiRf9a","gYfgWSxCpFdk2cZfE","CQsEwAyJP6NYvKZw6","JiLcxpWzCrnwkndsT","gpk8dARHBi7Mkmzt9","GrbeyZzp6NwzSWpds","9MZdwQ7u53oaRiBYX","gFyJgnu5vAbzELBM8","ouQNu3hhfKLBRuwR7","m5AH78nscsGjMbBwv","oKYWbXioKaANATxKY","cq5x4XDnLcBrYbb66","KjdP2WjWng6skwbY7","wfpdejMWog4vEDLDg","7F5jo5LD9FD7DpxCX","kDjKF2yFhFEWe4hgC","pWi5WmvDcN4Hn7Bo6","NGc3Yjecg9pDMznWq","xxvKhjpcTAJwvtbWM","DJnvFsZ2maKxPi7v7","zo9zKcz47JxDErFzQ","fyZBtNB3Ki3fM4a6Y","H4kadKrC2xLK24udn","BxersHYN2qcFoonwg","Ck5cgNS2Eozc8mBeJ","wr9dH2GjztvCz6pYX","EzAt4SbtQcXtDNhHK","syeBtmGoKvjJTH6sH","eWqFy8wESHbxNod7i","8cWMX6L8St8k9pPRC","jP583FwKepjiWbeoQ","rMfpnorsMoRwyn4iP","TKk7rShf9d5ePN7vR","fNJvYD6XqnX82i4jA","r8aAqSBeeeMNRtiYK","Gh2qQHrCg3teQen3c","3GAnfeG9KmsbsWeTj","JKgGvJCzNoBQss2bq","JjGs6mDZxeCWkg3ii","AzKx6EjaoaMuk595v","duAkuSqJhGDcfMaTA","pXLqpguHJzxSjDdx7","FbJYEn6eWA5JnGeGP","8GiTowD6XqTNzgCz7","qfDgEreMoSEtmLTws","96N8BT9tJvybLbn5z","SCs4KpcShb23hcTni","bDMoMvw2PYgijqZCC","nqwzrpkPvviLHWXaE","YuZXRxWSqaCoZHEXr","6YYmkpumigAmh3efu","SgszmZwrDHwG3qurr","9EyzaH3jzH3PyQtM5","eR7Su77N2nK3e5YRZ","GSBCw94DsxLgDat6r","cpdsMuAHSWhWnKdog","avvXAvGhhGgkJDDso","KnPN7ett8RszE79PH","ptmmK9PWgYTuWToaZ","XNhfw5Bqsi4SGNNBk","PKy8NuNPknenkDY74","3yqf6zJSwBF34Zbys","YpyW97jRbtvBAncAr","LwcKYR8bykM6vDHyo","H6hMugfY3tDQGfqYL","iyRpsScBa6y4rduEt","mLuQfS7gmfr4nwTdv","TrvkWBwYvvJjSqSCj","yXHcqrCpiHC5tDuEc","HKfBeWN8ufNdFgzG6","P8yeoeJ2bwmnD93mZ","kxW6q5YdTGWh5sWby","ksatPnddyZjHwZWwG","st7DiQP23YQSxumCt","tE7y8FZe7wSSzoRaS","L4HQ3gnSrBETRdcGu","eqxqgFxymP8hXDTt5","uKWXktrR7KpbgZAs4","h4vWsBBjASgiQ2pn6","DXBziiT2RFLcmLY9J","k42G2aaNhRNB7hdCJ","XSKQLeQnBupFo7GGC","BnDF5kejzQLqd5cjH","AMmqk74zWmvP8tXEJ","NQQzXpahhkb6f6ZCe","Tk5ovpucaqweCu4tu","9WX59u7g2sdKqnjDm","Xht9swezkGZLAxBrd","8c8AZq5hgifmnHKSN","nMNi86hgNjaNnh8iu","s3rAKTkdSHb6Hwwoz","rqnbrJhDKCoZvNGEZ","Ea8pt2dsrS6D4P54F","uN3wjp2K6TEQ2oAML","DAc4iuy4D3EiNBt9B","jqCz2X49FRn5Bgb5b","8hxvfZiqH24oqyr6y","puYfAEJJomeodeSsi","S54HKhxQyttNLATKu","igSPcmvTigCHxWt8x","4esQ684vtR9zcjHgW","yGaw4NqRha8hgx5ny","eHnupDgggBqDqT5eg","k7oxdbNaGATZbtEg3","bbGEiSmNiTpPrFhcQ","Z6dmoLyfBdmo6HEss","QcXuwQvvPkqcKZmXS","7FJRnxbRtT7Sbzizs","oBTkthd7h8sDpkiu2","cmiRk9XtT9Psnd3Yr","G6npMHwgRGSQDKavX","hwxj4gieR7FWNwYfa","yGycR8tFA3JJbvApp","jxy7rBcQink8a7C9b","vQNJrJqebXEWjJfnz","kjmpq33kHg7YpeRYW","FwYMuD2sNcaEpE5on","4rwABGAd9kZG8nf2P","GkXKvkLAcTm5ackCq","TrmMcujGZt5JAtMGg","gBpYo7mt2zNBmtBJd","aNRYQFnMQbA7uu99u","YMokuZdoY9tEDHjzv","MG8Yhsxqu9JY4xRPr","zEvqFtT4AtTztfYC4","fzeoYhKoYPR3tDYFT","8npC4KRcAJtGdErTq","AYbhqi65SWzHzy7Xx","N99KgncSXewWqkzMA","2KacvW34BbXFmDBtQ","tSgcorrgBnrCH8nL3","NHuLAS3oKZWr2X9hP","9hR2RmpJmxT8dyPo4","fwSDKTZvraSdmwFsj","Cf2zBkoocqcjnrNFD","MPj7t2w3nk4s9EYYh","TTPux7QFBpKxZtMKE","shcSdHGPhnLQkpSbX","M4w2rdYgCKctbADMn","hMjFMSQZb4swKugfv","mkrvsNi8cYGSjGqkh","DXcezGmnBcAYL2Y2u","Aq8BQMXRZX3BoFd4c","FoJSa8mgLPT83g9e8","Xt85tj6GQJCuuXT68","JAAHjm4iZ2j5Exfo2","sAiHxHkQrsYsRpKFP","6phFYpNQH9SmWL9Jt","Rkxj7TFxhbm59AKJh","rNFzvii8LtCL5joJo","Hw26MrLuhGWH7kBLm","Zvu6ZP47dMLHXMiG3","HByDKLLdaWEcA2QQD","7qhtuQLCCvmwCPfXK","FgjcHiWvADgsocE34","Lp4Q9kSGsJHLfoHX3","3xF66BNSC5caZuKyC","BseaxjsiDPKvGtDrm","Q924oPJzK92FifuFg","oJwJzeZ6ar2Hr7KAX","H7Rs8HqrwBDque8Ru","gEKHX8WKrXGM4roRC","FKB7iEergZaC7PvQf","suxvE2ddnYMPJN9HD","iETtCZcfmRyHp69w4","mz3hwS4c9bc9EHAm9","KFLdfuw35qkgjzWer","RApxEu3A4GnvGoEe2","XLbDQL2qYi9FDozvL","p4XpZWcQksSiCPG72","mB95aqTSJLNR9YyjH","2NaAhMPGub8F2Pbr7","BbM47qBPzdSRruY4z","dYnHLWMXCYdm9xu5j","qHpazCw3ryvBojGSa","wyYubb3eC5FS365nk","wmjPGE8TZKNLSKzm4","CBWSDdzjqfnexBurB","gBnSRErajRtvhMnDr","BfBF6T6HA82zBxPrv","dbDHEQyKqnMDDqq2G","doPejjd84w8BmERqj","PT8vSxsusqWuN7JXp","dKxX76SCfCvceJXHv","DSzpr8Y9299jdDLc9","hnLutdvjC8kPScPAj","vit9oWGj6WgXpRhce","CsKboswS3z5iaiutC","kjQXzkTGuixoJtQnq","RgJicDmXHDxcJ9Fsw","L6iFpR9ZyTmzHvYci","Z5ZBPEgufmDsm7LAv","PRAyQaiMWg2La7XQy","x6Kv7nxKHfLGtPJej","3pjv6uDvY9sqmsnvY","Aet2mbnK7GDDfrEQu","scL68JtnSr3iakuc6","3SG4WbNPoP8fsuZgs","XfpJ6WQBDcEcC8Mu4","iprqfLaDLCGoJFeiZ","frApEhpyKQAcFvbXJ","znBJwbuT3f5eWgM4E","cR7Zfrc4BtnFes46y","hbmsW2k9DxED5Z4eJ","SzecSPYxqRa5GCaSF","hxaq9MCaSrwWPmooZ","FSmPtu7foXwNYpWiB","WQWhXzALcrzrJtqRh","jYNT3Qihn2aAYaaPb","gebzzEwn2TaA6rGkc","WhHFvzFsYfMxgYCdo","tjxgbovwc5Ft7wrtc","2brqzQWfmNx5Agdrx","QaDwBio8MLqRvTREH","Jko7pt7MwwTBrfG3A","A9tJFJY7DsGTFKKkh","Wnqua6eQkewL3bqsF","DJB82jKwgJE5NsWgT","5b6YcFbEBCZbX6YSK","zk6RK3xFaDeJHsoym","FQqcejhNWGG8vHDch","srge9MCLHSiwzaX6r","DJRe5obJd7kqCkvRr","D8ds9idKWbwzCseCh","hTMFt3h7QqA2qecn7","9LXxgXySTFsnookkw","CHtwDXy63BsLkQx4n","u5RLu5F3zKTB3Qjnu","4tke3ibK9zfnvh9sE","2WngsveoLhFubuLMH","ADwayvunaJqBLzawa","NG6FrXgmqPd5Wn3mh","Ww5xKq5brC4xAJY7o","HL6x8zHo9BkuK3tic","PKBXczqhry7iK3Ruw","oBBzqkZwkxDvsKBGB","HuFZJkGptWDtRbkWs","iQWk5jYeDg5ACCmpx","RdpqsQ6xbHzyckW9m","sizjfDgCgAsuLJQmm","X3p8mxE5dHYDZNxCm","wZGpoZgDANdkwTrwt","uAc7bWgpEhrGwFcv7","3nDR23ksSQJ98WNDm","sMsvcdxbK2Xqx8EHr","evYFijNMdjfbPaCho","Psp8ZpYLCDJjshpRb","Zupr296Zy74wpihXT","68dHanLWsS6SEyZp9","x9FNKTEt68Rz6wQ6P","DWHkxqX4t79aThDkg","xLm9mgJRPvmPGpo7Q","6LzKRP88mhL9NKNrS","XYDsYSbBjqgPAgcoQ","eRohP4gbxuBuhqTbe","Wpf3Gsa8A89mmjkk8","PfcQguFpT8CDHcozj","XPwEptSSFRCnfHqFk","pohTfSGsNQZYbGpCy","zcPLNNw4wgBX5k8kQ","2meuc3kPRkBcRpj3R","bzhGBHrGrFfQss4Df","2269iGRnWruLHsZ5r","kj37Hzb2MsALwLqWt","Qz9GvoPbnFwGrHHQB","pJJdcZgB6mPNWoSWr","dtmmP4YdJEfK9y4Rc","QPqm5aj2meRmE7kR8","2oybbEw697CQgcRE5","TYTEJxzeK3jBMq2TZ","K4eDzqS2rbcBDsCLZ","FcRt3xAF4ynojfj6G","gMXsyhPiEJbGerF6F","9sguwESkteCgqFMbj","mvPfao35Moah8py46","kuDKtwwbsksAW4BG2","pL56xPoniLvtMDQ4J","ENBzEkoyvdakz4w5d","wM4bcDxEh75NDkhjo","YAkpzvjC768Jm2TYb","ExssKjAaXEEYcnzPd","n3LAgnHg6ashQK3fF","GMCs73dCPTL8dWYGq","8gapy2nLy4wysXSGL","dgFcJtHaYfaoByAK9","HhWhaSzQr6xmBki8F","CpvyhFy9WvCNsifkY","aan3jPEEwPhrcGZjj","mhA4vkeaRn9cpxkag","iA25AvZqAr6G8mAXR","C4tR3BEpuWviT7Sje","FghubkDy6Dp6mnxk7","RKz7pc6snBttndxXz","jiJquD34sa9Lyo5wc","c8EeJtqnsKyXdLtc5","ZGGGBR9sDgtLgMDaA","uM6mENiJi2pNPpdnC","o9dnstYoc7cwpgdhg","YSWa8rYeD3aDaofSP","pC47ZTsPNAkjavkXs","QtyKq4BDyuJ3tysoK","bYrF8rXFYwPqnfxTp","KbyRPCAsWv5GtfrbG","c2RzFadrxkzyRAFXa","9ZodFr54FtpLThHZh","xmoYza9vgcRvWD5PA","sbb9bZgojmEa7Yjrc","6yTShbTdtATxKonY5","BHYBdijDcAKQ6e45Z","qGEqpy7J78bZh3awf","KJbQyFbXiiYDDWbaS","PYtus925Gcg7cqTEq","yTvBSFrXhZfL8vr5a","Aud7CL7uhz55KL8jG","bXTNKjsD4y3fabhwR","AmNjHo8xXMKnZEWRS","CHD5m9fnosr7L3dto","MN4NRkMw7ggt9587K","CDXDnruBJe23rpdfC","y5GftLezdozEHdXkL","d6yNW5T6J9rtnGizc","pT48swb8LoPowiAzR","27AWRKbKyXuzQoaSk","vNHf7dx5QZA4SLSZb","KwbJFexa4MEdhJbs4","mja6jZ6k9gAwki9Nu","fW9n8bEuMpLwkxCx6","muXfZr5EYCfZqLmsb","5PBWgHiCiiJHjPRSn","PAYMMgPi2L3MPP967","RaxaXBNmStYe289gC","DMxe4XKXnjyMEAAGw","xF7gBJYsy6qenmmCS","gMszBSAX23uqYhytR","HbXXd2givHBBLxr3d","Z5wF8mdonsM2AuGgt","utySCY9nJt9xGYGGQ","gCz7cB6JG66EhweSS","krHDNc7cDvfEL8z9a","aNAFrGbzXddQBMDqh","sksP9Lkv9wqaAhXsA","p3s8RvkcyTwzu27ps","8ccTZ9ZxpJrvnxt4F","p7WXmG6Fbo3eaSwm3","CPBmbgYZpsGqkiz2R","yDRX2fdkm3HqfTpav","WbLAA8qZQNdbRgKte","75dnjiD8kv2khe9eQ","JZZENevaLzLLeC3zn","MgFDzAfCku9MSDLuw","PQtEqmyqHWDa2vf5H","zbqLuTgTCu365MNu9","P3uavjFmZD5RopJKk","8gqrbnW758qjHFTrH","pZaPhGg2hmmPwByHc","4hLcbXaqudM9wSeor","WxW6Gc6f2z3mzmqKs","j9HoG56Y6KuopSzdn","GhFoAxG49RXFzze5Y","rD57ysqawarsbry6v","LCfaLXcWnk8pujnX4","tAXrD8Y6hcJ8dt6Nt","af9MjBqF2hgu3EN6r","FRRb6Gqem8k69ocbi","LbyxFk8JmPKPAQBvL","PHmYhE4sKnwzYgvkh","fZJRxYLtNNzpbWZAA","kgmkdf3C7EkDX7dnT","Gs29k3beHiqWFZqnn","MMAK6eeMCH3JGuqeZ","cdB5f2adKoLGW8Ytc","5e49dHLDJoDpeXGnh","Ccsx339LE9Jhoii9K","PHnMDhfiadQt6Gj23","Jo89KvfAs9z7owoZp","fri4HdDkwhayCYFaE","tD9zEiHfkvakpnNam","xggxWfyzZmnz7hydm","JgBBuDf5uZHmpEMDs","vbcjYg6h3XzuqaaN8","hRohhttbtpY3SHmmD","6KzFwcDy7hsCkzJKY","F2DZXsMdhGyX4FPAd","esRZaPXSHgWzyB2NL","AqsjZwxHNqH64C2b6","4psQW7vRwt7PE5Pnj","voLHQgNncnjjgAPH7","aaHDA4X6cTzFrvuSX","LHtMNz7ua8zu4rSZr","zjMKpSB2Xccn9qi5t","BAzCGCys4BkzGDCWR","goC9qv4PWf2cjfnbm","Z2CuyKtkCmWGQtAEh","c3iQryHA4tnAvPZEv","vwLxd6hhFvPbvKmBH","Js34Ez9nrDeJCTYQL","fJvjin8ETkzhFdadC","W59Nb72sYJhMJKGB8","xiPMaYGTm2xfsB8WF","oPEWyxJjRo4oKHzMu","PjfsbKrK5MnJDDoFr","sBBGxdvhKcppQWZZE","vwM7hnT9ysE3suwfk","BzYmJYECAc3xyCTt6","uiyWHaTrz3ML7JqDX","vZssZr2wq7YrG3FMa","73QyjLymEak4L8RDC","6vcxuRHzeM99jYcYd","bG4PR9uSsZqHg2gYY","HoQ5Rp7Gs6rebusNP","9iA87EfNKnREgdTJN","QEYWkRoCn4fZxXQAY","kAgJJa3HLSZxsuSrf","FZaDFYbnRoHmde7F6","BNfL58ijGawgpkh9b","4gDbqL3Tods8kHDqs","DwqgLXn5qYC7GqExF","atcJqdhCxTZiJSxo2","zRn6cLtxyNodudzhw","P32AuYu9MqM2ejKKY","K2JBqDeETX2yEgyyZ","3FoMuCLqZggTxoC3S","LcEzxX2FNTKbB6KXS","o5F2p3krzT4JgzqQc","cy3BhHrGinZCp3LXE","zsG9yKcriht2doRhM","WYmmC3W6ZNhEgAmWG","EL4HNa92Z95FKL9R2","EKu66pFKDHFYPaZ6q","Pa5NqtxHBkGuCh98G","JKj5Krff5oKMb8TjT","vwt3wKXWaCvqZyF74","4basF9w9jaPZpoC8R","Bfq6ncLfYdtCb6sat","jDQm7YJxLnMnSNHFu","FDJnZt8Ks2djouQTZ","f3o9ydY7iPjFF2fyk","KnQs55tjxWopCzKsk","Ww2dxwWpSfkQB4NZb","ZawRiFR8ytvpqfBPX","ZGzDNfNCXzfx6hYAH","rFjhz5Ks685xHbMXW","Mrz2srZWc7EzbADSo","B4DuwmtqF3HhNwvua","zQKgKjecvR4W7oJw5","BSpdshJWGAW6TuNzZ","JHcTP4Ad8QAmRTCZm","GGn8MBiY8Xz6NdNdH","hQysqfSEzciRazx8k","AtfQFj8umeyBBkkxa","r99tazGiLgzqFX7ka","uFYQaGCRwt3wKtyZP","BFamedwSgRdGGKXQQ","teaxCFgtmCQ3E9fy8","ka8eveZpT7hXLhRTM","euJm4RwkAptZnP89i","LLRtjkvh9AackwuNB","yPLr2tnXbiFXkMWvk","ervaGwJ2ZcwqfCcLx","4AHXDwcGab5PhKhHT","NuueGqPZdotjMQKLu","qjSHfbjmSyMnGR9DS","xtzvtJBNofk4FPAtt","SkcM4hwgH3AP6iqjs","Br4xDbYu4Frwrb64a","HvcZmKS43SLCbJvRb","BEtzRE2M5m9YEAQpX","EhEZoTFzys9EDmEXn","bmoQ2wy7Nd7EiJdpg","pYcFPMBtQveAjcSfH","zb3hWt99i9Fm93KPq","W9rJv26sxs4g2B9bL","Dod9AWz8Rp4Svdpof","hQHuXuRGZxxWXaPgg","zB3ukZJqt3pQDw9jz","KheBaeW8Pi7LwewoF","Ek7M3xGAoXDdQkPZQ","guDcrPqLsnhEjrPZj","7XbcDaeigMaxW43EB","ttGbpJQ8shBi8hDhh","wJnm5cBiZGmKn595f","puhPJimawPuNZ5wAR","eoHbneGvqDu25Hasc","gHgs2e2J5azvGFatb","x5ASTMPKPowLKpLpZ","EhAbh2pQoAXkm9yor","jfq2BH5kfQqu2vYv3","Mf2MCkYgSZSJRz5nM","mXgsd5o9uuYaQKHMz","YM6Qgiz9RT7EmeFpp","PcfHSSAMNFMgdqFyB","uX3HjXo6BWos3Zgy5","nzmCvRvPm4xJuqztv","CMt3ijXYuCynhPWXa","Ndtb22KYBxpBsagpj","yFJ7vCjefBxnTchmG","SQ9cZtfrzDJmw9A2m","PJLABqQ962hZEqhdB","HmfxSWnqnK265GEFM","i3BTagvt3HbPMx6PN","ZEgQGAjQm5rTAnGuM","ctpkTaqTKbmm6uRgC","qEweugBipR5P2cMyK","xnPFYBuaGhpq869mY","YtvZxRpZjcFNwJecS","ido3qfidfDJbigTEQ","85J8hjEn48FicYfvp","N6vZEnCn6A95Xn39p","tJQsxD34maYw2g5E4","96TBXaHwLbFyeAxrg","ixZLTmFfnKRbaStA5","2x7fwbwb35sG8QmEt","oaqKjHbgsoqEXBMZ2","t2NN6JwMFaqANuLqH","J9pNx22bj5RuiRjAj","AN2cBr6xKWCB8dRQG","G5eMM3Wp3hbCuKKPE","y5jAuKqkShdjMNZab","vADtvr9iDeYsCDfxd","x4GmqcwjFTnWeRiud","5ntgky9ShzKKWu7us","z8usYeKX7dtTWsEnk","3S4nyoNEEuvNsbXt8","EEv9JeuY5xfuDDSgF","ASpGaS3HGEQCbJbjS","AXXaXJvf7WcTessog","QL7J9wmS6W2fWpofd","osYFcQtxnRKB4F4HA","MajyZJrsf8fAywWgY","bvqC4Ci7rXq4sN9df","GctJD5oCDRxCspEaZ","A9NxPTwbw6r6Awuwt","dKTh9Td3KaJ8QW6gw","oTX2LXHqXqYg2u4g6","LuXb6CZG4x7pDRBP8","hamma4XgeNrsvAJv5","BfTW9jmDzujYkhjAb","DoHcgTvyxdorAMquE","EbFABnst8LsidYs5Y","Sdx6A6yLByRRs8iLY","qbHLGo5vu8HD3JqEM","48WeP7oTec3kBEada","LgavAYtzFQZKg95WC","5QpufhoH2ASnppsjs","Kz9zMgWB5C27Pmdkh","qy5dF7bQcFjSKaW58","wkuDgmpxwbu2M2k3w","JcpzFpPBSmzuksmWM","zMxrkFrB6ka4Lb7fM","PX7AdEkpuChKqrNoj","ui6mDLdqXkaXiDMJ5","uXn3LyA8eNqpvdoZw","FwiPfF8Woe5JrzqEu","hzuSDMx7pd2uxFc5w","mHqQxwKuzZS69CXX5","yKXKcyoBzWtECzXrE","zHS4FJhByRjqsuH4o","a5JAiTdytou3Jg749","HEn2qiMxk5BggN83J","tYAvXXgSwHCzNTK8f","WXvt8bxYnwBYpy9oT","kLR5H4pbaBjzZxLv6","CtXaFo3hikGMWW4C9","4DBBQkEQvNEWafkek","qwdupkFd6kmeZHYXy","EHbJ69JDs4suovpLw","w5F4w8tNZc6LcBKRP","xqkGmfikqapbJ2YMj","yRAo2KEGWenKYZG9K","scwoBEju75C45W5n3","qJgz2YapqpFEDTLKn","aSQy7yHj6nPD44RNo","Ltey8BS83qSkd9M3u","9Yc7Pp7szcjPgPsjf","hN2aRnu798yas5b2k","ERWeEA8op6s6tYCKy","yJfBzcDL9fBHJfZ6P","BZ6XaCwN4QGgH9CxF","3nMpdmt8LrzxQnkGp","TNHQLZK5pHbxdnz4e","F6ZTtBXn2cFLmWPdM","neQ7eXuaXpiYw7SBy","k2SNji3jXaLGhBeYP","WsSybGTqpBoHpXJyQ","jtMXj24Masrnq3SpS","jqTeghCJ2anMHPPjG","B7P97C27rvHPz3s9B","uK6sQCNMw8WKzJeCQ","hurF9uFGkJYXzpHEE","xEHy9oivifjgFbnvc","33KewgYhNSxFpbpXg","c5GHf2kMGhA4Tsj4g","dC7mP5nSwvpL65Qu5","hpjou9ZnLZkSJR7sd","bshZiaLefDejvPKuS","AvjbBjAAbKBk73v5F","XqvnWFtRD2keJdwjX","KJ9MFBPwXGwNpadf2","37sHjeisS9uJufi4u","5iZTwGHv2tNfFmeDa","gziZACDg6EBpGZbJe","RYcoJdvmoBbi5Nax7","9o3QBg2xJXcRCxGjS","vs3kzjLhbdKsndnBy","bZ2w99pEAeAbKnKqo","bjjbp5i5G8bekJuxv","vwqLfDfsHmiavFAGP","Yp2vYb4zHXEeoTkJc","z6QQJbtpkEAX3Aojj","ubPAo3zGeJNqtZDqT","pfibDHFZ3waBo6pAc","cumc876woKaZLmQs5","Ty2tjPwv8uyPK9vrz","ZiQqsgGX6a42Sfpii","ybYBCK9D7MZCcdArB","pNcFYZnPdXyL2RfgA","rEBXN3x6kXgD4pLxs","no5jDTut5Byjqb4j5","qCsxiojX7BSLuuBgQ","uyBeAN5jPEATMqKkX","aHaqgTNnFzD7NGLMx","bQ6zpf6buWgP939ov","mkbGjzxD8d8XqKHzA","CKpByWmsZ8WmpHtYa","midXmMb2Xg37F2Kgn","reitXJgJXFzKpdKyd","LFNXiQuGrar3duBzJ","KcvJXhKqx4itFNWty","RWu8eZqbwgB9zaerh","EFQ3F6kmt4WHXRqik","FfPukic3Qskd9ZAkk","A2Qam9Bd9xpbb2wLQ","t9svvNPNmFf5Qa3TA","n5TqCuizyJDfAPjkr","Kbm6QnJv9dgWsPHQP","gFMH3Cqw4XxwL69iy","Kyc5dFDzBg4WccrbK","RWo4LwFzpHNQCTcYt","vbWBJGWyWyKyoxLBe","PsEppdvgRisz5xAHG","tscc3e5eujrsEeFN4","GG2rtBReAm6o3mrtn","E4cKD9iTWHaE7f3AJ","yCWPkLi8wJvewPbEp","AcKRB8wDpdaN6v6ru","LbrPTJ4fmABEdEnLf","rtM3jFaoQn3eoAiPh","eDicGjD9yte6FLSie","xg3hXCYQPJkwHyik2","bJ2haLkcGeLtTWaD5","PBRWb2Em5SNeWYwwB","7hFeMWC6Y5eaSixbD","aMHq4mA2PHSM2TMoH","wpZJvgQ4HvJE2bysy","2brqzQWfmNx5Agdrx","GLMFmFvXGyAcG25ni","NLBbCQeNLFvBJJkrt","bYrF8rXFYwPqnfxTp","v7c47vjta3mavY3QC","3MvaoZbGPxtRFCijw","TappK5n3kZmQzWEWD","tSemJckYr29Gnxod2","WdkLDpBGMCWhfByAY","EctieqKwDQcQHhqZy","hNqte2p48nqKux3wS","qw3Z79HELMsmLkL9F","zwDz9pgT43fRczkB4","Fafzj3wMvoCW4WjeF","kxW6q5YdTGWh5sWby","G5eMM3Wp3hbCuKKPE","kSiT2XjfTnDHKx44W","DSzpr8Y9299jdDLc9","wZGpoZgDANdkwTrwt","qajfiXo5qRThZQG7s","rRzZzBBQ36CrqhZTY","aP36QcAsxyuEispq6","TxcRbCYHaeL59aY7E","MFNJ7kQttCuCXHp8P","PQ3nutgxfTgvq69Xt","JJFphYfMsdFMuprBy","ythFNoiAotjvuEGkg","GZSzMqr8hAB2dR8pk","BBQ5HEnL3ShefQxEj","fzeoYhKoYPR3tDYFT","bXuAXCbzw9hsJSuEN","mbCccXJuuRBZdXdpH","m7THsgXyxxiEXgyHv","xtHd6sfdr2bZHa6Pb","pfaTqpWFghfrbvzaD","u8GMcpEN9Z6aQiCvp","gBpYo7mt2zNBmtBJd","rkpDX7j7va6c8Q7cZ","NGkBfd8LTqcpbQn5Z","GEPX7jgLMB8vR2qaK"]},"locale":"en-US","mapbox":{"apiKey":"pk.eyJ1IjoiaGFicnlrYSIsImEiOiJjaWxvcnhidzgwOGlodHJrbmJ2bmVmdjRtIn0.inr-_5rWOOslGQxY8iDFOA"},"petrov":{"afterTime":1727400080403,"beforeTime":1727376805595,"petrovPostId":"6LJ6xcHEjKF9zWKzs","petrovServerUrl":"https://forum.effectivealtruism.org/graphql","petrovGamePostId":"KTEciTeFwL2tTujZk"},"reacts":{"addNewReactKarmaThreshold":10,"downvoteExistingReactKarmaThreshold":20,"addNameToExistingReactKarmaThreshold":5},"stripe":{"publicKey":"pk_live_51HtKAwA2QvoATZCZiy9f2nc6hA52YS1BE81cFu9FEV1IKar0Bwx6hIpxxxYHnhaxO9KM7kRYofZId3sUUI7Q0NeO00tGni3Wza"},"algolia":{"appId":"fakeAppId","searchKey":"fakeSearchKey","indexPrefix":"test_"},"llmChat":{"userIds":["McgHKH6MMYSnPwQcm","6Fx2vQtkYSZkaCvAg","MEu8MdhruX5jfGsFQ","YaNNYeR5HjKLDBefQ","hBEAsEpoNHaZfefxR","NFmcwmaFeTWfgrvBN","ZnpELPxzzD2CiigNy","Q7NW4XaWQmfPfdcFj","NXeHNNSFHGESrYkPv","QDNJ93vrjoaRBesk2","iMBN2523tmh4Yicc3","5iPRfSnjako6iM6LG","aBHfQ4C5fSM4TPyTn","n4M37rPXGyL6p8ivK","e9ToWWzhwWp5GSE7P","TCjNiBLBPyhZq5BuM","XLwKyCK7JmC292ZCC","S3ydcLKdejjkodNut","ENgxBL95Sc7MRwYty","KCExMGwS2ETzN3Ksr","XGEcH5rmq4yGvD82A","YFiFbXgjBpDKZT93g","dZMo8p7fGCgPMfdfD","Pdca6FNZBrXj9z28n","LHbu27FubhwFv8ZJt","gYxdDBQ3AZbde8HgZ","5JqkvjdNcxwN8D86a","6c2KCEXTGogBZ9KoE","haTrhurXNmNN8EiXc"]},"logoUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1498011194/LessWrong_Logo_skglnw.svg","ckEditor":{"uploadUrl":"https://39669.cke-cs.com/easyimage/upload/","webSocketUrl":"39669.cke-cs.com/ws"},"recombee":{"enabled":true},"hasEvents":true,"logRocket":{"apiKey":"mtnxzn/lesswrong","sampleDensity":5},"reCaptcha":{"apiKey":"6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc"},"siteImage":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg","cloudinary":{"cloudName":"lesswrong-2-0","uploadPresetBanner":"navcjwf7","uploadPresetGridImage":"tz0mgw2s","uploadPresetSocialPreview":"nn5tppry"},"googleMaps":{"apiKey":"AIzaSyA3C48rl26gynG3qIuNuS-3Bh_Zz9jFXkY"},"adminAccount":{"email":"team@lesswrong.com","username":"LessWrong"},"annualReview":{"end":"2024-02-01T08:00:00Z","start":"2023-12-04T00:10:00Z","reviewPhaseEnd":"2024-01-15T08:00:00Z","votingPhaseEnd":"2024-02-01T08:00:00Z","nominationPhaseEnd":"2023-12-17T08:00:00Z","votingResultsPostId":"TSaJ9Zcvc3KWh3bjX","announcementPostPath":"/posts/B6CxEApaatATzown6/the-lesswrong-2022-review","reviewWinnerSectionsInfo":{"modeling":{"tag":"World Modeling","order":2,"title":"World","coords":{"leftXPct":0.05,"leftYPct":0,"rightXPct":0.57,"rightYPct":0,"middleXPct":0.31,"middleYPct":0,"leftFlipped":true,"leftWidthPct":0.26,"rightWidthPct":0.26,"middleWidthPct":0.26},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753450/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_15ba02c3-b268-45f1-a780-322bbaa6fc22_eu9l0l.png"},"ai safety":{"tag":"AI","order":5,"title":"Technical AI Safety","coords":{"leftXPct":0.2,"leftYPct":0.3,"rightXPct":0.554,"rightYPct":0.3,"middleXPct":0.467,"middleYPct":0.3,"leftFlipped":false,"leftWidthPct":0.267,"rightFlipped":true,"middleFlipped":false,"rightWidthPct":0.267,"middleWidthPct":0.267},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,fl_progressive,q_auto/v1708570131/lwbot_topographic_watercolor_artwork_of_a_giant_robot_hand_gent_e4e9f305-9611-4787-8768-d7af3d702ed4_ta2ii9.png"},"practical":{"tag":"Practical","order":3,"title":"Practical","coords":{"leftXPct":0.2,"leftYPct":0.05,"rightXPct":0.634,"rightYPct":0.05,"middleXPct":0.417,"middleYPct":0.05,"leftFlipped":false,"leftWidthPct":0.217,"rightWidthPct":0.217,"middleWidthPct":0.217},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708974564/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_4f6449e2-569b-48a3-b878-a400315b3ef0_hqutxe.png"},"ai strategy":{"tag":"AI","order":4,"title":"AI Strategy","coords":{"leftXPct":0,"leftYPct":0,"rightXPct":0.66,"rightYPct":0,"middleXPct":0.33,"middleYPct":0,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":true,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753570/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_8dda30ee-71d6-4b24-80c7-a8499a5b25c6_uacvgk.png"},"rationality":{"tag":"Rationality","order":0,"title":"Rationality","coords":{"leftXPct":0.12,"leftYPct":0,"rightXPct":0.72,"rightYPct":0,"middleXPct":0.42,"middleYPct":0,"leftFlipped":false,"leftWidthPct":0.3,"rightFlipped":true,"rightWidthPct":0.3,"middleWidthPct":0.3},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753260/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_09275054-eb84-43c4-9cfa-4a05e1818c9e_rmov5i.png"},"optimization":{"tag":"World Optimization","order":1,"title":"Optimization","coords":{"leftXPct":0.1,"leftYPct":0.2,"rightXPct":0.7,"rightYPct":0.2,"middleXPct":0.4,"middleYPct":0.2,"leftWidthPct":0.33,"rightFlipped":true,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753382/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_242eda7f-95a9-4c3b-8090-991a1b11286f_xcjhxq.png"}},"reviewWinnerYearGroupsInfo":{"2018":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.1,"rightXPct":0.72,"rightYPct":0.1,"middleXPct":0.34,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008323/ruby37_green_on_white_aquarelle_sketch_by_thomas_schaller_of_ri_7a3fa89a-ac7a-466f-929f-b396cb4d9bd5_p8rh9t.png"},"2019":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.1,"rightXPct":0.72,"rightYPct":0.1,"middleXPct":0.34,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008331/ruby37_blue_on_white_aquarelle_sketch_by_thomas_schaller_of_gre_f421cc99-2bb5-4357-b164-d05c2f4fe84e_aib1co.png"},"2020":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.01,"rightXPct":0.72,"rightYPct":0.01,"middleXPct":0.34,"middleYPct":0.01,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008346/ruby37_aquarelle_sketch_of_futuristic_landscape_by_thomas_schal_f07d5805-9fb0-4dcc-9295-7f063624e28c_slcokh.png"},"2021":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.1,"rightXPct":0.545,"rightYPct":0.1,"middleXPct":0.278,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.267,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.267,"middleWidthPct":0.267},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/a_270/q_auto,f_auto/ohabryka_Topographic_aquarelle_book_cover_by_Thomas_W._Schaller_f9c9dbbe-4880-4f12-8ebb-b8f0b900abc1_m4k6dy_734413"},"2022":{"tag":null,"coords":{"leftXPct":0,"leftYPct":0.1,"rightXPct":0.79,"rightYPct":0.1,"middleXPct":0.43,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":true,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008351/ruby37_aquarelle_sketch_of_a_woman_focusing_hard_studying_in_an_2ac568ef-408e-4561-acc8-84c76bb42fba_gwt8uq.png"}},"showReviewOnFrontPageIfActive":true},"googleVertex":{"enabled":true},"intercomAppId":"wtb8z7sj","commentInterval":15,"googleDocImport":{"enabled":true},"moderationEmail":"team@lesswrong.com","timeDecayFactor":1.15,"googleTagManager":{"apiKey":"GTM-TRC765W"},"textReplacements":{"Less Wrong":"Down Bad","Alignment Forum":"Standards Committee","Artificial Intelligence":"Fake News"},"alternateHomePage":false,"gatherTownMessage":"Schelling social hours on Tues 1pm and Thurs 6pm PT","bookDisplaySetting":false,"gardenOpenToPublic":false,"karmaRewarderId100":"iqWr6C3oEB4yWpzn5","legacyRouteAcronym":"lw","maxRenderQueueSize":3,"recommendationsTab":{"manuallyStickiedPostIds":[]},"frontpageScoreBonus":0,"karmaRewarderId1000":"mBBmKWkmw8bgJmGiG","defaultVisibilityTags":[{"tagId":"Ng8Gice9KNkncxqcj","tagName":"Rationality","filterMode":10},{"tagId":"3uE2pXvbcnS9nnZRE","tagName":"World Modeling","filterMode":10}],"enableGoodHeartProject":false,"maxDocumentsPerRequest":5000,"defaultSequenceBannerId":"sequences/vnyzzznenju0hzdv6pqb.jpg","defaultModeratorComments":[{"id":"FfMok764BCY6ScqWm","label":"Option A"},{"id":"yMHoNoYZdk5cKa3wQ","label":"Option B"}],"newUserIconKarmaThreshold":50,"dialogueMatchmakingEnabled":true,"hideUnreviewedAuthorComments":"2023-04-04T18:54:35.895Z","gatherTownUserTrackingIsBroken":true,"postModerationWarningCommentId":"sLay9Tv65zeXaQzR4","commentModerationWarningCommentId":"LbGNE5Ssnvs6MYnLu","performanceMetricLoggingEnax5bled":true,"firstCommentAcknowledgeMessageCommentId":"QgwD7PkQHFp3nfhjj"}</script><script>window.tabId = "5gjdqiijF4aCptf8q"</script><script>window.isReturningVisitor = false</script><script async="" src="eliezer_files/bundle.js"></script><title>Eliezer Yudkowsky - LessWrong</title><meta data-react-helmet="true" name="twitter:image:src" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"><meta data-react-helmet="true" property="og:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"><meta data-react-helmet="true" http-equiv="Accept-CH" content="DPR, Viewport-Width, Width"><meta data-react-helmet="true" charset="utf-8"><meta data-react-helmet="true" name="description" content="Eliezer Yudkowsky's profile on LessWrong — A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1"><meta data-react-helmet="true" name="twitter:description" content="Eliezer Yudkowsky's profile on LessWrong — A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:url" content="https://www.lesswrong.com/users/eliezer_yudkowsky"><meta data-react-helmet="true" property="og:description" content="Eliezer Yudkowsky's profile on LessWrong — A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" http-equiv="delegate-ch" content="sec-ch-dpr https://res.cloudinary.com;"><link data-react-helmet="true" rel="canonical" href="eliezer_files/eliezer_yudkowsky.html"><link data-react-helmet="true" rel="alternate" type="application/rss+xml" href="https://www.lesswrong.com/feed.xml"><script data-react-helmet="true" type="application/ld+json">{"@context":"http://schema.org","@type":"Person","name":"Eliezer Yudkowsky","url":"https://www.lesswrong.com/users/eliezer_yudkowsky","interactionStatistic":[{"@type":"InteractionCounter","interactionType":{"@type":"http://schema.org/LikeAction"},"userInteractionCount":144836},{"@type":"InteractionCounter","interactionType":{"@type":"http://schema.org/WriteAction"},"userInteractionCount":951}],"memberSince":"2009-02-23T21:58:56.739Z"}</script><meta name="twitter:card" content="summary"><script>window.themeOptions = {"name":"default"}</script><style id="jss-insertion-point"></style><style data-jss="" data-meta="MuiSvgIcon">
.MuiSvgIcon-root {
  fill: currentColor;
  width: 1em;
  height: 1em;
  display: inline-block;
  font-size: 24px;
  transition: fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  user-select: none;
  flex-shrink: 0;
}
.MuiSvgIcon-colorPrimary {
  color: #5f9b65;
}
.MuiSvgIcon-colorSecondary {
  color: #5f9b65;
}
.MuiSvgIcon-colorAction {
  color: rgba(0, 0, 0, 0.54);
}
.MuiSvgIcon-colorError {
  color: #bf360c;
}
.MuiSvgIcon-colorDisabled {
  color: rgba(0, 0, 0, 0.26);
}
.MuiSvgIcon-fontSizeInherit {
  font-size: inherit;
}
.MuiSvgIcon-fontSizeSmall {
  font-size: 20px;
}
.MuiSvgIcon-fontSizeLarge {
  font-size: 36px;
}
</style><style data-jss="" data-meta="MuiSvgIcon">
.MuiSvgIcon-root {
  fill: currentColor;
  width: 1em;
  height: 1em;
  display: inline-block;
  font-size: 24px;
  transition: fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  user-select: none;
  flex-shrink: 0;
}
.MuiSvgIcon-colorPrimary {
  color: #5f9b65;
}
.MuiSvgIcon-colorSecondary {
  color: #5f9b65;
}
.MuiSvgIcon-colorAction {
  color: rgba(0, 0, 0, 0.54);
}
.MuiSvgIcon-colorError {
  color: #bf360c;
}
.MuiSvgIcon-colorDisabled {
  color: rgba(0, 0, 0, 0.26);
}
.MuiSvgIcon-fontSizeInherit {
  font-size: inherit;
}
.MuiSvgIcon-fontSizeSmall {
  font-size: 20px;
}
.MuiSvgIcon-fontSizeLarge {
  font-size: 36px;
}
</style><style data-jss="" data-meta="MuiTouchRipple">
.MuiTouchRipple-root {
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: block;
  z-index: 0;
  position: absolute;
  overflow: hidden;
  border-radius: inherit;
  pointer-events: none;
}
.MuiTouchRipple-ripple {
  top: 0;
  left: 0;
  width: 50px;
  height: 50px;
  opacity: 0;
  position: absolute;
}
.MuiTouchRipple-rippleVisible {
  opacity: 0.3;
  transform: scale(1);
  animation: mui-ripple-enter 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-ripplePulsate {
  animation-duration: 200ms;
}
.MuiTouchRipple-child {
  width: 100%;
  height: 100%;
  opacity: 1;
  display: block;
  border-radius: 50%;
  background-color: currentColor;
}
.MuiTouchRipple-childLeaving {
  opacity: 0;
  animation: mui-ripple-exit 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-childPulsate {
  top: 0;
  left: 0;
  position: absolute;
  animation: mui-ripple-pulsate 2500ms cubic-bezier(0.4, 0, 0.2, 1) 200ms infinite;
}
@-moz-keyframes mui-ripple-enter {
  0% {
    opacity: 0.1;
    transform: scale(0);
  }
  100% {
    opacity: 0.3;
    transform: scale(1);
  }
}
@-moz-keyframes mui-ripple-exit {
  0% {
    opacity: 1;
  }
  100% {
    opacity: 0;
  }
}
@-moz-keyframes mui-ripple-pulsate {
  0% {
    transform: scale(1);
  }
  50% {
    transform: scale(0.92);
  }
  100% {
    transform: scale(1);
  }
}
</style><style data-jss="" data-meta="MuiTouchRipple">
.MuiTouchRipple-root {
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: block;
  z-index: 0;
  position: absolute;
  overflow: hidden;
  border-radius: inherit;
  pointer-events: none;
}
.MuiTouchRipple-ripple {
  top: 0;
  left: 0;
  width: 50px;
  height: 50px;
  opacity: 0;
  position: absolute;
}
.MuiTouchRipple-rippleVisible {
  opacity: 0.3;
  transform: scale(1);
  animation: mui-ripple-enter 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-ripplePulsate {
  animation-duration: 200ms;
}
.MuiTouchRipple-child {
  width: 100%;
  height: 100%;
  opacity: 1;
  display: block;
  border-radius: 50%;
  background-color: currentColor;
}
.MuiTouchRipple-childLeaving {
  opacity: 0;
  animation: mui-ripple-exit 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-childPulsate {
  top: 0;
  left: 0;
  position: absolute;
  animation: mui-ripple-pulsate 2500ms cubic-bezier(0.4, 0, 0.2, 1) 200ms infinite;
}
@-moz-keyframes mui-ripple-enter {
  0% {
    opacity: 0.1;
    transform: scale(0);
  }
  100% {
    opacity: 0.3;
    transform: scale(1);
  }
}
@-moz-keyframes mui-ripple-exit {
  0% {
    opacity: 1;
  }
  100% {
    opacity: 0;
  }
}
@-moz-keyframes mui-ripple-pulsate {
  0% {
    transform: scale(1);
  }
  50% {
    transform: scale(0.92);
  }
  100% {
    transform: scale(1);
  }
}
</style><style data-jss="" data-meta="MuiButtonBase">
.MuiButtonBase-root {
  color: inherit;
  border: 0;
  margin: 0;
  cursor: pointer;
  display: inline-flex;
  outline: none;
  padding: 0;
  position: relative;
  align-items: center;
  user-select: none;
  border-radius: 0;
  vertical-align: middle;
  justify-content: center;
  -moz-appearance: none;
  text-decoration: none;
  background-color: transparent;
  -webkit-appearance: none;
  -webkit-tap-highlight-color: transparent;
}
.MuiButtonBase-root::-moz-focus-inner {
  border-style: none;
}
.MuiButtonBase-root.MuiButtonBase-disabled {
  cursor: default;
  pointer-events: none;
}
</style><style data-jss="" data-meta="MuiButtonBase">
.MuiButtonBase-root {
  color: inherit;
  border: 0;
  margin: 0;
  cursor: pointer;
  display: inline-flex;
  outline: none;
  padding: 0;
  position: relative;
  align-items: center;
  user-select: none;
  border-radius: 0;
  vertical-align: middle;
  justify-content: center;
  -moz-appearance: none;
  text-decoration: none;
  background-color: transparent;
  -webkit-appearance: none;
  -webkit-tap-highlight-color: transparent;
}
.MuiButtonBase-root::-moz-focus-inner {
  border-style: none;
}
.MuiButtonBase-root.MuiButtonBase-disabled {
  cursor: default;
  pointer-events: none;
}
</style><style data-jss="" data-meta="MuiButton">
.MuiButton-root {
  color: rgba(0,0,0,0.87);
  padding: 8px 16px;
  font-size: 0.875rem;
  min-width: 64px;
  box-sizing: border-box;
  min-height: 36px;
  transition: background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  font-weight: 500;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  text-transform: uppercase;
}
.MuiButton-root:hover {
  text-decoration: none;
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiButton-root.MuiButton-disabled {
  color: rgba(0, 0, 0, 0.26);
}
@media (hover: none) {
  .MuiButton-root:hover {
    background-color: transparent;
  }
}
.MuiButton-root:hover.MuiButton-disabled {
  background-color: transparent;
}
.MuiButton-label {
  width: 100%;
  display: inherit;
  align-items: inherit;
  justify-content: inherit;
}
.MuiButton-textPrimary {
  color: #5f9b65;
}
.MuiButton-textPrimary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiButton-textPrimary:hover {
    background-color: transparent;
  }
}
.MuiButton-textSecondary {
  color: #5f9b65;
}
.MuiButton-textSecondary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiButton-textSecondary:hover {
    background-color: transparent;
  }
}
.MuiButton-outlined {
  border: 1px solid rgba(0, 0, 0, 0.23);
}
.MuiButton-outlinedPrimary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedPrimary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedPrimary.MuiButton-disabled {
  border: 1px solid rgba(0, 0, 0, 0.26);
}
.MuiButton-outlinedSecondary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedSecondary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedSecondary.MuiButton-disabled {
  border: 1px solid rgba(0, 0, 0, 0.26);
}
.MuiButton-contained {
  color: rgba(0, 0, 0, 0.87);
  box-shadow: 0px 1px 5px 0px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 3px 1px -2px rgba(0,0,0,0.12);
  background-color: #e0e0e0;
}
.MuiButton-contained.MuiButton-focusVisible {
  box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
}
.MuiButton-contained:active {
  box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);
}
.MuiButton-contained.MuiButton-disabled {
  color: rgba(0, 0, 0, 0.26);
  box-shadow: none;
  background-color: rgba(0, 0, 0, 0.12);
}
.MuiButton-contained:hover {
  background-color: #d5d5d5;
}
@media (hover: none) {
  .MuiButton-contained:hover {
    background-color: #e0e0e0;
  }
}
.MuiButton-contained:hover.MuiButton-disabled {
  background-color: rgba(0, 0, 0, 0.12);
}
.MuiButton-containedPrimary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiButton-containedPrimary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedPrimary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-containedSecondary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiButton-containedSecondary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedSecondary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-fab {
  width: 56px;
  height: 56px;
  padding: 0;
  min-width: 0;
  box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
  border-radius: 50%;
}
.MuiButton-fab:active {
  box-shadow: 0px 7px 8px -4px rgba(0,0,0,0.2),0px 12px 17px 2px rgba(0,0,0,0.14),0px 5px 22px 4px rgba(0,0,0,0.12);
}
.MuiButton-extendedFab {
  width: auto;
  height: 48px;
  padding: 0 16px;
  min-width: 48px;
  border-radius: 24px;
}
.MuiButton-colorInherit {
  color: inherit;
}
.MuiButton-mini {
  width: 40px;
  height: 40px;
}
.MuiButton-sizeSmall {
  padding: 7px 8px;
  min-width: 64px;
  font-size: 0.8125rem;
  min-height: 32px;
}
.MuiButton-sizeLarge {
  padding: 8px 24px;
  min-width: 112px;
  font-size: 0.9375rem;
  min-height: 40px;
}
.MuiButton-fullWidth {
  width: 100%;
}
</style><style data-jss="" data-meta="MuiButton">
.MuiButton-root {
  color: rgba(0,0,0,0.87);
  padding: 8px 16px;
  font-size: 0.875rem;
  min-width: 64px;
  box-sizing: border-box;
  min-height: 36px;
  transition: background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  font-weight: 500;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  text-transform: uppercase;
}
.MuiButton-root:hover {
  text-decoration: none;
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiButton-root.MuiButton-disabled {
  color: rgba(0, 0, 0, 0.26);
}
@media (hover: none) {
  .MuiButton-root:hover {
    background-color: transparent;
  }
}
.MuiButton-root:hover.MuiButton-disabled {
  background-color: transparent;
}
.MuiButton-label {
  width: 100%;
  display: inherit;
  align-items: inherit;
  justify-content: inherit;
}
.MuiButton-textPrimary {
  color: #5f9b65;
}
.MuiButton-textPrimary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiButton-textPrimary:hover {
    background-color: transparent;
  }
}
.MuiButton-textSecondary {
  color: #5f9b65;
}
.MuiButton-textSecondary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiButton-textSecondary:hover {
    background-color: transparent;
  }
}
.MuiButton-outlined {
  border: 1px solid rgba(0, 0, 0, 0.23);
}
.MuiButton-outlinedPrimary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedPrimary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedPrimary.MuiButton-disabled {
  border: 1px solid rgba(0, 0, 0, 0.26);
}
.MuiButton-outlinedSecondary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedSecondary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedSecondary.MuiButton-disabled {
  border: 1px solid rgba(0, 0, 0, 0.26);
}
.MuiButton-contained {
  color: rgba(0, 0, 0, 0.87);
  box-shadow: 0px 1px 5px 0px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 3px 1px -2px rgba(0,0,0,0.12);
  background-color: #e0e0e0;
}
.MuiButton-contained.MuiButton-focusVisible {
  box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
}
.MuiButton-contained:active {
  box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);
}
.MuiButton-contained.MuiButton-disabled {
  color: rgba(0, 0, 0, 0.26);
  box-shadow: none;
  background-color: rgba(0, 0, 0, 0.12);
}
.MuiButton-contained:hover {
  background-color: #d5d5d5;
}
@media (hover: none) {
  .MuiButton-contained:hover {
    background-color: #e0e0e0;
  }
}
.MuiButton-contained:hover.MuiButton-disabled {
  background-color: rgba(0, 0, 0, 0.12);
}
.MuiButton-containedPrimary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiButton-containedPrimary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedPrimary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-containedSecondary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiButton-containedSecondary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedSecondary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-fab {
  width: 56px;
  height: 56px;
  padding: 0;
  min-width: 0;
  box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
  border-radius: 50%;
}
.MuiButton-fab:active {
  box-shadow: 0px 7px 8px -4px rgba(0,0,0,0.2),0px 12px 17px 2px rgba(0,0,0,0.14),0px 5px 22px 4px rgba(0,0,0,0.12);
}
.MuiButton-extendedFab {
  width: auto;
  height: 48px;
  padding: 0 16px;
  min-width: 48px;
  border-radius: 24px;
}
.MuiButton-colorInherit {
  color: inherit;
}
.MuiButton-mini {
  width: 40px;
  height: 40px;
}
.MuiButton-sizeSmall {
  padding: 7px 8px;
  min-width: 64px;
  font-size: 0.8125rem;
  min-height: 32px;
}
.MuiButton-sizeLarge {
  padding: 8px 24px;
  min-width: 112px;
  font-size: 0.9375rem;
  min-height: 40px;
}
.MuiButton-fullWidth {
  width: 100%;
}
</style><style data-jss="" data-meta="MuiIconButton">
.MuiIconButton-root {
  flex: 0 0 auto;
  color: rgba(0, 0, 0, 0.54);
  padding: 12px;
  overflow: visible;
  font-size: 1.5rem;
  text-align: center;
  transition: background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  border-radius: 50%;
}
.MuiIconButton-root:hover {
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiIconButton-root.MuiIconButton-disabled {
  color: rgba(0, 0, 0, 0.26);
}
@media (hover: none) {
  .MuiIconButton-root:hover {
    background-color: transparent;
  }
}
.MuiIconButton-root:hover.MuiIconButton-disabled {
  background-color: transparent;
}
.MuiIconButton-colorInherit {
  color: inherit;
}
.MuiIconButton-colorPrimary {
  color: #5f9b65;
}
.MuiIconButton-colorPrimary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiIconButton-colorPrimary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-colorSecondary {
  color: #5f9b65;
}
.MuiIconButton-colorSecondary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiIconButton-colorSecondary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-label {
  width: 100%;
  display: flex;
  align-items: inherit;
  justify-content: inherit;
}
</style><style data-jss="" data-meta="MuiIconButton">
.MuiIconButton-root {
  flex: 0 0 auto;
  color: rgba(0, 0, 0, 0.54);
  padding: 12px;
  overflow: visible;
  font-size: 1.5rem;
  text-align: center;
  transition: background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  border-radius: 50%;
}
.MuiIconButton-root:hover {
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiIconButton-root.MuiIconButton-disabled {
  color: rgba(0, 0, 0, 0.26);
}
@media (hover: none) {
  .MuiIconButton-root:hover {
    background-color: transparent;
  }
}
.MuiIconButton-root:hover.MuiIconButton-disabled {
  background-color: transparent;
}
.MuiIconButton-colorInherit {
  color: inherit;
}
.MuiIconButton-colorPrimary {
  color: #5f9b65;
}
.MuiIconButton-colorPrimary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiIconButton-colorPrimary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-colorSecondary {
  color: #5f9b65;
}
.MuiIconButton-colorSecondary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiIconButton-colorSecondary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-label {
  width: 100%;
  display: flex;
  align-items: inherit;
  justify-content: inherit;
}
</style><style data-jss="" data-meta="MuiModal">
.MuiModal-root {
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 1300;
  position: fixed;
}
.MuiModal-hidden {
  visibility: hidden;
}
</style><style data-jss="" data-meta="MuiModal">
.MuiModal-root {
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 1300;
  position: fixed;
}
.MuiModal-hidden {
  visibility: hidden;
}
</style><style data-jss="" data-meta="MuiPopover">
.MuiPopover-paper {
  outline: none;
  position: absolute;
  min-width: 16px;
  max-width: calc(100% - 32px);
  overflow-y: auto;
  overflow-x: hidden;
  min-height: 16px;
  max-height: calc(100% - 32px);
}
</style><style data-jss="" data-meta="MuiPopover">
.MuiPopover-paper {
  outline: none;
  position: absolute;
  min-width: 16px;
  max-width: calc(100% - 32px);
  overflow-y: auto;
  overflow-x: hidden;
  min-height: 16px;
  max-height: calc(100% - 32px);
}
</style><style data-jss="" data-meta="MuiMenu">
.MuiMenu-paper {
  max-height: calc(100% - 96px);
  -webkit-overflow-scrolling: touch;
}
</style><style data-jss="" data-meta="MuiTooltip">
.MuiTooltip-popper {
  z-index: 1500;
  opacity: 0.9;
}
.MuiTooltip-tooltip {
  color: #fff;
  padding: 9.1px;
  z-index: 10000000;
  font-size: 13px;
  max-width: 300px;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  background-color: rgba(75,75,75,.94);
}
.MuiTooltip-touch {
  padding: 8px 16px;
  font-size: 0.875rem;
  line-height: 1.14286em;
}
.MuiTooltip-tooltipPlacementLeft {
  margin: 0 24px ;
  transform-origin: right center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementLeft {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementRight {
  margin: 0 24px;
  transform-origin: left center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementRight {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementTop {
  margin: 24px 0;
  transform-origin: center bottom;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementTop {
    margin: 14px 0;
  }
}
.MuiTooltip-tooltipPlacementBottom {
  margin: 24px 0;
  transform-origin: center top;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementBottom {
    margin: 14px 0;
  }
}
</style><style data-jss="" data-meta="MuiTooltip">
.MuiTooltip-popper {
  z-index: 1500;
  opacity: 0.9;
}
.MuiTooltip-tooltip {
  color: #fff;
  padding: 9.1px;
  z-index: 10000000;
  font-size: 13px;
  max-width: 300px;
  font-family: GreekFallback,Calibri,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  background-color: rgba(75,75,75,.94);
}
.MuiTooltip-touch {
  padding: 8px 16px;
  font-size: 0.875rem;
  line-height: 1.14286em;
}
.MuiTooltip-tooltipPlacementLeft {
  margin: 0 24px ;
  transform-origin: right center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementLeft {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementRight {
  margin: 0 24px;
  transform-origin: left center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementRight {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementTop {
  margin: 24px 0;
  transform-origin: center bottom;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementTop {
    margin: 14px 0;
  }
}
.MuiTooltip-tooltipPlacementBottom {
  margin: 24px 0;
  transform-origin: center top;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementBottom {
    margin: 14px 0;
  }
}
</style><style data-jss="" data-meta="MuiToolbar">
.MuiToolbar-root {
  display: flex;
  position: relative;
  align-items: center;
}
.MuiToolbar-gutters {
  padding-left: 16px;
  padding-right: 16px;
}
@media (min-width:600px) {
  .MuiToolbar-gutters {
    padding-left: 24px;
    padding-right: 24px;
  }
}
.MuiToolbar-regular {
  min-height: 56px;
}
@media (min-width:0px) and (orientation: landscape) {
  .MuiToolbar-regular {
    min-height: 48px;
  }
}
@media (min-width:600px) {
  .MuiToolbar-regular {
    min-height: 64px;
  }
}
.MuiToolbar-dense {
  min-height: 48px;
}
</style><style data-jss="" data-meta="MuiToolbar">
.MuiToolbar-root {
  display: flex;
  position: relative;
  align-items: center;
}
.MuiToolbar-gutters {
  padding-left: 16px;
  padding-right: 16px;
}
@media (min-width:600px) {
  .MuiToolbar-gutters {
    padding-left: 24px;
    padding-right: 24px;
  }
}
.MuiToolbar-regular {
  min-height: 56px;
}
@media (min-width:0px) and (orientation: landscape) {
  .MuiToolbar-regular {
    min-height: 48px;
  }
}
@media (min-width:600px) {
  .MuiToolbar-regular {
    min-height: 64px;
  }
}
.MuiToolbar-dense {
  min-height: 48px;
}
</style><style data-jss="" data-meta="MuiSnackbar">
.MuiSnackbar-root {
  left: 0;
  right: 0;
  z-index: 1400;
  display: flex;
  position: fixed;
  align-items: center;
  justify-content: center;
}
.MuiSnackbar-anchorOriginTopCenter {
  top: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginBottomCenter {
  bottom: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginTopRight {
  top: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopRight {
    top: 24px;
    left: auto;
    right: 24px;
  }
}
.MuiSnackbar-anchorOriginBottomRight {
  bottom: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomRight {
    left: auto;
    right: 24px;
    bottom: 24px;
  }
}
.MuiSnackbar-anchorOriginTopLeft {
  top: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopLeft {
    top: 24px;
    left: 24px;
    right: auto;
  }
}
.MuiSnackbar-anchorOriginBottomLeft {
  bottom: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomLeft {
    left: 24px;
    right: auto;
    bottom: 24px;
  }
}
</style><style data-jss="" data-meta="MuiSnackbar">
.MuiSnackbar-root {
  left: 0;
  right: 0;
  z-index: 1400;
  display: flex;
  position: fixed;
  align-items: center;
  justify-content: center;
}
.MuiSnackbar-anchorOriginTopCenter {
  top: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginBottomCenter {
  bottom: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginTopRight {
  top: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopRight {
    top: 24px;
    left: auto;
    right: 24px;
  }
}
.MuiSnackbar-anchorOriginBottomRight {
  bottom: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomRight {
    left: auto;
    right: 24px;
    bottom: 24px;
  }
}
.MuiSnackbar-anchorOriginTopLeft {
  top: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopLeft {
    top: 24px;
    left: 24px;
    right: auto;
  }
}
.MuiSnackbar-anchorOriginBottomLeft {
  bottom: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomLeft {
    left: 24px;
    right: auto;
    bottom: 24px;
  }
}
</style><style data-jss="" data-meta="MuiDrawer">
.MuiDrawer-docked {
  flex: 0 0 auto;
}
.MuiDrawer-paper {
  top: 0;
  flex: 1 0 auto;
  height: 100%;
  display: flex;
  z-index: 1200;
  outline: none;
  position: fixed;
  overflow-y: auto;
  flex-direction: column;
  -webkit-overflow-scrolling: touch;
}
.MuiDrawer-paperAnchorLeft {
  left: 0;
  right: auto;
}
.MuiDrawer-paperAnchorRight {
  left: auto;
  right: 0;
}
.MuiDrawer-paperAnchorTop {
  top: 0;
  left: 0;
  right: 0;
  bottom: auto;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorBottom {
  top: auto;
  left: 0;
  right: 0;
  bottom: 0;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorDockedLeft {
  border-right: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedTop {
  border-bottom: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedRight {
  border-left: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedBottom {
  border-top: 1px solid rgba(0, 0, 0, 0.12);
}
</style><style data-jss="" data-meta="MuiDrawer">
.MuiDrawer-docked {
  flex: 0 0 auto;
}
.MuiDrawer-paper {
  top: 0;
  flex: 1 0 auto;
  height: 100%;
  display: flex;
  z-index: 1200;
  outline: none;
  position: fixed;
  overflow-y: auto;
  flex-direction: column;
  -webkit-overflow-scrolling: touch;
}
.MuiDrawer-paperAnchorLeft {
  left: 0;
  right: auto;
}
.MuiDrawer-paperAnchorRight {
  left: auto;
  right: 0;
}
.MuiDrawer-paperAnchorTop {
  top: 0;
  left: 0;
  right: 0;
  bottom: auto;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorBottom {
  top: auto;
  left: 0;
  right: 0;
  bottom: 0;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorDockedLeft {
  border-right: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedTop {
  border-bottom: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedRight {
  border-left: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedBottom {
  border-top: 1px solid rgba(0, 0, 0, 0.12);
}
</style><style data-jss="">
.jss85 {
  top: 0;
  left: 0;
  bottom: 0;
  z-index: 1199;
  position: fixed;
}
.jss86 {
  right: auto;
}
.jss87 {
  left: auto;
  right: 0;
}
.jss88 {
  right: 0;
  bottom: auto;
}
.jss89 {
  top: auto;
  right: 0;
  bottom: 0;
}
</style><link id="main-styles" rel="stylesheet" type="text/css" onerror="window.missingMainStylesheet=true" href="eliezer_files/allStyles.css"><script async="" src="eliezer_files/wtb8z7sj"></script><meta property="og:title" content="Eliezer Yudkowsky - LessWrong" data-react-helmet="true"></head>
<body class="abTestNoEffect_group2 collectionsPageABTest_originalLayoutGroup booksProgressBarABTest_control welcomeBoxABTest_welcomeBox twoLineEventsSidebar_expanded dialogueFacilitationMessages_optIn frontpageDialogueReciprocityRecommendations_noShow showOpinionsInReciprocity_show showRecommendedContentInMatchForm_show checkNotificationMessageContent_v4 newFrontpagePostFeedsWithRecommendationsOptIn_classicFrontpage">
<script>0</script><div id="react-app"><div class="wrapper Layout-wrapper" id="wrapper"><div></div><span></span><div class="IntercomWrapper-intercomFrame" id="intercom-outer-frame"></div><noscript class="noscript-warning"> This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. </noscript><noscript></noscript><div class="Header-root"><div style="height: 64px;" class="Header-headroom headroom-wrapper"><div class="headroom headroom--unpinned headroom-disable-animation"><header class="Header-appBar"><div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><h2 class="Typography-root Typography-title Header-title"><div class="Header-hideSmDown"><div class="Header-titleSubtitleContainer"><a class="Header-titleLink" href="https://www.lesswrong.com/">LESSWRONG</a></div></div><div class="Header-hideMdUp"><a class="Header-titleLink" href="https://www.lesswrong.com/">LW</a></div></h2><div class="Header-rightHeaderItems"><div class="SearchBar-root"><div class="SearchBar-rootChild"><div class="SearchBar-searchInputArea SearchBar-searchInputAreaSmall"><div><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root SearchBar-searchIcon SearchBar-searchIconSmall" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button></div><div></div></div></div></div><div class="UsersAccountMenu-root"><button tabindex="0" class="MuiButtonBase-root MuiButton-root MuiButton-text MuiButton-flat" type="button"><span class="MuiButton-label"><span class="UsersAccountMenu-userButton">Login</span></span><span class="MuiTouchRipple-root"></span></button></div></div></div></header><div class="jss85 jss86" style="width: 20px;"></div></div></div></div><div class="Layout-standaloneNavFlex"><div class="Layout-searchResultsArea"></div><div class="Layout-main"><div class="flash-messages FlashMessages-root"></div><div class="page users-profile UsersProfile-profilePage"><div class="SingleColumnSection-root"><div class="UsersProfile-usernameTitle">Eliezer Yudkowsky</div><aside class="Typography-root Typography-body2 UsersProfile-userInfo"><div class="UsersProfile-meta"><span class="UsersProfile-userMetaInfo" title="144836 karma"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">144836</span></span><span class="UsersProfile-userMetaInfo" title="1831 karma on alignmentforum.org"><span class="OmegaIcon-root UsersProfile-icon UsersProfile-specificalz">Ω</span><span class="Typography-root Typography-body2 MetaInfo-root">1831</span></span><span class="UsersProfile-userMetaInfo" title="951 posts"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M14 2H6c-1.1 0-1.99.9-1.99 2L4 20c0 1.1.89 2 1.99 2H18c1.1 0 2-.9 2-2V8l-6-6zm2 16H8v-2h8v2zm0-4H8v-2h8v2zm-3-5V3.5L18.5 9H13z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">951</span></span><span class="UsersProfile-userMetaInfo" title="7584 comments"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M20 2H4c-1.1 0-1.99.9-1.99 2L2 22l4-4h14c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-2 12H6v-2h12v2zm0-3H6V9h12v2zm0-3H6V6h12v2z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">7584</span></span><span class="UsersProfile-userMetaInfo" title="324 wiki edits"><svg class="MuiSvgIcon-root UsersProfile-icon UsersProfile-specificalz" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M3 17.25V21h3.75L17.81 9.94l-3.75-3.75L3 17.25zM20.71 7.04c.39-.39.39-1.02 0-1.41l-2.34-2.34a.9959.9959 0 0 0-1.41 0l-1.83 1.83 3.75 3.75 1.83-1.83z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><span class="Typography-root Typography-body2 MetaInfo-root">324</span></span></div><div><a>Message</a></div><div class="UsersProfile-subscribeButton"><div><div><a>Subscribe</a></div></div></div></aside></div><div class="SingleColumnSection-root"></div><div class="SingleColumnSection-root"><div class="SectionTitle-root"><h1 id="sequences" class="Typography-root Typography-display1 SectionTitle-title">Sequences</h1><div class="SectionTitle-children"></div></div><div class="SequencesGridWrapper-gridWrapper"><div class="SequencesGrid-grid"><div class="SequencesGrid-gridContent"><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/ds26mimg1uvv82k5d63v.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Metaethics</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ePDpMhJoKCff6qnvh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/e0chotk2uafu1ic9kvny.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Quantum Physics</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/d3WgHDBAPYYScp5Em"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/ncfkdhspgrfhhjpbisaj.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Fun Theory</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/AmFb5xWbPWWQyQ244"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/nczv7w6hr10v4rumtusv.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Ethical Injunctions</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/LAop879LCQWrM5YnE"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/vnyzzznenju0hzdv6pqb.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">The Bayesian Conspiracy</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qWoFR4ytMpQ5vw3FT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/sio9b8jw1apesuispocg.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Three Worlds Collide</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/i2ogsvmipbdolntkew4a.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Highly Advanced Epistemology 101 for Beginners</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/oLGCcbnvabyibnG9d"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/vbhv0s06jdmonk6garvf.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Inadequate Equilibria</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#pvim9PZJ6qHRTMqD3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/yd3bzuj2zhnafkh9uavf.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">The Craft and the Community</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#3szfzHZr7EYGSWt92"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/bdj5ovunbpuswk1eipt9.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Challenging the Difficult</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#SXurf2mWFw8LX2mkG"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/nggx4r2f8zoyehbcqcia.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Yudkowsky's Coming of Age</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#waF2Pomid7YHjfEDt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/f5njri3ukki9fltibb8y.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Quantified Humanism</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#9bvAELWc8y2gYjRav"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/i9c5eiacx4ldzaezqmfn.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Value Theory</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#fqh9TLuoquxpducDb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/mm0ivmd2a1eizxatc0uz.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Fake Preferences</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#fxynfGCSHpY4FmBZy"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/qynhzqukuonidzl0c0kp.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Science and Rationality</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#Kqs6GR7F5xziuSyGZ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/wncutymtkeulpld7cbrm.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Quantum Physics and Many Worlds</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#FqgKAHZAiZn9JAjDo"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/fowl7xmsw7pq08qcyvtt.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Physicalism 201</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#6BFkmEgre7uwhDxDR"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/awsd65brbw8jl3p3txx5.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Joy in the Merely Real</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#p3TndjYbdYaiWwm9x"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/tggcycp4xgcp5awosmta.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Reductionism 101</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#oFePMp9rKftEeZDDr"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/ocof6os9fv9dfgiuyr36.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Lawful Truth</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#SGB7Y5WERh4skwtnb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/bbhnqs3en78i274e5fzx.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">A Human's Guide to Words</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#3HyeNiEpvbQQaqeoH"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/cl2cyyzxkous0oy9uir6.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Fragile Purposes</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#MH2b8NfWv22dBtrs8"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/zdvived9irhffc6rnz1e.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">The Simple Math of Evolution</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#5bZZZJ5psXrrD5BGb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/lzvmuizelhfnz4vflmqk.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Letting Go</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#M3TJ2fTCzoQq66NBJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/kfdosmh7m4gptzsdcl5e.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Death Spirals</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#pmHZDpak4NeRLLLCw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/a1qshk4kjvlctafdi5rw.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Seeing with Fresh Eyes</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#qqFS6Kw5fmPyzkLby"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/xl51zsusg8iahec404fm.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Against Doublethink</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#GSqFqc646rsRd2oyz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/hltj7lnof0lzwlgbeidd.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Against Rationalization</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#3ELrPerFTSo75WnrH"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/h6vrwdypijqgsop7xwa0.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Politics and Rationality</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#FrqfoG3LJeCZs96Ym"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/khst2yhr0iyt9vaq18ee.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Overly Convenient Excuses</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#5uZQHpecjn7955faL"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/qxrtikxmqtdgdrv8laod.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Mysterious Answers</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#zpCiuR4T343j9WkcK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/ff6hxn6crxxxzfrtnciq.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Noticing Confusion</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#7gRSERQZbqTuLX5re"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/vx5txwdtbsejuj6tbf4e.png"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Fake Beliefs</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#5g5TkQTe9rmPS5vvM"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/wwkkaskmbcajjogyv1hu.png"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">Predictably Wrong</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#SyvHDEqbCmB3va7HJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/cbtybpfyrqqqf7jciw1u.png"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">4: HJG and the Phoenix's Call</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#2jyo5h7xkyPiMD3AA"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/fecn7ysevbxwo1yfkiat.png"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">5: HJPEV and the Last Enemy</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#fhRcCn2AcDCYW8PHB"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/xsuilhn2dwx0eoxvrzoa.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">3: HJPEV and the Shadows of Death</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#EBuZhwCrYuJGp7ax4"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/tevykiajl66u3wfuaeae.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">2: HJPEV and the Professor's Games</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#u7ciTcKteyx2hqdBh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/r6niwnpcmu9r4ztfbl6n.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">6: HJPEV and the Philosopher's Stone</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#PtgH6ALi5CoJnPmGS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="eliezer_files/i9dkgkhw14vwar63i4xn.jpg"></div><div class="SequencesGridItem-meta SequencesGridItem-hiddenAuthor"><div class="SequencesGridItem-title">The Methods of Rationality</div></div></div></span></div></div></div></div></div><div class="SingleColumnSection-root"><div class="UsersProfile-postsTitle"><div class="SectionTitle-root"><h1 id="posts" class="Typography-root Typography-display1 SectionTitle-title">Posts</h1><div class="SectionTitle-children"><span class="SettingsButton-iconWithLabelGroup"><svg class="MuiSvgIcon-root SettingsButton-icon SettingsButton-iconWithLabel ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path transform="scale(1.2, 1.2)" fill="none" d="M0 0h20v20H0V0z"></path><path transform="scale(1.2, 1.2)" d="M15.95 10.78c.03-.25.05-.51.05-.78s-.02-.53-.06-.78l1.69-1.32c.15-.12.19-.34.1-.51l-1.6-2.77c-.1-.18-.31-.24-.49-.18l-1.99.8c-.42-.32-.86-.58-1.35-.78L12 2.34c-.03-.2-.2-.34-.4-.34H8.4c-.2 0-.36.14-.39.34l-.3 2.12c-.49.2-.94.47-1.35.78l-1.99-.8c-.18-.07-.39 0-.49.18l-1.6 2.77c-.1.18-.06.39.1.51l1.69 1.32c-.04.25-.07.52-.07.78s.02.53.06.78L2.37 12.1c-.15.12-.19.34-.1.51l1.6 2.77c.1.18.31.24.49.18l1.99-.8c.42.32.86.58 1.35.78l.3 2.12c.04.2.2.34.4.34h3.2c.2 0 .37-.14.39-.34l.3-2.12c.49-.2.94-.47 1.35-.78l1.99.8c.18.07.39 0 .49-.18l1.6-2.77c.1-.18.06-.39-.1-.51l-1.67-1.32zM10 13c-1.65 0-3-1.35-3-3s1.35-3 3-3 3 1.35 3 3-1.35 3-3 3z"></path></svg><span class="SettingsButton-label">Sorted by New</span></span></div></div></div><div class="ProfileShortform-root"><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">14</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/KYuR2HcWPEmXZqMZs/eliezer-yudkowsky-s-shortform"><span><span class="PostsTitle-sticky"><svg class="PostsTitle-stickyIcon ForumIcon-root" fill="currentColor" height="15" viewBox="0 0 10 15" width="10" xmlns="http://www.w3.org/2000/svg"><path d="M0 0h24v24H0z" fill="none"></path><path d="M 0.62965 7.43734C 0.504915 7.43692 0.383097 7.40021 0.279548 7.33183C 0.175999 7.26345 0.0953529 7.16646 0.0477722 7.05309C 0.000191541 6.93972 -0.0121941 6.81504 0.0121763 6.69475C 0.0365467 6.57447 0.0965826 6.46397 0.184718 6.37719L 1.77312 4.81248L 1.77312 1.75013L 1.32819 1.75013C 1.20359 1.75073 1.08025 1.72558 0.966163 1.67633C 0.852072 1.62708 0.749771 1.55483 0.665885 1.46423C 0.581999 1.37364 0.518398 1.26674 0.479198 1.15045C 0.439999 1.03415 0.426075 0.91106 0.438329 0.789139C 0.466198 0.56792 0.576593 0.364748 0.748122 0.218993C 0.919651 0.0732386 1.1401 -0.00472087 1.36675 0.000221379L 8.00217 0.000221379C 8.12677 -0.000372526 8.25011 0.0247692 8.3642 0.0740189C 8.47829 0.123269 8.58059 0.195528 8.66448 0.286119C 8.74837 0.37671 8.81197 0.483614 8.85117 0.599907C 8.89037 0.716201 8.90429 0.839293 8.89204 0.961214C 8.86417 1.18243 8.75377 1.38561 8.58224 1.53136C 8.41071 1.67711 8.19026 1.75507 7.96361 1.75013L 7.55724 1.75013L 7.55724 4.81248L 9.14861 6.37719C 9.23675 6.46397 9.29679 6.57447 9.32116 6.69475C 9.34553 6.81504 9.33314 6.93972 9.28556 7.05309C 9.23798 7.16646 9.15733 7.26345 9.05378 7.33183C 8.95023 7.40021 8.82842 7.43692 8.70368 7.43734L 0.62965 7.43734ZM 4.16834 13.562C 4.18174 13.6824 4.23985 13.7937 4.33154 13.8745C 4.42323 13.9553 4.54204 14 4.66518 14C 4.78833 14 4.90713 13.9553 4.99882 13.8745C 5.09051 13.7937 5.14863 13.6824 5.16202 13.562L 5.73747 8.74977L 3.5929 8.74977L 4.16834 13.562Z"></path></svg></span>Eliezer Yudkowsky's Shortform</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/KYuR2HcWPEmXZqMZs/eliezer-yudkowsky-s-shortform"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-04-01T22:43:50.929Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/KYuR2HcWPEmXZqMZs/eliezer-yudkowsky-s-shortform"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">0</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div></div><div class=""><div class="PostsList2-postsBoxShadow"><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma LWPostsItem-karmaPredictedReviewWinner"><span class="LWTooltip-root">182</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/F8sfrbPjCQj4KwJqn/the-sun-is-big-but-superintelligences-will-not-spare-earth-a"><span>The Sun is big, but superintelligences will not spare Earth a little sunlight</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2024-09-23T03:39:16.243Z">18d</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">136</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">273</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/fPvssZk3AoDzXwfwJ/universal-basic-income-and-poverty"><span>Universal Basic Income and Poverty</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2024-07-26T07:23:50.151Z">3mo</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">119</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma LWPostsItem-karmaPredictedReviewWinner"><span class="LWTooltip-root">172</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/LvKDMWQ3yLG9R3gHw/empiricism-as-anti-epistemology"><span>'Empiricism!' as Anti-Epistemology</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2024-03-14T02:02:59.723Z">7mo</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">90</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">205</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/EzSH9698DhBsXAcYY/my-current-lk99-questions"><span>My current LK99 questions</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="CuratedIcon-postIcon"><span class="LWTooltip-root"><a href="https://www.lesswrong.com/recommendations"><svg class="MuiSvgIcon-root CuratedIcon-curatedIcon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-08-13T04:07:43.668Z">1y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="CuratedIcon-postIcon"><span class="LWTooltip-root"><a href="https://www.lesswrong.com/recommendations"><svg class="MuiSvgIcon-root CuratedIcon-curatedIcon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 17.27L18.18 21l-1.64-7.03L22 9.24l-7.19-.61L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">38</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma LWPostsItem-karmaPredictedReviewWinner"><span class="LWTooltip-root">401</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators"><span>GPTs are Predictors, not Imitators</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-04-08T19:59:13.601Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">90</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma LWPostsItem-karmaPredictedReviewWinner"><span class="LWTooltip-root">253</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1"><span>Pausing AI Developments Isn't Enough. We Need to Shut it All Down</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-04-08T00:36:47.702Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">40</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">14</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/KYuR2HcWPEmXZqMZs/eliezer-yudkowsky-s-shortform"><span>Eliezer Yudkowsky's Shortform</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/KYuR2HcWPEmXZqMZs/eliezer-yudkowsky-s-shortform"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-04-01T22:43:50.929Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><svg class="MuiSvgIcon-root PostsItemIcons-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/KYuR2HcWPEmXZqMZs/eliezer-yudkowsky-s-shortform"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">0</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">116</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/uNepkB5EqETC8b9C2/manifold-if-okay-agi-why"><span>Manifold:  If okay AGI, why?</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://manifold.markets/EliezerYudkowsky/if-artificial-general-intelligence?r=RWxpZXplcll1ZGtvd3NreQ"><svg class="MuiSvgIcon-root PostsItemIcons-linkIcon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-03-25T22:43:53.820Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://manifold.markets/EliezerYudkowsky/if-artificial-general-intelligence?r=RWxpZXplcll1ZGtvd3NreQ"><svg class="MuiSvgIcon-root PostsItemIcons-linkIcon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">37</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background LWPostsItem-bottomBorder"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">174</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals"><span>Alexander and Yudkowsky on AGI goals</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2023-01-24T21:09:16.938Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">52</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div><div class="LWPostsItem-row"><div class="LWPostsItem-root LWPostsItem-background"><div><div class="LWPostsItem-postsItem LWPostsItem-withGrayHover"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo LWPostsItem-karma"><span class="LWTooltip-root">301</span></span><span class="LWPostsItem-title"><span><span class="PostsTitle-root"><span class="PostsTitle-eaTitleDesktopEllipsis"><a href="https://www.lesswrong.com/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1"><span>A challenge for AGI organizations, and a challenge for readers</span></a></span><span class="PostsTitle-hideXsDown"><div class="PostsTitle-interactionWrapper"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div></span></span></span></span><span class="LWPostsItem-spacer"></span><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo PostsItemDate-postedAt"><time datetime="2022-12-01T23:11:44.279Z">2y</time></span></span><div class="LWPostsItem-mobileSecondRowSpacer"></div><div class="LWPostsItem-mobileIcons"><span class="PostsItemIcons-iconSet"><span class="PostsItemIcons-postIcon"><span class="LWTooltip-root"><a href="https://alignmentforum.org/posts/tD9zEiHfkvakpnNam/a-challenge-for-agi-organizations-and-a-challenge-for-1"><span class="OmegaIcon-root PostsItemIcons-icon PostsItemIcons-alignmentIcon">Ω</span></a></span></span></span></div><div class="LWPostsItem-mobileActions"><div class="PostActionsButton-root"><div><svg class="MuiSvgIcon-root PostActionsButton-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M6 10c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm12 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2zm-6 0c-1.1 0-2 .9-2 2s.9 2 2 2 2-.9 2-2-.9-2-2-2z"></path></svg></div></div></div><div class="LWPostsItem-commentsIcon"><div class="PostsItemComments-commentsIconLarge"><svg class="MuiSvgIcon-root PostsItemComments-commentCountIcon PostsItemComments-noUnreadComments" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M21.99 4c0-1.1-.89-2-1.99-2H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h14l4 4-.01-18z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><div class="PostsItemComments-commentCount">33</div></div></div><div class="LWPostsItem-mobileDismissButton"></div></div></div><div class="PostsItemTrailingButtons-actions"></div></div></div></div><aside class="Typography-root Typography-body2 SectionFooter-root"><a class="LoadMore-root LoadMore-sectionFooterStyles" href="#">Load More</a></aside></div></div><div class="SingleColumnSection-root"><div class="SectionTitle-root"><h1 id="wiki-contributions" class="Typography-root Typography-display1 SectionTitle-title">Wiki Contributions</h1><div class="SectionTitle-children"></div></div><div class="TagEditsByUser-root"><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/dath-ilan">Dath Ilan</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 3y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+973</span>/<span class="ChangeMetricsDisplay-charsRemoved">-92</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/dath-ilan">Dath Ilan</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 3y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+1498</span>/<span class="ChangeMetricsDisplay-charsRemoved">-36</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/shut-up-and-multiply">Shut Up and Multiply</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 3y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+653</span>/<span class="ChangeMetricsDisplay-charsRemoved">-306</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/valley-of-bad-rationality">Valley of Bad Rationality</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+50</span>/<span class="ChangeMetricsDisplay-charsRemoved">-32</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/valley-of-bad-rationality">Valley of Bad Rationality</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+51</span>/<span class="ChangeMetricsDisplay-charsRemoved">-56</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/yudkowskys-84th-law">Yudkowsky's 84Th Law</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+10</span>/<span class="ChangeMetricsDisplay-charsRemoved">-10</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/yudkowskys-84th-law">Yudkowsky's 84Th Law</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root">(<span class="ChangeMetricsDisplay-charsAdded">+36</span>/<span class="ChangeMetricsDisplay-charsRemoved">-9</span>)</span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/yudkowskys-84th-law">Yudkowsky's 84Th Law</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root"><span class="ChangeMetricsDisplay-charsAdded">(+172)</span></span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/yudkowskys-law">Yudkowsky's Law</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span><div class="SingleLineTagUpdates-changeMetrics"><span class="ChangeMetricsDisplay-root"><span class="ChangeMetricsDisplay-charsAdded">(+29)</span></span></div></div></div><div class="SingleLineTagUpdates-root"><div class="SingleLineTagUpdates-metadata"><div class="SingleLineTagUpdates-title"><a href="https://www.lesswrong.com/tag/yudkowskys-84th-law">Yudkowsky's 84Th Law</a></div><span class="LWTooltip-root"><span class="Typography-root Typography-body2 PostsItem2MetaInfo-metaInfo SingleLineTagUpdates-postedAt"> 9y </span></span></div></div><a class="LoadMore-root" href="#">Load More</a></div></div><div class="SingleColumnSection-root"><div class="SectionTitle-root UsersProfile-commentSorting"><h1 class="Typography-root Typography-display1 SectionTitle-title"><a href="https://www.lesswrong.com/users/eliezer_yudkowsky/replies">Comments</a></h1><div class="SectionTitle-children">Sorted by <div class="InlineSelect-root"><a class="InlineSelect-link">Newest</a></div></div></div><div class="RecentComments-root"><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="oZtsFc5oCMA9Zk5Tg"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/qc7P2NwfxQMC3hdgm/?commentId=oZtsFc5oCMA9Zk5Tg">Rationalism before the Sequences</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/qc7P2NwfxQMC3hdgm/rationalism-before-the-sequences?commentId=oZtsFc5oCMA9Zk5Tg"><span class="LWTooltip-root"><time datetime="2024-08-26T15:56:32.900Z">1mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">47</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">0</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>I
 note that I haven't said out loud, and should say out loud, that I 
endorse this history. &nbsp;Not every single line of it (see my other 
comment on why I reject verificationism) but on the whole, this is 
well-informed and well-applied.</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class="NamesAttachedReactionsCommentBottom-footerReactions"><span><span><span class="NamesAttachedReactionsCommentBottom-footerReaction"><span><span class="ReactionIcon-invertIfDarkMode"><img src="eliezer_files/thankyou.svg" style="filter: opacity(0.5) saturate(0.6); transform: scale(0.9) translate(0px); width: 18px; height: 18px;" class="ReactionIcon-reactionSvg"></span></span><span class="NamesAttachedReactionsCommentBottom-reactionCount">4</span></span><span class="NamesAttachedReactionsCommentBottom-footerReactionSpacer"></span></span></span></span><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="cGCaQactztiBrxGjh"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/fjfWrKhEawwBGCTGs/?commentId=cGCaQactztiBrxGjh">A simple case for extreme inner misalignment</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/fjfWrKhEawwBGCTGs/a-simple-case-for-extreme-inner-misalignment?commentId=cGCaQactztiBrxGjh"><span class="LWTooltip-root"><time datetime="2024-08-09T18:14:34.806Z">2mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">12</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">20</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">5</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><blockquote><p>If
 you had to put a rough number on how likely it is that a misaligned 
superintelligence would primarily value "small molecular squiggles" 
versus other types of misaligned goals, would it be more like 1000:1 or 
1:1 or 1000:1 or something else?&nbsp;</p></blockquote><p>Value them <i>primarily?</i>
 &nbsp;Uhhh... maybe 1:3 against? &nbsp;I admit I have never actually 
pondered this question before today; but 1 in 4 uncontrolled 
superintelligences spending most of their resources on tiny squiggles 
doesn't sound off by, like, more than 1-2 orders of magnitude in either 
direction.</p><blockquote><p>Clocks are not actually very complicated; 
how plausible is it on your model that these goals are as complicated 
as, say, a typical human's preferences about how human civilization is 
structured?</p></blockquote><p>It wouldn't shock me if their goals end 
up far more complicated than human ones; the most obvious pathway for it
 is (a) gradient descent turning out to produce internal preferences 
much faster than natural selection + biological reinforcement learning 
and (b) some significant fraction of those preferences being retained 
under reflection. &nbsp;(Where (b) strikes me as way less probable than 
(a), but not wholly forbidden.) &nbsp;The second most obvious pathway is
 if a bunch of weird detailed noise appears in the first version of the 
reflective process and then freezes.</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="WapHz3gokGBd3KHKm"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/hzt9gHpNwA2oHtwKX/?commentId=WapHz3gokGBd3KHKm">Self-Other Overlap: A Neglected Approach to AI Alignment</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/hzt9gHpNwA2oHtwKX/self-other-overlap-a-neglected-approach-to-ai-alignment?commentId=WapHz3gokGBd3KHKm"><span class="LWTooltip-root"><time datetime="2024-08-09T02:19:13.583Z">2mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">16</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">37</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">13</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Not obviously stupid on a very quick skim. &nbsp;I will have to actually read it to figure out where it's stupid.</p><p>(I rarely give any review this positive on a first skim. &nbsp;Congrats.)</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="XGTupMJe2ccyKhgrQ"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/rP66bz34crvDudzcJ/?commentId=XGTupMJe2ccyKhgrQ">Decision theory does not imply that we get to have nice things</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/rP66bz34crvDudzcJ/decision-theory-does-not-imply-that-we-get-to-have-nice?commentId=XGTupMJe2ccyKhgrQ"><span class="LWTooltip-root"><time datetime="2024-07-29T19:13:47.897Z">2mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">5</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">7</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">2</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>By "dumb player" I did not mean <i>as dumb as a human</i>
 player. &nbsp;I meant "too dumb to compute the pseudorandom numbers, 
but not too dumb to simulate other players faithfully apart from that". 
&nbsp;I did not realize we were talking about humans at all. &nbsp;This 
jumps out more to me as a potential source of misunderstanding than it 
did 15 years ago, and for that I apologize.</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="vkCEJ5ivLWsnNnpbE"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/rP66bz34crvDudzcJ/?commentId=vkCEJ5ivLWsnNnpbE">Decision theory does not imply that we get to have nice things</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/rP66bz34crvDudzcJ/decision-theory-does-not-imply-that-we-get-to-have-nice?commentId=vkCEJ5ivLWsnNnpbE"><span class="LWTooltip-root"><time datetime="2024-07-28T21:30:21.852Z">2mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">6</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">12</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">8</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>I
 don't always remember my previous positions all that well, but I doubt I
 would have said at any point that sufficiently advanced LDT agents are 
friendly to each other, rather than that they coordinate well with each 
other (and not so with us)?</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="gAGnuYfDdwbAm9tSv"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/fjfWrKhEawwBGCTGs/?commentId=gAGnuYfDdwbAm9tSv">A simple case for extreme inner misalignment</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/fjfWrKhEawwBGCTGs/a-simple-case-for-extreme-inner-misalignment?commentId=gAGnuYfDdwbAm9tSv"><span class="LWTooltip-root"><time datetime="2024-07-16T19:59:20.949Z">3mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">11</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">26</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">11</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Actually, to slightly amend that: &nbsp;The part where squiggles are small is a <i>more than randomly likely</i> part of the prediction, but not a <i>load-bearing part of downstream predictions or the policy argument.</i>
 &nbsp;Most of the time we don't needlessly build our own paperclips to 
be the size of skyscrapers; even when having fun, we try to do the fun 
without vastly more resources, than are necessary to that amount of fun,
 because then we'll have needlessly used up all our resources and not 
get to have more fun. &nbsp;We buy cookies that cost a dollar instead of
 a hundred thousand dollars. &nbsp;A very wide variety of utility 
functions you could run over the outside universe will have optima 
around making lots of small things because each thing scores one point, 
and so to score as many points as possible, each thing is as small as it
 can be as still count as a thing. &nbsp;Nothing downstream depends on 
this part coming true and there are many ways for it to come false; but 
the part where the squiggles are small and molecular <i>is</i> an 
obvious kind of guess. &nbsp;"Great giant squiggles of nickel the size 
of a solar system would be no more valuable, even from a very embracing 
and cosmopolitan perspective on value" is the loadbearing part.</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="fTThxDyxEiNsaTwaP"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/fjfWrKhEawwBGCTGs/?commentId=fTThxDyxEiNsaTwaP">A simple case for extreme inner misalignment</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/fjfWrKhEawwBGCTGs/a-simple-case-for-extreme-inner-misalignment?commentId=fTThxDyxEiNsaTwaP"><span class="LWTooltip-root"><time datetime="2024-07-16T14:11:40.041Z">3mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">10</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">26</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">19</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>The
 part where squiggles are small and simple is unimportant.  They could 
be bigger and more complicated, like building giant mechanical clocks.  <span><span class=""><span class="InlineReactHoverableHighlight-highlight"><span>The
 part that matters is that squiggles/paperclips are of no value even 
from a very cosmopolitan and embracing perspective on value</span></span></span></span>.</p>
</div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class="NamesAttachedReactionsCommentBottom-footerReactions"><span><span><span class="NamesAttachedReactionsCommentBottom-footerReaction NamesAttachedReactionsCommentBottom-hasQuotes"><span><span class="ReactionIcon-invertIfDarkMode"><img src="eliezer_files/check.svg" style="filter: opacity(0.4) saturate(0.6); transform: scale(1) translate(0px); width: 18px; height: 18px;" class="ReactionIcon-reactionSvg"></span></span><span class="NamesAttachedReactionsCommentBottom-reactionCount">2</span></span><span class="NamesAttachedReactionsCommentBottom-footerReactionSpacer"></span></span></span></span><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="CixonSXNfLgAPh48Z"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/q8uNoJBgcpAe3bSBp/?commentId=CixonSXNfLgAPh48Z">My AI Model Delta Compared To Yudkowsky</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/q8uNoJBgcpAe3bSBp/my-ai-model-delta-compared-to-yudkowsky?commentId=CixonSXNfLgAPh48Z"><span class="LWTooltip-root"><time datetime="2024-06-11T18:07:24.402Z">4mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">145</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">34</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>I
 think that the AI's internal ontology is liable to have some noticeable
 alignments to human ontology w/r/t the purely predictive aspects of the
 natural world; it wouldn't surprise me to find distinct thoughts in 
there about electrons. &nbsp;As the internal ontology goes to be more 
about affordances and actions, I expect to find increasing disalignment.
 &nbsp;As the internal ontology takes on any <i>reflective</i> aspects, 
parts of the representation that mix with facts about the AI's 
internals, I expect to find much larger differences -- not just that the
 AI has a different concept boundary around "easy to understand", say, 
but that it maybe doesn't have any such internal notion as "easy to 
understand" at all, because easiness isn't in the environment and the AI
 doesn't have any such thing as "effort". &nbsp;Maybe it's got 
categories around yieldingness to seven different categories of methods,
 and/or some general notion of "can predict at all / can't predict at 
all", but no general notion that maps onto human "easy to understand" --
 though "easy to understand" is plausibly general-enough that I wouldn't
 be unsurprised to find a mapping after all.</p><p>Corrigibility and 
actual human values are both heavily reflective concepts. &nbsp;If you 
master a requisite level of the prerequisite skill of noticing when <span><span class=""><span class="InlineReactHoverableHighlight-highlight"><span>a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment</span></span></span></span> -- which of course <span><span class=""><span class="InlineReactHoverableHighlight-highlight"><span>most people can't do because they project the category boundary onto the environment</span></span></span></span>,
 but I have some credit that John Wentworth might be able to do it some 
-- and then you start mapping out concept definitions about 
corrigibility or values or god help you CEV, that might help highlight 
where some of my concern about unnatural abstractions comes in.</p><p>&nbsp;</p><p><i>Entirely separately,</i> I have concerns about the ability of ML-based technology to robustly point the AI in <i>any </i>builder-intended direction whatsoever, <i>even if</i>
 there exists some not-too-large adequate mapping from that intended 
direction onto the AI's internal ontology at training time. &nbsp;My 
guess is that more of the disagreement lies here.</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class="NamesAttachedReactionsCommentBottom-footerReactions"><span><span><span class="NamesAttachedReactionsCommentBottom-footerReaction"><span><span class="ReactionIcon-invertIfDarkMode"><img src="eliezer_files/thankyou.svg" style="filter: opacity(0.5) saturate(0.6); transform: scale(0.9) translate(0px); width: 18px; height: 18px;" class="ReactionIcon-reactionSvg"></span></span><span class="NamesAttachedReactionsCommentBottom-reactionCount">12</span></span><span class="NamesAttachedReactionsCommentBottom-footerReactionSpacer"></span></span></span><span><span><span class="NamesAttachedReactionsCommentBottom-footerReaction NamesAttachedReactionsCommentBottom-hasQuotes"><span><span class="ReactionIcon-invertIfDarkMode"><img src="eliezer_files/lightbulb.svg" style="filter: opacity(0.4) saturate(0.6); transform: scale(1) translate(0px); width: 18px; height: 18px;" class="ReactionIcon-reactionSvg"></span></span><span class="NamesAttachedReactionsCommentBottom-reactionCount">3</span></span><span class="NamesAttachedReactionsCommentBottom-footerReactionSpacer"></span></span></span><span><span><span class="NamesAttachedReactionsCommentBottom-footerReaction NamesAttachedReactionsCommentBottom-hasQuotes"><span><span class="ReactionIcon-invertIfDarkMode"><img src="eliezer_files/shapes.svg" style="filter: opacity(0.6) saturate(0.6); transform: scale(1) translate(0px); width: 18px; height: 18px;" class="ReactionIcon-reactionSvg"></span></span><span class="NamesAttachedReactionsCommentBottom-reactionCount">2</span></span><span class="NamesAttachedReactionsCommentBottom-footerReactionSpacer"></span></span></span></span><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node af CommentFrame-answerLeafComment" id="ww5p8EEtKndRnWazN"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/?commentId=ww5p8EEtKndRnWazN">GPTs are Predictors, not Imitators</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators?commentId=ww5p8EEtKndRnWazN"><span class="LWTooltip-root"><time datetime="2024-04-21T01:37:40.889Z">6mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="LWTooltip-root"><span class="OverallVoteAxis-secondaryScore"><span class="OverallVoteAxis-secondarySymbol">Ω</span><span class="OverallVoteAxis-secondaryScoreNumber">9</span></span></span><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">16</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">8</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>What
 the main post is responding to is the argument: &nbsp;"We're just 
training AIs to imitate human text, right, so that process can't make 
them get any smarter than the text they're imitating, right? &nbsp;So 
AIs shouldn't learn abilities that humans don't have; because why would 
you need those abilities to learn to imitate humans?" &nbsp;And to this 
the main post says, "Nope."</p><p>The main post is <i>not </i>arguing: 
&nbsp;"If you abstract away the tasks humans evolved to solve, from 
human levels of performance at those tasks, the tasks AIs are being 
trained to solve are harder than those tasks in principle even if they 
were being solved perfectly." &nbsp;I agree this is just false, and did 
not think my post said otherwise.</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><div><div><div class="comments-node CommentFrame-commentsNodeRoot comments-node-root comments-node-odd CommentFrame-node CommentFrame-answerLeafComment" id="rSzwbLHLSdj5dEnSY"><div><div class="CommentsItem-root recent-comments-node"><div class="CommentsItem-postTitleRow"><span class="LWTooltip-root"><a class="CommentsItem-postTitle" href="https://www.lesswrong.com/posts/LvKDMWQ3yLG9R3gHw/?commentId=rSzwbLHLSdj5dEnSY">'Empiricism!' as Anti-Epistemology</a></span></div><div class="CommentsItem-body"><div class="CommentsItemMeta-root"><span class="LWTooltip-root"><span class="ShowParentComment-root"><svg class="MuiSvgIcon-root ShowParentComment-icon" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0V0z"></path><path d="M11 9l1.42 1.42L8.83 14H18V4h2v12H8.83l3.59 3.58L11 21l-6-6 6-6z"></path></svg></span></span><span class="CommentsItemMeta-username CommentUserName-author"><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span><span class="CommentsItemDate-root CommentsItemDate-date"><a rel="nofollow" href="https://www.lesswrong.com/posts/LvKDMWQ3yLG9R3gHw/empiricism-as-anti-epistemology?commentId=rSzwbLHLSdj5dEnSY"><span class="LWTooltip-root"><time datetime="2024-03-20T00:13:11.196Z">7mo</time></span><svg class="MuiSvgIcon-root CommentsItemDate-icon ForumIcon-root ForumIcon-linkRotation" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a></span><span class="NamesAttachedReactionsVoteOnComment-root"><span class="OverallVoteAxis-vote"><span class="OverallVoteAxis-overallSection OverallVoteAxis-overallSectionBox"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-left" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span><span class="LWTooltip-root"><span class="OverallVoteAxis-voteScore">15</span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteArrowIcon-root VoteArrowIcon-right" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root VoteArrowIcon-smallArrow" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteArrowIcon-bigArrow VoteArrowIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></button></span></span></span><span class="AgreementVoteAxis-agreementSection"><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-clear VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallArrowBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigClear VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span></span></button></span><span class="AgreementVoteAxis-agreementScore"><span class="LWTooltip-root"><span class="AgreementVoteAxis-voteScore">-3</span></span></span><span class="LWTooltip-root"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root VoteAgreementIcon-root" type="button"><span class="MuiIconButton-label"><span class="VoteAgreementIcon-iconsContainer"><svg class="MuiSvgIcon-root VoteAgreementIcon-check VoteAgreementIcon-noClickCatch" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation" style="color: inherit;"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-smallCheckBigVoted VoteAgreementIcon-noClickCatch VoteAgreementIcon-hideIcon" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg><svg class="MuiSvgIcon-root VoteAgreementIcon-bigCheck VoteAgreementIcon-noClickCatch VoteAgreementIcon-exited" focusable="false" viewBox="6 6 12 12" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M9 16.17L4.83 12l-1.42 1.41L9 19 21 7l-1.41-1.41z"></path></svg></span></span></button></span></span></span><span class="CommentsItemMeta-rightSection"></span></div><div class="InlineReactSelectionWrapper-root"><div class="CommentBody-root ContentStyles-base content ContentStyles-commentBody"><div class="CommentBody-commentStyling"><p>Unless
 I'm greatly misremembering, you did pick out what you said was your 
strongest item from Lethalities, separately from this, and I responded 
to it. &nbsp;You'd just straightforwardly misunderstood my argument in 
that case, so it wasn't a long response, but I responded. &nbsp;Asking 
for a second try is one thing, but I don't think it's cool to act like 
you never picked out any one item or I never responded to it.</p><p>EDIT:
 I'm misremembering, it was Quintin's strongest point about the Bankless
 podcast. 
&nbsp;https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-
to-we-re-all-gonna-die-with-eliezer-yudkowsky?
commentId=cr54ivfjndn6dxraD</p></div></div></div><div class="CommentBottom-bottom CommentBottom-bottomWithReacts"><a class="comments-item-reply-link CommentsItem-replyLink">Reply</a><span class="NamesAttachedReactionsCommentBottom-footerReactionsRow"><span class=""><span class="NamesAttachedReactionsCommentBottom-addReactionButton react-hover-style"><svg fill="none" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><g fill="currentColor"><path d="m13 7c0-3.31371-2.6863-6-6-6-3.31371 0-6 2.68629-6 6 0 3.3137 2.68629 6 6 6 .08516 0 .1699-.0018.25419-.0053-.11154-.3168-.18862-.6499-.22673-.9948l-.02746.0001c-2.76142 0-5-2.23858-5-5s2.23858-5 5-5 5 2.23858 5 5l-.0001.02746c.3449.03811.678.11519.9948.22673.0035-.08429.0053-.16903.0053-.25419z"></path><path d="m7.11191 10.4982c.08367-.368.21246-.71893.38025-1.04657-.15911.03174-.32368.04837-.49216.04837-.74037 0-1.40506-.3212-1.86354-.83346-.18417-.20576-.50026-.22327-.70603-.03911-.20576.18417-.22327.50026-.03911.70603.64016.71524 1.57205 1.16654 2.60868 1.16654.03744 0 .07475-.0006.11191-.0018z"></path><path d="m6 6c0 .41421-.33579.75-.75.75s-.75-.33579-.75-.75.33579-.75.75-.75.75.33579.75.75z"></path><path d="m8.75 6.75c.41421 0 .75-.33579.75-.75s-.33579-.75-.75-.75-.75.33579-.75.75.33579.75.75.75z"></path><path d="m15 11.5c0 1.933-1.567 3.5-3.5 3.5s-3.5-1.567-3.5-3.5 1.567-3.5 3.5-3.5 3.5 1.567 3.5 3.5zm-3-2c0-.27614-.2239-.5-.5-.5s-.5.22386-.5.5v1.5h-1.5c-.27614 0-.5.2239-.5.5s.22386.5.5.5h1.5v1.5c0 .2761.2239.5.5.5s.5-.2239.5-.5v-1.5h1.5c.2761 0 .5-.2239.5-.5s-.2239-.5-.5-.5h-1.5z"></path></g></svg></span></span></span></div></div></div></div></div></div></div><a class="LoadMore-root" href="#">Load More</a></div></div></div><div class="Footer-root"></div></div></div></div></div>

<script>window.ssrRenderedAt = "2024-10-10T21:04:31.332Z"</script>
<script>window.ssrMetadata = {"renderedAt":"2024-10-10T21:04:31.332Z","cacheFriendly":false,"timezone":"Atlantic/Reykjavik"}</script>
<script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","currentUser":null,"unreadNotificationCounts":{"__typename":"NotificationCounts","unreadNotifications":0,"unreadPrivateMessages":0,"faviconBadgeNumber":0,"checkedAt":"2024-10-10T21:04:31.346Z"},"users({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":10,\"slug\":\"eliezer_yudkowsky\",\"view\":\"usersProfile\"}}})":{"__typename":"MultiUserOutput","results":[{"__ref":"User:nmk3nLpQE89dMRzzN"}],"totalCount":null},"revisions({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":10,\"userId\":\"nmk3nLpQE89dMRzzN\",\"view\":\"revisionsByUser\"}}})":{"__typename":"MultiRevisionOutput","results":[{"__ref":"Revision:iqfCAag2mikQJ6Rr8"},{"__ref":"Revision:6mevXpDx3TkGCtdCZ"},{"__ref":"Revision:zuKhWCcgFFBNKmbuG"},{"__ref":"Revision:5f5c37ee1b5cdee568cfc775"},{"__ref":"Revision:5f5c37ee1b5cdee568cfc774"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd77f"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd77e"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd77d"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd780"},{"__ref":"Revision:5f5c37ee1b5cdee568cfd77c"}],"totalCount":null},"post({\"input\":{\"selector\":{\"documentId\":\"KYuR2HcWPEmXZqMZs\"}}})":{"__typename":"SinglePostOutput","result":{"__ref":"Post:KYuR2HcWPEmXZqMZs"}},"sequences({\"input\":{\"enableCache\":false,\"enableTotal\":true,\"terms\":{\"limit\":9,\"userId\":\"nmk3nLpQE89dMRzzN\",\"view\":\"userProfile\"}}})":{"__typename":"MultiSequenceOutput","results":[{"__ref":"Sequence:W2fkmatEzyrmbbrDt"},{"__ref":"Sequence:ePDpMhJoKCff6qnvh"},{"__ref":"Sequence:d3WgHDBAPYYScp5Em"},{"__ref":"Sequence:AmFb5xWbPWWQyQ244"},{"__ref":"Sequence:LAop879LCQWrM5YnE"},{"__ref":"Sequence:qWoFR4ytMpQ5vw3FT"},{"__ref":"Sequence:SqFbMbtxGybdS2gRs"},{"__ref":"Sequence:oLGCcbnvabyibnG9d"},{"__ref":"Sequence:pvim9PZJ6qHRTMqD3"}],"totalCount":40},"user({\"input\":{\"selector\":{\"documentId\":\"nmk3nLpQE89dMRzzN\"}}})":{"__typename":"SingleUserOutput","result":{"__ref":"User:nmk3nLpQE89dMRzzN"}},"comments({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"authorIsUnreviewed\":null,\"limit\":10,\"userId\":\"nmk3nLpQE89dMRzzN\",\"view\":\"profileComments\"}}})":{"__typename":"MultiCommentOutput","results":[{"__ref":"Comment:oZtsFc5oCMA9Zk5Tg"},{"__ref":"Comment:cGCaQactztiBrxGjh"},{"__ref":"Comment:WapHz3gokGBd3KHKm"},{"__ref":"Comment:XGTupMJe2ccyKhgrQ"},{"__ref":"Comment:vkCEJ5ivLWsnNnpbE"},{"__ref":"Comment:gAGnuYfDdwbAm9tSv"},{"__ref":"Comment:fTThxDyxEiNsaTwaP"},{"__ref":"Comment:CixonSXNfLgAPh48Z"},{"__ref":"Comment:ww5p8EEtKndRnWazN"},{"__ref":"Comment:rSzwbLHLSdj5dEnSY"}],"totalCount":null},"posts({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"authorIsUnreviewed\":null,\"excludeEvents\":true,\"limit\":10,\"userId\":\"nmk3nLpQE89dMRzzN\",\"view\":\"userPosts\"}}})":{"__typename":"MultiPostOutput","results":[{"__ref":"Post:F8sfrbPjCQj4KwJqn"},{"__ref":"Post:fPvssZk3AoDzXwfwJ"},{"__ref":"Post:LvKDMWQ3yLG9R3gHw"},{"__ref":"Post:EzSH9698DhBsXAcYY"},{"__ref":"Post:nH4c3Q9t9F3nJ7y8W"},{"__ref":"Post:oM9pEezyCb4dCsuKq"},{"__ref":"Post:KYuR2HcWPEmXZqMZs"},{"__ref":"Post:uNepkB5EqETC8b9C2"},{"__ref":"Post:rwkkcgSpnAyE8oNo3"},{"__ref":"Post:tD9zEiHfkvakpnNam"}],"totalCount":null}},"Revision:nmk3nLpQE89dMRzzN_moderationGuidelines":{"_id":"nmk3nLpQE89dMRzzN_moderationGuidelines","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2021-04-03T23:33:58.374Z","userId":"XtphY3uYHwruKqDyG","html":"<p>I will enforce the same standards here as I would on my personal Facebook garden.  If it looks like it would be unhedonic to spend time interacting with you, I will ban you from commenting on my posts.<\/p>\n<p>Specific guidelines:<\/p>\n<ul>\n<li>Argue against ideas rather than people.<\/li>\n<li>Don't accuse others of committing the Being Wrong Fallacy (\"Wow, I can't believe you're so wrong!  And you believe you're right!  That's even more wrong!\").<\/li>\n<li>I consider tone-policing to be a self-fulfilling prophecy and will delete it.<\/li>\n<li>If I think your own tone is counterproductive, I will try to remember to politely delete your comment instead of rudely saying so in a public reply.<\/li>\n<li>If you have helpful personal advice to someone that could perhaps be taken as lowering their status, say it to them in private rather than in a public comment.<\/li>\n<li>The censorship policy of the Reign of Terror is not part of the content of the post itself and may not be debated on the post.  If you think Censorship!! is a terrible idea and invalidates discussion, feel free not to read the comments section.<\/li>\n<li>The Internet is full of things to read that will not make you angry.  If it seems like you choose to spend a lot of time reading things that will give you a chance to be angry and push down others so you can be above them, you're not an interesting plant to have in my garden and you will be weeded.  I don't consider it fun to get angry at such people, and I will choose to read something else instead.<\/li>\n<\/ul>\n","commitMessage":"","wordCount":279,"htmlHighlight":"<p>I will enforce the same standards here as I would on my personal Facebook garden.  If it looks like it would be unhedonic to spend time interacting with you, I will ban you from commenting on my posts.<\/p>\n<p>Specific guidelines:<\/p>\n<ul>\n<li>Argue against ideas rather than people.<\/li>\n<li>Don't accuse others of committing the Being Wrong Fallacy (\"Wow, I can't believe you're so wrong!  And you believe you're right!  That's even more wrong!\").<\/li>\n<li>I consider tone-policing to be a self-fulfilling prophecy and will delete it.<\/li>\n<li>If I think your own tone is counterproductive, I will try to remember to politely delete your comment instead of rudely saying so in a public reply.<\/li>\n<li>If you have helpful personal advice to someone that could perhaps be taken as lowering their status, say it to them in private rather than in a public comment.<\/li>\n<li>The censorship policy of the Reign of Terror is not part of the content of the post itself and may not be debated on the post.  If you think Censorship!! is a terrible idea and invalidates discussion, feel free not to read the comments section.<\/li>\n<li>The Internet is full of things to read that will not make you angry.  If it seems like you choose to spend a lot of time reading things that will give you a chance to be angry and push down others so you can be above them, you're not an interesting plant to have in my garden and you will be weeded.  I don't consider it fun to get angry at such people, and I will choose to read something else instead.<\/li>\n<\/ul>","plaintextDescription":"I will enforce the same standards here as I would on my personal Facebook garden. If it looks like it would be unhedonic to spend time interacting with you, I will ban you from commenting on my posts.\n\nSpecific guidelines:\n\n * Argue against ideas rather than people.\n * Don't accuse others of committing the Being Wrong Fallacy (\"Wow, I can't believe you're so wrong! And you believe you're right! That's even more wrong!\").\n * I consider tone-policing to be a self-fulfilling prophecy and will delete it.\n * If I think your own tone is counterproductive, I will try to remember to politely delete your comment instead of rudely saying so in a public reply.\n * If you have helpful personal advice to someone that could perhaps be taken as lowering their status, say it to them in private rather than in a public comment.\n * The censorship policy of the Reign of Terror is not part of the content of the post itself and may not be debated on the post. If you think Censorship!! is a terrible idea and invalidates discussion, feel free not to read the comments section.\n * The Internet is full of things to read that will not make you angry. If it seems like you choose to spend a lot of time reading things that will give you a chance to be angry and push down others so you can be above them, you're not an interesting plant to have in my garden and you will be weeded. I don't consider it fun to get angry at such people, and I will choose to read something else instead."},"User:nmk3nLpQE89dMRzzN":{"_id":"nmk3nLpQE89dMRzzN","__typename":"User","oldSlugs":["eliezer-yudkowsky"],"groups":["trustLevel1","alignmentVoters","alignmentForum","canModeratePersonal"],"jobTitle":null,"organization":null,"careerStage":null,"biography":null,"howOthersCanHelpMe":null,"howICanHelpOthers":null,"profileTagIds":[],"profileTags":[],"organizerOfGroupIds":[],"organizerOfGroups":[],"programParticipation":null,"website":null,"linkedinProfileURL":null,"facebookProfileURL":null,"twitterProfileURL":null,"githubProfileURL":null,"frontpagePostCount":445,"afSequenceCount":0,"afSequenceDraftCount":0,"sequenceDraftCount":0,"moderationStyle":"reign-of-terror","moderationGuidelines":{"__ref":"Revision:nmk3nLpQE89dMRzzN_moderationGuidelines"},"bannedUserIds":["sBWszXPhPsNNemv4Q","YBHSPmZEfyyY2E2au"],"location":null,"googleLocation":null,"mapLocation":null,"mapLocationSet":false,"mapMarkerText":null,"htmlMapMarkerText":"","mongoLocation":null,"shortformFeedId":"KYuR2HcWPEmXZqMZs","viewUnreviewedComments":null,"auto_subscribe_to_my_posts":true,"auto_subscribe_to_my_comments":true,"autoSubscribeAsOrganizer":true,"petrovPressedButtonDate":null,"petrovOptOut":false,"sortDraftsBy":null,"email":null,"emails":null,"banned":null,"noindex":false,"paymentEmail":null,"paymentInfo":null,"goodHeartTokens":994,"postingDisabled":null,"allCommentingDisabled":null,"commentingOnOtherUsersDisabled":null,"conversationsDisabled":null,"slug":"eliezer_yudkowsky","createdAt":"2009-02-23T21:58:56.739Z","username":"Eliezer_Yudkowsky","displayName":"Eliezer Yudkowsky","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":144836,"afKarma":1831,"deleted":false,"isAdmin":false,"htmlBio":"","postCount":951,"commentCount":7584,"sequenceCount":40,"afPostCount":18,"afCommentCount":116,"spamRiskScore":1,"tagRevisionCount":324,"reviewedByUserId":"r38pkCm7wF4M44MDQ","walledGardenInvite":null,"hideWalledGardenUI":null,"walledGardenPortalOnboarded":null,"taggingDashboardCollapsed":null,"usernameUnset":null,"moderatorAssistance":true,"maxCommentCount":7586,"maxPostCount":1090,"voteCount":null,"smallUpvoteCount":null,"bigUpvoteCount":null,"smallDownvoteCount":null,"bigDownvoteCount":null,"reviewedAt":null,"signUpReCaptchaRating":null,"needsReview":null,"sunshineNotes":null,"sunshineFlagged":null,"snoozedUntilContentCount":null,"nullifyVotes":false,"deleteContent":false,"moderatorActions":null,"usersContactedBeforeReview":null,"associatedClientIds":null,"altAccountsDetected":null,"voteReceivedCount":null,"smallUpvoteReceivedCount":null,"bigUpvoteReceivedCount":null,"smallDownvoteReceivedCount":null,"bigDownvoteReceivedCount":null,"recentKarmaInfo":{"last20Karma":1136,"lastMonthKarma":182,"last20PostKarma":4377,"last20CommentKarma":526,"downvoterCount":0,"postDownvoterCount":0,"commentDownvoterCount":0,"lastMonthDownvoterCount":0},"lastNotificationsCheck":null},"Tag:EmaCLRKb4baBFq4ra":{"_id":"EmaCLRKb4baBFq4ra","__typename":"Tag","userId":"sKAL2jzfkYkDbQmx9","name":"Dath Ilan","shortName":null,"slug":"dath-ilan","core":false,"postCount":34,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2021-10-18T15:53:10.881Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:iqfCAag2mikQJ6Rr8":{"_id":"iqfCAag2mikQJ6Rr8","__typename":"Revision","tag":{"__ref":"Tag:EmaCLRKb4baBFq4ra"},"documentId":"EmaCLRKb4baBFq4ra","changeMetrics":{"added":973,"removed":92},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.4.0","editedAt":"2021-11-02T00:02:19.989Z","commitMessage":"","userId":"nmk3nLpQE89dMRzzN","score":13,"baseScore":13,"extendedScore":null,"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:6mevXpDx3TkGCtdCZ":{"_id":"6mevXpDx3TkGCtdCZ","__typename":"Revision","tag":{"__ref":"Tag:EmaCLRKb4baBFq4ra"},"documentId":"EmaCLRKb4baBFq4ra","changeMetrics":{"added":1498,"removed":36},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.3.0","editedAt":"2021-11-01T23:53:26.473Z","commitMessage":"","userId":"nmk3nLpQE89dMRzzN","score":13,"baseScore":11,"extendedScore":null,"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb187":{"_id":"5f5c37ee1b5cdee568cfb187","__typename":"Tag","userId":"qf77EiaoMw7tH3GSr","name":"Shut Up and Multiply","shortName":null,"slug":"shut-up-and-multiply","core":false,"postCount":33,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.100Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:zuKhWCcgFFBNKmbuG":{"_id":"zuKhWCcgFFBNKmbuG","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb187"},"documentId":"5f5c37ee1b5cdee568cfb187","changeMetrics":{"added":653,"removed":306},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.25.0","editedAt":"2021-04-17T19:55:22.338Z","commitMessage":"","userId":"nmk3nLpQE89dMRzzN","score":10,"baseScore":10,"extendedScore":null,"voteCount":5,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb1f2":{"_id":"5f5c37ee1b5cdee568cfb1f2","__typename":"Tag","userId":"9c2mQkLQq6gQSksMs","name":"Valley of Bad Rationality","shortName":null,"slug":"valley-of-bad-rationality","core":false,"postCount":15,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.277Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:5f5c37ee1b5cdee568cfc775":{"_id":"5f5c37ee1b5cdee568cfc775","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb1f2"},"documentId":"5f5c37ee1b5cdee568cfb1f2","changeMetrics":{"added":50,"removed":32},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.10.0","editedAt":"2016-03-03T23:15:55.000Z","commitMessage":"","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfc774":{"_id":"5f5c37ee1b5cdee568cfc774","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb1f2"},"documentId":"5f5c37ee1b5cdee568cfb1f2","changeMetrics":{"added":51,"removed":56},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.9.0","editedAt":"2016-03-03T23:15:02.000Z","commitMessage":"it's not a universal experience","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb32c":{"_id":"5f5c37ee1b5cdee568cfb32c","__typename":"Tag","userId":"KC7mjSorWj2XsdL3v","name":"Yudkowsky's 84Th Law","shortName":null,"slug":"yudkowskys-84th-law","core":false,"postCount":0,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.820Z","wikiOnly":true,"deleted":false,"isSubforum":false,"noindex":false},"Revision:5f5c37ee1b5cdee568cfd77f":{"_id":"5f5c37ee1b5cdee568cfd77f","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb32c"},"documentId":"5f5c37ee1b5cdee568cfb32c","changeMetrics":{"added":10,"removed":10},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.4.0","editedAt":"2016-02-08T06:51:57.000Z","commitMessage":"","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd77e":{"_id":"5f5c37ee1b5cdee568cfd77e","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb32c"},"documentId":"5f5c37ee1b5cdee568cfb32c","changeMetrics":{"added":36,"removed":9},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.3.0","editedAt":"2016-02-08T06:51:45.000Z","commitMessage":"","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd77d":{"_id":"5f5c37ee1b5cdee568cfd77d","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb32c"},"documentId":"5f5c37ee1b5cdee568cfb32c","changeMetrics":{"added":172,"removed":0},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.2.0","editedAt":"2016-02-08T06:51:07.000Z","commitMessage":"added reason for rename","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb32d":{"_id":"5f5c37ee1b5cdee568cfb32d","__typename":"Tag","userId":"nmk3nLpQE89dMRzzN","name":"Yudkowsky's Law","shortName":null,"slug":"yudkowskys-law","core":false,"postCount":0,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.822Z","wikiOnly":true,"deleted":false,"isSubforum":false,"noindex":false},"Revision:5f5c37ee1b5cdee568cfd780":{"_id":"5f5c37ee1b5cdee568cfd780","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb32d"},"documentId":"5f5c37ee1b5cdee568cfb32d","changeMetrics":{"added":29,"removed":0},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.0.0","editedAt":"2016-02-08T06:50:02.000Z","commitMessage":"Eliezer Yudkowsky moved page [[Yudkowsky's Law]] to [[Yudkowsky's 84th Law]]: I've got like 83 other laws that are more important than this one, it does not get to claim that title.","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:5f5c37ee1b5cdee568cfd77c":{"_id":"5f5c37ee1b5cdee568cfd77c","__typename":"Revision","tag":{"__ref":"Tag:5f5c37ee1b5cdee568cfb32c"},"documentId":"5f5c37ee1b5cdee568cfb32c","changeMetrics":{"added":0,"removed":0},"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"version":"1.1.0","editedAt":"2016-02-08T06:50:02.000Z","commitMessage":"Eliezer Yudkowsky moved page [[Yudkowsky's Law]] to [[Yudkowsky's 84th Law]]: I've got like 83 other laws that are more important than this one, it does not get to claim that title.","userId":"nmk3nLpQE89dMRzzN","score":2,"baseScore":2,"extendedScore":null,"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:KYuR2HcWPEmXZqMZs":{"_id":"KYuR2HcWPEmXZqMZs","__typename":"SocialPreviewType","imageUrl":""},"Post:KYuR2HcWPEmXZqMZs":{"_id":"KYuR2HcWPEmXZqMZs","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":null,"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[],"socialPreviewData":{"__ref":"SocialPreviewType:KYuR2HcWPEmXZqMZs"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2023-04-01T22:43:50.929Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":{},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":null,"commentCount":0,"voteCount":2,"baseScore":14,"extendedScore":null,"emojiReactors":{},"unlisted":false,"score":0.0002570841461420059,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2023-04-01T22:43:50.929Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":null,"suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":6,"afExtendedScore":null,"afCommentCount":0,"afLastCommentedAt":"2023-04-01T22:43:50.929Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":true,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"eliezer-yudkowsky-s-shortform","title":"Eliezer Yudkowsky's Shortform","draft":null,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:W2fkmatEzyrmbbrDt_contents":{"_id":"W2fkmatEzyrmbbrDt_contents","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2023-02-26T07:35:56.034Z","userId":"r38pkCm7wF4M44MDQ","html":"<p>Posts about metaethics from between June and August 2008.<\/p><p>Imported from <a href=\"https://wiki.lesswrong.com/wiki/Metaethics_sequence\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav\">Value Theory<\/a> and <a href=\"https://www.lesswrong.com/s/pmHZDpak4NeRLLLCw\">Seeing with Fresh Eyes<\/a>.<\/p>","commitMessage":"","wordCount":22,"htmlHighlight":"<p>Posts about metaethics from between June and August 2008.<\/p><p>Imported from <a href=\"https://wiki.lesswrong.com/wiki/Metaethics_sequence\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav\">Value Theory<\/a> and <a href=\"https://www.lesswrong.com/s/pmHZDpak4NeRLLLCw\">Seeing with Fresh Eyes<\/a>.<\/p>","plaintextDescription":"Posts about metaethics from between June and August 2008.\n\nImported from the wiki. Overlaps with Value Theory and Seeing with Fresh Eyes."},"Sequence:W2fkmatEzyrmbbrDt":{"_id":"W2fkmatEzyrmbbrDt","__typename":"Sequence","createdAt":"2018-11-01T01:23:48.607Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:W2fkmatEzyrmbbrDt_contents"},"gridImageId":"sequencesgrid/ds26mimg1uvv82k5d63v","bannerImageId":"sequences/dvzxpuvyvsd74j3csftl","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":null,"userProfileOrder":null,"af":false,"postsCount":38,"readPostsCount":0,"title":"Metaethics","canonicalCollection":null},"Revision:ePDpMhJoKCff6qnvh_contents":{"_id":"ePDpMhJoKCff6qnvh_contents","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2023-02-26T07:36:11.343Z","userId":"r38pkCm7wF4M44MDQ","html":"<p>A <em>non-mysterious <\/em>introduction to quantum mechanics, intended to be accessible to anyone who can grok algebra and complex numbers. Cleaning up the old confusion about QM is used to introduce basic issues in rationality (such as the technical version of <a href=\"https://wiki.lesswrong.com/wiki/Occam%27s_Razor\">Occams Razor<\/a>), epistemology, reductionism, naturalism, and philosophy of science. <em>Not<\/em> dispensable reading, even though the exact reasons for the digression are hard to explain in advance of reading.<\/p><p>Imported from <a href=\"https://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ\">Quantum Physics and Many Worlds<\/a> and <a href=\"https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy\">Science and Rationality<\/a>.<\/p>","commitMessage":"","wordCount":83,"htmlHighlight":"<p>A <em>non-mysterious <\/em>introduction to quantum mechanics, intended to be accessible to anyone who can grok algebra and complex numbers. Cleaning up the old confusion about QM is used to introduce basic issues in rationality (such as the technical version of <a href=\"https://wiki.lesswrong.com/wiki/Occam%27s_Razor\">Occams Razor<\/a>), epistemology, reductionism, naturalism, and philosophy of science. <em>Not<\/em> dispensable reading, even though the exact reasons for the digression are hard to explain in advance of reading.<\/p><p>Imported from <a href=\"https://wiki.lesswrong.com/wiki/The_Quantum_Physics_Sequence\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ\">Quantum Physics and Many Worlds<\/a> and <a href=\"https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy\">Science and Rationality<\/a>.<\/p>","plaintextDescription":"A non-mysterious introduction to quantum mechanics, intended to be accessible to anyone who can grok algebra and complex numbers. Cleaning up the old confusion about QM is used to introduce basic issues in rationality (such as the technical version of Occams Razor), epistemology, reductionism, naturalism, and philosophy of science. Not dispensable reading, even though the exact reasons for the digression are hard to explain in advance of reading.\n\nImported from the wiki. Overlaps with Quantum Physics and Many Worlds and Science and Rationality."},"Sequence:ePDpMhJoKCff6qnvh":{"_id":"ePDpMhJoKCff6qnvh","__typename":"Sequence","createdAt":"2018-09-23T09:26:00.447Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:ePDpMhJoKCff6qnvh_contents"},"gridImageId":"sequencesgrid/e0chotk2uafu1ic9kvny","bannerImageId":"sequences/puthl5ngdvqx5tkylwfu","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":null,"userProfileOrder":null,"af":false,"postsCount":55,"readPostsCount":0,"title":"Quantum Physics","canonicalCollection":null},"Revision:d3WgHDBAPYYScp5Em_contents":{"_id":"d3WgHDBAPYYScp5Em_contents","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2022-10-13T07:26:27.113Z","userId":"r38pkCm7wF4M44MDQ","html":"<p>A concrete theory of transhuman values. How much fun is there in the universe? Will we ever run out of fun? Are we having fun yet? Could we be having more fun? Part of the <a href=\"https://wiki.lesswrong.com/wiki/Complexity_of_value\">complexity of value<\/a> thesis.<\/p><p>Also forms part of the fully general answer to religious theodicy.<\/p><p>Sequence by <a href=\"https://www.lesswrong.com/users/eliezer_yudkowsky\">Eliezer Yudkowsky<\/a>, imported from <a href=\"https://wiki.lesswrong.com/wiki/The_Fun_Theory_Sequence\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav\">Value Theory<\/a>.<\/p>","commitMessage":"","wordCount":62,"htmlHighlight":"<p>A concrete theory of transhuman values. How much fun is there in the universe? Will we ever run out of fun? Are we having fun yet? Could we be having more fun? Part of the <a href=\"https://wiki.lesswrong.com/wiki/Complexity_of_value\">complexity of value<\/a> thesis.<\/p><p>Also forms part of the fully general answer to religious theodicy.<\/p><p>Sequence by <a href=\"https://www.lesswrong.com/users/eliezer_yudkowsky\">Eliezer Yudkowsky<\/a>, imported from <a href=\"https://wiki.lesswrong.com/wiki/The_Fun_Theory_Sequence\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav\">Value Theory<\/a>.<\/p>","plaintextDescription":"A concrete theory of transhuman values. How much fun is there in the universe? Will we ever run out of fun? Are we having fun yet? Could we be having more fun? Part of the complexity of value thesis.\n\nAlso forms part of the fully general answer to religious theodicy.\n\nSequence by Eliezer Yudkowsky, imported from the wiki. Overlaps with Value Theory."},"Sequence:d3WgHDBAPYYScp5Em":{"_id":"d3WgHDBAPYYScp5Em","__typename":"Sequence","createdAt":"2018-09-22T01:17:01.044Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:d3WgHDBAPYYScp5Em_contents"},"gridImageId":"sequencesgrid/ncfkdhspgrfhhjpbisaj","bannerImageId":"sequences/kusdtqumjqf6bymmhaso","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":1,"userProfileOrder":null,"af":false,"postsCount":29,"readPostsCount":0,"title":"Fun Theory","canonicalCollection":null},"Revision:AmFb5xWbPWWQyQ244_contents":{"_id":"AmFb5xWbPWWQyQ244_contents","__typename":"Revision","version":"1.2.0","updateType":"minor","editedAt":"2023-02-26T07:36:23.926Z","userId":"r38pkCm7wF4M44MDQ","html":"<p><strong>Ethical injunctions<\/strong> are rules not to do something even when it's the right thing to do. (That is, you refrain \"even when your brain has computed it's the right thing to do\", but this will just <em>seem like<\/em> \"the right thing to do\".)<\/p>\n<p>For example, you shouldn't rob banks even if you plan to give the money to a good cause.<\/p>\n<p>This is to protect you from your own cleverness (especially taking bad black swan bets), and the <a href=\"https://wiki.lesswrong.com/wiki/Corrupted_hardware\">Corrupted hardware<\/a> you're running on.<\/p>\n<p>Imported from <a href=\"https://wiki.lesswrong.com/wiki/Ethical_Injunctions\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/waF2Pomid7YHjfEDt\">Quantified Humanism<\/a> and <a href=\"https://www.lesswrong.com/s/GSqFqc646rsRd2oyz\">Against Rationalization<\/a>.<\/p>\n","commitMessage":"","wordCount":94,"htmlHighlight":"<p><strong>Ethical injunctions<\/strong> are rules not to do something even when it's the right thing to do. (That is, you refrain \"even when your brain has computed it's the right thing to do\", but this will just <em>seem like<\/em> \"the right thing to do\".)<\/p>\n<p>For example, you shouldn't rob banks even if you plan to give the money to a good cause.<\/p>\n<p>This is to protect you from your own cleverness (especially taking bad black swan bets), and the <a href=\"https://wiki.lesswrong.com/wiki/Corrupted_hardware\">Corrupted hardware<\/a> you're running on.<\/p>\n<p>Imported from <a href=\"https://wiki.lesswrong.com/wiki/Ethical_Injunctions\">the wiki<\/a>. Overlaps with <a href=\"https://www.lesswrong.com/s/waF2Pomid7YHjfEDt\">Quantified Humanism<\/a> and <a href=\"https://www.lesswrong.com/s/GSqFqc646rsRd2oyz\">Against Rationalization<\/a>.<\/p>","plaintextDescription":"Ethical injunctions are rules not to do something even when it's the right thing to do. (That is, you refrain \"even when your brain has computed it's the right thing to do\", but this will just seem like \"the right thing to do\".)\n\nFor example, you shouldn't rob banks even if you plan to give the money to a good cause.\n\nThis is to protect you from your own cleverness (especially taking bad black swan bets), and the Corrupted hardware you're running on.\n\nImported from the wiki. Overlaps with Quantified Humanism and Against Rationalization."},"Sequence:AmFb5xWbPWWQyQ244":{"_id":"AmFb5xWbPWWQyQ244","__typename":"Sequence","createdAt":"2018-09-22T00:39:11.385Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:AmFb5xWbPWWQyQ244_contents"},"gridImageId":"sequencesgrid/nczv7w6hr10v4rumtusv","bannerImageId":"sequences/r9ovcbou7w0z8mzmvefo","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":null,"userProfileOrder":null,"af":false,"postsCount":8,"readPostsCount":0,"title":"Ethical Injunctions","canonicalCollection":null},"Revision:LAop879LCQWrM5YnE_contents":{"_id":"LAop879LCQWrM5YnE_contents","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2018-08-30T18:42:14.286Z","userId":"nmk3nLpQE89dMRzzN","html":"<p>The <em>beisutsukai<\/em> are members of a secret society, also known as the Bayesian Conspiracy.<\/p><p>Derived from &quot;beisu&quot;, a Japanese transliteration of Bayes (though more accurate Japanese pronunciation would be bei-zu, not bei-su), plus &quot;tsukai&quot;, Japanese &quot;user&quot;, therefore &quot;Bayes-user&quot;. A casual, shortened pronunciation would probably come out &quot;bayes-tskai&quot;.<\/p>","commitMessage":null,"wordCount":46,"htmlHighlight":"<p>The <em>beisutsukai<\/em> are members of a secret society, also known as the Bayesian Conspiracy.<\/p><p>Derived from &quot;beisu&quot;, a Japanese transliteration of Bayes (though more accurate Japanese pronunciation would be bei-zu, not bei-su), plus &quot;tsukai&quot;, Japanese &quot;user&quot;, therefore &quot;Bayes-user&quot;. A casual, shortened pronunciation would probably come out &quot;bayes-tskai&quot;.<\/p>","plaintextDescription":"The beisutsukai are members of a secret society, also known as the Bayesian Conspiracy.\n\nDerived from \"beisu\", a Japanese transliteration of Bayes (though more accurate Japanese pronunciation would be bei-zu, not bei-su), plus \"tsukai\", Japanese \"user\", therefore \"Bayes-user\". A casual, shortened pronunciation would probably come out \"bayes-tskai\"."},"Sequence:LAop879LCQWrM5YnE":{"_id":"LAop879LCQWrM5YnE","__typename":"Sequence","createdAt":"2018-08-30T18:42:14.286Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:LAop879LCQWrM5YnE_contents"},"gridImageId":null,"bannerImageId":null,"canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":null,"userProfileOrder":null,"af":false,"postsCount":5,"readPostsCount":0,"title":"The Bayesian Conspiracy","canonicalCollection":null},"Revision:qWoFR4ytMpQ5vw3FT_contents":{"_id":"qWoFR4ytMpQ5vw3FT_contents","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2018-07-06T20:02:30.941Z","userId":"nmk3nLpQE89dMRzzN","html":"<blockquote>&quot;The kind of classic fifties-era first-contact story that Jonathan Swift might have written, if Jonathan Swift had had a background in game theory.&quot;<br/>      -- (Hugo nominee) Peter Watts, &quot;<a href=\"http://www.rifters.com/crawl/?p=266\">In Praise of Baby-Eating<\/a>&quot;<\/blockquote><p><\/p><p><em>Three Worlds Collide<\/em> is a story I wrote to illustrate some points on naturalistic metaethics and diverse other issues of rational conduct.  It grew, as such things do, into a small novella.  On publication, it proved widely popular and widely criticized.  Be warned that the story, as it wrote itself, ended up containing some profanity and PG-13 content.<\/p><p>(PDF is <a href=\"http://robinhanson.typepad.com/files/three-worlds-collide.pdf\">here<\/a>. Old contents post with comments is <a href=\"https://www.lesswrong.com/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8\">here<\/a>.)<\/p>","commitMessage":null,"wordCount":102,"htmlHighlight":"<blockquote>&quot;The kind of classic fifties-era first-contact story that Jonathan Swift might have written, if Jonathan Swift had had a background in game theory.&quot;<br/>      -- (Hugo nominee) Peter Watts, &quot;<a href=\"http://www.rifters.com/crawl/?p=266\">In Praise of Baby-Eating<\/a>&quot;<\/blockquote><p><\/p><p><em>Three Worlds Collide<\/em> is a story I wrote to illustrate some points on naturalistic metaethics and diverse other issues of rational conduct.  It grew, as such things do, into a small novella.  On publication, it proved widely popular and widely criticized.  Be warned that the story, as it wrote itself, ended up containing some profanity and PG-13 content.<\/p><p>(PDF is <a href=\"http://robinhanson.typepad.com/files/three-worlds-collide.pdf\">here<\/a>. Old contents post with comments is <a href=\"https://www.lesswrong.com/posts/HawFh7RvDM4RyoJ2d/three-worlds-collide-0-8\">here<\/a>.)<\/p>","plaintextDescription":"> \"The kind of classic fifties-era first-contact story that Jonathan Swift might have written, if Jonathan Swift had had a background in game theory.\"\n>       -- (Hugo nominee) Peter Watts, \"In Praise of Baby-Eating\"\n\n\n\nThree Worlds Collide is a story I wrote to illustrate some points on naturalistic metaethics and diverse other issues of rational conduct.  It grew, as such things do, into a small novella.  On publication, it proved widely popular and widely criticized.  Be warned that the story, as it wrote itself, ended up containing some profanity and PG-13 content.\n\n(PDF is here. Old contents post with comments is here.)"},"Sequence:qWoFR4ytMpQ5vw3FT":{"_id":"qWoFR4ytMpQ5vw3FT","__typename":"Sequence","createdAt":"2018-07-06T20:02:30.941Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:qWoFR4ytMpQ5vw3FT_contents"},"gridImageId":"sequencesgrid/sio9b8jw1apesuispocg","bannerImageId":"sequences/evlaa0fe6nysqqhhqwyd","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":1,"userProfileOrder":null,"af":false,"postsCount":8,"readPostsCount":0,"title":"Three Worlds Collide","canonicalCollection":null},"Revision:SqFbMbtxGybdS2gRs_contents":{"_id":"SqFbMbtxGybdS2gRs_contents","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2020-04-19T19:13:02.750Z","userId":"XtphY3uYHwruKqDyG","html":"<p>These essays include a discussion of truth, formal logic, causality, and metaethics, and are a good way for more ambitious readers to quickly get up to speed.<\/p>","commitMessage":null,"wordCount":27,"htmlHighlight":"<p>These essays include a discussion of truth, formal logic, causality, and metaethics, and are a good way for more ambitious readers to quickly get up to speed.<\/p>","plaintextDescription":"These essays include a discussion of truth, formal logic, causality, and metaethics, and are a good way for more ambitious readers to quickly get up to speed."},"Sequence:SqFbMbtxGybdS2gRs":{"_id":"SqFbMbtxGybdS2gRs","__typename":"Sequence","createdAt":"2018-01-22T08:55:37.700Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:SqFbMbtxGybdS2gRs_contents"},"gridImageId":"sequencesgrid/i2ogsvmipbdolntkew4a","bannerImageId":"sequences/kx9ydblgnzt7f16remz3","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":1,"userProfileOrder":null,"af":false,"postsCount":16,"readPostsCount":0,"title":"Highly Advanced Epistemology 101 for Beginners","canonicalCollection":null},"Revision:oLGCcbnvabyibnG9d_contents":{"_id":"oLGCcbnvabyibnG9d_contents","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-11-07T03:42:59.852Z","userId":"nmk3nLpQE89dMRzzN","html":"<p>Inadequate Equilibria is a book about a generalized notion of efficient markets, and how we can use this notion to guess where society will or won’t be effective at pursuing some widely desired goal.<\/p>","commitMessage":null,"wordCount":34,"htmlHighlight":"<p>Inadequate Equilibria is a book about a generalized notion of efficient markets, and how we can use this notion to guess where society will or won’t be effective at pursuing some widely desired goal.<\/p>","plaintextDescription":"Inadequate Equilibria is a book about a generalized notion of efficient markets, and how we can use this notion to guess where society will or won’t be effective at pursuing some widely desired goal."},"Sequence:oLGCcbnvabyibnG9d":{"_id":"oLGCcbnvabyibnG9d","__typename":"Sequence","createdAt":"2017-11-07T03:42:59.852Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:oLGCcbnvabyibnG9d_contents"},"gridImageId":"sequencesgrid/vbhv0s06jdmonk6garvf","bannerImageId":"sequences/gl1xzufmu65v6cn66wrm","canonicalCollectionSlug":null,"draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":1,"userProfileOrder":null,"af":false,"postsCount":9,"readPostsCount":0,"title":"Inadequate Equilibria","canonicalCollection":null},"Revision:pvim9PZJ6qHRTMqD3_contents":{"_id":"pvim9PZJ6qHRTMqD3_contents","__typename":"Revision","version":"1.0.0","updateType":null,"editedAt":"2017-08-24T04:52:04.379Z","userId":"nmk3nLpQE89dMRzzN","html":"<p>Discusses rationality groups and group rationality, raising the questions:<\/p><ul><li>Can rationality be learned and taught?<\/li><li>If so, how much improvement is possible?<\/li><\/ul><p>How can we be confident we&#x27;re seeing a real effect in a rationality intervention, and picking out the right cause?<\/p><ul><li>What community norms would make this process of bettering ourselves easier?<\/li><li>Can we effectively collaborate on large-scale problems without sacrificing our freedom of thought and conduct?<\/li><\/ul><p>Above all: What’s missing? What should be in the next generation of rationality primers—the ones that replace this text, improve on its style, test its prescriptions, supplement its content, and branch out in altogether new directions?<\/p>","commitMessage":null,"wordCount":110,"htmlHighlight":"<p>Discusses rationality groups and group rationality, raising the questions:<\/p><ul><li>Can rationality be learned and taught?<\/li><li>If so, how much improvement is possible?<\/li><\/ul><p>How can we be confident we&#x27;re seeing a real effect in a rationality intervention, and picking out the right cause?<\/p><ul><li>What community norms would make this process of bettering ourselves easier?<\/li><li>Can we effectively collaborate on large-scale problems without sacrificing our freedom of thought and conduct?<\/li><\/ul><p>Above all: What’s missing? What should be in the next generation of rationality primers—the ones that replace this text, improve on its style, test its prescriptions, supplement its content, and branch out in altogether new directions?<\/p>","plaintextDescription":"Discusses rationality groups and group rationality, raising the questions:\n\n * Can rationality be learned and taught?\n * If so, how much improvement is possible?\n\nHow can we be confident we're seeing a real effect in a rationality intervention, and picking out the right cause?\n\n * What community norms would make this process of bettering ourselves easier?\n * Can we effectively collaborate on large-scale problems without sacrificing our freedom of thought and conduct?\n\nAbove all: What’s missing? What should be in the next generation of rationality primers—the ones that replace this text, improve on its style, test its prescriptions, supplement its content, and branch out in altogether new directions?"},"Collection:oneQyj4pw77ynzwAF":{"_id":"oneQyj4pw77ynzwAF","__typename":"Collection","title":"Rationality: A-Z"},"Sequence:pvim9PZJ6qHRTMqD3":{"_id":"pvim9PZJ6qHRTMqD3","__typename":"Sequence","createdAt":"2017-08-24T04:52:04.379Z","userId":"nmk3nLpQE89dMRzzN","user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"contents":{"__ref":"Revision:pvim9PZJ6qHRTMqD3_contents"},"gridImageId":"sequencesgrid/yd3bzuj2zhnafkh9uavf","bannerImageId":"sequences/zxtxxob2ltpnviyavqdk","canonicalCollectionSlug":"rationality","draft":false,"isDeleted":false,"hidden":false,"hideFromAuthorPage":false,"noindex":false,"curatedOrder":null,"userProfileOrder":null,"af":false,"postsCount":22,"readPostsCount":0,"title":"The Craft and the Community","canonicalCollection":{"__ref":"Collection:oneQyj4pw77ynzwAF"}},"Post:qc7P2NwfxQMC3hdgm":{"_id":"qc7P2NwfxQMC3hdgm","__typename":"Post","isRead":null,"slug":"rationalism-before-the-sequences","title":"Rationalism before the Sequences","draft":null,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"pmG94FcKP7oqLWPEq","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:oZtsFc5oCMA9Zk5Tg_contents":{"_id":"oZtsFc5oCMA9Zk5Tg_contents","__typename":"Revision","html":"<p>I note that I haven't said out loud, and should say out loud, that I endorse this history. &nbsp;Not every single line of it (see my other comment on why I reject verificationism) but on the whole, this is well-informed and well-applied.<\/p>","plaintextMainText":"I note that I haven't said out loud, and should say out loud, that I endorse this history.  Not every single line of it (see my other comment on why I reject verificationism) but on the whole, this is well-informed and well-applied.","wordCount":42},"Comment:oZtsFc5oCMA9Zk5Tg":{"_id":"oZtsFc5oCMA9Zk5Tg","__typename":"Comment","post":{"__ref":"Post:qc7P2NwfxQMC3hdgm"},"tag":null,"postId":"qc7P2NwfxQMC3hdgm","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:oZtsFc5oCMA9Zk5Tg_contents"},"postedAt":"2024-08-26T15:56:32.900Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":47,"extendedScore":{"reacts":{"thanks":[{"karma":36777,"userId":"XtphY3uYHwruKqDyG","reactType":"created","displayName":"habryka"},{"karma":32279,"userId":"EQNTWXLKMeWMp2FQS","reactType":"seconded","displayName":"Ben Pace"},{"karma":766,"userId":"P7t9L2tKiCkjt8dkq","reactType":"seconded","displayName":"sunwillrise"},{"karma":535,"userId":"XGEcH5rmq4yGvD82A","reactType":"seconded","displayName":"Saul Munn"}]},"agreement":0,"approvalVoteCount":20,"agreementVoteCount":0},"score":0.015258999541401863,"voteCount":24,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":23,"afExtendedScore":{"reacts":{"thanks":[{"karma":36777,"userId":"XtphY3uYHwruKqDyG","reactType":"created","displayName":"habryka"},{"karma":32279,"userId":"EQNTWXLKMeWMp2FQS","reactType":"seconded","displayName":"Ben Pace"},{"karma":766,"userId":"P7t9L2tKiCkjt8dkq","reactType":"seconded","displayName":"sunwillrise"}]},"agreement":0,"approvalVoteCount":13,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.4.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-08-26T15:56:32.909Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Post:fjfWrKhEawwBGCTGs":{"_id":"fjfWrKhEawwBGCTGs","__typename":"Post","isRead":null,"slug":"a-simple-case-for-extreme-inner-misalignment","title":"A simple case for extreme inner misalignment","draft":null,"shortform":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"BCmzFRdQhqLPREvat","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:cGCaQactztiBrxGjh_contents":{"_id":"cGCaQactztiBrxGjh_contents","__typename":"Revision","html":"<blockquote><p>If you had to put a rough number on how likely it is that a misaligned superintelligence would primarily value \"small molecular squiggles\" versus other types of misaligned goals, would it be more like 1000:1 or 1:1 or 1000:1 or something else?&nbsp;<\/p><\/blockquote><p>Value them <i>primarily?<\/i> &nbsp;Uhhh... maybe 1:3 against? &nbsp;I admit I have never actually pondered this question before today; but 1 in 4 uncontrolled superintelligences spending most of their resources on tiny squiggles doesn't sound off by, like, more than 1-2 orders of magnitude in either direction.<\/p><blockquote><p>Clocks are not actually very complicated; how plausible is it on your model that these goals are as complicated as, say, a typical human's preferences about how human civilization is structured?<\/p><\/blockquote><p>It wouldn't shock me if their goals end up far more complicated than human ones; the most obvious pathway for it is (a) gradient descent turning out to produce internal preferences much faster than natural selection + biological reinforcement learning and (b) some significant fraction of those preferences being retained under reflection. &nbsp;(Where (b) strikes me as way less probable than (a), but not wholly forbidden.) &nbsp;The second most obvious pathway is if a bunch of weird detailed noise appears in the first version of the reflective process and then freezes.<\/p>","plaintextMainText":"Value them primarily?  Uhhh... maybe 1:3 against?  I admit I have never actually pondered this question before today; but 1 in 4 uncontrolled superintelligences spending most of their resources on tiny squiggles doesn't sound off by, like, more than 1-2 orders of magnitude in either direction.\n\nIt wouldn't shock me if their goals end up far more complicated than human ones; the most obvious pathway for it is (a) gradient descent turning out to produce internal preferences much faster than natural selection + biological reinforcement learning and (b) some significant fraction of those preferences being retained under reflection.  (Where (b) strikes me as way less probable than (a), but not wholly forbidden.)  The second most obvious pathway is if a bunch of weird detailed noise appears in the first version of the reflective process and then freezes.","wordCount":212},"Comment:cGCaQactztiBrxGjh":{"_id":"cGCaQactztiBrxGjh","__typename":"Comment","post":{"__ref":"Post:fjfWrKhEawwBGCTGs"},"tag":null,"postId":"fjfWrKhEawwBGCTGs","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"v2d94vmiKYdujYzji","topLevelCommentId":"fTThxDyxEiNsaTwaP","descendentCount":0,"title":null,"contents":{"__ref":"Revision:cGCaQactztiBrxGjh_contents"},"postedAt":"2024-08-09T18:14:34.806Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":20,"extendedScore":{"reacts":{},"agreement":5,"approvalVoteCount":12,"agreementVoteCount":3},"score":0.004726925399154425,"voteCount":12,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":12,"afExtendedScore":{"reacts":{},"agreement":4,"approvalVoteCount":9,"agreementVoteCount":2},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.6.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-08-09T18:14:34.812Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Post:hzt9gHpNwA2oHtwKX":{"_id":"hzt9gHpNwA2oHtwKX","__typename":"Post","isRead":null,"slug":"self-other-overlap-a-neglected-approach-to-ai-alignment","title":"Self-Other Overlap: A Neglected Approach to AI Alignment","draft":null,"shortform":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"BY9TJYzgxkR5GYya8","coauthorStatuses":[{"userId":"isB2XPxy3a4xyvNzQ","confirmed":true,"requested":true},{"userId":"wN6u4c4hDAn7ydasg","confirmed":true,"requested":true},{"userId":"iydKdeJA9C8phpy6K","confirmed":true,"requested":true},{"userId":"XZeuzy2MWbgcRfgoc","confirmed":true,"requested":false},{"userId":"W9SdD7zsbzZdk8Riq","confirmed":true,"requested":true}],"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:WapHz3gokGBd3KHKm_contents":{"_id":"WapHz3gokGBd3KHKm_contents","__typename":"Revision","html":"<p>Not obviously stupid on a very quick skim. &nbsp;I will have to actually read it to figure out where it's stupid.<\/p><p>(I rarely give any review this positive on a first skim. &nbsp;Congrats.)<\/p>","plaintextMainText":"Not obviously stupid on a very quick skim.  I will have to actually read it to figure out where it's stupid.\n\n(I rarely give any review this positive on a first skim.  Congrats.)","wordCount":33},"Comment:WapHz3gokGBd3KHKm":{"_id":"WapHz3gokGBd3KHKm","__typename":"Comment","post":{"__ref":"Post:hzt9gHpNwA2oHtwKX"},"tag":null,"postId":"hzt9gHpNwA2oHtwKX","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":5,"title":null,"contents":{"__ref":"Revision:WapHz3gokGBd3KHKm_contents"},"postedAt":"2024-08-09T02:19:13.583Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":37,"extendedScore":{"reacts":{},"agreement":13,"approvalVoteCount":43,"agreementVoteCount":15},"score":0.008204000070691109,"voteCount":48,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":16,"afExtendedScore":{"reacts":{},"agreement":14,"approvalVoteCount":21,"agreementVoteCount":9},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.12.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-08-14T21:48:24.542Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":2,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Post:rP66bz34crvDudzcJ":{"_id":"rP66bz34crvDudzcJ","__typename":"Post","isRead":null,"slug":"decision-theory-does-not-imply-that-we-get-to-have-nice","title":"Decision theory does not imply that we get to have nice things","draft":null,"shortform":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"xSfc2APSi8WzFxp7i","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:XGTupMJe2ccyKhgrQ_contents":{"_id":"XGTupMJe2ccyKhgrQ_contents","__typename":"Revision","html":"<p>By \"dumb player\" I did not mean <i>as dumb as a human<\/i> player. &nbsp;I meant \"too dumb to compute the pseudorandom numbers, but not too dumb to simulate other players faithfully apart from that\". &nbsp;I did not realize we were talking about humans at all. &nbsp;This jumps out more to me as a potential source of misunderstanding than it did 15 years ago, and for that I apologize.<\/p>","plaintextMainText":"By \"dumb player\" I did not mean as dumb as a human player.  I meant \"too dumb to compute the pseudorandom numbers, but not too dumb to simulate other players faithfully apart from that\".  I did not realize we were talking about humans at all.  This jumps out more to me as a potential source of misunderstanding than it did 15 years ago, and for that I apologize.","wordCount":68},"Comment:XGTupMJe2ccyKhgrQ":{"_id":"XGTupMJe2ccyKhgrQ","__typename":"Comment","post":{"__ref":"Post:rP66bz34crvDudzcJ"},"tag":null,"postId":"rP66bz34crvDudzcJ","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"sYybWGmhQNhb9QeMR","topLevelCommentId":"ABCGngfphSgXQkCeA","descendentCount":1,"title":null,"contents":{"__ref":"Revision:XGTupMJe2ccyKhgrQ_contents"},"postedAt":"2024-07-29T19:13:47.897Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":7,"extendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":4,"agreementVoteCount":1},"score":0.0014562956057488918,"voteCount":4,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":5,"afExtendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":3,"agreementVoteCount":1},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"0.4.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-07-30T05:03:20.304Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Revision:vkCEJ5ivLWsnNnpbE_contents":{"_id":"vkCEJ5ivLWsnNnpbE_contents","__typename":"Revision","html":"<p>I don't always remember my previous positions all that well, but I doubt I would have said at any point that sufficiently advanced LDT agents are friendly to each other, rather than that they coordinate well with each other (and not so with us)?<\/p>","plaintextMainText":"I don't always remember my previous positions all that well, but I doubt I would have said at any point that sufficiently advanced LDT agents are friendly to each other, rather than that they coordinate well with each other (and not so with us)?","wordCount":44},"Comment:vkCEJ5ivLWsnNnpbE":{"_id":"vkCEJ5ivLWsnNnpbE","__typename":"Comment","post":{"__ref":"Post:rP66bz34crvDudzcJ"},"tag":null,"postId":"rP66bz34crvDudzcJ","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"ABCGngfphSgXQkCeA","topLevelCommentId":"ABCGngfphSgXQkCeA","descendentCount":8,"title":null,"contents":{"__ref":"Revision:vkCEJ5ivLWsnNnpbE_contents"},"postedAt":"2024-07-28T21:30:21.852Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":12,"extendedScore":{"reacts":{},"agreement":8,"approvalVoteCount":5,"agreementVoteCount":3},"score":0.0022631261963397264,"voteCount":6,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":6,"afExtendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":4,"agreementVoteCount":1},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"0.4.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-07-30T06:06:00.975Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Revision:gAGnuYfDdwbAm9tSv_contents":{"_id":"gAGnuYfDdwbAm9tSv_contents","__typename":"Revision","html":"<p>Actually, to slightly amend that: &nbsp;The part where squiggles are small is a <i>more than randomly likely<\/i> part of the prediction, but not a <i>load-bearing part of downstream predictions or the policy argument.<\/i> &nbsp;Most of the time we don't needlessly build our own paperclips to be the size of skyscrapers; even when having fun, we try to do the fun without vastly more resources, than are necessary to that amount of fun, because then we'll have needlessly used up all our resources and not get to have more fun. &nbsp;We buy cookies that cost a dollar instead of a hundred thousand dollars. &nbsp;A very wide variety of utility functions you could run over the outside universe will have optima around making lots of small things because each thing scores one point, and so to score as many points as possible, each thing is as small as it can be as still count as a thing. &nbsp;Nothing downstream depends on this part coming true and there are many ways for it to come false; but the part where the squiggles are small and molecular <i>is<\/i> an obvious kind of guess. &nbsp;\"Great giant squiggles of nickel the size of a solar system would be no more valuable, even from a very embracing and cosmopolitan perspective on value\" is the loadbearing part.<\/p>","plaintextMainText":"Actually, to slightly amend that:  The part where squiggles are small is a more than randomly likely part of the prediction, but not a load-bearing part of downstream predictions or the policy argument.  Most of the time we don't needlessly build our own paperclips to be the size of skyscrapers; even when having fun, we try to do the fun without vastly more resources, than are necessary to that amount of fun, because then we'll have needlessly used up all our resources and not get to have more fun.  We buy cookies that cost a dollar instead of a hundred thousand dollars.  A very wide variety of utility functions you could run over the outside universe will have optima around making lots of small things because each thing scores one point, and so to score as many points as possible, each thing is as small as it can be as still count as a thing.  Nothing downstream depends on this part coming true and there are many ways for it to come false; but the part where the squiggles are small and molecular is an obvious kind of guess.  \"Great giant squiggles of nickel the size of a solar system would be no more valuable, even from a very embracing and cosmopolitan perspective on value\" is the loadbearing part.","wordCount":219},"Comment:gAGnuYfDdwbAm9tSv":{"_id":"gAGnuYfDdwbAm9tSv","__typename":"Comment","post":{"__ref":"Post:fjfWrKhEawwBGCTGs"},"tag":null,"postId":"fjfWrKhEawwBGCTGs","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"fTThxDyxEiNsaTwaP","topLevelCommentId":"fTThxDyxEiNsaTwaP","descendentCount":2,"title":null,"contents":{"__ref":"Revision:gAGnuYfDdwbAm9tSv_contents"},"postedAt":"2024-07-16T19:59:20.949Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":26,"extendedScore":{"reacts":{},"agreement":11,"approvalVoteCount":15,"agreementVoteCount":7},"score":0.004105086904019117,"voteCount":15,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":11,"afExtendedScore":{"reacts":{},"agreement":8,"approvalVoteCount":9,"agreementVoteCount":4},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.6.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-08-09T18:14:35.079Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Revision:fTThxDyxEiNsaTwaP_contents":{"_id":"fTThxDyxEiNsaTwaP_contents","__typename":"Revision","html":"<p>The part where squiggles are small and simple is unimportant.  They could be bigger and more complicated, like building giant mechanical clocks.  The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value.<\/p>\n","plaintextMainText":"The part where squiggles are small and simple is unimportant. They could be bigger and more complicated, like building giant mechanical clocks. The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value.","wordCount":43},"Comment:fTThxDyxEiNsaTwaP":{"_id":"fTThxDyxEiNsaTwaP","__typename":"Comment","post":{"__ref":"Post:fjfWrKhEawwBGCTGs"},"tag":null,"postId":"fjfWrKhEawwBGCTGs","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":3,"title":null,"contents":{"__ref":"Revision:fTThxDyxEiNsaTwaP_contents"},"postedAt":"2024-07-16T14:11:40.041Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":26,"extendedScore":{"reacts":{"agree":[{"karma":764,"quotes":["The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value"],"userId":"P7t9L2tKiCkjt8dkq","reactType":"created","displayName":"sunwillrise"},{"karma":904,"quotes":["The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value"],"userId":"oxqFdPeNG7DuSmYGD","reactType":"seconded","displayName":"Mikhail Samin"}]},"agreement":19,"approvalVoteCount":15,"agreementVoteCount":10},"score":0.004091599490493536,"voteCount":17,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":10,"afExtendedScore":{"reacts":{"agree":[{"karma":764,"quotes":["The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value"],"userId":"P7t9L2tKiCkjt8dkq","reactType":"created","displayName":"sunwillrise"}]},"agreement":15,"approvalVoteCount":9,"agreementVoteCount":6},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.6.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-08-09T18:14:35.079Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Post:q8uNoJBgcpAe3bSBp":{"_id":"q8uNoJBgcpAe3bSBp","__typename":"Post","isRead":null,"slug":"my-ai-model-delta-compared-to-yudkowsky","title":"My AI Model Delta Compared To Yudkowsky","draft":null,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"MEu8MdhruX5jfGsFQ","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:CixonSXNfLgAPh48Z_contents":{"_id":"CixonSXNfLgAPh48Z_contents","__typename":"Revision","html":"<p>I think that the AI's internal ontology is liable to have some noticeable alignments to human ontology w/r/t the purely predictive aspects of the natural world; it wouldn't surprise me to find distinct thoughts in there about electrons. &nbsp;As the internal ontology goes to be more about affordances and actions, I expect to find increasing disalignment. &nbsp;As the internal ontology takes on any <i>reflective<\/i> aspects, parts of the representation that mix with facts about the AI's internals, I expect to find much larger differences -- not just that the AI has a different concept boundary around \"easy to understand\", say, but that it maybe doesn't have any such internal notion as \"easy to understand\" at all, because easiness isn't in the environment and the AI doesn't have any such thing as \"effort\". &nbsp;Maybe it's got categories around yieldingness to seven different categories of methods, and/or some general notion of \"can predict at all / can't predict at all\", but no general notion that maps onto human \"easy to understand\" -- though \"easy to understand\" is plausibly general-enough that I wouldn't be unsurprised to find a mapping after all.<\/p><p>Corrigibility and actual human values are both heavily reflective concepts. &nbsp;If you master a requisite level of the prerequisite skill of noticing when a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment -- which of course most people can't do because they project the category boundary onto the environment, but I have some credit that John Wentworth might be able to do it some -- and then you start mapping out concept definitions about corrigibility or values or god help you CEV, that might help highlight where some of my concern about unnatural abstractions comes in.<\/p><p>&nbsp;<\/p><p><i>Entirely separately,<\/i> I have concerns about the ability of ML-based technology to robustly point the AI in <i>any <\/i>builder-intended direction whatsoever, <i>even if<\/i> there exists some not-too-large adequate mapping from that intended direction onto the AI's internal ontology at training time. &nbsp;My guess is that more of the disagreement lies here.<\/p>","plaintextMainText":"I think that the AI's internal ontology is liable to have some noticeable alignments to human ontology w/r/t the purely predictive aspects of the natural world; it wouldn't surprise me to find distinct thoughts in there about electrons.  As the internal ontology goes to be more about affordances and actions, I expect to find increasing disalignment.  As the internal ontology takes on any reflective aspects, parts of the representation that mix with facts about the AI's internals, I expect to find much larger differences -- not just that the AI has a different concept boundary around \"easy to understand\", say, but that it maybe doesn't have any such internal notion as \"easy to understand\" at all, because easiness isn't in the environment and the AI doesn't have any such thing as \"effort\".  Maybe it's got categories around yieldingness to seven different categories of methods, and/or some general notion of \"can predict at all / can't predict at all\", but no general notion that maps onto human \"easy to understand\" -- though \"easy to understand\" is plausibly general-enough that I wouldn't be unsurprised to find a mapping after all.\n\nCorrigibility and actual human values are both heavily reflective concepts.  If you master a requisite level of the prerequisite skill of noticing when a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment -- which of course most people can't do because they project the category boundary onto the environment, but I have some credit that John Wentworth might be able to do it some -- and then you start mapping out concept definitions about corrigibility or values or god help you CEV, that might help highlight where some of my concern about unnatural abstractions comes in.\n\n \n\nEntirely separately, I have concerns about the ability of ML-based technology to robustly point the AI in any builder-intended direction whatsoever, even if there exists some not-too-large adequate ma","wordCount":347},"Comment:CixonSXNfLgAPh48Z":{"_id":"CixonSXNfLgAPh48Z","__typename":"Comment","post":{"__ref":"Post:q8uNoJBgcpAe3bSBp"},"tag":null,"postId":"q8uNoJBgcpAe3bSBp","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":23,"title":null,"contents":{"__ref":"Revision:CixonSXNfLgAPh48Z_contents"},"postedAt":"2024-06-11T18:07:24.402Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":145,"extendedScore":{"reacts":{"thanks":[{"karma":47500,"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"},{"karma":3183,"userId":"grecHJcgkb3KW5wnM","reactType":"seconded","displayName":"RobertM"},{"karma":2230,"userId":"Q6MFeYKvpT2WT4QPe","reactType":"seconded","displayName":"Adele Lopez"},{"karma":3774,"userId":"nDpieb7g8huozpx9j","reactType":"created","displayName":"Thane Ruthenis"},{"karma":3027,"userId":"c96TaP5ZJFYPabnpH","reactType":"seconded","displayName":"Nathan Helm-Burger"},{"karma":483,"userId":"ZDuEswM7CQCQh9wmf","reactType":"seconded","displayName":"Simon Lermen"},{"karma":119,"userId":"defNx3yJYcQxH6cE3","reactType":"seconded","displayName":"JuliaHP"},{"karma":43682,"userId":"N9zj5qpTfqmbn9dro","reactType":"created","displayName":"Zvi"},{"karma":369,"userId":"fgimQSg2Hkv94HCBS","reactType":"seconded","displayName":"Ebenezer Dukakis"},{"karma":887,"userId":"5JqkvjdNcxwN8D86a","reactType":"seconded","displayName":"Mateusz Bagiński"},{"karma":237,"userId":"iv7RbsrFdbNYgn393","reactType":"seconded","displayName":"Épiphanie Gédéon"},{"karma":440,"userId":"ENgxBL95Sc7MRwYty","reactType":"created","displayName":"quila"}],"examples":[{"karma":369,"quotes":["most people can't do because they project the category boundary onto the environment"],"userId":"fgimQSg2Hkv94HCBS","reactType":"created","displayName":"Ebenezer Dukakis"},{"karma":136,"userId":"8uGxcmL9rXkvEZtig","reactType":"created","displayName":"artemium"}],"insightful":[{"karma":703,"quotes":["a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment "],"userId":"YKBBvKHvZymmDpwwx","reactType":"created","displayName":"Tapatakt"},{"karma":887,"quotes":["a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment "],"userId":"5JqkvjdNcxwN8D86a","reactType":"seconded","displayName":"Mateusz Bagiński"},{"karma":299,"quotes":["a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment "],"userId":"sjh6ETexxGbauitEN","reactType":"seconded","displayName":"MiguelDev"}]},"agreement":34,"approvalVoteCount":74,"agreementVoteCount":27},"score":0.01519506610929966,"voteCount":90,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":60,"afExtendedScore":{"reacts":{"thanks":[{"karma":47500,"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"},{"karma":3183,"userId":"grecHJcgkb3KW5wnM","reactType":"seconded","displayName":"RobertM"},{"karma":2230,"userId":"Q6MFeYKvpT2WT4QPe","reactType":"seconded","displayName":"Adele Lopez"},{"karma":3774,"userId":"nDpieb7g8huozpx9j","reactType":"created","displayName":"Thane Ruthenis"},{"karma":3027,"userId":"c96TaP5ZJFYPabnpH","reactType":"seconded","displayName":"Nathan Helm-Burger"},{"karma":483,"userId":"ZDuEswM7CQCQh9wmf","reactType":"seconded","displayName":"Simon Lermen"},{"karma":43682,"userId":"N9zj5qpTfqmbn9dro","reactType":"created","displayName":"Zvi"},{"karma":887,"userId":"5JqkvjdNcxwN8D86a","reactType":"seconded","displayName":"Mateusz Bagiński"},{"karma":237,"userId":"iv7RbsrFdbNYgn393","reactType":"seconded","displayName":"Épiphanie Gédéon"}],"insightful":[{"karma":703,"quotes":["a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment "],"userId":"YKBBvKHvZymmDpwwx","reactType":"created","displayName":"Tapatakt"},{"karma":887,"quotes":["a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment "],"userId":"5JqkvjdNcxwN8D86a","reactType":"seconded","displayName":"Mateusz Bagiński"},{"karma":299,"quotes":["a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment "],"userId":"sjh6ETexxGbauitEN","reactType":"seconded","displayName":"MiguelDev"}]},"agreement":25,"approvalVoteCount":43,"agreementVoteCount":16},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-07-11T02:23:58.688Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":8,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Post:nH4c3Q9t9F3nJ7y8W":{"_id":"nH4c3Q9t9F3nJ7y8W","__typename":"Post","isRead":false,"slug":"gpts-are-predictors-not-imitators","title":"GPTs are Predictors, not Imitators","draft":null,"shortform":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"nmk3nLpQE89dMRzzN","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false,"currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":{"__ref":"PodcastEpisode:QofXg7g4WwFkgDmot"},"deletedDraft":false,"contents":{"__ref":"Revision:mrjg6LFtNc2byucH8"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"uDXyphdhaWxvAzwkZ"},"readTimeMinutes":4,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:YWzByWvtXunfrBu5b"},{"__ref":"Tag:xjNvvmvQ5BH3cfEBr"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:nH4c3Q9t9F3nJ7y8W"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2023-04-08T19:59:13.601Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-04-08T22:18:01.483Z","meta":false,"postCategory":"post","tagRelevance":{"YWzByWvtXunfrBu5b":2,"sYm3HiWcfZvrGu3ui":1,"xjNvvmvQ5BH3cfEBr":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"mrjg6LFtNc2byucH8","commentCount":90,"voteCount":252,"baseScore":401,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":245,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.007507828995585442,"lastVisitedAt":null,"isFuture":false,"lastCommentedAt":"2023-04-08T19:59:13.601Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":109,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":98,"agreementVoteCount":0},"afCommentCount":10,"afLastCommentedAt":"2024-04-22T11:43:02.633Z","afSticky":false,"hideAuthor":false,"moderationStyle":"reign-of-terror","ignoreRateLimits":null,"submitToFrontpage":true,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"NpcRdjYj2xsWcqXjlS5V","annualReviewMarketProbability":0.603054894017781,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2023,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-gpts-are-predictors-not-imitat","group":null,"podcastEpisodeId":"QofXg7g4WwFkgDmot","forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[]},"Revision:ww5p8EEtKndRnWazN_contents":{"_id":"ww5p8EEtKndRnWazN_contents","__typename":"Revision","html":"<p>What the main post is responding to is the argument: &nbsp;\"We're just training AIs to imitate human text, right, so that process can't make them get any smarter than the text they're imitating, right? &nbsp;So AIs shouldn't learn abilities that humans don't have; because why would you need those abilities to learn to imitate humans?\" &nbsp;And to this the main post says, \"Nope.\"<\/p><p>The main post is <i>not <\/i>arguing: &nbsp;\"If you abstract away the tasks humans evolved to solve, from human levels of performance at those tasks, the tasks AIs are being trained to solve are harder than those tasks in principle even if they were being solved perfectly.\" &nbsp;I agree this is just false, and did not think my post said otherwise.<\/p>","plaintextMainText":"What the main post is responding to is the argument:  \"We're just training AIs to imitate human text, right, so that process can't make them get any smarter than the text they're imitating, right?  So AIs shouldn't learn abilities that humans don't have; because why would you need those abilities to learn to imitate humans?\"  And to this the main post says, \"Nope.\"\n\nThe main post is not arguing:  \"If you abstract away the tasks humans evolved to solve, from human levels of performance at those tasks, the tasks AIs are being trained to solve are harder than those tasks in principle even if they were being solved perfectly.\"  I agree this is just false, and did not think my post said otherwise.","wordCount":123},"Comment:ww5p8EEtKndRnWazN":{"_id":"ww5p8EEtKndRnWazN","__typename":"Comment","post":{"__ref":"Post:nH4c3Q9t9F3nJ7y8W"},"tag":null,"postId":"nH4c3Q9t9F3nJ7y8W","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"rdXw2qwE9uyNRJFBK","topLevelCommentId":"St8zjZ2xatojvnFxW","descendentCount":1,"title":null,"contents":{"__ref":"Revision:ww5p8EEtKndRnWazN_contents"},"postedAt":"2024-04-21T01:37:40.889Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":16,"extendedScore":{"reacts":{},"agreement":8,"approvalVoteCount":10,"agreementVoteCount":6},"score":0.001134387799538672,"voteCount":11,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":9,"afExtendedScore":{"reacts":{},"agreement":5,"approvalVoteCount":5,"agreementVoteCount":3},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-04-22T11:43:02.862Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Post:LvKDMWQ3yLG9R3gHw":{"_id":"LvKDMWQ3yLG9R3gHw","__typename":"Post","isRead":false,"slug":"empiricism-as-anti-epistemology","title":"'Empiricism!' as Anti-Epistemology","draft":null,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"nmk3nLpQE89dMRzzN","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false,"currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:yntvBLoyDndDFvTib"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":30,"rejectedReason":null,"customHighlight":{"__ref":"Revision:LvKDMWQ3yLG9R3gHw_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:xgpBASEThXPuKRhbS"},{"__ref":"Tag:cHoCqtfE9cF7aSs9d"},{"__ref":"Tag:yXNtYNHJB54T3bGm3"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"}],"socialPreviewData":{"__ref":"SocialPreviewType:LvKDMWQ3yLG9R3gHw"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2024-03-14T02:02:59.723Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-03-14T02:03:23.285Z","meta":false,"postCategory":"post","tagRelevance":{"Ng8Gice9KNkncxqcj":1,"cHoCqtfE9cF7aSs9d":1,"sYm3HiWcfZvrGu3ui":1,"xgpBASEThXPuKRhbS":2,"yXNtYNHJB54T3bGm3":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"yntvBLoyDndDFvTib","commentCount":90,"voteCount":123,"baseScore":172,"extendedScore":{"reacts":{"miss":[{"karma":18700,"quotes":["\"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\"\n\n"],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":313,"quotes":["\"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\"\n\n"],"userId":"xjac7rhMMk9Dak3a2","reactType":"seconded","displayName":"DavidW"}],"typo":[{"karma":8523,"quotes":["iiv"],"userId":"9hHLLhkuQwtjjykdk","reactType":"created","displayName":"Vanessa Kosoy"},{"karma":1134,"quotes":["smilled"],"userId":"6A4zgssQTdyLjsEAw","reactType":"created","displayName":"mike_hawke"},{"karma":26,"quotes":["smilled"],"userId":"v5tRMn37otvdotY8h","reactType":"seconded","displayName":"Suh_Prance_Alot"},{"karma":8566,"quotes":["iiv"],"userId":"dfZAq9eZxs4BB4Ji5","reactType":"disagreed","displayName":"ryan_greenblatt"},{"karma":403,"quotes":["smilled"],"userId":"BPKDqwXFX9FK2EkmB","reactType":"seconded","displayName":"roryokane"},{"karma":8,"quotes":["smilled"],"userId":"fuxNvTfsZYe2XomfM","reactType":"seconded","displayName":"LTM"}],"laugh":[{"karma":28,"quotes":["\"Dude,\" said the Scientist in a gender-neutral way."],"userId":"a2kKLwQSBqgH4seG4","reactType":"created","displayName":"lesswronguser123"}],"confused":[{"karma":18700,"quotes":["Has it threatened humans? no not that time with Bing Sydney"],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"}],"strawman":[{"karma":18700,"quotes":["\"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\""],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":18700,"quotes":["what if they cried 'Unfalsifiable!' when we couldn't predict whether a phase shift would occur within the next two years exactly?"],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":313,"quotes":["what if they cried 'Unfalsifiable!' when we couldn't predict whether a phase shift would occur within the next two years exactly?"],"userId":"xjac7rhMMk9Dak3a2","reactType":"seconded","displayName":"DavidW"}],"unnecessarily-combative":[{"karma":18700,"quotes":["\"Nobody could possibly be foolish enough to reason from the apparently good behavior of AI models too dumb to fool us or scheme, to AI models smart enough to kill everyone; it wouldn't fly even as a parable, and would just be confusing as a metaphor.\""],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":18700,"quotes":["an attempt to trigger a naive intuition, and then abuse epistemology in order to prevent you from doing the further thinking that would undermine that naive intuition, which would be transparently untrustworthy if you were allowed to think about it instead of getting shut down with a cry of \"Empiricism!\""],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":2153,"quotes":["\"Nobody could possibly be foolish enough to reason from the apparently good behavior of AI models too dumb to fool us or scheme, to AI models smart enough to kill everyone; it wouldn't fly even as a parable, and would just be confusing as a metaphor.\""],"userId":"75YZ666ipe3weoJaS","reactType":"disagreed","displayName":"dr_s"}]},"agreement":0,"approvalVoteCount":114,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.010052665136754513,"lastVisitedAt":null,"isFuture":false,"lastCommentedAt":"2024-09-30T01:37:44.758Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":61,"afExtendedScore":{"reacts":{"miss":[{"karma":18700,"quotes":["\"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\"\n\n"],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":313,"quotes":["\"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\"\n\n"],"userId":"xjac7rhMMk9Dak3a2","reactType":"seconded","displayName":"DavidW"}],"typo":[{"karma":8523,"quotes":["iiv"],"userId":"9hHLLhkuQwtjjykdk","reactType":"created","displayName":"Vanessa Kosoy"},{"karma":8566,"quotes":["iiv"],"userId":"dfZAq9eZxs4BB4Ji5","reactType":"disagreed","displayName":"ryan_greenblatt"}],"confused":[{"karma":18700,"quotes":["Has it threatened humans? no not that time with Bing Sydney"],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"}],"strawman":[{"karma":18700,"quotes":["\"that the AI-permitting faction says to you, that you ought to not trust all that complicated thinking about all these stages, and should instead just trust the observations that the early models hadn't yet been caught planning how to exterminate humanity; or at least, not caught doing it at a level of intelligence that anyone thought was a credible threat or reflected a real inner tendency in that direction.  They come to you and say:  You should just take the observable, 'Has a superintelligence tried to destroy us yet?' and the past time-series of answers 'NO, NO, NO' and extrapolate.  They say that only this simple extrapolation is robust and reliable, rather than all that reasoning you were trying to do.\""],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":18700,"quotes":["what if they cried 'Unfalsifiable!' when we couldn't predict whether a phase shift would occur within the next two years exactly?"],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":313,"quotes":["what if they cried 'Unfalsifiable!' when we couldn't predict whether a phase shift would occur within the next two years exactly?"],"userId":"xjac7rhMMk9Dak3a2","reactType":"seconded","displayName":"DavidW"}],"unnecessarily-combative":[{"karma":18700,"quotes":["\"Nobody could possibly be foolish enough to reason from the apparently good behavior of AI models too dumb to fool us or scheme, to AI models smart enough to kill everyone; it wouldn't fly even as a parable, and would just be confusing as a metaphor.\""],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"},{"karma":18700,"quotes":["an attempt to trigger a naive intuition, and then abuse epistemology in order to prevent you from doing the further thinking that would undermine that naive intuition, which would be transparently untrustworthy if you were allowed to think about it instead of getting shut down with a cry of \"Empiricism!\""],"userId":"pgi5MqvGrtvQozEH8","reactType":"created","displayName":"TurnTrout"}]},"agreement":0,"approvalVoteCount":52,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-03-14T02:02:59.723Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"iYsgya1Ad6SWLPjoyixe","annualReviewMarketProbability":0.8000000000000002,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-empiricism-as-antiepistemology","group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[]},"Revision:rSzwbLHLSdj5dEnSY_contents":{"_id":"rSzwbLHLSdj5dEnSY_contents","__typename":"Revision","html":"<p>Unless I'm greatly misremembering, you did pick out what you said was your strongest item from Lethalities, separately from this, and I responded to it. &nbsp;You'd just straightforwardly misunderstood my argument in that case, so it wasn't a long response, but I responded. &nbsp;Asking for a second try is one thing, but I don't think it's cool to act like you never picked out any one item or I never responded to it.<\/p><p>EDIT: I'm misremembering, it was Quintin's strongest point about the Bankless podcast. &nbsp;https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=cr54ivfjndn6dxraD<\/p>","plaintextMainText":"Unless I'm greatly misremembering, you did pick out what you said was your strongest item from Lethalities, separately from this, and I responded to it.  You'd just straightforwardly misunderstood my argument in that case, so it wasn't a long response, but I responded.  Asking for a second try is one thing, but I don't think it's cool to act like you never picked out any one item or I never responded to it.\n\nEDIT: I'm misremembering, it was Quintin's strongest point about the Bankless podcast.  https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=cr54ivfjndn6dxraD","wordCount":86},"Comment:rSzwbLHLSdj5dEnSY":{"_id":"rSzwbLHLSdj5dEnSY","__typename":"Comment","post":{"__ref":"Post:LvKDMWQ3yLG9R3gHw"},"tag":null,"postId":"LvKDMWQ3yLG9R3gHw","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"T3oPTu7ZEwmxt88Jb","topLevelCommentId":"dRax6PHpQhxmNDnaN","descendentCount":1,"title":null,"contents":{"__ref":"Revision:rSzwbLHLSdj5dEnSY_contents"},"postedAt":"2024-03-20T00:13:11.196Z","repliesBlockedUntil":null,"userId":"nmk3nLpQE89dMRzzN","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":15,"extendedScore":{"reacts":{},"agreement":-3,"approvalVoteCount":11,"agreementVoteCount":6},"score":0.000865986046846956,"voteCount":14,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":8,"afExtendedScore":{"reacts":{},"agreement":-4,"approvalVoteCount":8,"agreementVoteCount":4},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"0.3.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-03-20T18:32:42.324Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null},"Revision:khkMzcMarzjrhNeDd":{"_id":"khkMzcMarzjrhNeDd","__typename":"Revision","htmlHighlight":"<p><i>Crossposted <\/i><a href=\"https://x.com/ESYudkowsky/status/1838042097561305254\"><i>from Twitter<\/i><\/a><i> with Eliezer's permission<\/i><\/p><p>&nbsp;<\/p><h1>i.<\/h1><p>A common claim among e/accs is that, since the solar system is big, Earth will be left alone by superintelligences. A simple rejoinder is that just because Bernard Arnault has $170 billion, does not mean that he'll give you $77.18.<\/p><p>Earth subtends only 4.54e-10 = 0.0000000454% of the angular area around the Sun, according to GPT-o1.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"r7bra1u49p7\" role=\"doc-noteref\" id=\"fnrefr7bra1u49p7\"><sup><a href=\"#fnr7bra1u49p7\">[1]<\/a><\/sup><\/span><\/p><p>Asking an ASI to leave a hole in a Dyson Shell, so that Earth could get some sunlight not transformed to infrared, would cost It 4.5e-10 of Its income.&nbsp;<\/p><p>This is like asking Bernard Arnalt to send you $77.18 of his $170 billion of wealth.<\/p><p>In real life, Arnalt says no.<\/p><p>But wouldn't humanity be able to trade with ASIs, and pay Them to give us sunlight? This is like planning to get $77 from Bernard Arnalt by selling him an Oreo cookie.<\/p><p>To extract $77 from Arnalt, it's not a sufficient condition that:<\/p><ul><li>Arnalt wants one Oreo cookie.<\/li><li>Arnalt would derive over $77 of use-value from one cookie.<\/li><li>You have one cookie.<\/li><\/ul><p>It also requires that Arnalt can't buy the cookie more cheaply from anyone or anywhere else.<\/p><p>There's a basic rule in economics, Ricardo's Law of Comparative Advantage, which shows that even if the country of Freedonia is more productive in every way than the country of Sylvania, both countries still benefit from trading with each other.<\/p><p>For example! &nbsp;Let's say that in Freedonia:<\/p><ul><li>It takes 6 hours to produce 10 hotdogs.<\/li><li>It takes 4 hours to produce 15 hotdog buns.<\/li><\/ul><p>And in Sylvania:<\/p><ul><li>It takes 10 hours to produce 10 hotdogs.<\/li><li>It takes 10 hours to produce 15 hotdog buns.<\/li><\/ul><p>For each country to, alone, without trade, produce 30 hotdogs and 30 buns:<\/p><ul><li>Freedonia needs 6*3 + 4*2 = 26 hours of labor.<\/li><li>Sylvania needs 10*3 + 10*2 = 50 hours of labor.<\/li><\/ul><p>But if Freedonia spends 8 hours of labor to produce 30 hotdog buns, and trades them for 15 hotdogs from Sylvania:<\/p><ul><li>Freedonia produces: 60 buns, 15 dogs = 4*4+6*1.5 = 25 hours&nbsp;<\/li><li>Sylvania produces: 0 buns, 45 dogs = 10*0 + 10*4.5 = 45 hours<\/li><\/ul><p>Both countries are better off from trading, even though Freedonia was more productive in creating every article being traded!<\/p><p>Midwits are often very impressed with themselves for knowing a fancy economic rule like Ricardo's Law of Comparative Advantage!<\/p><p>To be fair, even smart people sometimes take pride that humanity knows it. &nbsp;It's a great noble truth that was missed by a lot of earlier civilizations.<\/p><p>The thing about mi... <\/p>","plaintextDescription":"Crossposted from Twitter with Eliezer's permission\n\n \n\n\ni.\nA common claim among e/accs is that, since the solar system is big, Earth will be left alone by superintelligences. A simple rejoinder is that just because Bernard Arnault has $170 billion, does not mean that he'll give you $77.18.\n\nEarth subtends only 4.54e-10 = 0.0000000454% of the angular area around the Sun, according to GPT-o1.[1]\n\nAsking an ASI to leave a hole in a Dyson Shell, so that Earth could get some sunlight not transformed to infrared, would cost It 4.5e-10 of Its income. \n\nThis is like asking Bernard Arnalt to send you $77.18 of his $170 billion of wealth.\n\nIn real life, Arnalt says no.\n\nBut wouldn't humanity be able to trade with ASIs, and pay Them to give us sunlight? This is like planning to get $77 from Bernard Arnalt by selling him an Oreo cookie.\n\nTo extract $77 from Arnalt, it's not a sufficient condition that:\n\n * Arnalt wants one Oreo cookie.\n * Arnalt would derive over $77 of use-value from one cookie.\n * You have one cookie.\n\nIt also requires that Arnalt can't buy the cookie more cheaply from anyone or anywhere else.\n\nThere's a basic rule in economics, Ricardo's Law of Comparative Advantage, which shows that even if the country of Freedonia is more productive in every way than the country of Sylvania, both countries still benefit from trading with each other.\n\nFor example!  Let's say that in Freedonia:\n\n * It takes 6 hours to produce 10 hotdogs.\n * It takes 4 hours to produce 15 hotdog buns.\n\nAnd in Sylvania:\n\n * It takes 10 hours to produce 10 hotdogs.\n * It takes 10 hours to produce 15 hotdog buns.\n\nFor each country to, alone, without trade, produce 30 hotdogs and 30 buns:\n\n * Freedonia needs 6*3 + 4*2 = 26 hours of labor.\n * Sylvania needs 10*3 + 10*2 = 50 hours of labor.\n\nBut if Freedonia spends 8 hours of labor to produce 30 hotdog buns, and trades them for 15 hotdogs from Sylvania:\n\n * Freedonia produces: 60 buns, 15 dogs = 4*4+6*1.5 = 25 hours \n * Sylvania produces: 0 buns, 4","wordCount":4034,"version":"1.3.0"},"Revision:F8sfrbPjCQj4KwJqn_customHighlight":{"_id":"F8sfrbPjCQj4KwJqn_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Revision:sYm3HiWcfZvrGu3ui_description":{"_id":"sYm3HiWcfZvrGu3ui_description","__typename":"Revision","htmlHighlight":"<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/deceptive-alignment?showPostCount=true&amp;useTagName=true\">Deceptive Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\">Fixed Point Theorems<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"https://www.lesswrong.com/tag/gradient-hacking?showPostCount=true&amp;useTagName=true\">Gradient Hacking<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/multipolar-scenarios?showPostCount=true&amp;useTagName=true\">Multipolar Scenarios<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/power-seeking-ai?showPostCount=true&amp;useTagName=true\">Power Seeking (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/simulator-theory\">Simulator Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/sharp-left-turn?showPostCount=true&amp;useTagName=true\">Sharp Left Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence<\/a><br><a href=\"https://www.lesswrong.com/tag/symbol-grounding\">Symbol Grounding<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\">Treacherous Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/agent-foundations?showPostCount=true&amp;useTagName=true\">Agent Foundations<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment?showPostCount=true&amp;useTagName=true\">AI-assisted Alignment&nbsp;<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\">Conservatism (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk\">Eliciting Latent Knowledge (ELK)<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/rlhf?showPostCount=true&amp;useTagName=true\">RLHF<\/a><br><a href=\"https://www.lesswrong.com/tag/shard-theory?showPostCount=true&amp;useTagName=true\">Shard Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\">Transparency / Interpretability<\/a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\">Tripwire<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://aisafety.world/map/\">Full map here<\/a><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/alignment-research-center\">Alignment Resea<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>... "},"Tag:sYm3HiWcfZvrGu3ui":{"_id":"sYm3HiWcfZvrGu3ui","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:sYm3HiWcfZvrGu3ui_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"AI","shortName":null,"slug":"ai","core":true,"postCount":9865,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":2000,"createdAt":"2020-06-14T22:24:22.097Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:F8sfrbPjCQj4KwJqn":{"_id":"F8sfrbPjCQj4KwJqn","__typename":"SocialPreviewType","imageUrl":""},"Post:F8sfrbPjCQj4KwJqn":{"_id":"F8sfrbPjCQj4KwJqn","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:khkMzcMarzjrhNeDd"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":16,"rejectedReason":null,"customHighlight":{"__ref":"Revision:F8sfrbPjCQj4KwJqn_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:F8sfrbPjCQj4KwJqn"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2024-09-23T03:39:16.243Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-09-23T15:59:52.992Z","meta":false,"postCategory":"post","tagRelevance":{"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"khkMzcMarzjrhNeDd","commentCount":136,"voteCount":155,"baseScore":182,"extendedScore":{"reacts":{"typo":[{"karma":168,"quotes":["Arnalt t"],"userId":"rAd5XfAPowrEDjTPv","reactType":"created","displayName":"EZ97"},{"karma":168,"quotes":["Arnalt s"],"userId":"rAd5XfAPowrEDjTPv","reactType":"created","displayName":"EZ97"}],"thinking":[{"karma":171,"quotes":["satisfices expected utility"],"userId":"n4M37rPXGyL6p8ivK","reactType":"created","displayName":"ProgramCrafter"}]},"agreement":0,"approvalVoteCount":148,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.18123266100883484,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2024-10-05T11:35:36.931Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":54,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":68,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-09-23T03:39:16.243Z","afSticky":false,"hideAuthor":false,"moderationStyle":"norm-enforcing","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"1ek8vcwajt","annualReviewMarketProbability":0.2873460730682494,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-asis-will-not-leave-just-a-lit","group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"the-sun-is-big-but-superintelligences-will-not-spare-earth-a","title":"The Sun is big, but superintelligences will not spare Earth a little sunlight","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:gt7JhGewtqiNQqCiB":{"_id":"gt7JhGewtqiNQqCiB","__typename":"Revision","htmlHighlight":"<p><i>(Crossposted <\/i><a href=\"https://x.com/ESYudkowsky/status/1815090947514142759\"><i>from Twitter<\/i><\/a><i>)<\/i><\/p><p>I'm skeptical that Universal Basic Income can get rid of grinding poverty, since somehow humanity's 100-fold productivity increase (since the days of agriculture) didn't eliminate poverty.<\/p><p>Some of my friends reply, \"What do you mean, poverty is still around? &nbsp;'Poor' people today, in Western countries, have a lot to legitimately be miserable about, don't get me wrong; but they also have amounts of clothing and fabric that only rich merchants could afford a thousand years ago; they often own more than one pair of shoes; why, they even have cellphones, as not even an emperor of the olden days could have had at any price. &nbsp;They're relatively poor, sure, and they have a lot of things to be legitimately sad about. &nbsp;But in what sense is almost-anyone in a high-tech country 'poor' by the standards of a thousand years earlier? &nbsp;Maybe UBI works the same way; maybe some people are still comparing themselves to the Joneses, and consider themselves relatively poverty-stricken, and in fact have many things to be sad about; but their actual lives are much wealthier and better, such that poor people today would hardly recognize them. &nbsp;UBI is still worth doing, if that's the result; even if, afterwards, many people still self-identify as 'poor'.\"<\/p><p>Or to sum up their answer: &nbsp;\"What do you mean, humanity's 100-fold productivity increase, since the days of agriculture, has managed not to eliminate poverty? &nbsp;What people a thousand years ago used to call 'poverty' has essentially disappeared in the high-tech countries. &nbsp;'Poor' people no longer starve in winter when their farm's food storage runs out. &nbsp;There's still something we call 'poverty' but that's just because 'poverty' is a moving target, not because there's some real and puzzlingly persistent form of misery that resisted all economic growth, and would also resist redistribution via UBI.\"<\/p><p>And this is a sensible question; but let me try out a new answer to it.<\/p><p>Consider the imaginary society of Anoxistan, in which every citizen who can't afford better lives in a government-provided 1,000 square-meter apartment; which the government can afford to provide as a fallback, because building skyscrapers is legal in Anoxistan. &nbsp;Anoxistan has free high-quality food (not fast food made of mostly seed oils) available to every citizen, if anyone ever runs out of mone... <\/p>","plaintextDescription":"(Crossposted from Twitter)\n\nI'm skeptical that Universal Basic Income can get rid of grinding poverty, since somehow humanity's 100-fold productivity increase (since the days of agriculture) didn't eliminate poverty.\n\nSome of my friends reply, \"What do you mean, poverty is still around?  'Poor' people today, in Western countries, have a lot to legitimately be miserable about, don't get me wrong; but they also have amounts of clothing and fabric that only rich merchants could afford a thousand years ago; they often own more than one pair of shoes; why, they even have cellphones, as not even an emperor of the olden days could have had at any price.  They're relatively poor, sure, and they have a lot of things to be legitimately sad about.  But in what sense is almost-anyone in a high-tech country 'poor' by the standards of a thousand years earlier?  Maybe UBI works the same way; maybe some people are still comparing themselves to the Joneses, and consider themselves relatively poverty-stricken, and in fact have many things to be sad about; but their actual lives are much wealthier and better, such that poor people today would hardly recognize them.  UBI is still worth doing, if that's the result; even if, afterwards, many people still self-identify as 'poor'.\"\n\nOr to sum up their answer:  \"What do you mean, humanity's 100-fold productivity increase, since the days of agriculture, has managed not to eliminate poverty?  What people a thousand years ago used to call 'poverty' has essentially disappeared in the high-tech countries.  'Poor' people no longer starve in winter when their farm's food storage runs out.  There's still something we call 'poverty' but that's just because 'poverty' is a moving target, not because there's some real and puzzlingly persistent form of misery that resisted all economic growth, and would also resist redistribution via UBI.\"\n\nAnd this is a sensible question; but let me try out a new answer to it.\n\nConsider the imaginary society of Anoxist","wordCount":2712,"version":"1.0.0"},"Revision:fPvssZk3AoDzXwfwJ_customHighlight":{"_id":"fPvssZk3AoDzXwfwJ_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Revision:PDJ6KqJBRzvKPfuS3_description":{"_id":"PDJ6KqJBRzvKPfuS3_description","__typename":"Revision","htmlHighlight":"<p><strong>Economics<\/strong> is the social science that studies how humans and other agents interact in a universe with scarce resources. It deals with topics such as trade, specialization of labor, accumulation of capital, technology, and resource consumption. Agents in economics are generally assumed to have utility functions, which they try to maximize under various constraints.<\/p><p>Economics is usually separated into microeconomics and macroeconomics. Microeconomics concerns the behavior of agents as they interact in a market. More narrowly, it studies the price mechanism, a decentralized system of allocating goods and services based on an evolving system of prices and trade, which all actors in a market economy contribute towards. The price mechanism is closely related to the concept of the <a href=\"https://en.wikipedia.org/wiki/Invisible_hand\">invisible hand<\/a>, first introduced by <a href=\"https://en.wikipedia.org/wiki/Adam_Smith\">Adam Smith<\/a>. <a href=\"https://www.lesswrong.com/tag/game-theory\">Game theory<\/a> is the mathematical study of rational agency, which formalizes many standard results in microeconomics.<\/p><p>Macroeconomics concerns the aggregate behavior of entire economies. For example, it studies economic growth, inflation, international trade and unemployment. An ongoing debate concerns to what extent the <a href=\"https://www.lesswrong.com/tag/economic-consequences-of-agi\">impacts of artificial intelligence<\/a> should be viewed through the lens of economics.<\/p>"},"Tag:PDJ6KqJBRzvKPfuS3":{"_id":"PDJ6KqJBRzvKPfuS3","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:PDJ6KqJBRzvKPfuS3_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"Economics","shortName":null,"slug":"economics","core":false,"postCount":462,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-14T22:24:48.135Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:o9aQASibdsECTfYF6_description":{"_id":"o9aQASibdsECTfYF6_description","__typename":"Revision","htmlHighlight":"<p><strong>Moloch<\/strong> is the personification of the forces that coerce competing individuals to take actions which, although locally optimal, ultimately lead to situations where everyone is worse off. Moreover, no individual is able to unilaterally break out of the dynamic. The situation is a bad Nash equilibrium. A trap.<\/p><p>It happens when \"In some competition optimizing for X, the opportunity arises to throw some other value under the bus for improved X. Those who take it prosper. Those who don’t take it die out. Eventually, everyone’s relative status is about the same as before, but everyone’s absolute status is worse than before. The process continues until all other values that can be traded off have been – in other words, until human ingenuity cannot possibly figure out a way to make things any worse.\" - Scott Alexander<\/p><p>One example of a Molochian dynamic is a <a href=\"https://en.wikipedia.org/wiki/Red_Queen%27s_race\">Red Queen race<\/a> between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they have all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who still does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.<\/p><p>The topic of Moloch receives a formal treatment in the sequence <a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\">Inadequate Equilibria<\/a>, particularly in the chapter <a href=\"https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/moloch-s-toolbox-1-2\">Moloch's Toolbox<\/a>.<\/p>\n<h2>Origin<\/h2>\n<p><a href=\"https://www.lesswrong.com/users/yvain?sortedBy=top\">Scott Alexander<\/a> &nbsp;linked the name to the concept in his eponymous post, <a href=\"https://www.lesswrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch\">Meditations on Moloch<\/a>. &nbsp;The post intersperses lines of Allan Ginsberg's poem, <a href=\"https://www.poetryfoundation.org/poems/49303/howl\">Howl<\/a>, with multiples examples of the dynamic including: the Prisoner's Dilemma, dollar auctions, <a href=\"https://web.archive.org/web/20160928190322/http://raikoth.net/libertarian.html\">fish farming story<\/a>, Malthusian trap, capitalism, two-income trap, agriculture, arms races, races to the bottom, education system, science, and government corruption and corporate welfare.<\/p><p>From Allan Ginsberg's <a href=\"https://www.poetryfoundation.org/poems/49303/howl\">Howl<\/a>:<\/p>\n<blockquote>\n<p><em>What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?<\/em><br>\n<em>Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!<\/em><br>\n<em>Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!<\/em><br>\n<em>Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of s<\/em><\/p><\/blockquote>... "},"Tag:o9aQASibdsECTfYF6":{"_id":"o9aQASibdsECTfYF6","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:o9aQASibdsECTfYF6_description"},"canVoteOnRels":null,"userId":"73yyrm8KF6GDK9sRy","name":"Moloch","shortName":null,"slug":"moloch","core":false,"postCount":73,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-25T20:18:44.249Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:rZCxz9XBonCFLdwja_description":{"_id":"rZCxz9XBonCFLdwja_description","__typename":"Revision","htmlHighlight":""},"Tag:rZCxz9XBonCFLdwja":{"_id":"rZCxz9XBonCFLdwja","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:rZCxz9XBonCFLdwja_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"Poverty","shortName":null,"slug":"poverty","core":false,"postCount":7,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-05-25T01:57:41.919Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:fPvssZk3AoDzXwfwJ":{"_id":"fPvssZk3AoDzXwfwJ","__typename":"SocialPreviewType","imageUrl":""},"Post:fPvssZk3AoDzXwfwJ":{"_id":"fPvssZk3AoDzXwfwJ","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:gt7JhGewtqiNQqCiB"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":11,"rejectedReason":null,"customHighlight":{"__ref":"Revision:fPvssZk3AoDzXwfwJ_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:PDJ6KqJBRzvKPfuS3"},{"__ref":"Tag:o9aQASibdsECTfYF6"},{"__ref":"Tag:rZCxz9XBonCFLdwja"}],"socialPreviewData":{"__ref":"SocialPreviewType:fPvssZk3AoDzXwfwJ"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2024-07-26T07:23:50.151Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-07-26T17:55:18.986Z","meta":false,"postCategory":"post","tagRelevance":{"PDJ6KqJBRzvKPfuS3":5,"o9aQASibdsECTfYF6":2,"rZCxz9XBonCFLdwja":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"gt7JhGewtqiNQqCiB","commentCount":119,"voteCount":186,"baseScore":273,"extendedScore":{"reacts":{"typo":[{"karma":3293,"quotes":["porverty"],"userId":"9zJ7ffPXRTMyAqfPh","reactType":"created","displayName":"mako yass"},{"karma":2397,"quotes":["porverty"],"userId":"55XxDBpfKkkBPm9H8","reactType":"disagreed","displayName":"kave"}],"laugh":[{"karma":184,"quotes":["which the government can afford to provide as a fallback, because building skyscrapers is legal in Anoxistan. "],"userId":"znqXX8kdMBTq77Nip","reactType":"created","displayName":"stavros"}],"important":[{"karma":20,"quotes":["(No, it's not a conspiracy of rich people, such as some people fondly imagine are solely and purposefully responsible for all the world's awfulness.  I have known some rich people.  They don't act as a coordinated group almost ever; and the group they don't form, is flatly not capable of accurately predicting and deliberately directing world-historical equilibria over centuries.)"],"userId":"HiwXy9f3cpTbmcz9q","reactType":"created","displayName":"Дмитрий Зеленский"},{"karma":54,"quotes":["(No, it's not a conspiracy of rich people, such as some people fondly imagine are solely and purposefully responsible for all the world's awfulness.  I have known some rich people.  They don't act as a coordinated group almost ever; and the group they don't form, is flatly not capable of accurately predicting and deliberately directing world-historical equilibria over centuries.)"],"userId":"oATdvK97DFGHrSXYY","reactType":"seconded","displayName":"NoriMori1992"}],"hitsTheMark":[{"karma":184,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"znqXX8kdMBTq77Nip","reactType":"created","displayName":"stavros"},{"karma":22687,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"yGwDggdsbvyLf49wm","reactType":"seconded","displayName":"Viliam"},{"karma":12082,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"FoKb35gJijkSFYeXa","reactType":"seconded","displayName":"Duncan Sabien (Deactivated)"},{"karma":34,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"a2kKLwQSBqgH4seG4","reactType":"seconded","displayName":"lesswronguser123"},{"karma":54,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"oATdvK97DFGHrSXYY","reactType":"seconded","displayName":"NoriMori1992"},{"karma":862,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"g5EfzjTpbE8i6wxxA","reactType":"seconded","displayName":"eggsyntax"}]},"agreement":0,"approvalVoteCount":180,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.05047634616494179,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2024-10-07T05:27:26.096Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":57,"afExtendedScore":{"reacts":{"typo":[{"karma":3293,"quotes":["porverty"],"userId":"9zJ7ffPXRTMyAqfPh","reactType":"created","displayName":"mako yass"},{"karma":2397,"quotes":["porverty"],"userId":"55XxDBpfKkkBPm9H8","reactType":"disagreed","displayName":"kave"}],"hitsTheMark":[{"karma":22687,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"yGwDggdsbvyLf49wm","reactType":"seconded","displayName":"Viliam"},{"karma":862,"quotes":["It's the quality of working yourself until you can't work any longer; of taking on jobs that are painful to do, and require groveling submission to bosses, because that's what it takes to get the few scraps to hang on.\n\n"],"userId":"g5EfzjTpbE8i6wxxA","reactType":"seconded","displayName":"eggsyntax"}]},"agreement":0,"approvalVoteCount":61,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-07-26T07:23:50.151Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"la92bddfsl","annualReviewMarketProbability":0.12842159581323223,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-universal-basic-income-and-pov","group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"universal-basic-income-and-poverty","title":"Universal Basic Income and Poverty","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:yntvBLoyDndDFvTib":{"_id":"yntvBLoyDndDFvTib","__typename":"Revision","htmlHighlight":"<p><i>(Crossposted by <\/i><a href=\"https://www.lesswrong.com/users/habryka4\"><i>habryka<\/i><\/a><i> after asking Eliezer whether I could post it under his account)<\/i><\/p><h3><strong>i.<\/strong><\/h3><p>\"Ignore all these elaborate, abstract, theoretical predictions,\" the Spokesperson for Ponzi Pyramid Incorporated said in a firm, reassuring tone. &nbsp;\"Empirically, everyone who's invested in Bernie Bankman has received back 144% of what they invested two years later.\"<\/p><p>\"That's not how 'empiricism' works,\" said the Epistemologist. &nbsp;\"You're still making the assumption that --\"<\/p><p>\"You could only believe that something different would happen in the future, if you believed in elaborate theoretical analyses of Bernie Bankman's unobservable internal motives and internal finances,\" said the spokesperson for Ponzi Pyramid Incorporated. &nbsp;\"If you are a virtuous skeptic who doesn't trust in overcomplicated arguments, you'll believe that future investments will also pay back 144%, just like in the past. &nbsp;That's the prediction you make if you predict based purely on empirical observations, instead of theories about a future nobody has seen!\"<\/p><p>\"That's not how anything works,\" said the Epistemologist. &nbsp;\"Every future prediction has a theory connecting it to our past observations. &nbsp;There's no such thing as going from past observations directly to future predictions, with no theory, no assumptions, to cross the gap --\"<\/p><p>\"Sure there's such a thing as a purely empirical prediction,\" said the Ponzi spokesperson. &nbsp;\"I just made one. &nbsp;Not to mention, my dear audience, are you really going to trust anything as complicated as epistemology?\"<\/p><p>\"The alternative to thinking about epistemology is letting other people do your thinking about it for you,\" said the Epistemologist. &nbsp;\"You're saying, 'If we observe proposition X \"past investors in the Ponzi Pyramid getting paid back 144% in two years\", that implies prediction Y \"this next set of investors in the Ponzi Pyramid will get paid back 144% in two years\"'. &nbsp;X and Y are distinct propositions, so you must have some theory saying 'X -&gt; Y' that lets you put in X and get out Y.\"<\/p><p>\"But my theory is empirically proven, unlike yours!\" said the Spokesperson.<\/p><p>\"...nnnnoooo it's not,\" said the Epistemologist. &nbsp;\"I agree we've observed your X, that past investors in the Ponzi Pyramid got 144% returns in 2 years -- those investors who withdrew their money instead of leaving it in to accumulate future returns, that is, not qu... <\/p>","plaintextDescription":"(Crossposted by habryka after asking Eliezer whether I could post it under his account)\n\n\ni.\n\"Ignore all these elaborate, abstract, theoretical predictions,\" the Spokesperson for Ponzi Pyramid Incorporated said in a firm, reassuring tone.  \"Empirically, everyone who's invested in Bernie Bankman has received back 144% of what they invested two years later.\"\n\n\"That's not how 'empiricism' works,\" said the Epistemologist.  \"You're still making the assumption that --\"\n\n\"You could only believe that something different would happen in the future, if you believed in elaborate theoretical analyses of Bernie Bankman's unobservable internal motives and internal finances,\" said the spokesperson for Ponzi Pyramid Incorporated.  \"If you are a virtuous skeptic who doesn't trust in overcomplicated arguments, you'll believe that future investments will also pay back 144%, just like in the past.  That's the prediction you make if you predict based purely on empirical observations, instead of theories about a future nobody has seen!\"\n\n\"That's not how anything works,\" said the Epistemologist.  \"Every future prediction has a theory connecting it to our past observations.  There's no such thing as going from past observations directly to future predictions, with no theory, no assumptions, to cross the gap --\"\n\n\"Sure there's such a thing as a purely empirical prediction,\" said the Ponzi spokesperson.  \"I just made one.  Not to mention, my dear audience, are you really going to trust anything as complicated as epistemology?\"\n\n\"The alternative to thinking about epistemology is letting other people do your thinking about it for you,\" said the Epistemologist.  \"You're saying, 'If we observe proposition X \"past investors in the Ponzi Pyramid getting paid back 144% in two years\", that implies prediction Y \"this next set of investors in the Ponzi Pyramid will get paid back 144% in two years\"'.  X and Y are distinct propositions, so you must have some theory saying 'X -> Y' that lets you put in X","wordCount":7591,"version":"1.0.0"},"Revision:LvKDMWQ3yLG9R3gHw_customHighlight":{"_id":"LvKDMWQ3yLG9R3gHw_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Revision:xgpBASEThXPuKRhbS_description":{"_id":"xgpBASEThXPuKRhbS_description","__typename":"Revision","htmlHighlight":"<p><strong>Epistemology<\/strong> is the study of how we know the world. It's both a topic in philosophy and a practical concern for how we come to believe things are true.<\/p><p><strong>Related Sequences:<\/strong> <a href=\"https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs\">Highly Advanced Epistemology 101 for Beginners<\/a>, <a href=\"https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm\">Concepts in formal epistemology<\/a>, <a href=\"https://www.lesswrong.com/s/GTEay24Lxm3xoE4hy\">Novum Organum<\/a><\/p><p>Projects to help people improve their epistemics and individuals to contact:<\/p><ul><li>Pastcasting - forecasting past results so you get instant feedback<ul><li><a href=\"https://www.quantifiedintuitions.org/pastcasting\">https://www.quantifiedintuitions.org/pastcasting<\/a><\/li><\/ul><\/li><li>Calibration - see how justified your own confidence is<ul><li>https://www.quantifiedintuitions.org/calibration<\/li><\/ul><\/li><li>Historical Base rates - much thinking requires knowing roughly how often things have happened in the past<ul><li>Seems like Our World In Data might do some work here<\/li><li>There is at least one other effort<\/li><\/ul><\/li><li>Increasing summarisation - making it easier to get a basic understanding on EA/LessWrong topics<ul><li><a href=\"https://www.super-linear.org/twitter-thread?recordId=rec08e5GTz7A8U4kG\">Nonlinear offer a range of prizes for summaries of such works<\/a><ul><li>Nathan Young<\/li><\/ul><\/li><\/ul><\/li><\/ul><p>&nbsp;<\/p><p>Potential projects:<\/p><ul><li>Displaying estimates<ul><li>The <a href=\"https://www.lesswrong.com/tag/squiggle\">Squiggle<\/a> team seem focused on the first step of this<ul><li>Ozzie Gooen<\/li><\/ul><\/li><li>Nathan Young is interested in it<\/li><\/ul><\/li><li>&nbsp;<\/li><\/ul><p>Note:<\/p><ul><li>It is hard to build epistemic infrastructure among rationalists, because anyone who is capable of doing it can work on AI safety and most do<\/li><\/ul>"},"Tag:xgpBASEThXPuKRhbS":{"_id":"xgpBASEThXPuKRhbS","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:xgpBASEThXPuKRhbS_description"},"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"Epistemology","shortName":null,"slug":"epistemology","core":false,"postCount":349,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-08T18:14:19.353Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:cHoCqtfE9cF7aSs9d_description":{"_id":"cHoCqtfE9cF7aSs9d_description","__typename":"Revision","htmlHighlight":"<p><strong>Deception<\/strong> is the act of sharing information in a way which intentionally misleads others.<\/p><p><strong>Related Pages:<\/strong> <a href=\"https://www.lesswrong.com/tag/deceptive-alignment\">Deceptive Alignment,<\/a> <a href=\"https://www.lesswrong.com/tag/honesty\">Honesty<\/a>, <a href=\"https://www.lesswrong.com/tag/meta-honesty\">Meta-Honesty<\/a>, <a href=\"https://www.lesswrong.com/tag/self-deception\">Self-Deception<\/a>, <a href=\"https://www.lesswrong.com/tag/simulacrum-levels\">Simulacrum Levels<\/a><\/p>"},"Tag:cHoCqtfE9cF7aSs9d":{"_id":"cHoCqtfE9cF7aSs9d","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:cHoCqtfE9cF7aSs9d_description"},"canVoteOnRels":null,"userId":"mPipmBTniuABY5PQy","name":"Deception","shortName":null,"slug":"deception","core":false,"postCount":120,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-09T05:53:15.445Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:yXNtYNHJB54T3bGm3_description":{"_id":"yXNtYNHJB54T3bGm3_description","__typename":"Revision","htmlHighlight":"<p>Concepts are sometimes best illustrated through fictional dialogue between two or more characters holding different viewpoints. While the conversation itself is fictional, the subject matter is not.<\/p><p><strong>Related Pages:<\/strong> <a href=\"https://www.lesswrong.com/tag/fiction\">Fiction<\/a>, <a href=\"https://www.lesswrong.com/tag/interviews\">Interviews<\/a><\/p>"},"Tag:yXNtYNHJB54T3bGm3":{"_id":"yXNtYNHJB54T3bGm3","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:yXNtYNHJB54T3bGm3_description"},"canVoteOnRels":null,"userId":"QBvPFLFyZyuHcBwFm","name":"Dialogue (format)","shortName":null,"slug":"dialogue-format","core":false,"postCount":60,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-06T07:53:15.370Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:Ng8Gice9KNkncxqcj_description":{"_id":"Ng8Gice9KNkncxqcj_description","__typename":"Revision","htmlHighlight":"<p><strong>Rationality<\/strong> is the art of thinking in ways that result in <a href=\"https://www.lesswrong.com/tag/world-modeling\">accurate beliefs<\/a> and <a href=\"https://www.lesswrong.com/tag/decision-theory\">good decisions<\/a>. It is the primary topic of LessWrong.<br><br>Rationality is not only about avoiding the vices of <a href=\"https://www.lesswrong.com/tag/self-deception\">self-deception<\/a> and obfuscation (the failure to <a href=\"https://www.lesswrong.com/tag/conversation-topic\">communicate clearly<\/a>), but also about the virtue of <a href=\"https://www.lesswrong.com/tag/curiosity\">curiosity<\/a>, seeing the world more clearly than before, and <a href=\"https://www.lesswrong.com/tag/ambition\">achieving things<\/a> <a href=\"https://www.lesswrong.com/tag/skill-building\">previously unreachable<\/a> <a href=\"https://www.lesswrong.com/tag/coordination-cooperation\">to you<\/a>. The study of rationality on LessWrong includes a theoretical understanding of ideal cognitive algorithms, as well as building a practice that uses these idealized algorithms to inform <a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\">heuristics<\/a>, <a href=\"https://www.lesswrong.com/tag/habits\">habits<\/a>, and <a href=\"https://www.lesswrong.com/tag/techniques\">techniques<\/a>, to successfully reason and make decisions in the real world.<\/p><p>Topics covered in rationality include (but are not limited to): normative and theoretical explorations of <a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">ideal<\/a> <a href=\"https://www.lesswrong.com/tag/probability-and-statistics\">reasoning<\/a>; the <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\">capabilities and limitations<\/a> <a href=\"https://www.lesswrong.com/tag/neuroscience\">of our brain<\/a>, <a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\">mind and psychology<\/a>; applied advice such as <a href=\"https://www.lesswrong.com/tag/introspection\">introspection<\/a> techniques and <a href=\"https://www.lesswrong.com/tag/group-rationality\">how to achieve truth collaboratively<\/a>; practical techniques and methodologies for figuring out what’s true ranging from rough quantitative modeling to full research guides.<\/p><p>Note that content about <i>how the world is <\/i>can be found under <a href=\"https://www.lesswrong.com/tag/world-modeling\">World Modeling<\/a>, and practical advice about <i>how to change the world<\/i> is categorized under <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a> or <a href=\"/tag/practical\">Practical<\/a>.<\/p><hr><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Theory / Concepts<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><u>Anticipated Experiences<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/aumann-s-agreement-theorem?showPostCount=true&amp;useTagName=true\">Aumann's Agreement Theorem<\/a><br><a href=\"http://www.lesswrong.com/tag/bayes-theorem?showPostCount=true&amp;useTagName=true\"><u>Bayes Theorem<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/bounded-rationality?showPostCount=true&amp;useTagName=true\">Bounded Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence?showPostCount=true&amp;useTagName=true\">Conservation of Expected<\/a><br><a href=\"http://www.lesswrong.com/tag/contrarianism?showPostCount=true&amp;useTagName=true\">Contrarianism<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/epistemology?showPostCount=true&amp;useTagName=true\"><u>Epistemology<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><u>Gears-Level<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/hansonian-pre-rationality?useTagName=true&amp;showPostCount=true\">Hansonian Pre-Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/law-thinking?showPostCount=true&amp;useTagName=true\">Law-Thinking<\/a><br><a href=\"http://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"http://www.lesswrong.com/tag/occam-s-razor?showPostCount=true&amp;useTagName=true\">Occam's razor<\/a><br><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/truth-semantics-and-meaning?showPostCount=true&amp;useTagName=true\">Truth, Semantics, &amp; Meaning<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Applied Topics<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/alief?showPostCount=true&amp;useTagName=true\"><u>Alief<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\">Betting<\/a><br><a href=\"http://www.lesswrong.com/tag/cached-thoughts?showPostCount=true&amp;useTagName=true\">Cached Thoughts<\/a><br><a href=\"http://www.lesswrong.com/tag/calibration?showPostCount=true&amp;useTagName=true\">Calibration<\/a><br><a href=\"https://www.lesswrong.com/tag/dark-arts?showPostCount=true&amp;useTagName=true\">Dark Arts<\/a><br><a href=\"http://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism<\/a><br><a href=\"http://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\">Epistemic Modesty<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/identity?showPostCount=true&amp;useTagName=true\">Identity<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/introspection?showPostCount=true&amp;useTagName=true\"><u>Introspection<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/intuition?showPostCount=true&amp;useTagName=true\">Intuition<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><u>Practice &amp; Philosophy of Science<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><br><a href=\"http://www.lesswrong.com/tag/taking-ideas-seriously?showPostCount=true&amp;useTagName=true\">Taking Ideas Seriously<\/a><br><a href=\"https://www.lesswrong.com/tag/value-of-information?showPostCount=true&amp;useTagName=true\">Value of Information<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Failure Modes<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic<\/a><br><a href=\"https://www.lesswrong.com/tag/bucket-errors?showPostCount=true&amp;useTagName=true\">Bucket Errors<\/a><br><a href=\"https://www.lesswrong.com/tag/compartmentalization?showPostCount=true&amp;useTagName=true\">Compartmentalization<\/a><br><a href=\"https://www.lesswrong.com/tag/confirmation-bias?showPostCount=true&amp;useTagName=true\"><u>Confirmation Bias<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/logical-fallacies?showPostCount=true&amp;useTagName=true\">Fallacies<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart’s Law<\/a><br><a href=\"http://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><u>Groupthink<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\">Heuristics and Biases<\/a><br><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy?showPostCount=true&amp;useTagName=true\">Mind Projection Fallacy<\/a><br><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><u>Motivated Reasoning<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true&amp;useTagName=true\">Pica<\/a><br><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality?showPostCount=true&amp;useTagName=true\">Pitfalls of Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalization<\/a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\">Self-Deception<\/a><br><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy?showPostCount=true&amp;useTagName=true\">Sunk-Cost Fallacy<\/a><\/p><\/td><\/tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Communication<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true\"><u>Conversation<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing?showPostCount=true&amp;useTagName=true\"><u>Decoupling vs Contextualizing<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/disagreement?showPostCount=true&amp;useTagName=true\"><u>Disagreement<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/good-explanations-advice?showPostCount=true&amp;useTagName=true\">Good Explanations (Advice)<\/a><br><a href=\"http://www.lesswrong.com/tag/ideological-turing-tests?showPostCount=true&amp;useTagName=true\">Ideological Turing Tests<\/a><br><a href=\"https://www.lesswrong.com/tag/inferential-distance?showPostCount=true&amp;useTagName=true\">Inferential Distance<\/a><br><a href=\"https://www.lesswrong.com/tag/information-cascades?showPostCount=true&amp;useTagName=true\">Information Cascades<\/a><br><a href=\"https://www.lesswrong.com/tag/memetic-immune-system?showPostCount=true&amp;useTagName=true\">Memetic Immune System<\/a><br><a href=\"https://www.lesswrong.com/tag/philosophy-of-language?showPostCount=true&amp;useTagName=true\"><u>Philos<\/u><\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>... "},"Tag:Ng8Gice9KNkncxqcj":{"_id":"Ng8Gice9KNkncxqcj","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:Ng8Gice9KNkncxqcj_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"Rationality","shortName":null,"slug":"rationality","core":true,"postCount":3828,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":100,"createdAt":"2020-06-14T22:24:17.072Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:LvKDMWQ3yLG9R3gHw":{"_id":"LvKDMWQ3yLG9R3gHw","__typename":"SocialPreviewType","imageUrl":""},"Revision:uR75KaPDLgRBBAbrJ":{"_id":"uR75KaPDLgRBBAbrJ","__typename":"Revision","htmlHighlight":"<p>So this morning I thought to myself, \"Okay, now I will actually try to study the LK99 question, instead of betting based on nontechnical priors and market sentiment reckoning.\" &nbsp;(My initial entry into the affray, having been driven by people online presenting as confidently YES when the prediction markets were not confidently YES.) &nbsp;And then I thought to myself, \"This LK99 issue seems complicated enough that it'd be worth doing an actual Bayesian calculation on it\"--a rare thought; I don't think I've done an actual explicit numerical Bayesian update in at least a year.<\/p><p>In the process of trying to set up an explicit calculation, I realized I felt very unsure about some critically important quantities, to the point where it no longer seemed worth trying to do the calculation with numbers. &nbsp;This is the System Working As Intended.<\/p><hr><p>On July 30th, Danielle Fong <a href=\"https://twitter.com/DanielleFong/status/1685748984999202816\">said<\/a> of <a href=\"https://twitter.com/marshray/status/1685525342008823808\">this temperature-current-voltage graph<\/a>,&nbsp;<\/p><blockquote><p>'Normally as current increases, voltage drop across a material increases. in a *superconductor*, voltage stays nearly constant, 0. that appears to be what's happening here -- up to a critical current. with higher currents available at lower temperatures &nbsp;deeply in the \"fraud or superconduct\" territory, imo. like you don't get this by accident -- you either faked it, or really found something.'<\/p><\/blockquote><p>The graph Fong is talking about only appears in the initial paper put forth by Young-Wan Kwon, allegedly without authorization. &nbsp;A different graph, though similar, appears in Fig. 6 on p. 12 of <a href=\"https://arxiv.org/abs/2307.12037\">the 6-author LK-endorsed paper rushed out in response<\/a>.<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/cdhfzqptro0tancxr9xt\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/zvjtjv9z8g2hbazllpky 130w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/p9wraztv0rhijrcrtiyy 260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/zyichrwa6ibcllfffbuk 390w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/gkmh9lhrnkrhuyjsempl 520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/euiqj1ivgws73scyciin 650w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/pp9fhryybrkib7tgv7bq 780w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/pormb9ggddy81m2ccjov 910w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/unvulpkcwspgykngwdch 1040w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/m3jchr9hvr8olpkp9rzm 1170w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/h0k21z6tg08apirhnjmf 1266w\"><\/figure><p>Is it currently widely held by expert opinion, that this diagram has no obvious or likely explanation except \"superconductivity\" or \"fraud\"? &nbsp;If the authors discovered something weird that wasn't a superconductor, or if they just hopefully measured over and over until they started getting some sort of measurement error, is there any known, any obvious way they could have gotten the same graph?<\/p><p>One person alleges an online rumor that poorly connected electrical leads can produce the same graph. &nbsp;Is <i>that<\/i> a conventional view?<\/p><p>Alternatively: &nbsp;If this material <i>is <\/i>a superconductor, have we seen what we expected to see? &nbsp;Is the diminishing current capacity with increased temperature usual? &nbsp;How does this alleged direct measurement of superconductivity square up with the current-story-as... <\/p>","plaintextDescription":"So this morning I thought to myself, \"Okay, now I will actually try to study the LK99 question, instead of betting based on nontechnical priors and market sentiment reckoning.\"  (My initial entry into the affray, having been driven by people online presenting as confidently YES when the prediction markets were not confidently YES.)  And then I thought to myself, \"This LK99 issue seems complicated enough that it'd be worth doing an actual Bayesian calculation on it\"--a rare thought; I don't think I've done an actual explicit numerical Bayesian update in at least a year.\n\nIn the process of trying to set up an explicit calculation, I realized I felt very unsure about some critically important quantities, to the point where it no longer seemed worth trying to do the calculation with numbers.  This is the System Working As Intended.\n\n----------------------------------------\n\nOn July 30th, Danielle Fong said of this temperature-current-voltage graph, \n\n> 'Normally as current increases, voltage drop across a material increases. in a *superconductor*, voltage stays nearly constant, 0. that appears to be what's happening here -- up to a critical current. with higher currents available at lower temperatures  deeply in the \"fraud or superconduct\" territory, imo. like you don't get this by accident -- you either faked it, or really found something.'\n\nThe graph Fong is talking about only appears in the initial paper put forth by Young-Wan Kwon, allegedly without authorization.  A different graph, though similar, appears in Fig. 6 on p. 12 of the 6-author LK-endorsed paper rushed out in response.\n\nIs it currently widely held by expert opinion, that this diagram has no obvious or likely explanation except \"superconductivity\" or \"fraud\"?  If the authors discovered something weird that wasn't a superconductor, or if they just hopefully measured over and over until they started getting some sort of measurement error, is there any known, any obvious way they could have gotten the same","wordCount":1571,"version":"1.1.1"},"Revision:4fxcbJ8xSv4SAYkkx_description":{"_id":"4fxcbJ8xSv4SAYkkx_description","__typename":"Revision","htmlHighlight":"<p><strong>Bayesianism<\/strong> is the broader philosophy inspired by <a href=\"https://www.lesswrong.com/tag/bayes-theorem\">Bayes' theorem<\/a>. The core claim behind all varieties of Bayesianism is that <em>probabilities are subjective degrees of belief --<\/em> often operationalized as willingness to bet.<\/p><p><em>See also<\/em>: <a href=\"https://www.lesswrong.com/tag/bayes-theorem\">Bayes theorem<\/a>, <a href=\"https://www.lesswrong.com/tag/bayesian-probability\">Bayesian probability<\/a>, <a href=\"https://www.lesswrong.com/tag/radical-probabilism\">Radical Probabilism<\/a>, <a href=\"https://www.lesswrong.com/tag/priors\">Priors<\/a>, <a href=\"https://www.lesswrong.com/tag/rational-evidence\">Rational evidence<\/a>, <a href=\"https://www.lesswrong.com/tag/probability-theory\">Probability theory<\/a>, <a href=\"https://www.lesswrong.com/tag/decision-theory\">Decision theory<\/a>, <a href=\"https://www.lesswrong.com/tag/lawful-intelligence\">Lawful intelligence<\/a>, <a href=\"https://www.lesswrong.com/tag/bayesian-conspiracy\">Bayesian Conspiracy<\/a>.<\/p><p>This stands in contrast to other interpretations of probability, which attempt greater objectivity. The <a href=\"https://en.wikipedia.org/wiki/Frequentist_probability\">frequentist<\/a> interpretation of probability has a focus on repeatable <em>experiments;<\/em> probabilities are <em>the limiting frequency of an event if you performed the experiment an infinite number of times<\/em>.<\/p><p>Another contender is the <a href=\"https://en.wikipedia.org/wiki/Propensity_probability\">propensity<\/a> interpretation, which grounds probability in <em>the propensity for things to happen<\/em>. A perfectly balanced 6-sided die would have a 1/6 propensity to land on each side. A propensity theorist sees this as a basic fact about dice not derived from infinite sequences of experiments or subjective viewpoints.<\/p><p>Note how both of these alternative interpretations ground the meaning of probability in an external objective fact which cannot be directly accessed.<\/p><p>As a consequence of the subjective interpretation of probability theory, Bayesians are more inclined to apply Bayes' Theorem in practical statistical inference. The primary example of this is statistical hypothesis testing. Frequentists take the application of Bayes' Theorem to be inappropriate, because \"the probability of a hypothesis\" is meaningless: a hypothesis is either true or false; you cannot define a repeated experiment in which it is sometimes true and sometimes false, so you cannot assign it an intermediate probability.<\/p>\n<h2>Bayesianism &amp; Rationality<\/h2>\n<p>Bayesians conceive rationality as a technical codeword used by cognitive scientists to mean \"rational\". Bayesian <a href=\"https://www.lesswrong.com/tag/probability-theory\">probability theory<\/a> is the math of <a href=\"https://wiki.lesswrong.com/wiki/epistemic_rationality\">epistemic rationality<\/a>, Bayesian <a href=\"https://www.lesswrong.com/tag/decision-theory\">decision theory<\/a> is the math of <a href=\"https://wiki.lesswrong.com/wiki/instrumental_rationality\">instrumental rationality<\/a>. Right up there with <a href=\"https://wiki.lesswrong.com/wiki/cognitive_bias\">cognitive bias<\/a> as an absolutely fundamental concept on Less Wrong.<\/p>\n<h2>Other usages<\/h2>\n<p>The term \"Bayesian\" may also refer to an ideal rational agent implementing precise, perfect Bayesian probability theory and decision theory (see, for example, <a href=\"https://www.lesswrong.com/tag/aumann-s-agreement-theorem\">Aumann's agreement theorem<\/a>).<\/p>"},"Tag:4fxcbJ8xSv4SAYkkx":{"_id":"4fxcbJ8xSv4SAYkkx","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:4fxcbJ8xSv4SAYkkx_description"},"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"Bayesianism","shortName":null,"slug":"bayesianism","core":false,"postCount":47,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-09-12T02:42:21.141Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Tag:csMv9MvvjYJyeHqoo":{"_id":"csMv9MvvjYJyeHqoo","__typename":"Tag","parentTag":null,"subTags":[],"description":null,"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"Physics","shortName":null,"slug":"physics","core":false,"postCount":233,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-07T21:07:09.006Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:5f5c37ee1b5cdee568cfb1a2_description":{"_id":"5f5c37ee1b5cdee568cfb1a2_description","__typename":"Revision","htmlHighlight":"<p><strong>Science<\/strong> is a method for developing true beliefs about the world. It works by developing hypotheses about the world, creating experiments that would allow the hypotheses to be tested, and running the experiments. By having people publish their falsifiable predictions and their experimental results, science protects itself from individuals deceiving themselves or others.<\/p><h2>Blog posts<\/h2><ul><li><a href=\"http://lesswrong.com/lw/in/scientific_evidence_legal_evidence_rational/\">Scientific Evidence, Legal Evidence, Rational Evidence<\/a><\/li><li><a href=\"http://lesswrong.com/lw/io/is_molecular_nanotechnology_scientific/\">Is Molecular Nanotechnology \"Scientific\"?<\/a><\/li><li><a href=\"http://lesswrong.com/lw/kj/no_one_knows_what_science_doesnt_know/\">No One Knows What Science Doesn't Know<\/a><\/li><li><a href=\"http://lesswrong.com/lw/ow/the_beauty_of_settled_science/\">The Beauty of Settled Science<\/a><\/li><li><a href=\"http://lesswrong.com/lw/qa/the_dilemma_science_or_bayes/\">The Dilemma: Science or Bayes?<\/a> and <a href=\"http://lesswrong.com/lw/qb/science_doesnt_trust_your_rationality/\">Science Doesn't Trust Your Rationality<\/a><\/li><li><a href=\"http://lesswrong.com/lw/qc/when_science_cant_help/\">When Science Can't Help<\/a>, <a href=\"http://lesswrong.com/lw/qd/science_isnt_strict_enough/\">Science Isn't Strict Enough<\/a>, <a href=\"http://lesswrong.com/lw/qe/do_scientists_already_know_this_stuff/\">Do Scientists Already Know This Stuff?<\/a> and <a href=\"http://lesswrong.com/lw/qf/no_safe_defense_not_even_science/\">No Safe Defense, Not Even Science<\/a><\/li><\/ul><h2>See also<\/h2><ul><li><a href=\"https://www.lesswrong.com/tag/rational-evidence\">Rational evidence<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/general-knowledge\">General knowledge<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\">Scholarship<\/a><\/li><li><a href=\"https://www.lesswrong.com/tag/traditional-rationality\">Traditional rationality<\/a><\/li><li><a href=\"https://wiki.lesswrong.com/wiki/No_one_knows_what_science_doesn't_know\">No one knows what science doesn't know<\/a><\/li><\/ul>"},"Tag:5f5c37ee1b5cdee568cfb1a2":{"_id":"5f5c37ee1b5cdee568cfb1a2","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:5f5c37ee1b5cdee568cfb1a2_description"},"canVoteOnRels":null,"userId":"XzXbiS2zWYNdZdLW8","name":"Science","shortName":null,"slug":"science","core":false,"postCount":13,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.140Z","wikiOnly":true,"deleted":false,"isSubforum":false,"noindex":false},"Revision:3uE2pXvbcnS9nnZRE_description":{"_id":"3uE2pXvbcnS9nnZRE_description","__typename":"Revision","htmlHighlight":"<p><strong>World Modeling<\/strong> is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, <i>finding out how the damn thing works,<\/i> and writing it down for the rest of us.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.<\/i><\/p><p>—<a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality\"><u>Twelve Virtues of Rationality<\/u><\/a><\/p><\/blockquote><hr><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Modeling Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics<\/a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics<\/a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology<\/a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics<\/a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming<\/a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers<\/a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews<\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History<\/a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics<\/a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies<\/a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease<\/a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis<\/a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution<\/a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology<\/a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness<\/a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution<\/a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology<\/a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine<\/a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience<\/a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus<\/a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence<\/a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review<\/a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models<\/a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science<\/a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets<\/a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism<\/a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><p>&nbsp;<\/p><h2>A definition by elimination<\/h2><p>Properly considered, the overwhelming majority of content LessWrong is about <i>modeling how the world is<\/i>, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality<\/a>, <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a>, ... <\/p>"},"Tag:3uE2pXvbcnS9nnZRE":{"_id":"3uE2pXvbcnS9nnZRE","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:3uE2pXvbcnS9nnZRE_description"},"canVoteOnRels":null,"userId":"r38pkCm7wF4M44MDQ","name":"World Modeling","shortName":null,"slug":"world-modeling","core":true,"postCount":5001,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":27,"createdAt":"2020-06-14T22:24:50.898Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:EzSH9698DhBsXAcYY":{"_id":"EzSH9698DhBsXAcYY","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/EzSH9698DhBsXAcYY/cdhfzqptro0tancxr9xt"},"Post:EzSH9698DhBsXAcYY":{"_id":"EzSH9698DhBsXAcYY","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:uR75KaPDLgRBBAbrJ"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":6,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:4fxcbJ8xSv4SAYkkx"},{"__ref":"Tag:csMv9MvvjYJyeHqoo"},{"__ref":"Tag:5f5c37ee1b5cdee568cfb1a2"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}],"socialPreviewData":{"__ref":"SocialPreviewType:EzSH9698DhBsXAcYY"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2023-08-01T22:48:00.733Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-08-01T23:48:05.123Z","meta":false,"postCategory":"post","tagRelevance":{"3uE2pXvbcnS9nnZRE":2,"4fxcbJ8xSv4SAYkkx":2,"Ng8Gice9KNkncxqcj":2,"csMv9MvvjYJyeHqoo":2,"5f5c37ee1b5cdee568cfb1a2":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"uR75KaPDLgRBBAbrJ","commentCount":38,"voteCount":109,"baseScore":205,"extendedScore":null,"emojiReactors":{},"unlisted":false,"score":0.005391045473515987,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2023-10-23T07:54:19.375Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2023-08-13T04:07:43.668Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"grecHJcgkb3KW5wnM","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":"grecHJcgkb3KW5wnM","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":55,"afExtendedScore":null,"afCommentCount":0,"afLastCommentedAt":"2023-08-01T22:48:00.733Z","afSticky":false,"hideAuthor":false,"moderationStyle":"reign-of-terror","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"my-current-lk99-questions","title":"My current LK99 questions","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Podcast:k6u8zvJXgjQioznQ7":{"_id":"k6u8zvJXgjQioznQ7","__typename":"Podcast","title":"LessWrong Curated Podcast","applePodcastLink":"https://podcasts.apple.com/us/podcast/lesswrong-curated-podcast/id1630783021","spotifyPodcastLink":"https://open.spotify.com/show/7vqBzO0ejqiLiXyTECEeBY"},"PodcastEpisode:QofXg7g4WwFkgDmot":{"_id":"QofXg7g4WwFkgDmot","__typename":"PodcastEpisode","title":"GPTs are Predictors, not Imitators","podcast":{"__ref":"Podcast:k6u8zvJXgjQioznQ7"},"episodeLink":"https://www.buzzsprout.com/2037297/12634893-gpts-are-predictors-not-imitators-by-eliezer-yudkowski.js?container_id=buzzsprout-player-12634893&player=small","externalEpisodeId":"12634893"},"Revision:mrjg6LFtNc2byucH8":{"_id":"mrjg6LFtNc2byucH8","__typename":"Revision","htmlHighlight":"<p>(Related text posted to <a href=\"https://twitter.com/ESYudkowsky/status/1644760694771048449\">Twitter<\/a>; this version is edited and has a more advanced final section.)<\/p><p>Imagine yourself in a box, trying to predict the next word - assign as much probability mass to the next token as possible - for all the text on the Internet.<\/p><p>Koan: &nbsp;Is this a task whose difficulty caps out as human intelligence, or at the intelligence level of the smartest human who wrote any Internet text? &nbsp;What factors make that task easier, or harder? &nbsp;(If you don't have an answer, maybe take a minute to generate one, or alternatively, try to predict what I'll say next; if you do have an answer, take a moment to review it inside your mind, or maybe say the words out loud.)<\/p><hr><p>Consider that somewhere on the internet is probably a list of thruples: &lt;product of 2 prime numbers, first prime, second prime&gt;.<\/p><p>GPT obviously isn't going to predict that successfully for significantly-sized primes, but it illustrates the basic point:<\/p><p>There is no law saying that a predictor only needs to be as intelligent as the generator, in order to predict the generator's next token.<\/p><p>Indeed, in general, you've got to be more intelligent to predict particular X, than to generate realistic X. &nbsp;GPTs are being trained to a much harder task than GANs.<\/p><p>Same spirit: &lt;Hash, plaintext&gt; pairs, which you can't <i>predict <\/i>without cracking the hash algorithm, but which you could far more easily <i>generate typical instances of<\/i> if you were trying to pass a GAN's discriminator about it (assuming a discriminator that had learned to compute hash functions).<\/p><hr><p>Consider that some of the text on the Internet isn't humans casually chatting. It's the results section of a science paper. It's news stories that say what happened on a particular day, where maybe no human would be smart enough to predict the next thing that happened in the news story in advance of it happening.<\/p><p>As Ilya Sutskever compactly put it, to learn to predict text, is to learn to predict the causal processes of which the text is a shadow.<\/p><p>Lots of what's shadowed on the Internet has a *complicated* causal process generating it.<\/p><hr><p>Consider that sometimes human beings, in the course of talking, make errors.<\/p><p>GPTs are not being trained to imitate human error. They're being trained to *predict* human error.<\/p><p>Consider the asymmetry between you, who makes an error, and an outside mind that knows you well enough and in enough detail to predic... <\/p>","plaintextDescription":"(Related text posted to Twitter; this version is edited and has a more advanced final section.)\n\nImagine yourself in a box, trying to predict the next word - assign as much probability mass to the next token as possible - for all the text on the Internet.\n\nKoan:  Is this a task whose difficulty caps out as human intelligence, or at the intelligence level of the smartest human who wrote any Internet text?  What factors make that task easier, or harder?  (If you don't have an answer, maybe take a minute to generate one, or alternatively, try to predict what I'll say next; if you do have an answer, take a moment to review it inside your mind, or maybe say the words out loud.)\n\n----------------------------------------\n\nConsider that somewhere on the internet is probably a list of thruples: <product of 2 prime numbers, first prime, second prime>.\n\nGPT obviously isn't going to predict that successfully for significantly-sized primes, but it illustrates the basic point:\n\nThere is no law saying that a predictor only needs to be as intelligent as the generator, in order to predict the generator's next token.\n\nIndeed, in general, you've got to be more intelligent to predict particular X, than to generate realistic X.  GPTs are being trained to a much harder task than GANs.\n\nSame spirit: <Hash, plaintext> pairs, which you can't predict without cracking the hash algorithm, but which you could far more easily generate typical instances of if you were trying to pass a GAN's discriminator about it (assuming a discriminator that had learned to compute hash functions).\n\n----------------------------------------\n\nConsider that some of the text on the Internet isn't humans casually chatting. It's the results section of a science paper. It's news stories that say what happened on a particular day, where maybe no human would be smart enough to predict the next thing that happened in the news story in advance of it happening.\n\nAs Ilya Sutskever compactly put it, to learn to predict text, ","wordCount":994,"version":"1.1.0"},"Revision:YWzByWvtXunfrBu5b_description":{"_id":"YWzByWvtXunfrBu5b_description","__typename":"Revision","htmlHighlight":"<p><strong>GPT<\/strong> (Generative Pretrained Transformer) is a family of large transformer-based <a href=\"https://lesswrong.com/tag/language-models\">language models<\/a> created by <a href=\"https://lesswrong.com/tag/openai\">OpenAI<\/a>. Its ability to generate remarkably human-like responses has relevance to discussions on AGI.<\/p><p>External links:<\/p><p><a href=\"https://arxiv.org/abs/2005.14165\">GPT-3 Paper<\/a><\/p><p><a href=\"https://openai.com/api/\">GPT-3 Website<\/a><\/p>"},"Tag:YWzByWvtXunfrBu5b":{"_id":"YWzByWvtXunfrBu5b","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:YWzByWvtXunfrBu5b_description"},"canVoteOnRels":null,"userId":"nLbwLhBaQeG6tCNDN","name":"GPT","shortName":null,"slug":"gpt","core":false,"postCount":430,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-04-10T18:07:00.605Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:xjNvvmvQ5BH3cfEBr_description":{"_id":"xjNvvmvQ5BH3cfEBr_description","__typename":"Revision","htmlHighlight":"<p>Simulator theory in the context of AI refers to an ontology or frame for understanding the working of large generative models, such as the GPT series from OpenAI. Broadly it views these models as simulating a learned distribution with various degrees of fidelity, which in the case of language models trained on a large corpus of text is the mechanics underlying our world.<\/p><p>It can also refer to an alignment research agenda, that deals with better understanding simulator conditionals, effects of downstream training, alignment-relevant properties such as myopia and agency in the context of language models, and using them as alignment research accelerators. See also: <a href=\"https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism\">Cyborgism<\/a><\/p>"},"Tag:xjNvvmvQ5BH3cfEBr":{"_id":"xjNvvmvQ5BH3cfEBr","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:xjNvvmvQ5BH3cfEBr_description"},"canVoteOnRels":null,"userId":"e9ToWWzhwWp5GSE7P","name":"Simulator Theory","shortName":null,"slug":"simulator-theory","core":false,"postCount":102,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-12-06T12:53:39.358Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:nH4c3Q9t9F3nJ7y8W":{"_id":"nH4c3Q9t9F3nJ7y8W","__typename":"SocialPreviewType","imageUrl":""},"Revision:RYEAvp5H7jpECPiCq":{"_id":"RYEAvp5H7jpECPiCq","__typename":"Revision","htmlHighlight":"<p>(<i>Published in&nbsp;<\/i><a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\"><i><u>TIME<\/u><\/i><\/a><i> on March 29.<\/i>)<\/p><p>&nbsp;<\/p><p>An&nbsp;<a href=\"https://futureoflife.org/open-letter/pause-giant-ai-experiments/\"><u>open letter<\/u><\/a> published today calls for “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”<\/p><p>This 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It’s an improvement on the margin.<\/p><p>I refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.<\/p><p>The key issue is not “human-competitive” intelligence (as the open letter puts it); it’s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can’t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.<\/p><p>Many researchers steeped in these&nbsp;<a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\"><u>issues<\/u><\/a>, including myself,&nbsp;<a href=\"https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results\"><u>expect<\/u><\/a> that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in “maybe possibly some remote chance,” but as in “that is the obvious thing that would happen.” It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.<\/p><p>Without that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that&nbsp;<i>could in principle<\/i> be imbued into an AI but&nbsp;<i>we are not ready&nbsp;<\/i>and&nbsp;<i>do not currently know how.<\/i><\/p><p>Absent that caring, we get “the AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.”<\/p><p>The likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include “a 10-year-old trying to play chess against Stockfish 15”, “the 11th century trying to fight the 21st century,” and “<i>Australopithecus<\/i> trying to fight&nbsp;<i>Homo sapiens<\/i>“.<\/p><p>To visualize a hostile superhuman AI, don’t imagine a lifeless book-smart thinker dwelling inside the internet and sending ill-intentioned emails. Visualize an entire alien civilization, thinking at millions of times h... <\/p>","plaintextDescription":"(Published in TIME on March 29.)\n\n \n\nAn open letter published today calls for “all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.”\n\nThis 6-month moratorium would be better than no moratorium. I have respect for everyone who stepped up and signed it. It’s an improvement on the margin.\n\nI refrained from signing because I think the letter is understating the seriousness of the situation and asking for too little to solve it.\n\nThe key issue is not “human-competitive” intelligence (as the open letter puts it); it’s what happens after AI gets to smarter-than-human intelligence. Key thresholds there may not be obvious, we definitely can’t calculate in advance what happens when, and it currently seems imaginable that a research lab would cross critical lines without noticing.\n\nMany researchers steeped in these issues, including myself, expect that the most likely result of building a superhumanly smart AI, under anything remotely like the current circumstances, is that literally everyone on Earth will die. Not as in “maybe possibly some remote chance,” but as in “that is the obvious thing that would happen.” It’s not that you can’t, in principle, survive creating something much smarter than you; it’s that it would require precision and preparation and new scientific insights, and probably not having AI systems composed of giant inscrutable arrays of fractional numbers.\n\nWithout that precision and preparation, the most likely outcome is AI that does not do what we want, and does not care for us nor for sentient life in general. That kind of caring is something that could in principle be imbued into an AI but we are not ready and do not currently know how.\n\nAbsent that caring, we get “the AI does not love you, nor does it hate you, and you are made of atoms it can use for something else.”\n\nThe likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include “a 10-year-old","wordCount":3742,"version":"1.0.0"},"Revision:oM9pEezyCb4dCsuKq_customHighlight":{"_id":"oM9pEezyCb4dCsuKq_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Revision:sioN8jTw4MSgf8MpL_description":{"_id":"sioN8jTw4MSgf8MpL_description","__typename":"Revision","htmlHighlight":""},"Tag:sioN8jTw4MSgf8MpL":{"_id":"sioN8jTw4MSgf8MpL","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:sioN8jTw4MSgf8MpL_description"},"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"AI Development Pause","shortName":null,"slug":"ai-development-pause","core":false,"postCount":29,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2023-04-06T17:32:08.013Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:qHDus5MuMNqQxJbjD_description":{"_id":"qHDus5MuMNqQxJbjD_description","__typename":"Revision","htmlHighlight":"<p><strong>AI Governance<em> <\/em><\/strong>asks how we can ensure society benefits at large from increasingly powerful AI systems. While solving technical AI alignment is a necessary step towards this goal, it is by no means sufficient.<\/p><p>Governance includes policy, economics, sociology, law, and many other fields.<\/p>"},"Tag:qHDus5MuMNqQxJbjD":{"_id":"qHDus5MuMNqQxJbjD","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:qHDus5MuMNqQxJbjD_description"},"canVoteOnRels":null,"userId":"QBvPFLFyZyuHcBwFm","name":"AI Governance","shortName":null,"slug":"ai-governance","core":false,"postCount":541,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-09T18:31:56.709Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:AqwjXSSy7DuF2pKdm_description":{"_id":"AqwjXSSy7DuF2pKdm_description","__typename":"Revision","htmlHighlight":""},"Tag:AqwjXSSy7DuF2pKdm":{"_id":"AqwjXSSy7DuF2pKdm","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:AqwjXSSy7DuF2pKdm_description"},"canVoteOnRels":null,"userId":"BpBzKEueak7J8vHNi","name":"Slowing Down AI","shortName":null,"slug":"slowing-down-ai-1","core":false,"postCount":38,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-12-24T09:12:37.075Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:xexCWMyds6QLWognu_description":{"_id":"xexCWMyds6QLWognu_description","__typename":"Revision","htmlHighlight":"<p><strong>World Optimization<\/strong> is the full use of our agency. It is extending the reach of human civilization. It is building cities and democracies and economic systems and computers and flight and science and space rockets and the internet. World optimization is about adding to that list.&nbsp;<br><br>But it’s not just about growth, it’s also about preservation. We are still in the dawn of civilization, with most of civilization in the billions of years ahead. We mustn’t let this light go out.<\/p><hr><h1>World Optimization Sub-Topics<\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Moral Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/altruism?showPostCount=true&amp;useTagName=true\">Altruism<\/a><br><a href=\"https://www.lesswrong.com/tag/consequentialism?showPostCount=true&amp;useTagName=true\">Consequentialism<\/a><br><a href=\"https://www.lesswrong.com/tag/deontology?showPostCount=true&amp;useTagName=true\">Deontology<\/a><br><a href=\"http://www.lesswrong.com/tag/ethics-and-morality?showPostCount=true&amp;useTagName=true\"><u>Ethics &amp; Morality<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/metaethics?showPostCount=true&amp;useTagName=true\">Metaethics<\/a><br><a href=\"http://www.lesswrong.com/tag/moral-uncertainty?showPostCount=true&amp;useTagName=true\"><u>Moral Uncertainty<\/u><\/a><\/p><p>&nbsp;<\/p><p>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\"><p><strong>Causes / Interventions<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/animal-welfare?showPostCount=true&amp;useTagName=true\">Animal Welfare<\/a><br><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\">Existential Risk<\/a><br><a href=\"http://www.lesswrong.com/tag/futurism?showPostCount=true&amp;useTagName=true\">Futurism<\/a><br><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=true&amp;useTagName=true\">Mind Uploading<\/a><br><a href=\"https://www.lesswrong.com/tag/life-extension?showPostCount=true&amp;useTagName=true\">Life Extension<\/a><br><a href=\"http://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&amp;useTagName=false\"><u>S-risks<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/transhumanism?showPostCount=true&amp;useTagName=true\"><u>Transhumanism<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/voting-theory?showPostCount=true&amp;useTagName=true\">Voting Theory<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\"><p><strong>Working with Humans<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><u>Coalitional Instincts<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coordination / Cooperation<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/institution-design?showPostCount=true&amp;useTagName=true\">Institution Design<\/a><br><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/simulacrum-levels?showPostCount=true&amp;useTagName=true\">Simulacrum Levels<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Applied Topics<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/blackmail?showPostCount=true&amp;useTagName=true\">Blackmail<\/a><br><a href=\"http://www.lesswrong.com/tag/censorship?showPostCount=true&amp;useTagName=true\">Censorship<\/a><br><a href=\"http://www.lesswrong.com/tag/chesterton-s-fence?showPostCount=true&amp;useTagName=true\">Chesterton's Fence<\/a><br><a href=\"http://www.lesswrong.com/tag/death?showPostCount=true&amp;useTagName=true\">Death<\/a><br><a href=\"https://www.lesswrong.com/tag/deception?showPostCount=true&amp;useTagName=true\">Deception<\/a><br><a href=\"https://www.lesswrong.com/tag/honesty?showPostCount=true&amp;useTagName=true\">Honesty<\/a><br><a href=\"https://www.lesswrong.com/tag/hypocrisy?showPostCount=true&amp;useTagName=true\">Hypocrisy<\/a><br><a href=\"https://www.lesswrong.com/tag/information-hazards?showPostCount=true&amp;useTagName=true\">Information Hazards<\/a><br><a href=\"https://www.lesswrong.com/tag/meta-honesty?showPostCount=true&amp;useTagName=true\">Meta-Honesty<\/a><br><a href=\"http://www.lesswrong.com/tag/pascal-s-mugging?showPostCount=true&amp;useTagName=true\">Pascal's Mugging<\/a><br><a href=\"http://www.lesswrong.com/tag/war?showPostCount=true&amp;useTagName=true\">War<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Value &amp; Virtue<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/ambition?showPostCount=true&amp;useTagName=true\">Ambition<\/a><br><a href=\"https://www.lesswrong.com/tag/art?showPostCount=true&amp;useTagName=true\">Art<\/a><br><a href=\"https://www.lesswrong.com/tag/aesthetics?showPostCount=true&amp;useTagName=true\">Aesthetics<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"http://www.lesswrong.com/tag/courage?showPostCount=true&amp;useTagName=true\">Courage<\/a><br><a href=\"http://www.lesswrong.com/tag/fun-theory?showPostCount=true&amp;useTagName=true\">Fun Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/principles?showPostCount=true&amp;useTagName=true\">Principles<\/a><br><a href=\"http://www.lesswrong.com/tag/suffering?showPostCount=true&amp;useTagName=true\"><u>Suffering<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/superstimuli?showPostCount=true&amp;useTagName=true\">Superstimuli<\/a><br><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\">Wireheading<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Meta<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cause-prioritization?showPostCount=true&amp;useTagName=true\">Cause Prioritization<\/a><br><a href=\"http://www.lesswrong.com/tag/center-on-long-term-risk-clr?useTagName=true&amp;showPostCount=true\">Center for Long-term Risk<\/a><br><a href=\"https://www.lesswrong.com/tag/effective-altruism?showPostCount=true&amp;useTagName=true\">Effective Altruism<\/a><br><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\">Heroic Responsibility<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><hr><p>Content which describes <i>how the world is <\/i>that directly bears upon choices one makes to optimize the world fall under this tag. Examples include discussion of the moral patienthood of different animals, the potential of human civilization, and the most effective interventions against a global health threat.<\/p><p>Some material has both immediate relevance to world optimization decisions but also can inform broader world models. This material might be included under both <a href=\"https://www.lesswrong.com/tag/world-modeling\">World Modeling<\/a> tag and this tag.<\/p>"},"Tag:xexCWMyds6QLWognu":{"_id":"xexCWMyds6QLWognu","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:xexCWMyds6QLWognu_description"},"canVoteOnRels":null,"userId":"XtphY3uYHwruKqDyG","name":"World Optimization","shortName":null,"slug":"world-optimization","core":true,"postCount":2711,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":20,"createdAt":"2020-06-14T03:38:23.532Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:oM9pEezyCb4dCsuKq":{"_id":"oM9pEezyCb4dCsuKq","__typename":"SocialPreviewType","imageUrl":""},"Post:oM9pEezyCb4dCsuKq":{"_id":"oM9pEezyCb4dCsuKq","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:RYEAvp5H7jpECPiCq"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"Cs5Zx3ncLWcoYKSog"},"readTimeMinutes":15,"rejectedReason":null,"customHighlight":{"__ref":"Revision:oM9pEezyCb4dCsuKq_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:sioN8jTw4MSgf8MpL"},{"__ref":"Tag:qHDus5MuMNqQxJbjD"},{"__ref":"Tag:AqwjXSSy7DuF2pKdm"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"},{"__ref":"Tag:xexCWMyds6QLWognu"}],"socialPreviewData":{"__ref":"SocialPreviewType:oM9pEezyCb4dCsuKq"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2023-04-08T00:36:47.702Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-04-08T00:47:00.484Z","meta":false,"postCategory":"post","tagRelevance":{"AqwjXSSy7DuF2pKdm":1,"qHDus5MuMNqQxJbjD":1,"sYm3HiWcfZvrGu3ui":2,"sioN8jTw4MSgf8MpL":2,"xexCWMyds6QLWognu":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"RYEAvp5H7jpECPiCq","commentCount":40,"voteCount":137,"baseScore":253,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":134,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.004790006671100855,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2024-06-14T06:49:55.758Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":69,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":51,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2023-04-08T00:36:47.702Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"0187ZtudwDiKbMLI6wKI","annualReviewMarketProbability":0.5143220880463569,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2023,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-pausing-ai-developments-isnt-e","group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1","title":"Pausing AI Developments Isn't Enough. We Need to Shut it All Down","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:qznzMQANdMvXF45oR":{"_id":"qznzMQANdMvXF45oR","__typename":"Revision","htmlHighlight":"<p>Arguably the most important topic about which a prediction market has yet been run: &nbsp;Conditional on an okay outcome with AGI, how did that happen?<\/p><figure class=\"media\"><div data-oembed-url=\"https://manifold.markets/EliezerYudkowsky/if-artificial-general-intelligence\">\n\t\t\t\t<div class=\"manifold-preview\">\n\t\t\t\t\t<iframe src=\"https://manifold.markets/embed/EliezerYudkowsky/if-artificial-general-intelligence\">\n\t\t\t\t<\/iframe><\/div>\n\t\t\t<\/div><\/figure>","plaintextDescription":"Arguably the most important topic about which a prediction market has yet been run:  Conditional on an okay outcome with AGI, how did that happen?\n\n","wordCount":25,"version":"1.2.0"},"Revision:uNepkB5EqETC8b9C2_customHighlight":{"_id":"uNepkB5EqETC8b9C2_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Revision:33BrBRSrRQS4jEHdk_description":{"_id":"33BrBRSrRQS4jEHdk_description","__typename":"Revision","htmlHighlight":"<p><strong>Forecasts (Specific Predictions)<\/strong>.<strong> <\/strong>This tag is for specific predictions about unknown facts or future events. Discussion of the practice of making forecasts can be found in <a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\">Forecasting and Prediction<\/a>. Related: <a href=\"https://www.lesswrong.com/tag/betting\">Betting<\/a>.<\/p><blockquote><p><i>Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. &nbsp;– <\/i><a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><i>Making Beliefs Pay Rent<\/i><\/a><\/p><\/blockquote>"},"Tag:33BrBRSrRQS4jEHdk":{"_id":"33BrBRSrRQS4jEHdk","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:33BrBRSrRQS4jEHdk_description"},"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"Forecasts (Specific Predictions)","shortName":null,"slug":"forecasts-specific-predictions","core":false,"postCount":150,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-12T06:31:37.542Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:uNepkB5EqETC8b9C2":{"_id":"uNepkB5EqETC8b9C2","__typename":"SocialPreviewType","imageUrl":""},"Post:uNepkB5EqETC8b9C2":{"_id":"uNepkB5EqETC8b9C2","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:qznzMQANdMvXF45oR"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":{"__ref":"Revision:uNepkB5EqETC8b9C2_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:33BrBRSrRQS4jEHdk"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:uNepkB5EqETC8b9C2"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://manifold.markets/EliezerYudkowsky/if-artificial-general-intelligence?r=RWxpZXplcll1ZGtvd3NreQ","postedAt":"2023-03-25T22:43:53.820Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-03-26T06:57:27.774Z","meta":false,"postCategory":"linkpost","tagRelevance":{"33BrBRSrRQS4jEHdk":2,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"qznzMQANdMvXF45oR","commentCount":37,"voteCount":56,"baseScore":116,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":55,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0022337576374411583,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2024-03-19T03:51:37.460Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":27,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":26,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2023-03-25T22:43:53.820Z","afSticky":false,"hideAuthor":false,"moderationStyle":"reign-of-terror","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"8cxeGxx0rOMstzdtSQC0","annualReviewMarketProbability":0.13000000000000003,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2023,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-manifold-if-okay-agi-why-make","group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"manifold-if-okay-agi-why","title":"Manifold:  If okay AGI, why?","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:fCbsyCLtdktHxFMLz":{"_id":"fCbsyCLtdktHxFMLz","__typename":"Revision","htmlHighlight":"<p>This is a lightly edited transcript of a chatroom conversation between Scott Alexander and Eliezer Yudkowsky last year, following up on the <a href=\"https://www.lesswrong.com/s/n945eovrA3oDueqtq\">Late 2021 MIRI Conversations<\/a>. Questions discussed include \"How hard is it to get the right goals into AGI systems?\" and \"In what contexts do AI systems exhibit 'consequentialism'?\".<\/p><p>&nbsp;<\/p><h2>1. Analogies to human moral development<\/h2><figure class=\"table\"><table><tbody><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:29]<\/strong><i>&nbsp;<strong>&nbsp;<\/strong><\/i><\/p><p>@ScottAlexander ready when you are<\/p><\/td><\/tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:31]<\/strong><i> &nbsp;<\/i><\/p><p>Okay, how do you want to do this?<\/p><\/td><\/tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:32]<\/strong><i> &nbsp;<\/i><\/p><p>If you have an agenda of Things To Ask, you can follow it; otherwise I can start by posing a probing question or you can?<\/p><p>We've been very much winging it on these and that has worked... as well as you have seen it working!<\/p><\/td><\/tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:34]<\/strong><i> &nbsp;<\/i><\/p><p>Okay. I'll post from my agenda. I'm assuming we both have the right to edit logs before releasing them? I have one question where I ask about a specific party where your real answer might offend some people it's bad to offend - if that happens, maybe we just have that discussion and then decide if we want to include it later?<\/p><\/td><\/tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Yudkowsky][13:34]<\/strong> &nbsp;<i> &nbsp;<\/i><\/p><p>Yup, both parties have rights to edit before releasing.<\/p><\/td><\/tr><tr><td style=\"border:1pt solid #000000;vertical-align:top\"><p><strong>[Alexander][13:34]<\/strong>&nbsp;<i>&nbsp;<\/i><\/p><p>Okay.<\/p><p>One story that psychologists tell goes something like this: a child does something socially proscribed (eg steal). Their parents punish them. They learn some combination of \"don't steal\" and \"don't get caught stealing\". A few people (eg sociopaths) learn only \"don't get caught stealing\", but most of the rest of us get at least some genuine aversion to stealing that eventually generalizes into a real sense of ethics. If a sociopath got absolute power, they would probably steal all the time. But there are at least a few people whose ethics would successfully restrain them.<\/p><p>I interpret a major strain in your thought as being that we're going to train fledgling AIs to do things like not steal, and they're going to learn not to get caught stealing by anyone who can punish them. Then, once they're superintelligent and have absolute power, they'll reveal that it was all a lie, and steal whenever they want. Is this worry at the level of \"we can't be sure they won't do this\"? Or do you think it's overwhelmingly likely? If the latter, what makes you think AIs won't internalize ethical prohibitions, even though most children do? Is it that evolution has given us priors to interpret rew<\/p><\/td><\/tr><\/tbody><\/table><\/figure>... ","plaintextDescription":"This is a lightly edited transcript of a chatroom conversation between Scott Alexander and Eliezer Yudkowsky last year, following up on the Late 2021 MIRI Conversations. Questions discussed include \"How hard is it to get the right goals into AGI systems?\" and \"In what contexts do AI systems exhibit 'consequentialism'?\".\n\n \n\n\n1. Analogies to human moral development\n[Yudkowsky][13:29]  \n\n@ScottAlexander ready when you are\n\n[Alexander][13:31]  \n\nOkay, how do you want to do this?\n\n[Yudkowsky][13:32]  \n\nIf you have an agenda of Things To Ask, you can follow it; otherwise I can start by posing a probing question or you can?\n\nWe've been very much winging it on these and that has worked... as well as you have seen it working!\n\n[Alexander][13:34]  \n\nOkay. I'll post from my agenda. I'm assuming we both have the right to edit logs before releasing them? I have one question where I ask about a specific party where your real answer might offend some people it's bad to offend - if that happens, maybe we just have that discussion and then decide if we want to include it later?\n\n[Yudkowsky][13:34]    \n\nYup, both parties have rights to edit before releasing.\n\n[Alexander][13:34]  \n\nOkay.\n\nOne story that psychologists tell goes something like this: a child does something socially proscribed (eg steal). Their parents punish them. They learn some combination of \"don't steal\" and \"don't get caught stealing\". A few people (eg sociopaths) learn only \"don't get caught stealing\", but most of the rest of us get at least some genuine aversion to stealing that eventually generalizes into a real sense of ethics. If a sociopath got absolute power, they would probably steal all the time. But there are at least a few people whose ethics would successfully restrain them.\n\nI interpret a major strain in your thought as being that we're going to train fledgling AIs to do things like not steal, and they're going to learn not to get caught stealing by anyone who can punish them. Then, once they're superi","wordCount":7740,"version":"1.0.0"},"Revision:rwkkcgSpnAyE8oNo3_customHighlight":{"_id":"rwkkcgSpnAyE8oNo3_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Revision:ZFrgTgzwEfStg26JL_description":{"_id":"ZFrgTgzwEfStg26JL_description","__typename":"Revision","htmlHighlight":"<p><strong>AI Risk<\/strong> is analysis of the risks associated with building powerful AI systems.<\/p><p><i>Related: <\/i><a href=\"https://www.lesswrong.com/tag/ai\"><i>AI<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis\"><i>Orthogonality thesis<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><i>Complexity of value<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><i>Goodhart's law<\/i><\/a><i>, <\/i><a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\"><i>Paperclip maximiser<\/i><\/a><\/p>"},"Tag:ZFrgTgzwEfStg26JL":{"_id":"ZFrgTgzwEfStg26JL","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:ZFrgTgzwEfStg26JL_description"},"canVoteOnRels":null,"userId":"EQNTWXLKMeWMp2FQS","name":"AI Risk","shortName":null,"slug":"ai-risk","core":false,"postCount":1358,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-16T10:29:25.410Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:rwkkcgSpnAyE8oNo3":{"_id":"rwkkcgSpnAyE8oNo3","__typename":"SocialPreviewType","imageUrl":""},"User:XgYW5s8njaYrtyP7q":{"_id":"XgYW5s8njaYrtyP7q","__typename":"User","biography":null,"profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"scottalexander","createdAt":"2009-02-28T15:53:46.032Z","username":"Yvain","displayName":"Scott Alexander","previousDisplayName":null,"fullName":null,"karma":42640,"afKarma":73,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":217,"commentCount":1577,"sequenceCount":15,"afPostCount":1,"afCommentCount":1,"spamRiskScore":1,"tagRevisionCount":19,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:rwkkcgSpnAyE8oNo3":{"_id":"rwkkcgSpnAyE8oNo3","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:fCbsyCLtdktHxFMLz"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":31,"rejectedReason":null,"customHighlight":{"__ref":"Revision:rwkkcgSpnAyE8oNo3_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:ZFrgTgzwEfStg26JL"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:rwkkcgSpnAyE8oNo3"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2023-01-24T21:09:16.938Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2023-01-24T21:15:12.919Z","meta":false,"postCategory":"post","tagRelevance":{"ZFrgTgzwEfStg26JL":3,"sYm3HiWcfZvrGu3ui":4},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"fCbsyCLtdktHxFMLz","commentCount":52,"voteCount":82,"baseScore":174,"extendedScore":{"agreement":0,"approvalVoteCount":82,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0029144317377358675,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2023-02-05T13:24:58.836Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XgYW5s8njaYrtyP7q","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":58,"afExtendedScore":{"agreement":0,"approvalVoteCount":41,"agreementVoteCount":0},"afCommentCount":20,"afLastCommentedAt":"2023-02-05T13:24:58.621Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:XgYW5s8njaYrtyP7q"},"coauthors":[{"__ref":"User:nmk3nLpQE89dMRzzN"}],"slug":"alexander-and-yudkowsky-on-agi-goals","title":"Alexander and Yudkowsky on AGI goals","draft":null,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"nmk3nLpQE89dMRzzN","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:zZ5fGvjpavd2ZsK6P":{"_id":"zZ5fGvjpavd2ZsK6P","__typename":"Revision","htmlHighlight":"<p><i>(Note: This post is a write-up by Rob of a point Eliezer wanted to broadcast. Nate helped with the editing, and endorses the post’s main points.)<\/i><\/p><p>&nbsp;<\/p><p>Eliezer Yudkowsky and Nate Soares (my co-workers) want to broadcast strong support for OpenAI’s recent decision to release a blog post (\"<a href=\"https://openai.com/blog/our-approach-to-alignment-research/\"><u>Our approach to<\/u>&nbsp;<u>alignment research<\/u><\/a>\") that states their current plan as an organization.<\/p><p>Although Eliezer and Nate disagree with OpenAI's proposed approach — a variant of \"use relatively unaligned AI to align AI\" — they view it as very important that OpenAI&nbsp;<i>has a plan<\/i> and has said what it is.<\/p><p>We want to challenge Anthropic and DeepMind, the other major AGI organizations with a stated concern for existential risk, to do the same: come up with a plan (possibly a branching one, if there are crucial uncertainties you expect to resolve later), write it up in some form, and publicly announce that plan (with sensitive parts fuzzed out) as the organization's current alignment plan.<\/p><p>Currently, Eliezer’s impression is that neither Anthropic nor DeepMind has a secret plan that's better than OpenAI's, nor a secret plan that's worse than OpenAI's. His impression is that they don't have a plan at all.<span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefjajubgo6rle\"><sup><a href=\"#fnjajubgo6rle\">[1]<\/a><\/sup><\/span><\/p><p>Having a plan is critically important for an AGI project, not because anyone should expect everything to play out as planned, but because plans force the project to concretely state their crucial assumptions in one place. This provides an opportunity to notice and address inconsistencies, and to notice updates to the plan (and fully propagate those updates to downstream beliefs, strategies, and policies) as new information comes in.<\/p><p>It's also healthy for the field to be able to debate plans and think about the big picture, and for orgs to be in some sense \"competing\" to have the most sane and reasonable plan.<\/p><p>We acknowledge that there are reasons organizations might want to be&nbsp;<i>abstract<\/i> about some steps in their plans — e.g., to avoid immunizing people to good-but-weird ideas, in a public document where it’s hard to fully explain and justify a chain of reasoning; or to avoid sharing capabilities insights, if parts of your plan depend on your inside-view model of how AGI works.<\/p><p>We’d be happy to see plans that fuzz out some details, but are still much more concrete than (e.g.) “figure out how to build AGI and expect this to go well because we'll be particularly conscientious about safe... <\/p>","plaintextDescription":"(Note: This post is a write-up by Rob of a point Eliezer wanted to broadcast. Nate helped with the editing, and endorses the post’s main points.)\n\n \n\nEliezer Yudkowsky and Nate Soares (my co-workers) want to broadcast strong support for OpenAI’s recent decision to release a blog post (\"Our approach to alignment research\") that states their current plan as an organization.\n\nAlthough Eliezer and Nate disagree with OpenAI's proposed approach — a variant of \"use relatively unaligned AI to align AI\" — they view it as very important that OpenAI has a plan and has said what it is.\n\nWe want to challenge Anthropic and DeepMind, the other major AGI organizations with a stated concern for existential risk, to do the same: come up with a plan (possibly a branching one, if there are crucial uncertainties you expect to resolve later), write it up in some form, and publicly announce that plan (with sensitive parts fuzzed out) as the organization's current alignment plan.\n\nCurrently, Eliezer’s impression is that neither Anthropic nor DeepMind has a secret plan that's better than OpenAI's, nor a secret plan that's worse than OpenAI's. His impression is that they don't have a plan at all.[1]\n\nHaving a plan is critically important for an AGI project, not because anyone should expect everything to play out as planned, but because plans force the project to concretely state their crucial assumptions in one place. This provides an opportunity to notice and address inconsistencies, and to notice updates to the plan (and fully propagate those updates to downstream beliefs, strategies, and policies) as new information comes in.\n\nIt's also healthy for the field to be able to debate plans and think about the big picture, and for orgs to be in some sense \"competing\" to have the most sane and reasonable plan.\n\nWe acknowledge that there are reasons organizations might want to be abstract about some steps in their plans — e.g., to avoid immunizing people to good-but-weird ideas, in a public docum","wordCount":527,"version":"1.0.0"},"Revision:tD9zEiHfkvakpnNam_customHighlight":{"_id":"tD9zEiHfkvakpnNam_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Tag:KoXbd2HmbdRfqLngk":{"_id":"KoXbd2HmbdRfqLngk","__typename":"Tag","parentTag":null,"subTags":[],"description":null,"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"Planning & Decision-Making","shortName":null,"slug":"planning-and-decision-making","core":false,"postCount":128,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-17T21:17:27.266Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:nKtyrL5u4Y5kmMWT5_description":{"_id":"nKtyrL5u4Y5kmMWT5_description","__typename":"Revision","htmlHighlight":"<p><strong>DeepMind <\/strong>is an AI research laboratory that was founded in 2010 and acquired by Google in 2014. It is known for several record-breaking AI algorithms, often named with the prefix \"Alpha\", e.g. AlphaGo, AlphaZero, AlphaStar, AlphaFold, AlphaCode, and AlphaTensor.<\/p>"},"Tag:nKtyrL5u4Y5kmMWT5":{"_id":"nKtyrL5u4Y5kmMWT5","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:nKtyrL5u4Y5kmMWT5_description"},"canVoteOnRels":null,"userId":"gXeEWGjTWyqgrQTzR","name":"DeepMind","shortName":null,"slug":"deepmind","core":false,"postCount":76,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-14T22:52:47.865Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:H4n4rzs33JfEgkf8b_description":{"_id":"H4n4rzs33JfEgkf8b_description","__typename":"Revision","htmlHighlight":"<p><strong>OpenAI <\/strong>is an organisation that performs <a href=\"https://ai\">AI<\/a> research, and houses a substantial amount of AI alignment research. Its stated mission is \"Discovering and enacting the path to safe artificial general intelligence.\"<\/p><p>This tag is for explicit discussion of the organisation, not for all work published by researchers at that organisation.<\/p><h3>See also:<\/h3><p>OpenAI projects: <a href=\"https://lesswrong.com/tag/gpt\">GPT<\/a>, <a href=\"https://lesswrong.com/tag/dall-e\">DALL-E<\/a><\/p><p>Other related tags: <a href=\"https://lesswrong.com/tag/language-models\">Language Models<\/a>, <a href=\"https://lesswrong.com/tag/machine-learning\">Machine Learning<\/a><\/p>"},"Tag:H4n4rzs33JfEgkf8b":{"_id":"H4n4rzs33JfEgkf8b","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:H4n4rzs33JfEgkf8b_description"},"canVoteOnRels":null,"userId":"EQNTWXLKMeWMp2FQS","name":"OpenAI","shortName":null,"slug":"openai","core":false,"postCount":192,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-16T10:24:25.105Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"Revision:aHay2tebonHAYKtac_description":{"_id":"aHay2tebonHAYKtac_description","__typename":"Revision","htmlHighlight":"<p><strong>Anthropic <\/strong>is an AI organization.<\/p><p>Not to be confused with <a href=\"anthropics\">anthropics<\/a>.<\/p>"},"Tag:aHay2tebonHAYKtac":{"_id":"aHay2tebonHAYKtac","__typename":"Tag","parentTag":null,"subTags":[],"description":{"__ref":"Revision:aHay2tebonHAYKtac_description"},"canVoteOnRels":null,"userId":"qgdGA4ZEyW7zNdK84","name":"Anthropic (org)","shortName":null,"slug":"anthropic-org","core":false,"postCount":51,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2021-12-24T23:24:39.241Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false},"SocialPreviewType:tD9zEiHfkvakpnNam":{"_id":"tD9zEiHfkvakpnNam","__typename":"SocialPreviewType","imageUrl":""},"Revision:2aoRX3ookcCozcb3m_biography":{"_id":"2aoRX3ookcCozcb3m_biography","__typename":"Revision","version":"1.1.0","updateType":"minor","editedAt":"2024-06-06T22:35:28.626Z","userId":"2aoRX3ookcCozcb3m","html":"<p>Communications @ MIRI. Unless otherwise indicated, my posts and comments here reflect my own views, and not necessarily my employer's. (Though we agree about an awful lot.)<\/p>","commitMessage":"","wordCount":27,"htmlHighlight":"<p>Communications @ MIRI. Unless otherwise indicated, my posts and comments here reflect my own views, and not necessarily my employer's. (Though we agree about an awful lot.)<\/p>","plaintextDescription":"Communications @ MIRI. Unless otherwise indicated, my posts and comments here reflect my own views, and not necessarily my employer's. (Though we agree about an awful lot.)"},"User:2aoRX3ookcCozcb3m":{"_id":"2aoRX3ookcCozcb3m","__typename":"User","biography":{"__ref":"Revision:2aoRX3ookcCozcb3m_biography"},"profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":false,"slug":"robbbb","createdAt":"2012-08-10T00:50:11.669Z","username":"RobbBB","displayName":"Rob Bensinger","previousDisplayName":null,"fullName":null,"karma":21767,"afKarma":1379,"deleted":false,"isAdmin":true,"htmlBio":"<p>Communications @ MIRI. Unless otherwise indicated, my posts and comments here reflect my own views, and not necessarily my employer's. (Though we agree about an awful lot.)<\/p>","jobTitle":null,"organization":null,"postCount":123,"commentCount":2212,"sequenceCount":5,"afPostCount":12,"afCommentCount":189,"spamRiskScore":1,"tagRevisionCount":117,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:tD9zEiHfkvakpnNam":{"_id":"tD9zEiHfkvakpnNam","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"podcastEpisode":null,"deletedDraft":false,"contents":{"__ref":"Revision:zZ5fGvjpavd2ZsK6P"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"Lr99AGm4czFK7bsgj"},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":{"__ref":"Revision:tD9zEiHfkvakpnNam_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:KoXbd2HmbdRfqLngk"},{"__ref":"Tag:ZFrgTgzwEfStg26JL"},{"__ref":"Tag:nKtyrL5u4Y5kmMWT5"},{"__ref":"Tag:H4n4rzs33JfEgkf8b"},{"__ref":"Tag:aHay2tebonHAYKtac"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:tD9zEiHfkvakpnNam"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2022-12-01T23:11:44.279Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2022-12-01T23:37:55.819Z","meta":false,"postCategory":"post","tagRelevance":{"H4n4rzs33JfEgkf8b":3,"KoXbd2HmbdRfqLngk":4,"ZFrgTgzwEfStg26JL":3,"aHay2tebonHAYKtac":1,"nKtyrL5u4Y5kmMWT5":3,"sYm3HiWcfZvrGu3ui":4},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"zZ5fGvjpavd2ZsK6P","commentCount":33,"voteCount":148,"baseScore":301,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":147,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0044694519601762295,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2023-04-12T21:24:23.514Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"2aoRX3ookcCozcb3m","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"grecHJcgkb3KW5wnM","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":null,"reviewForAlignmentUserId":null,"afBaseScore":78,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":69,"agreementVoteCount":0},"afCommentCount":17,"afLastCommentedAt":"2022-12-13T10:46:18.914Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":2,"positiveReviewVoteCount":1,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:2aoRX3ookcCozcb3m"},"coauthors":[{"__ref":"User:nmk3nLpQE89dMRzzN"}],"slug":"a-challenge-for-agi-organizations-and-a-challenge-for-1","title":"A challenge for AGI organizations, and a challenge for readers","draft":null,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"nmk3nLpQE89dMRzzN","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false}}</script>
<script>window.__APOLLO_FOREIGN_STATE__ = {}</script>

<script src="eliezer_files/api.js"></script><iframe id="intercom-frame" style="position: absolute !important; opacity: 0 !important; width: 1px !important; height: 1px !important; top: 0 !important; left: 0 !important; border: none !important; display: block !important; z-index: -1 !important; pointer-events: none;" aria-hidden="true" tabindex="-1" title="Intercom"></iframe><div class="intercom-lightweight-app"><div class="intercom-lightweight-app-launcher intercom-launcher" role="button" tabindex="0" aria-label="Open Intercom Messenger" aria-live="polite"><div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 28 32"><path d="M28 32s-4.714-1.855-8.527-3.34H3.437C1.54 28.66 0 27.026 0 25.013V3.644C0 1.633 1.54 0 3.437 0h21.125c1.898 0 3.437 1.632 3.437 3.645v18.404H28V32zm-4.139-11.982a.88.88 0 00-1.292-.105c-.03.026-3.015 2.681-8.57 2.681-5.486 0-8.517-2.636-8.571-2.684a.88.88 0 00-1.29.107 1.01 1.01 0 00-.219.708.992.992 0 00.318.664c.142.128 3.537 3.15 9.762 3.15 6.226 0 9.621-3.022 9.763-3.15a.992.992 0 00.317-.664 1.01 1.01 0 00-.218-.707z"></path></svg></div><div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-minimize"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path fill-rule="evenodd" clip-rule="evenodd" d="M18.601 8.39897C18.269 8.06702 17.7309 8.06702 17.3989 8.39897L12 13.7979L6.60099 8.39897C6.26904 8.06702 5.73086 8.06702 5.39891 8.39897C5.06696 8.73091 5.06696 9.2691 5.39891 9.60105L11.3989 15.601C11.7309 15.933 12.269 15.933 12.601 15.601L18.601 9.60105C18.9329 9.2691 18.9329 8.73091 18.601 8.39897Z" fill="white"></path>
</svg>
</div></div><style id="intercom-lightweight-app-style" type="text/css">
  @keyframes intercom-lightweight-app-launcher {
    from {
      opacity: 0;
      transform: scale(0.5);
    }
    to {
      opacity: 1;
      transform: scale(1);
    }
  }

  @keyframes intercom-lightweight-app-gradient {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  @keyframes intercom-lightweight-app-messenger {
    0% {
      opacity: 0;
      transform: scale(0);
    }
    40% {
      opacity: 1;
    }
    100% {
      transform: scale(1);
    }
  }

  .intercom-lightweight-app {
    position: fixed;
    z-index: 2147483001;
    width: 0;
    height: 0;
    font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif;
  }

  .intercom-lightweight-app-gradient {
    position: fixed;
    z-index: 2147483002;
    width: 500px;
    height: 500px;
    bottom: 0;
    right: 0;
    pointer-events: none;
    background: radial-gradient(
      ellipse at bottom right,
      rgba(29, 39, 54, 0.16) 0%,
      rgba(29, 39, 54, 0) 72%);
    animation: intercom-lightweight-app-gradient 200ms ease-out;
  }

  .intercom-lightweight-app-launcher {
    position: fixed;
    z-index: 2147483003;
    padding: 0 !important;
    margin: 0 !important;
    border: none;
    bottom: 20px;
    right: 20px;
    max-width: 48px;
    width: 48px;
    max-height: 48px;
    height: 48px;
    border-radius: 50%;
    background: #f5f5f5;
    cursor: pointer;
    box-shadow: 0 1px 6px 0 rgba(0, 0, 0, 0.06), 0 2px 32px 0 rgba(0, 0, 0, 0.16);
    transition: transform 167ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
    box-sizing: content-box;
  }


  .intercom-lightweight-app-launcher:hover {
    transition: transform 250ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
    transform: scale(1.1)
  }

  .intercom-lightweight-app-launcher:active {
    transform: scale(0.85);
    transition: transform 134ms cubic-bezier(0.45, 0, 0.2, 1);
  }


  .intercom-lightweight-app-launcher:focus {
    outline: none;

    
  }

  .intercom-lightweight-app-launcher-icon {
    display: flex;
    align-items: center;
    justify-content: center;
    position: absolute;
    top: 0;
    left: 0;
    width: 48px;
    height: 48px;
    transition: transform 100ms linear, opacity 80ms linear;
  }

  .intercom-lightweight-app-launcher-icon-open {
    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-open svg {
    width: 24px;
    height: 24px;
  }

  .intercom-lightweight-app-launcher-icon-open svg path {
    fill: rgb(0, 0, 0);
  }

  .intercom-lightweight-app-launcher-icon-self-serve {
    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-self-serve svg {
    height: 44px;
  }

  .intercom-lightweight-app-launcher-icon-self-serve svg path {
    fill: rgb(0, 0, 0);
  }

  .intercom-lightweight-app-launcher-custom-icon-open {
    max-height: 24px;
    max-width: 24px;

    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-minimize {
    
        opacity: 0;
        transform: rotate(-60deg) scale(0);
      
  }

  .intercom-lightweight-app-launcher-icon-minimize svg path {
    fill: rgb(0, 0, 0);
  }

  .intercom-lightweight-app-messenger {
    position: fixed;
    z-index: 2147483003;
    overflow: hidden;
    background-color: white;
    animation: intercom-lightweight-app-messenger 250ms cubic-bezier(0, 1, 1, 1);
    transform-origin: bottom right;

    
        width: 400px;
        height: calc(100% - 104px);
        max-height: 704px;
        min-height: 250px;
        right: 20px;
        bottom: 84px;
        box-shadow: 0 5px 40px rgba(0,0,0,0.16);
      

    border-radius: 16px;
  }

  .intercom-lightweight-app-messenger-header {
    height: 64px;
    border-bottom: none;
    background: #f5f5f5

    
  }

  .intercom-lightweight-app-messenger-footer{
    position:absolute;
    bottom:0;
    width: 100%;
    height: 80px;
    background: #fff;
    font-size: 14px;
    line-height: 21px;
    border-top: 1px solid rgba(0, 0, 0, 0.05);
    box-shadow: 0px 0px 25px rgba(0, 0, 0, 0.05);
    
  }

  @media print {
    .intercom-lightweight-app {
      display: none;
    }
  }
</style></div><div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe title="reCAPTCHA" width="256" height="60" role="presentation" name="a-avlouh5whdtw" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation" src="eliezer_files/anchor.html"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;"></iframe></div></body></html>