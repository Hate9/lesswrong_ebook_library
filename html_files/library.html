<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link rel="preload" as="style" href="library_files/allStyles.css"><link rel="stylesheet" type="text/css" href="library_files/icon.css"><link rel="stylesheet" type="text/css" href="library_files/reset-min.css"><link rel="stylesheet" type="text/css" href="library_files/css.css"><link rel="stylesheet" type="text/css" href="library_files/jvr1gjm.css"><link rel="stylesheet" type="text/css" href="library_files/tqv5rhd.css"><script type="text/javascript" async="" charset="utf-8" src="library_files/recaptcha__en.js" crossorigin="anonymous" integrity="sha384-0uUcqAX/lKvnfFMvCM7U5wcjfgBvv/1q+xxZKV6ZhBH4ikGcgTDEC4vEZPTt3l8O"></script><script async="" src="library_files/google-analytics_analytics.js"></script><script>window.publicInstanceSettings = {"forumType":"LessWrong","title":"LessWrong","siteNameWithArticle":"LessWrong","sentry":{"url":"https://1ab1949fc8d04608b43132f37bb2a1b0@sentry.io/1301611","environment":"production","release":"69f0f3c5d57b596e8249571383f8a280eff9bb23"},"debug":false,"aboutPostId":"bJ2haLkcGeLtTWaD5","faqPostId":"2rWKkWuPrgTMpLRbp","contactPostId":"ehcYkvyz7dh9L7Wt8","expectedDatabaseId":"production","tagline":"A community blog devoted to refining the art of rationality","faviconUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico","faviconWithBadge":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_with_badge.ico","forumSettings":{"headerTitle":"LESSWRONG","shortForumTitle":"LW","tabTitle":"LessWrong"},"analytics":{"environment":"lesswrong.com"},"cluster":{"enabled":true,"numWorkers":2},"testServer":false,"fmCrosspost":{"siteName":"the EA Forum","baseUrl":"https://forum.effectivealtruism.org/"},"allowTypeIIIPlayer":true,"hasRejectedContentSection":true,"hasCuratedPosts":true,"performanceMetricLogging":{"enabled":true,"batchSize":100},"reviewBotId":"tBchiz3RM7rPwujrJ","recombee":{"databaseId":"lightcone-infrastructure-lesswrong-prod-2","publicApiToken":"sb95OJbQ7mKLQAm1abPog2m5vCPj7XqZlVYdHGyANcjzqaHT5fX6HEgB0vCfiLav"},"taggingName":"wikitag","taggingUrlCustomBase":"w","homepagePosts":{"feeds":[{"name":"forum-classic","label":"Latest","description":"The classic LessWrong frontpage algorithm that combines karma with time discounting, plus any tag-based weighting if applied.","showToLoggedOut":true},{"name":"recombee-hybrid","label":"Enriched","description":"An equal mix of Latest and Recommended.","showSparkleIcon":true,"defaultTab":true,"showToLoggedOut":true},{"name":"recombee-lesswrong-custom","label":"Recommended","description":"Personalized recommendations from the history of LessWrong, using a machine learning model that takes into account posts you've read and/or voted on.","showSparkleIcon":true,"showToLoggedOut":true},{"name":"forum-subscribed-authors","label":"Subscribed","description":"Posts and comments by people you've explicitly subscribed to.","isInfiniteScroll":true},{"name":"vertex-default","label":"Vertex","description":"Experimental feed for Google Vertex recommendations.","showLabsIcon":true,"adminOnly":true},{"name":"forum-bookmarks","label":"Bookmarks","description":"A list of posts you saved because you wanted to have them findable later."},{"name":"forum-continue-reading","label":"Resume Reading","description":"Further posts in post sequences that you started reading.","disabled":true}]}}</script><link rel="shortcut icon" href="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1497915096/favicon_lncumn.ico"><script>window.publicSettings = {"forum":{"numberOfDays":10,"postInterval":30,"numberOfWeeks":4,"numberOfYears":4,"maxPostsPerDay":5,"numberOfMonths":4},"type3":{"cutoffDate":"2023-07-01","explicitlyAllowedPostIds":["SvKSwT6xYfYahH4XN","2weRdcvqANDq3zdPH","Zm7WAJMTaFvuh2Wc7","HcjL8ydHxPezj6wrt","pgGiqLQg2KWsaz5RE","jFzovY2CERF5bd2EW","sm6npdgZArSn4afeZ","CfX6pGepdjQYELSpK","NyFuuKQ8uCEDtd2du","LCjtqsQWapoSfDHqK","MxyRNd6qJsYAcXKuw","reG3g4wwzwJcKnFfh","zfeWGvTrS6wKQeeoF","oHsMeXehPy4jHcmwy","ofL22R6KZsfrvdmwg","655TmdcwAgryPGPWS","hhrv8aAcmkzJxvP58","iqQJiKcephtMgzJgN","mnpkM57R6ZbjnwrYw","6mRv7Cr57AJAtRFHv","ak9wY2t9K3K4GxCXv","Ay6GBGNcCgP55dRQ7","aH4mjhgqNPyYvJT85","JpoLCHytYiCm7fwNA","efMgZujzfjP9B9H4R","BjLxPLsev54LFCS3A","HTGCGASf9xfB6edAh","H6LnGwjKiGvDyR5yo","qL8Z9TBCNWQyN6yLq","F4xwRTrFQyazHufjD","LY7Nca846X8kcT8Jk","K9aLcuxAPyf5jGyFX","2AuvBPw6Rb7yxkvKc","muhtBvbh4etjkKXd9","ALEYMFAuFSCz8v5YE","CJxSgaqG6y7z6Rbij","k5TpDCEHeK4qwnJt8","4Y2J7NtuweW2B8JvB","BpYDqQNZ2NZNCqPp6","oMiogKLkK8L59WzDe","TyQSMmoJpRG3HBv5S","8KHR3tfa4SJjMSkXd","g4pi2jfQHFF6mPdjw","znEhB9hJtwXica5s3","Sd2r7H8bCmd9ChGbX","P2nYKqwmHdYKARTG8","FW3DEYbKPZJh5A8Bj","K3hFLRn7MvYacL466","ouSpHCCPgsXkwxAGb","w9SuQtRJLbDpeir6L","yPQGYn9rSme9RRpiQ","BD6WYC4GT6dnWaJRN","c8khnHoRTSGjmHLLf","TaPr4YSBbiakeKdwX","pyNPXST7feDX45ygt","ERPL3v2Y976W7XG3j","XpXQ4KNzLa9ZHYw8p","PBhrHw5X8sDmHDWkX","8KhThQXzsAEZ59iko","iYJo382hY28K7eCrP","KrEwDMN4YXp5YWD45","rNJ39yQmzTnseh8nL","hMQPyLDbg3bA7P6aN","3Jqz6JE8K6vyQ9hJ5","SQAfPKZBAAKYMjx25","Y345zuBetHqGnotwm","pZerSnxv6FPqvgoYu","3bPH2az479gzxDMbf","QXShCBvPydkwafekn","iLMkKDKmfbMkDuQBm","iNCg6mjw584r9BWZK","9oqF382ASmjaGBo7z","DdNB42JgBzbbvmAum","JP7eZYHB7aY6fA4TR","snwX7hXgLFikqDBr6","CsKrQdQJJCFPjfKjF","vhxywjnBH6ioRnnt3","A4MK9RQqSAJZjanQD","PCpzG9NJeviXM5YSq","KCcdhZK7omEMwBdju","kdmCm5NQTpqhJmGm6","2p8BWvcJvKkXGMsch","FLnDFnXyWrKr6eiT6","2gWs8SScqeDFidqyv","2HafkDSNdtMzptzcN","cTQRGJTQ2eGKm5G9g","qaHHJ3kkCQS4nsoGJ","gS8Jmcfoa9FAh92YK","eRhFaibbTeGbjdaaf","xij43oLTBRnEQv2bT","BZMc9Xzqw5WcCMHrr","2jZykdLg9fBGqKd46","gBChm3THPGFcrq5eH","9HSwh2mE3tX6xvZ2W","tEHJXNhw6t87foqJL","T5McDuWDeCvDZKeSj","PeTL97v92LxRJBsrM","Cq45AuedYnzekp3LX","pfmZ5cYQCahABGZzi","3wBj8BPquskZAbXu9","xPJKZyPCvap4Fven8","BPKvZuLRyiJBjfNbg","um7w5RogAHhxGy8Ti","CcyGR3pp3FCDuW6Pf","BfaAADSQ88cuxLQoD","ckuuDa8DmJ4pdFeD8","pczHfyxmnFhtKthqR","dymK5c7BkpgXH4acw","B4AyJXYPpGbBmxQzd","xNBRkPNHAGQ6EQaLS","88TN6y9M5xxAHHNwW","Lt8Rn4rkYwqiTXGPy","QdXrkWoK2Pp6XhNuQ","NjzBrtvDS4jXi5Krp","ZWC3n9c6v4s35rrZ3","Fy2b55mLtghd4fQpx","eaczwARbFnrisFx8E","KLjQedNYNEP4tW73W","DSnamjnW7Ad8vEEKd","7iDtkfyn322nPzTP4","eaSJtg8Kvc56bFBdt","AmaWMMWPzuQ62Ernf","jkf2YjuH8Z2E7hKBA","BroeiXGh9PrKZEkJ5","9Tw5RqnEzqEtaoEkq","EMJ3egz48BtZS8Pws","MkKcnPdTZ3pQ9F5yC","kjArXFinD3deRZNRu","Q8zqoBWBBHD2RjDuS","ePA4NDzZkunz98tLx","4xKeNKFXFB458f5N8","irbREZtZzPi7WEYex","QxZs5Za4qXBegXCgu","ZmQv4DFx6y4jFbhLy","M7rwT264CSYY6EdR3","z3cTkXbA7jgwGWPcv","9thqSN8HDLM3LTxK5","MtNnFg4uN32YPoKNa","Ep2Z42hYqj68QZz6w","ibk7q8msSYxZXmfCf","EgDpZS4HHeh5vqJPe","5dhWhjfxn4tPfFQdi","Wh8HAK6LR5CAoPCCC","Yy7mgec8tsbTAuTqb","azoP7WeKYYfgCozoh","Zh9AiXNjQaYXjmNaC","bJiyYJeCyh4HcKHub","aPrCzeFfbBmRsvzby","vXCK3kptLLggEfojX","M2LWXsJxKS626QNEA","LQp9cZPzJncFKh5c8","CZnBQtvDw33rmWpBD","miHttwTgajY2sjY3L","K2JBqDeETX2yEgyyZ","r8aAqSBeeeMNRtiYK","Gh2qQHrCg3teQen3c","mja6jZ6k9gAwki9Nu","qjSHfbjmSyMnGR9DS","Sx26Aj3xuMzmnKE4A","P3uavjFmZD5RopJKk","pJJdcZgB6mPNWoSWr","oGezscrQvPDgGvrbt","AYbhqi65SWzHzy7Xx","E4cKD9iTWHaE7f3AJ","x9FNKTEt68Rz6wQ6P","HAEPbGaMygJq8L59k","znBJwbuT3f5eWgM4E","yJfBzcDL9fBHJfZ6P","YAkpzvjC768Jm2TYb","LTtNXM9shNM9AC2mp","9hR2RmpJmxT8dyPo4","WQWhXzALcrzrJtqRh","p7WXmG6Fbo3eaSwm3","KheBaeW8Pi7LwewoF","A2Qam9Bd9xpbb2wLQ","asmZvCPHcB4SkSCMW","euJm4RwkAptZnP89i","r8stxYL29NF9w53am","6yTShbTdtATxKonY5","yDRX2fdkm3HqfTpav","EhEZoTFzys9EDmEXn","YSWa8rYeD3aDaofSP","rwkkcgSpnAyE8oNo3","HmfxSWnqnK265GEFM","Ltey8BS83qSkd9M3u","atcJqdhCxTZiJSxo2","pC47ZTsPNAkjavkXs","wJnm5cBiZGmKn595f","GrtbTAPfkJa4D6jjH","LgavAYtzFQZKg95WC","reitXJgJXFzKpdKyd","ZiQqsgGX6a42Sfpii","neQ7eXuaXpiYw7SBy","hQHuXuRGZxxWXaPgg","9kcTNWopvXFncXgPy","baTWMegR42PAsH9qJ","Kbm6QnJv9dgWsPHQP","gFMH3Cqw4XxwL69iy","R6M4vmShiowDn56of","6Fpvch8RR29qLEWNH","N6WM6hs7RQMKDhYjB","pdaGN6pQyQarFHXF4","SA9hDewwsYgnuscae","i9xyZBS3qzA8nFXNQ","bx3gkHJehRCYZAF3r","Jk9yMXpBLMWNTFLzh","JvZhhzycHu2Yd57RN","vzfz4AS6wbooaTeQk","gHefoxiznGfsbiAu9","sbcmACvB6DqYXYidL","kipMvuaK3NALvFHc9","xdwbX9pFEr7Pomaxv","XvN2QQpKTuEzgkZHY","uFNgRumrDTpBfQGrs","ii4xtogen7AyYmN6B","kpPnReyBC54KESiSn","FRv7ryoqtvSuqBxuT","u8GMcpEN9Z6aQiCvp","B2CfMNfay2P8f2yyc","JD7fwtRQ27yc8NoqS","mRwJce3npmzbKfxws","3rxMBRCYEmHCNDLhu","FWvzwCDRgcjb9sigb","KrJfoZzpSDpnrv9va","LpM3EAakwYdS6aRKf","Cf2xxC3Yx9g6w7yXN","qHCDysDnvhteW7kRd","mELQFMi9egPn5EAjK","qDmnyEMtJkE9Wrpau","4ZvJab25tDebB8FGE","4QemtxDFaGXyGSrGD","Psr9tnQFuEXiuqGcR","qmXqHKpgRfg83Nif9","ximou2kyQorm6MPjX","eccTPEonRe4BAvNpD","2cYebKxNp47PapHTL","pv7Qpu8WSge8NRbpB","PqMT9zGrNsGJNfiFR","B9kP6x5rpmuCzpfWb","zB4f7QqKhBHa5b37a","qc7P2NwfxQMC3hdgm","RcifQCKkRc9XTjxC2","YABJKJ3v97k9sbxwg","bNXdnRTpSXk9p4zmi","fRsjBseRuvRhMPPE5","MzKKi7niyEqkBPnyu","NQgWL7tvAPgN2LTLn","cujpciCqNbawBihhQ","wEebEiPpEwjYvnyqq","AqbWna2S85pFTsHH4","Nwgdq6kHke5LY692J","8xLtE3BwgegJ7WBbf","SWxnP5LZeJzuT3ccd","Tr7tAyt5zZpdTwTQK","ax695frGJEzGxFBK4","FkgsxrGf3QxhfLWHG","vJ7ggyjuP4u2yHNcP","X5RyaEDHNq5qutSHK","xhD6SHAAE9ghKZ9HS","AyNHoTWWAJ5eb99ji","F5ktR95qqpmGXXmLq","znfkdCoHMANwqc2WE","jbE85wCkRr9z7tqmD","4K5pJnKBGkqqTbyxx","yeADMcScw8EW9yxpH","9QxnfMYccz9QRgZ5z","X2i9dQQK3gETCyqh2","4XRjPocTprL4L8tmB","D6trAzh6DApKPhbv4","BcYfsi7vmhDvzQGiF","i42Dfoh4HtsCAfXxL","zp5AEENssb8ZDnoZR","KwdcMts8P8hacqwrX","RQpNHSiWaXTvDxt6R","nSjavaKcBrtNktzGa","hNqte2p48nqKux3wS","7im8at9PmhbT4JHsW","SwcyMEgLyd4C3Dern","AHhCrJ2KpTjsCSwbt","rz73eva3jv267Hy7B","E4zGWYzh6ZiG85b2z","hvGoYXi2kgnS3vxqb","D4hHASaZuLCW92gMy","v7c47vjta3mavY3QC","G5TwJ9BGxcgh5DsmQ","YRgMCXMbkKBZgMz4M","ham9i5wf4JCexXnkN","a4jRN9nbD79PAhWTB","xJyY5QkQvNJpZLJRo","ivpKSjM4D6FbqF4pZ","p7x32SEt43ZMC9r7r","f886riNJcArmpFahm","xhE4TriBSPywGuhqi","ThvvCE2HsLohJYd7b","diruo47z32eprenTg","JJFphYfMsdFMuprBy","ZDZmopKquzHYPRNxq","KkwtLtroaNToWs2H6","vKErZy7TFhjxtyBuG","3L46WGauGpr7nYubu","CSZnj2YNMKGfsMbZA","G2Lne2Fi7Qra5Lbuf","x6hpkYyzMG6Bf8T3W","aFaKhG86tTrKvtAnT","PrCmeuBPC4XLDQz8C","dYspinGtiba5oDCcv","9cbEPEuCa9E7uHMXT","N5Jm6Nj4HkNKySA5Z","asmZvCPHcB4SkSCMW","duxy4Hby5qMsv42i8","Djs38EWYZG8o7JMWY","A8iGaZ3uHNNGgJeaD","XYYyzgyuRH5rFN64K","2jfiMgKkh7qw9z8Do","JPan54R525D68NoEt","o4cgvYmNZnfS4xhxL","CeZXDmp8Z363XaM6b","DQKgYhEYP86PLW7tZ","niQ3heWwF6SydhS7R","gvK5QWRLk3H8iqcNy","fnkbdwckdfHS2H22Q","YicoiQurNBxSp7a65","JBFHzfPkXHB2XfDGj","tj8QP2EFdP8p54z6i","9fB4gvoooNYa4t56S","zTfSXQracE7TW8x4w","YcdArE79SDxwWAuyF","8xRSjC76HasLnMGSf","CvKnhXTu9BPcdKE4W","DtcbfwSrcewFubjxp","NxF5G6CJiof6cemTw","4ZwGqkMTyAvANYEDw","EF5M6CmKRd6qZk27Z","cCMihiwtZx7kdcKgt","Qz6w4GYZpgeDp6ATB","TPjbTXntR54XSZ3F2","x3fNwSe5aWZb5yXEG","bnY3L48TtDrKTzGRb","ZFtesgbY9XwtqqyZ5","S7csET9CgBtpi7sCh","tTWL6rkfEuQN9ivxj","L6Ktf952cwdMJnzWm","P6fSj3t4oApQQTB7E","4s2gbwMHSdh2SByyZ","sTwW3QLptTQKuyRXx","EYd63hYSzadcNnZTD","tF8z9HBoBn783Cirz","hyShz2ABiKX56j5tJ","YN6daWakNnkXEeznB","6DuJxY8X45Sco4bS2","TMFNQoRZxM4CuRCY6","q3JY4iRzjq56FyjGF","diutNaWF669WgEt3v","5okDRahtDewnWfFmz","r3NHPD3dLFNk9QE2Y","ALkH4o53ofm862vxc","N9oKuQKuf7yvCCtfq","WjsyEBHgSstgfXTvm","2G8j8D5auZKKAjSfY","rBkZvbGDQZhEymReM","nNqXfnjiezYukiMJi","36Dhz325MZNq3Cs6B","f2GF3q6fgyx8TqZcn","byewoxJiAfwE6zpep","nEBbw2Bc2CnN2RMxy","w4aeAFzSAguvqA5qu","xFotXGEotcKouifky","rzqACeBGycZtqCfaX","DoPo4PDjgSySquHX8","o3RLHYviTE4zMb9T9","5gfqG3Xcopscta3st","GNhMPAWcfBCASy8e6","uXH4r6MmKPedk8rMA","Gg9a4y8reWKtLe3Tn","bBdfbWfWxHN9Chjcq","sT6NxFxso6Z9xjS7o","k9dsbn8LZ6tTesDS3","exa5kmvopeRyfJgCy","YTJp5WBcktBimdxBG","X79Rc5cA5mSWBexnd","SvKpaPbZ2tibeDpgh","rQKstXH8ZMAdN5iqD","vQKbgEKjGZcpbCqDs","Z9cbwuevS9cqaR96h","pHHaNkG8xDcaq5DJF","sjRG35aq5fosJ6mdG","pPWiLGsWCtN92vLwu","D5BP9CxKHkcjA7gLv","57sq9qA3wurjres4K","t2LGSDwT7zSnAGybG","7Pq9KwZhG6vejmYpo","g3PwPgcdcWiP33pYn","zcriHTKgKNehSSdyG","kvLPC5YWgSujcHSkY","HnC29723hm6kJT7KP","CRiJuJxgArjBMJLvK","dyJfGeWo5GX2u6NGi","QLmSFeFexgTLsNeeA","kmT47aLQmqzcw329Y","givHhuPu6G43g8kWN","83DimRqppcaoyYAsy","vvzfFcbmKgEsDBRHh","FfNEt8mpi6qanNmXg","MrAfiomDNWCzxjei5","73kwTFKgi4AagxFHJ","iBBK4j6RWC7znEiDv","W8vSrHAM9qoWdzFoP","Rx9GLepCxctXDqCPc","4X9JLr2SpB6v68twG","yxTP9FckrwoMjxPc4","FuZ7MoR3dJEJuoRbN","xRyLxfytmLFZ6qz5s","mwGAyWmsSqzMz4WMd","xxC3Ka7axphW8kJ9E","KT8Mf3ey6uwQAkWek","GDT6tKH5ajphXHGny","ZXaRHHLsxaTTQQsZb","CHdsSaQGAvtkXBzmJ","HAEPbGaMygJq8L59k","SmDziGM9hBjW9DKmf","8NKu9WES7KeKRWEKK","NfdHG6oHBJ8Qxc26s","LTtNXM9shNM9AC2mp","uKp6tBFStnsvrot5t","baTWMegR42PAsH9qJ","Xqcorq5EyJBpZcCrN","7cAsBPGh98pGyrhz9","ZbgCx2ntD5eu8Cno9","9kcTNWopvXFncXgPy","HxWdXMqoQtjDhhNGA","xwBuoE9p8GE7RAuhd","inedT6KkbLSDwZvfd","sWLLdG6DWJEy3CH7n","dhj9dhiwhq3DX6W8z","yLLkWMDbC9ZNKbjDG","P3Yt66Wh5g7SbkKuT","brXr7PJ2W4Na2EW2q","45mNHCMaZgsvfDXbw","7izSBpNJSEXSAbaFh","pfoZSkZ389gnz5nZm","jfG6vdJZCwTQmG7kb","sGnPTfjE5JthAStqg","gvA4j8pGYG4xtaTkw","PZtsoaoSLpKjjbMqM","jnDibtfvWNHLucf4D","GrtbTAPfkJa4D6jjH","zEWJBFFMvQ835nq6h","64FdKLwmea8MCLWkE","Dx9LoqsEh3gHNJMDk","FMkQtPvzsriQAow5q","XuLG6M7sHuenYWbfC","PGv9THs68ArPur7yP","NcGBmDEe5qXB7dFBF","tEDXpFgsHsm5T8sWz","7gsehrZnvXo2YGiT7","x4n4jcoDP7xh5LWLq","boBZkTqPdboX5u7g9","CJw2tNHaEimx6nwNy","CcC8MocynqKPmMPwL","Rrt7uPJ8r3sYuLrXo","rwjv8bZfSuE9ZAigH","khYYedgupgrHonWNc","wrkEnGrTTrM2mnmGa","f9s7pHub6hbsX7YKT","YduZEfz8usGbJXN4x","55SHk8kh9dDvaDTCC","SFG9Cm7mf5eP4juKs","eLRSCC7r4KinuxqZX","oW6mbA3XHzcfJTwNq","kWMkDoy3izRTobZFe","LtsJLfnP4YwhGdaCf","w9kwayt5SWqBQe8Nx","h5CGM5qwivGk2f5T9","iPGpENE4ARKbzzQmt","PQ3nutgxfTgvq69Xt","3zZjF3YKJ257x79mu","9Qwignbzu4ddXLTsT","aiCtrN9EF2FjKz5sv","JcpwEKbmNHdwhpq5n","idipkijjz5PoxAwju","F7RgpHHDpZYBjZGia","xWTSHJASRaLABgHWc","Fg8dtE8HHkDoiGcwt","zPJE7MDtL25RpN7Cc","qqhdj3W3vSfB5E9ss","9SE67uz98kh6x2CxR","gR6H3egpRPNYnoTrA","qPoaA5ZSedivA4xJa","H6L7fuEN9qXDanQ6W","gfexKxsBDM6v2sCMo","7uJnA3XDpTgemRH2c","stb3Jjumzhv49zCEb","XjMkPyaPYTf7LrKiT","XuyRMxky6G8gq7a69","huRxRzwcvwTzvtEPY","8bWbNwiSGbGi9jXPS","sq3WkpyqGANT7hGRP","AyfDnnAdjG7HHeD3d","WmfapdnpFfHWzkdXY","8rYxw9xZfwy86jkpG","zFhhDCxz87yKwqYQf","doiMq8aH2yiZaCJsT","MQzbaHoiQutiHkx2M","ra9Pt2JkEDnKW4jsc","9YDk52NPrfq7nqLvd","KTEciTeFwL2tTujZk","6bSjRezJDxR2omHKE","r5H6YCmnn8DMtBtxt","JbcWQCxKWn3y49bNB","R4FX6wDmppvZ2JqpB","9vnWFwng8QzEnBT8z","XCtFBWoMeFwG8myYh","6uwLq8kofo4Tzxfe2","G993PFTwqqdQv4eTg","DWgWbXRfXLGHPgZJM","K7wtTqTEoKXC9Kb24","hmai5Lru5kWXpH7Ju","w4jjwDPa853m9P4ag","xvAkpCSdqgtYhEceo","6vMBpZtoRw4ia2JrK","Wzjjynmp8gMmdX6dt","CsN6WxwDnPzxAFhps","CLXkgEerPi9MpJCem","BKjJJH2cRpJcAnP7T","qXtbBAxmFkAQLQEJE","jES7mcPvKpfmzMTgC","D7epkkJb3CqDTYgX9","FpcgSoJDNNEZ4BQfj","mF8dkhZF9hAuLHXaD","camG6t6SxzfasF42i","HALKHS4pMbfghxsjD","HDXLTFnSndhpLj2XZ","fgYQjTktBmNZvMqce","fwNskn4dosKng9BCB","B5auLtDfQrvwEkw4Q","z7YvA5osMotdL5F4w","Hoh6umyMWSqzPGMJZ","vHSrtmr3EBohcw6t8","nsCwdYJEpmW5Hw5Xm","LKAXgTen4Xbqb8eZY","22GrdspteQc8EonMn","TSaJ9Zcvc3KWh3bjX","sJK6HN5vTPPnuuNgQ","mh3xapTix6fFtd3xM","JBnaLpsrYXLXjFocu","uR8c2NPp4bWHQ5u45","d4YGxMpzmvxknHfbe","wcNEXDHowiWkRxDNv","scNCmwaduCgJmCBYh","LsXtcLyzyfGg3gT5R","McN9BNtNcbYNfdCB5","4tzEAgdbNTwB6nKyL","sCFGEhwcB8MX3FQf5","G4uMdBzgDsxMsTNmr","34Gkqus9vusXRevR8","7MCqRnZzvszsxgtJi","HXxHcRCxR4oHrAsEr","cmrtpfG7hGEL9Zh9f","oHk9T3jbx2J5zJ39P","sYt3ZCrBq2QAf3rak","r8stxYL29NF9w53am","zymnWfGwf6BdDt64c","yyDrMYBfvYtKbmPmm","4gevjbK77NQS6hybY","jnjjzkH8Fdzg4D6EK","XKfQF73YnyMRiRf9a","gYfgWSxCpFdk2cZfE","CQsEwAyJP6NYvKZw6","JiLcxpWzCrnwkndsT","gpk8dARHBi7Mkmzt9","GrbeyZzp6NwzSWpds","9MZdwQ7u53oaRiBYX","gFyJgnu5vAbzELBM8","ouQNu3hhfKLBRuwR7","m5AH78nscsGjMbBwv","oKYWbXioKaANATxKY","cq5x4XDnLcBrYbb66","KjdP2WjWng6skwbY7","wfpdejMWog4vEDLDg","7F5jo5LD9FD7DpxCX","kDjKF2yFhFEWe4hgC","pWi5WmvDcN4Hn7Bo6","NGc3Yjecg9pDMznWq","xxvKhjpcTAJwvtbWM","DJnvFsZ2maKxPi7v7","zo9zKcz47JxDErFzQ","fyZBtNB3Ki3fM4a6Y","H4kadKrC2xLK24udn","BxersHYN2qcFoonwg","Ck5cgNS2Eozc8mBeJ","wr9dH2GjztvCz6pYX","EzAt4SbtQcXtDNhHK","syeBtmGoKvjJTH6sH","eWqFy8wESHbxNod7i","8cWMX6L8St8k9pPRC","jP583FwKepjiWbeoQ","rMfpnorsMoRwyn4iP","TKk7rShf9d5ePN7vR","fNJvYD6XqnX82i4jA","r8aAqSBeeeMNRtiYK","Gh2qQHrCg3teQen3c","3GAnfeG9KmsbsWeTj","JKgGvJCzNoBQss2bq","JjGs6mDZxeCWkg3ii","AzKx6EjaoaMuk595v","duAkuSqJhGDcfMaTA","pXLqpguHJzxSjDdx7","FbJYEn6eWA5JnGeGP","8GiTowD6XqTNzgCz7","qfDgEreMoSEtmLTws","96N8BT9tJvybLbn5z","SCs4KpcShb23hcTni","bDMoMvw2PYgijqZCC","nqwzrpkPvviLHWXaE","YuZXRxWSqaCoZHEXr","6YYmkpumigAmh3efu","SgszmZwrDHwG3qurr","9EyzaH3jzH3PyQtM5","eR7Su77N2nK3e5YRZ","GSBCw94DsxLgDat6r","cpdsMuAHSWhWnKdog","avvXAvGhhGgkJDDso","KnPN7ett8RszE79PH","ptmmK9PWgYTuWToaZ","XNhfw5Bqsi4SGNNBk","PKy8NuNPknenkDY74","3yqf6zJSwBF34Zbys","YpyW97jRbtvBAncAr","LwcKYR8bykM6vDHyo","H6hMugfY3tDQGfqYL","iyRpsScBa6y4rduEt","mLuQfS7gmfr4nwTdv","TrvkWBwYvvJjSqSCj","yXHcqrCpiHC5tDuEc","HKfBeWN8ufNdFgzG6","P8yeoeJ2bwmnD93mZ","kxW6q5YdTGWh5sWby","ksatPnddyZjHwZWwG","st7DiQP23YQSxumCt","tE7y8FZe7wSSzoRaS","L4HQ3gnSrBETRdcGu","eqxqgFxymP8hXDTt5","uKWXktrR7KpbgZAs4","h4vWsBBjASgiQ2pn6","DXBziiT2RFLcmLY9J","k42G2aaNhRNB7hdCJ","XSKQLeQnBupFo7GGC","BnDF5kejzQLqd5cjH","AMmqk74zWmvP8tXEJ","NQQzXpahhkb6f6ZCe","Tk5ovpucaqweCu4tu","9WX59u7g2sdKqnjDm","Xht9swezkGZLAxBrd","8c8AZq5hgifmnHKSN","nMNi86hgNjaNnh8iu","s3rAKTkdSHb6Hwwoz","rqnbrJhDKCoZvNGEZ","Ea8pt2dsrS6D4P54F","uN3wjp2K6TEQ2oAML","DAc4iuy4D3EiNBt9B","jqCz2X49FRn5Bgb5b","8hxvfZiqH24oqyr6y","puYfAEJJomeodeSsi","S54HKhxQyttNLATKu","igSPcmvTigCHxWt8x","4esQ684vtR9zcjHgW","yGaw4NqRha8hgx5ny","eHnupDgggBqDqT5eg","k7oxdbNaGATZbtEg3","bbGEiSmNiTpPrFhcQ","Z6dmoLyfBdmo6HEss","QcXuwQvvPkqcKZmXS","7FJRnxbRtT7Sbzizs","oBTkthd7h8sDpkiu2","cmiRk9XtT9Psnd3Yr","G6npMHwgRGSQDKavX","hwxj4gieR7FWNwYfa","yGycR8tFA3JJbvApp","jxy7rBcQink8a7C9b","vQNJrJqebXEWjJfnz","kjmpq33kHg7YpeRYW","FwYMuD2sNcaEpE5on","4rwABGAd9kZG8nf2P","GkXKvkLAcTm5ackCq","TrmMcujGZt5JAtMGg","gBpYo7mt2zNBmtBJd","aNRYQFnMQbA7uu99u","YMokuZdoY9tEDHjzv","MG8Yhsxqu9JY4xRPr","zEvqFtT4AtTztfYC4","fzeoYhKoYPR3tDYFT","8npC4KRcAJtGdErTq","AYbhqi65SWzHzy7Xx","N99KgncSXewWqkzMA","2KacvW34BbXFmDBtQ","tSgcorrgBnrCH8nL3","NHuLAS3oKZWr2X9hP","9hR2RmpJmxT8dyPo4","fwSDKTZvraSdmwFsj","Cf2zBkoocqcjnrNFD","MPj7t2w3nk4s9EYYh","TTPux7QFBpKxZtMKE","shcSdHGPhnLQkpSbX","M4w2rdYgCKctbADMn","hMjFMSQZb4swKugfv","mkrvsNi8cYGSjGqkh","DXcezGmnBcAYL2Y2u","Aq8BQMXRZX3BoFd4c","FoJSa8mgLPT83g9e8","Xt85tj6GQJCuuXT68","JAAHjm4iZ2j5Exfo2","sAiHxHkQrsYsRpKFP","6phFYpNQH9SmWL9Jt","Rkxj7TFxhbm59AKJh","rNFzvii8LtCL5joJo","Hw26MrLuhGWH7kBLm","Zvu6ZP47dMLHXMiG3","HByDKLLdaWEcA2QQD","7qhtuQLCCvmwCPfXK","FgjcHiWvADgsocE34","Lp4Q9kSGsJHLfoHX3","3xF66BNSC5caZuKyC","BseaxjsiDPKvGtDrm","Q924oPJzK92FifuFg","oJwJzeZ6ar2Hr7KAX","H7Rs8HqrwBDque8Ru","gEKHX8WKrXGM4roRC","FKB7iEergZaC7PvQf","suxvE2ddnYMPJN9HD","iETtCZcfmRyHp69w4","mz3hwS4c9bc9EHAm9","KFLdfuw35qkgjzWer","RApxEu3A4GnvGoEe2","XLbDQL2qYi9FDozvL","p4XpZWcQksSiCPG72","mB95aqTSJLNR9YyjH","2NaAhMPGub8F2Pbr7","BbM47qBPzdSRruY4z","dYnHLWMXCYdm9xu5j","qHpazCw3ryvBojGSa","wyYubb3eC5FS365nk","wmjPGE8TZKNLSKzm4","CBWSDdzjqfnexBurB","gBnSRErajRtvhMnDr","BfBF6T6HA82zBxPrv","dbDHEQyKqnMDDqq2G","doPejjd84w8BmERqj","PT8vSxsusqWuN7JXp","dKxX76SCfCvceJXHv","DSzpr8Y9299jdDLc9","hnLutdvjC8kPScPAj","vit9oWGj6WgXpRhce","CsKboswS3z5iaiutC","kjQXzkTGuixoJtQnq","RgJicDmXHDxcJ9Fsw","L6iFpR9ZyTmzHvYci","Z5ZBPEgufmDsm7LAv","PRAyQaiMWg2La7XQy","x6Kv7nxKHfLGtPJej","3pjv6uDvY9sqmsnvY","Aet2mbnK7GDDfrEQu","scL68JtnSr3iakuc6","3SG4WbNPoP8fsuZgs","XfpJ6WQBDcEcC8Mu4","iprqfLaDLCGoJFeiZ","frApEhpyKQAcFvbXJ","znBJwbuT3f5eWgM4E","cR7Zfrc4BtnFes46y","hbmsW2k9DxED5Z4eJ","SzecSPYxqRa5GCaSF","hxaq9MCaSrwWPmooZ","FSmPtu7foXwNYpWiB","WQWhXzALcrzrJtqRh","jYNT3Qihn2aAYaaPb","gebzzEwn2TaA6rGkc","WhHFvzFsYfMxgYCdo","tjxgbovwc5Ft7wrtc","2brqzQWfmNx5Agdrx","QaDwBio8MLqRvTREH","Jko7pt7MwwTBrfG3A","A9tJFJY7DsGTFKKkh","Wnqua6eQkewL3bqsF","DJB82jKwgJE5NsWgT","5b6YcFbEBCZbX6YSK","zk6RK3xFaDeJHsoym","FQqcejhNWGG8vHDch","srge9MCLHSiwzaX6r","DJRe5obJd7kqCkvRr","D8ds9idKWbwzCseCh","hTMFt3h7QqA2qecn7","9LXxgXySTFsnookkw","CHtwDXy63BsLkQx4n","u5RLu5F3zKTB3Qjnu","4tke3ibK9zfnvh9sE","2WngsveoLhFubuLMH","ADwayvunaJqBLzawa","NG6FrXgmqPd5Wn3mh","Ww5xKq5brC4xAJY7o","HL6x8zHo9BkuK3tic","PKBXczqhry7iK3Ruw","oBBzqkZwkxDvsKBGB","HuFZJkGptWDtRbkWs","iQWk5jYeDg5ACCmpx","RdpqsQ6xbHzyckW9m","sizjfDgCgAsuLJQmm","X3p8mxE5dHYDZNxCm","wZGpoZgDANdkwTrwt","uAc7bWgpEhrGwFcv7","3nDR23ksSQJ98WNDm","sMsvcdxbK2Xqx8EHr","evYFijNMdjfbPaCho","Psp8ZpYLCDJjshpRb","Zupr296Zy74wpihXT","68dHanLWsS6SEyZp9","x9FNKTEt68Rz6wQ6P","DWHkxqX4t79aThDkg","xLm9mgJRPvmPGpo7Q","6LzKRP88mhL9NKNrS","XYDsYSbBjqgPAgcoQ","eRohP4gbxuBuhqTbe","Wpf3Gsa8A89mmjkk8","PfcQguFpT8CDHcozj","XPwEptSSFRCnfHqFk","pohTfSGsNQZYbGpCy","zcPLNNw4wgBX5k8kQ","2meuc3kPRkBcRpj3R","bzhGBHrGrFfQss4Df","2269iGRnWruLHsZ5r","kj37Hzb2MsALwLqWt","Qz9GvoPbnFwGrHHQB","pJJdcZgB6mPNWoSWr","dtmmP4YdJEfK9y4Rc","QPqm5aj2meRmE7kR8","2oybbEw697CQgcRE5","TYTEJxzeK3jBMq2TZ","K4eDzqS2rbcBDsCLZ","FcRt3xAF4ynojfj6G","gMXsyhPiEJbGerF6F","9sguwESkteCgqFMbj","mvPfao35Moah8py46","kuDKtwwbsksAW4BG2","pL56xPoniLvtMDQ4J","ENBzEkoyvdakz4w5d","wM4bcDxEh75NDkhjo","YAkpzvjC768Jm2TYb","ExssKjAaXEEYcnzPd","n3LAgnHg6ashQK3fF","GMCs73dCPTL8dWYGq","8gapy2nLy4wysXSGL","dgFcJtHaYfaoByAK9","HhWhaSzQr6xmBki8F","CpvyhFy9WvCNsifkY","aan3jPEEwPhrcGZjj","mhA4vkeaRn9cpxkag","iA25AvZqAr6G8mAXR","C4tR3BEpuWviT7Sje","FghubkDy6Dp6mnxk7","RKz7pc6snBttndxXz","jiJquD34sa9Lyo5wc","c8EeJtqnsKyXdLtc5","ZGGGBR9sDgtLgMDaA","uM6mENiJi2pNPpdnC","o9dnstYoc7cwpgdhg","YSWa8rYeD3aDaofSP","pC47ZTsPNAkjavkXs","QtyKq4BDyuJ3tysoK","bYrF8rXFYwPqnfxTp","KbyRPCAsWv5GtfrbG","c2RzFadrxkzyRAFXa","9ZodFr54FtpLThHZh","xmoYza9vgcRvWD5PA","sbb9bZgojmEa7Yjrc","6yTShbTdtATxKonY5","BHYBdijDcAKQ6e45Z","qGEqpy7J78bZh3awf","KJbQyFbXiiYDDWbaS","PYtus925Gcg7cqTEq","yTvBSFrXhZfL8vr5a","Aud7CL7uhz55KL8jG","bXTNKjsD4y3fabhwR","AmNjHo8xXMKnZEWRS","CHD5m9fnosr7L3dto","MN4NRkMw7ggt9587K","CDXDnruBJe23rpdfC","y5GftLezdozEHdXkL","d6yNW5T6J9rtnGizc","pT48swb8LoPowiAzR","27AWRKbKyXuzQoaSk","vNHf7dx5QZA4SLSZb","KwbJFexa4MEdhJbs4","mja6jZ6k9gAwki9Nu","fW9n8bEuMpLwkxCx6","muXfZr5EYCfZqLmsb","5PBWgHiCiiJHjPRSn","PAYMMgPi2L3MPP967","RaxaXBNmStYe289gC","DMxe4XKXnjyMEAAGw","xF7gBJYsy6qenmmCS","gMszBSAX23uqYhytR","HbXXd2givHBBLxr3d","Z5wF8mdonsM2AuGgt","utySCY9nJt9xGYGGQ","gCz7cB6JG66EhweSS","krHDNc7cDvfEL8z9a","aNAFrGbzXddQBMDqh","sksP9Lkv9wqaAhXsA","p3s8RvkcyTwzu27ps","8ccTZ9ZxpJrvnxt4F","p7WXmG6Fbo3eaSwm3","CPBmbgYZpsGqkiz2R","yDRX2fdkm3HqfTpav","WbLAA8qZQNdbRgKte","75dnjiD8kv2khe9eQ","JZZENevaLzLLeC3zn","MgFDzAfCku9MSDLuw","PQtEqmyqHWDa2vf5H","zbqLuTgTCu365MNu9","P3uavjFmZD5RopJKk","8gqrbnW758qjHFTrH","pZaPhGg2hmmPwByHc","4hLcbXaqudM9wSeor","WxW6Gc6f2z3mzmqKs","j9HoG56Y6KuopSzdn","GhFoAxG49RXFzze5Y","rD57ysqawarsbry6v","LCfaLXcWnk8pujnX4","tAXrD8Y6hcJ8dt6Nt","af9MjBqF2hgu3EN6r","FRRb6Gqem8k69ocbi","LbyxFk8JmPKPAQBvL","PHmYhE4sKnwzYgvkh","fZJRxYLtNNzpbWZAA","kgmkdf3C7EkDX7dnT","Gs29k3beHiqWFZqnn","MMAK6eeMCH3JGuqeZ","cdB5f2adKoLGW8Ytc","5e49dHLDJoDpeXGnh","Ccsx339LE9Jhoii9K","PHnMDhfiadQt6Gj23","Jo89KvfAs9z7owoZp","fri4HdDkwhayCYFaE","tD9zEiHfkvakpnNam","xggxWfyzZmnz7hydm","JgBBuDf5uZHmpEMDs","vbcjYg6h3XzuqaaN8","hRohhttbtpY3SHmmD","6KzFwcDy7hsCkzJKY","F2DZXsMdhGyX4FPAd","esRZaPXSHgWzyB2NL","AqsjZwxHNqH64C2b6","4psQW7vRwt7PE5Pnj","voLHQgNncnjjgAPH7","aaHDA4X6cTzFrvuSX","LHtMNz7ua8zu4rSZr","zjMKpSB2Xccn9qi5t","BAzCGCys4BkzGDCWR","goC9qv4PWf2cjfnbm","Z2CuyKtkCmWGQtAEh","c3iQryHA4tnAvPZEv","vwLxd6hhFvPbvKmBH","Js34Ez9nrDeJCTYQL","fJvjin8ETkzhFdadC","W59Nb72sYJhMJKGB8","xiPMaYGTm2xfsB8WF","oPEWyxJjRo4oKHzMu","PjfsbKrK5MnJDDoFr","sBBGxdvhKcppQWZZE","vwM7hnT9ysE3suwfk","BzYmJYECAc3xyCTt6","uiyWHaTrz3ML7JqDX","vZssZr2wq7YrG3FMa","73QyjLymEak4L8RDC","6vcxuRHzeM99jYcYd","bG4PR9uSsZqHg2gYY","HoQ5Rp7Gs6rebusNP","9iA87EfNKnREgdTJN","QEYWkRoCn4fZxXQAY","kAgJJa3HLSZxsuSrf","FZaDFYbnRoHmde7F6","BNfL58ijGawgpkh9b","4gDbqL3Tods8kHDqs","DwqgLXn5qYC7GqExF","atcJqdhCxTZiJSxo2","zRn6cLtxyNodudzhw","P32AuYu9MqM2ejKKY","K2JBqDeETX2yEgyyZ","3FoMuCLqZggTxoC3S","LcEzxX2FNTKbB6KXS","o5F2p3krzT4JgzqQc","cy3BhHrGinZCp3LXE","zsG9yKcriht2doRhM","WYmmC3W6ZNhEgAmWG","EL4HNa92Z95FKL9R2","EKu66pFKDHFYPaZ6q","Pa5NqtxHBkGuCh98G","JKj5Krff5oKMb8TjT","vwt3wKXWaCvqZyF74","4basF9w9jaPZpoC8R","Bfq6ncLfYdtCb6sat","jDQm7YJxLnMnSNHFu","FDJnZt8Ks2djouQTZ","f3o9ydY7iPjFF2fyk","KnQs55tjxWopCzKsk","Ww2dxwWpSfkQB4NZb","ZawRiFR8ytvpqfBPX","ZGzDNfNCXzfx6hYAH","rFjhz5Ks685xHbMXW","Mrz2srZWc7EzbADSo","B4DuwmtqF3HhNwvua","zQKgKjecvR4W7oJw5","BSpdshJWGAW6TuNzZ","JHcTP4Ad8QAmRTCZm","GGn8MBiY8Xz6NdNdH","hQysqfSEzciRazx8k","AtfQFj8umeyBBkkxa","r99tazGiLgzqFX7ka","uFYQaGCRwt3wKtyZP","BFamedwSgRdGGKXQQ","teaxCFgtmCQ3E9fy8","ka8eveZpT7hXLhRTM","euJm4RwkAptZnP89i","LLRtjkvh9AackwuNB","yPLr2tnXbiFXkMWvk","ervaGwJ2ZcwqfCcLx","4AHXDwcGab5PhKhHT","NuueGqPZdotjMQKLu","qjSHfbjmSyMnGR9DS","xtzvtJBNofk4FPAtt","SkcM4hwgH3AP6iqjs","Br4xDbYu4Frwrb64a","HvcZmKS43SLCbJvRb","BEtzRE2M5m9YEAQpX","EhEZoTFzys9EDmEXn","bmoQ2wy7Nd7EiJdpg","pYcFPMBtQveAjcSfH","zb3hWt99i9Fm93KPq","W9rJv26sxs4g2B9bL","Dod9AWz8Rp4Svdpof","hQHuXuRGZxxWXaPgg","zB3ukZJqt3pQDw9jz","KheBaeW8Pi7LwewoF","Ek7M3xGAoXDdQkPZQ","guDcrPqLsnhEjrPZj","7XbcDaeigMaxW43EB","ttGbpJQ8shBi8hDhh","wJnm5cBiZGmKn595f","puhPJimawPuNZ5wAR","eoHbneGvqDu25Hasc","gHgs2e2J5azvGFatb","x5ASTMPKPowLKpLpZ","EhAbh2pQoAXkm9yor","jfq2BH5kfQqu2vYv3","Mf2MCkYgSZSJRz5nM","mXgsd5o9uuYaQKHMz","YM6Qgiz9RT7EmeFpp","PcfHSSAMNFMgdqFyB","uX3HjXo6BWos3Zgy5","nzmCvRvPm4xJuqztv","CMt3ijXYuCynhPWXa","Ndtb22KYBxpBsagpj","yFJ7vCjefBxnTchmG","SQ9cZtfrzDJmw9A2m","PJLABqQ962hZEqhdB","HmfxSWnqnK265GEFM","i3BTagvt3HbPMx6PN","ZEgQGAjQm5rTAnGuM","ctpkTaqTKbmm6uRgC","qEweugBipR5P2cMyK","xnPFYBuaGhpq869mY","YtvZxRpZjcFNwJecS","ido3qfidfDJbigTEQ","85J8hjEn48FicYfvp","N6vZEnCn6A95Xn39p","tJQsxD34maYw2g5E4","96TBXaHwLbFyeAxrg","ixZLTmFfnKRbaStA5","2x7fwbwb35sG8QmEt","oaqKjHbgsoqEXBMZ2","t2NN6JwMFaqANuLqH","J9pNx22bj5RuiRjAj","AN2cBr6xKWCB8dRQG","G5eMM3Wp3hbCuKKPE","y5jAuKqkShdjMNZab","vADtvr9iDeYsCDfxd","x4GmqcwjFTnWeRiud","5ntgky9ShzKKWu7us","z8usYeKX7dtTWsEnk","3S4nyoNEEuvNsbXt8","EEv9JeuY5xfuDDSgF","ASpGaS3HGEQCbJbjS","AXXaXJvf7WcTessog","QL7J9wmS6W2fWpofd","osYFcQtxnRKB4F4HA","MajyZJrsf8fAywWgY","bvqC4Ci7rXq4sN9df","GctJD5oCDRxCspEaZ","A9NxPTwbw6r6Awuwt","dKTh9Td3KaJ8QW6gw","oTX2LXHqXqYg2u4g6","LuXb6CZG4x7pDRBP8","hamma4XgeNrsvAJv5","BfTW9jmDzujYkhjAb","DoHcgTvyxdorAMquE","EbFABnst8LsidYs5Y","Sdx6A6yLByRRs8iLY","qbHLGo5vu8HD3JqEM","48WeP7oTec3kBEada","LgavAYtzFQZKg95WC","5QpufhoH2ASnppsjs","Kz9zMgWB5C27Pmdkh","qy5dF7bQcFjSKaW58","wkuDgmpxwbu2M2k3w","JcpzFpPBSmzuksmWM","zMxrkFrB6ka4Lb7fM","PX7AdEkpuChKqrNoj","ui6mDLdqXkaXiDMJ5","uXn3LyA8eNqpvdoZw","FwiPfF8Woe5JrzqEu","hzuSDMx7pd2uxFc5w","mHqQxwKuzZS69CXX5","yKXKcyoBzWtECzXrE","zHS4FJhByRjqsuH4o","a5JAiTdytou3Jg749","HEn2qiMxk5BggN83J","tYAvXXgSwHCzNTK8f","WXvt8bxYnwBYpy9oT","kLR5H4pbaBjzZxLv6","CtXaFo3hikGMWW4C9","4DBBQkEQvNEWafkek","qwdupkFd6kmeZHYXy","EHbJ69JDs4suovpLw","w5F4w8tNZc6LcBKRP","xqkGmfikqapbJ2YMj","yRAo2KEGWenKYZG9K","scwoBEju75C45W5n3","qJgz2YapqpFEDTLKn","aSQy7yHj6nPD44RNo","Ltey8BS83qSkd9M3u","9Yc7Pp7szcjPgPsjf","hN2aRnu798yas5b2k","ERWeEA8op6s6tYCKy","yJfBzcDL9fBHJfZ6P","BZ6XaCwN4QGgH9CxF","3nMpdmt8LrzxQnkGp","TNHQLZK5pHbxdnz4e","F6ZTtBXn2cFLmWPdM","neQ7eXuaXpiYw7SBy","k2SNji3jXaLGhBeYP","WsSybGTqpBoHpXJyQ","jtMXj24Masrnq3SpS","jqTeghCJ2anMHPPjG","B7P97C27rvHPz3s9B","uK6sQCNMw8WKzJeCQ","hurF9uFGkJYXzpHEE","xEHy9oivifjgFbnvc","33KewgYhNSxFpbpXg","c5GHf2kMGhA4Tsj4g","dC7mP5nSwvpL65Qu5","hpjou9ZnLZkSJR7sd","bshZiaLefDejvPKuS","AvjbBjAAbKBk73v5F","XqvnWFtRD2keJdwjX","KJ9MFBPwXGwNpadf2","37sHjeisS9uJufi4u","5iZTwGHv2tNfFmeDa","gziZACDg6EBpGZbJe","RYcoJdvmoBbi5Nax7","9o3QBg2xJXcRCxGjS","vs3kzjLhbdKsndnBy","bZ2w99pEAeAbKnKqo","bjjbp5i5G8bekJuxv","vwqLfDfsHmiavFAGP","Yp2vYb4zHXEeoTkJc","z6QQJbtpkEAX3Aojj","ubPAo3zGeJNqtZDqT","pfibDHFZ3waBo6pAc","cumc876woKaZLmQs5","Ty2tjPwv8uyPK9vrz","ZiQqsgGX6a42Sfpii","ybYBCK9D7MZCcdArB","pNcFYZnPdXyL2RfgA","rEBXN3x6kXgD4pLxs","no5jDTut5Byjqb4j5","qCsxiojX7BSLuuBgQ","uyBeAN5jPEATMqKkX","aHaqgTNnFzD7NGLMx","bQ6zpf6buWgP939ov","mkbGjzxD8d8XqKHzA","CKpByWmsZ8WmpHtYa","midXmMb2Xg37F2Kgn","reitXJgJXFzKpdKyd","LFNXiQuGrar3duBzJ","KcvJXhKqx4itFNWty","RWu8eZqbwgB9zaerh","EFQ3F6kmt4WHXRqik","FfPukic3Qskd9ZAkk","A2Qam9Bd9xpbb2wLQ","t9svvNPNmFf5Qa3TA","n5TqCuizyJDfAPjkr","Kbm6QnJv9dgWsPHQP","gFMH3Cqw4XxwL69iy","Kyc5dFDzBg4WccrbK","RWo4LwFzpHNQCTcYt","vbWBJGWyWyKyoxLBe","PsEppdvgRisz5xAHG","tscc3e5eujrsEeFN4","GG2rtBReAm6o3mrtn","E4cKD9iTWHaE7f3AJ","yCWPkLi8wJvewPbEp","AcKRB8wDpdaN6v6ru","LbrPTJ4fmABEdEnLf","rtM3jFaoQn3eoAiPh","eDicGjD9yte6FLSie","xg3hXCYQPJkwHyik2","bJ2haLkcGeLtTWaD5","PBRWb2Em5SNeWYwwB","7hFeMWC6Y5eaSixbD","aMHq4mA2PHSM2TMoH","wpZJvgQ4HvJE2bysy","2brqzQWfmNx5Agdrx","GLMFmFvXGyAcG25ni","NLBbCQeNLFvBJJkrt","bYrF8rXFYwPqnfxTp","v7c47vjta3mavY3QC","3MvaoZbGPxtRFCijw","TappK5n3kZmQzWEWD","tSemJckYr29Gnxod2","WdkLDpBGMCWhfByAY","EctieqKwDQcQHhqZy","hNqte2p48nqKux3wS","qw3Z79HELMsmLkL9F","zwDz9pgT43fRczkB4","Fafzj3wMvoCW4WjeF","kxW6q5YdTGWh5sWby","G5eMM3Wp3hbCuKKPE","kSiT2XjfTnDHKx44W","DSzpr8Y9299jdDLc9","wZGpoZgDANdkwTrwt","qajfiXo5qRThZQG7s","rRzZzBBQ36CrqhZTY","aP36QcAsxyuEispq6","TxcRbCYHaeL59aY7E","MFNJ7kQttCuCXHp8P","PQ3nutgxfTgvq69Xt","JJFphYfMsdFMuprBy","ythFNoiAotjvuEGkg","GZSzMqr8hAB2dR8pk","BBQ5HEnL3ShefQxEj","fzeoYhKoYPR3tDYFT","bXuAXCbzw9hsJSuEN","mbCccXJuuRBZdXdpH","m7THsgXyxxiEXgyHv","xtHd6sfdr2bZHa6Pb","pfaTqpWFghfrbvzaD","u8GMcpEN9Z6aQiCvp","gBpYo7mt2zNBmtBJd","rkpDX7j7va6c8Q7cZ","NGkBfd8LTqcpbQn5Z","GEPX7jgLMB8vR2qaK"]},"locale":"en-US","mapbox":{"apiKey":"pk.eyJ1IjoiaGFicnlrYSIsImEiOiJjaWxvcnhidzgwOGlodHJrbmJ2bmVmdjRtIn0.inr-_5rWOOslGQxY8iDFOA"},"petrov":{"afterTime":1727400080403,"beforeTime":1727376805595,"petrovPostId":"6LJ6xcHEjKF9zWKzs","petrovServerUrl":"https://forum.effectivealtruism.org/graphql","petrovGamePostId":"KTEciTeFwL2tTujZk"},"reacts":{"addNewReactKarmaThreshold":10,"downvoteExistingReactKarmaThreshold":20,"addNameToExistingReactKarmaThreshold":5},"stripe":{"publicKey":"pk_live_51HtKAwA2QvoATZCZiy9f2nc6hA52YS1BE81cFu9FEV1IKar0Bwx6hIpxxxYHnhaxO9KM7kRYofZId3sUUI7Q0NeO00tGni3Wza"},"algolia":{"appId":"fakeAppId","searchKey":"fakeSearchKey","indexPrefix":"test_"},"llmChat":{"userIds":["McgHKH6MMYSnPwQcm","6Fx2vQtkYSZkaCvAg","MEu8MdhruX5jfGsFQ","YaNNYeR5HjKLDBefQ","hBEAsEpoNHaZfefxR","NFmcwmaFeTWfgrvBN","ZnpELPxzzD2CiigNy","Q7NW4XaWQmfPfdcFj","NXeHNNSFHGESrYkPv","QDNJ93vrjoaRBesk2","iMBN2523tmh4Yicc3","5iPRfSnjako6iM6LG","aBHfQ4C5fSM4TPyTn","n4M37rPXGyL6p8ivK","e9ToWWzhwWp5GSE7P","TCjNiBLBPyhZq5BuM","XLwKyCK7JmC292ZCC","S3ydcLKdejjkodNut","ENgxBL95Sc7MRwYty","KCExMGwS2ETzN3Ksr","XGEcH5rmq4yGvD82A","YFiFbXgjBpDKZT93g","dZMo8p7fGCgPMfdfD","Pdca6FNZBrXj9z28n","LHbu27FubhwFv8ZJt","gYxdDBQ3AZbde8HgZ","5JqkvjdNcxwN8D86a","6c2KCEXTGogBZ9KoE","haTrhurXNmNN8EiXc","cJnvyeYrotgZgfG8W"]},"logoUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1498011194/LessWrong_Logo_skglnw.svg","ckEditor":{"uploadUrl":"https://39669.cke-cs.com/easyimage/upload/","webSocketUrl":"39669.cke-cs.com/ws"},"recombee":{"enabled":true},"hasEvents":true,"logRocket":{"apiKey":"mtnxzn/lesswrong","sampleDensity":5},"reCaptcha":{"apiKey":"6LfFgqEUAAAAAHKdMgzGO-1BRBhHw1x6_8Ly1cXc"},"siteImage":"https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg","cloudinary":{"cloudName":"lesswrong-2-0","uploadPresetBanner":"navcjwf7","uploadPresetGridImage":"tz0mgw2s","uploadPresetSocialPreview":"nn5tppry"},"googleMaps":{"apiKey":"AIzaSyA3C48rl26gynG3qIuNuS-3Bh_Zz9jFXkY"},"adminAccount":{"email":"team@lesswrong.com","username":"LessWrong"},"annualReview":{"end":"2024-02-01T08:00:00Z","start":"2023-12-04T00:10:00Z","reviewPhaseEnd":"2024-01-15T08:00:00Z","votingPhaseEnd":"2024-02-01T08:00:00Z","nominationPhaseEnd":"2023-12-17T08:00:00Z","votingResultsPostId":"TSaJ9Zcvc3KWh3bjX","announcementPostPath":"/posts/B6CxEApaatATzown6/the-lesswrong-2022-review","reviewWinnerSectionsInfo":{"modeling":{"tag":"World Modeling","order":2,"title":"World","coords":{"leftXPct":0.05,"leftYPct":0,"rightXPct":0.57,"rightYPct":0,"middleXPct":0.31,"middleYPct":0,"leftFlipped":true,"leftWidthPct":0.26,"rightWidthPct":0.26,"middleWidthPct":0.26},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753450/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_15ba02c3-b268-45f1-a780-322bbaa6fc22_eu9l0l.png"},"ai safety":{"tag":"AI","order":5,"title":"Technical AI Safety","coords":{"leftXPct":0.2,"leftYPct":0.3,"rightXPct":0.554,"rightYPct":0.3,"middleXPct":0.467,"middleYPct":0.3,"leftFlipped":false,"leftWidthPct":0.267,"rightFlipped":true,"middleFlipped":false,"rightWidthPct":0.267,"middleWidthPct":0.267},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,fl_progressive,q_auto/v1708570131/lwbot_topographic_watercolor_artwork_of_a_giant_robot_hand_gent_e4e9f305-9611-4787-8768-d7af3d702ed4_ta2ii9.png"},"practical":{"tag":"Practical","order":3,"title":"Practical","coords":{"leftXPct":0.2,"leftYPct":0.05,"rightXPct":0.634,"rightYPct":0.05,"middleXPct":0.417,"middleYPct":0.05,"leftFlipped":false,"leftWidthPct":0.217,"rightWidthPct":0.217,"middleWidthPct":0.217},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708974564/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_4f6449e2-569b-48a3-b878-a400315b3ef0_hqutxe.png"},"ai strategy":{"tag":"AI","order":4,"title":"AI Strategy","coords":{"leftXPct":0,"leftYPct":0,"rightXPct":0.66,"rightYPct":0,"middleXPct":0.33,"middleYPct":0,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":true,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753570/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_8dda30ee-71d6-4b24-80c7-a8499a5b25c6_uacvgk.png"},"rationality":{"tag":"Rationality","order":0,"title":"Rationality","coords":{"leftXPct":0.12,"leftYPct":0,"rightXPct":0.72,"rightYPct":0,"middleXPct":0.42,"middleYPct":0,"leftFlipped":false,"leftWidthPct":0.3,"rightFlipped":true,"rightWidthPct":0.3,"middleWidthPct":0.3},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753260/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_09275054-eb84-43c4-9cfa-4a05e1818c9e_rmov5i.png"},"optimization":{"tag":"World Optimization","order":1,"title":"Optimization","coords":{"leftXPct":0.1,"leftYPct":0.2,"rightXPct":0.7,"rightYPct":0.2,"middleXPct":0.4,"middleYPct":0.2,"leftWidthPct":0.33,"rightFlipped":true,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1708753382/ohabryka_Aquarelle_sketch_by_Thomas_W._Schaller_inspired_by_top_242eda7f-95a9-4c3b-8090-991a1b11286f_xcjhxq.png"}},"reviewWinnerYearGroupsInfo":{"2018":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.1,"rightXPct":0.72,"rightYPct":0.1,"middleXPct":0.34,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008323/ruby37_green_on_white_aquarelle_sketch_by_thomas_schaller_of_ri_7a3fa89a-ac7a-466f-929f-b396cb4d9bd5_p8rh9t.png"},"2019":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.1,"rightXPct":0.72,"rightYPct":0.1,"middleXPct":0.34,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008331/ruby37_blue_on_white_aquarelle_sketch_by_thomas_schaller_of_gre_f421cc99-2bb5-4357-b164-d05c2f4fe84e_aib1co.png"},"2020":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.01,"rightXPct":0.72,"rightYPct":0.01,"middleXPct":0.34,"middleYPct":0.01,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008346/ruby37_aquarelle_sketch_of_futuristic_landscape_by_thomas_schal_f07d5805-9fb0-4dcc-9295-7f063624e28c_slcokh.png"},"2021":{"tag":null,"coords":{"leftXPct":0.01,"leftYPct":0.1,"rightXPct":0.545,"rightYPct":0.1,"middleXPct":0.278,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.267,"rightFlipped":false,"middleFlipped":false,"rightWidthPct":0.267,"middleWidthPct":0.267},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/a_270/q_auto,f_auto/ohabryka_Topographic_aquarelle_book_cover_by_Thomas_W._Schaller_f9c9dbbe-4880-4f12-8ebb-b8f0b900abc1_m4k6dy_734413"},"2022":{"tag":null,"coords":{"leftXPct":0,"leftYPct":0.1,"rightXPct":0.79,"rightYPct":0.1,"middleXPct":0.43,"middleYPct":0.1,"leftFlipped":false,"leftWidthPct":0.33,"rightFlipped":true,"rightWidthPct":0.33,"middleWidthPct":0.33},"imgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1709008351/ruby37_aquarelle_sketch_of_a_woman_focusing_hard_studying_in_an_2ac568ef-408e-4561-acc8-84c76bb42fba_gwt8uq.png"}},"showReviewOnFrontPageIfActive":true},"googleVertex":{"enabled":false},"intercomAppId":"wtb8z7sj","commentInterval":15,"googleDocImport":{"enabled":true},"moderationEmail":"team@lesswrong.com","timeDecayFactor":1.15,"googleTagManager":{"apiKey":"GTM-TRC765W"},"textReplacements":{"Less Wrong":"Down Bad","Alignment Forum":"Standards Committee","Artificial Intelligence":"Fake News"},"alternateHomePage":false,"gatherTownMessage":"Schelling social hours on Tues 1pm and Thurs 6pm PT","bookDisplaySetting":false,"gardenOpenToPublic":false,"karmaRewarderId100":"iqWr6C3oEB4yWpzn5","legacyRouteAcronym":"lw","maxRenderQueueSize":3,"recommendationsTab":{"manuallyStickiedPostIds":[]},"frontpageScoreBonus":0,"karmaRewarderId1000":"mBBmKWkmw8bgJmGiG","lightconeFundraiser":{"active":false,"postId":"5n2ZQcbc7r4R8mvqc","paymentLinkId":"plink_1QPdGLBlb9vL5IMTvkJ3LZ6v","unsyncedAmount":2082623.2,"thermometerBgUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/q_auto,f_auto,h_400/v1732869999/Group_1_b4ap4h.png","thermometerGoalAmount":1000000,"thermometerGoal2Amount":2000000},"defaultVisibilityTags":[{"tagId":"Ng8Gice9KNkncxqcj","tagName":"Rationality","filterMode":10},{"tagId":"3uE2pXvbcnS9nnZRE","tagName":"World Modeling","filterMode":10}],"enableGoodHeartProject":false,"maxDocumentsPerRequest":5000,"defaultSequenceBannerId":"sequences/vnyzzznenju0hzdv6pqb.jpg","defaultModeratorComments":[{"id":"FfMok764BCY6ScqWm","label":"Option A"},{"id":"yMHoNoYZdk5cKa3wQ","label":"Option B"}],"newUserIconKarmaThreshold":50,"dialogueMatchmakingEnabled":true,"hideUnreviewedAuthorComments":"2023-04-04T18:54:35.895Z","gatherTownUserTrackingIsBroken":true,"postModerationWarningCommentId":"sLay9Tv65zeXaQzR4","commentModerationWarningCommentId":"LbGNE5Ssnvs6MYnLu","performanceMetricLoggingEnax5bled":true,"firstCommentAcknowledgeMessageCommentId":"QgwD7PkQHFp3nfhjj"}</script><script>window.tabId = "pKbeEf6Siq7b7NKEN"</script><script>window.isReturningVisitor = true</script><script async="" src="library_files/bundle.js"></script><title>The Library — LessWrong</title><meta data-react-helmet="true" name="twitter:image:src" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"><meta data-react-helmet="true" property="og:image" content="https://res.cloudinary.com/lesswrong-2-0/image/upload/v1654295382/new_mississippi_river_fjdmww.jpg"><meta data-react-helmet="true" http-equiv="Accept-CH" content="DPR, Viewport-Width, Width"><meta data-react-helmet="true" charset="utf-8"><meta data-react-helmet="true" name="description" content="A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1"><meta data-react-helmet="true" name="twitter:description" content="A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:description" content="A community blog devoted to refining the art of rationality"><meta data-react-helmet="true" http-equiv="delegate-ch" content="sec-ch-dpr https://res.cloudinary.com;"><link data-react-helmet="true" rel="alternate" type="application/rss+xml" href="https://www.lesswrong.com/feed.xml"><meta name="twitter:card" content="summary"><script>window.themeOptions = {"name":"default"}</script><style id="jss-insertion-point"></style><style data-jss="" data-meta="MuiSvgIcon">
.MuiSvgIcon-root {
  fill: currentColor;
  width: 1em;
  height: 1em;
  display: inline-block;
  font-size: 24px;
  transition: fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  user-select: none;
  flex-shrink: 0;
}
.MuiSvgIcon-colorPrimary {
  color: #5f9b65;
}
.MuiSvgIcon-colorSecondary {
  color: #5f9b65;
}
.MuiSvgIcon-colorAction {
  color: rgba(0, 0, 0, 0.54);
}
.MuiSvgIcon-colorError {
  color: #bf360c;
}
.MuiSvgIcon-colorDisabled {
  color: rgba(0, 0, 0, 0.26);
}
.MuiSvgIcon-fontSizeInherit {
  font-size: inherit;
}
.MuiSvgIcon-fontSizeSmall {
  font-size: 20px;
}
.MuiSvgIcon-fontSizeLarge {
  font-size: 36px;
}
</style><style data-jss="" data-meta="MuiSnackbar">
.MuiSnackbar-root {
  left: 0;
  right: 0;
  z-index: 1400;
  display: flex;
  position: fixed;
  align-items: center;
  justify-content: center;
}
.MuiSnackbar-anchorOriginTopCenter {
  top: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginBottomCenter {
  bottom: 0;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomCenter {
    left: 50%;
    right: auto;
    transform: translateX(-50%);
  }
}
.MuiSnackbar-anchorOriginTopRight {
  top: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopRight {
    top: 24px;
    left: auto;
    right: 24px;
  }
}
.MuiSnackbar-anchorOriginBottomRight {
  bottom: 0;
  justify-content: flex-end;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomRight {
    left: auto;
    right: 24px;
    bottom: 24px;
  }
}
.MuiSnackbar-anchorOriginTopLeft {
  top: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginTopLeft {
    top: 24px;
    left: 24px;
    right: auto;
  }
}
.MuiSnackbar-anchorOriginBottomLeft {
  bottom: 0;
  justify-content: flex-start;
}
@media (min-width:960px) {
  .MuiSnackbar-anchorOriginBottomLeft {
    left: 24px;
    right: auto;
    bottom: 24px;
  }
}
</style><style data-jss="" data-meta="MuiTouchRipple">
.MuiTouchRipple-root {
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  display: block;
  z-index: 0;
  position: absolute;
  overflow: hidden;
  border-radius: inherit;
  pointer-events: none;
}
.MuiTouchRipple-ripple {
  top: 0;
  left: 0;
  width: 50px;
  height: 50px;
  opacity: 0;
  position: absolute;
}
.MuiTouchRipple-rippleVisible {
  opacity: 0.3;
  transform: scale(1);
  animation: mui-ripple-enter 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-ripplePulsate {
  animation-duration: 200ms;
}
.MuiTouchRipple-child {
  width: 100%;
  height: 100%;
  opacity: 1;
  display: block;
  border-radius: 50%;
  background-color: currentColor;
}
.MuiTouchRipple-childLeaving {
  opacity: 0;
  animation: mui-ripple-exit 550ms cubic-bezier(0.4, 0, 0.2, 1);
}
.MuiTouchRipple-childPulsate {
  top: 0;
  left: 0;
  position: absolute;
  animation: mui-ripple-pulsate 2500ms cubic-bezier(0.4, 0, 0.2, 1) 200ms infinite;
}
@-moz-keyframes mui-ripple-enter {
  0% {
    opacity: 0.1;
    transform: scale(0);
  }
  100% {
    opacity: 0.3;
    transform: scale(1);
  }
}
@-moz-keyframes mui-ripple-exit {
  0% {
    opacity: 1;
  }
  100% {
    opacity: 0;
  }
}
@-moz-keyframes mui-ripple-pulsate {
  0% {
    transform: scale(1);
  }
  50% {
    transform: scale(0.92);
  }
  100% {
    transform: scale(1);
  }
}
</style><style data-jss="" data-meta="MuiButtonBase">
.MuiButtonBase-root {
  color: inherit;
  border: 0;
  margin: 0;
  cursor: pointer;
  display: inline-flex;
  outline: none;
  padding: 0;
  position: relative;
  align-items: center;
  user-select: none;
  border-radius: 0;
  vertical-align: middle;
  justify-content: center;
  -moz-appearance: none;
  text-decoration: none;
  background-color: transparent;
  -webkit-appearance: none;
  -webkit-tap-highlight-color: transparent;
}
.MuiButtonBase-root::-moz-focus-inner {
  border-style: none;
}
.MuiButtonBase-root.MuiButtonBase-disabled {
  cursor: default;
  pointer-events: none;
}
</style><style data-jss="" data-meta="MuiButton">
.MuiButton-root {
  color: rgba(0,0,0,0.87);
  padding: 8px 16px;
  font-size: 0.875rem;
  min-width: 64px;
  box-sizing: border-box;
  min-height: 36px;
  transition: background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  font-weight: 500;
  font-family: GreekFallback,Calibri,gill-sans-nova,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  text-transform: uppercase;
}
.MuiButton-root:hover {
  text-decoration: none;
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiButton-root.MuiButton-disabled {
  color: rgba(0, 0, 0, 0.26);
}
@media (hover: none) {
  .MuiButton-root:hover {
    background-color: transparent;
  }
}
.MuiButton-root:hover.MuiButton-disabled {
  background-color: transparent;
}
.MuiButton-label {
  width: 100%;
  display: inherit;
  align-items: inherit;
  justify-content: inherit;
}
.MuiButton-textPrimary {
  color: #5f9b65;
}
.MuiButton-textPrimary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiButton-textPrimary:hover {
    background-color: transparent;
  }
}
.MuiButton-textSecondary {
  color: #5f9b65;
}
.MuiButton-textSecondary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiButton-textSecondary:hover {
    background-color: transparent;
  }
}
.MuiButton-outlined {
  border: 1px solid rgba(0, 0, 0, 0.23);
}
.MuiButton-outlinedPrimary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedPrimary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedPrimary.MuiButton-disabled {
  border: 1px solid rgba(0, 0, 0, 0.26);
}
.MuiButton-outlinedSecondary {
  border: 1px solid rgba(95, 155, 101, 0.5);
}
.MuiButton-outlinedSecondary:hover {
  border: 1px solid #5f9b65;
}
.MuiButton-outlinedSecondary.MuiButton-disabled {
  border: 1px solid rgba(0, 0, 0, 0.26);
}
.MuiButton-contained {
  color: rgba(0, 0, 0, 0.87);
  box-shadow: 0px 1px 5px 0px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 3px 1px -2px rgba(0,0,0,0.12);
  background-color: #e0e0e0;
}
.MuiButton-contained.MuiButton-focusVisible {
  box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
}
.MuiButton-contained:active {
  box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);
}
.MuiButton-contained.MuiButton-disabled {
  color: rgba(0, 0, 0, 0.26);
  box-shadow: none;
  background-color: rgba(0, 0, 0, 0.12);
}
.MuiButton-contained:hover {
  background-color: #d5d5d5;
}
@media (hover: none) {
  .MuiButton-contained:hover {
    background-color: #e0e0e0;
  }
}
.MuiButton-contained:hover.MuiButton-disabled {
  background-color: rgba(0, 0, 0, 0.12);
}
.MuiButton-containedPrimary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiButton-containedPrimary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedPrimary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-containedSecondary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiButton-containedSecondary:hover {
  background-color: #426c46;
}
@media (hover: none) {
  .MuiButton-containedSecondary:hover {
    background-color: #5f9b65;
  }
}
.MuiButton-fab {
  width: 56px;
  height: 56px;
  padding: 0;
  min-width: 0;
  box-shadow: 0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);
  border-radius: 50%;
}
.MuiButton-fab:active {
  box-shadow: 0px 7px 8px -4px rgba(0,0,0,0.2),0px 12px 17px 2px rgba(0,0,0,0.14),0px 5px 22px 4px rgba(0,0,0,0.12);
}
.MuiButton-extendedFab {
  width: auto;
  height: 48px;
  padding: 0 16px;
  min-width: 48px;
  border-radius: 24px;
}
.MuiButton-colorInherit {
  color: inherit;
}
.MuiButton-mini {
  width: 40px;
  height: 40px;
}
.MuiButton-sizeSmall {
  padding: 7px 8px;
  min-width: 64px;
  font-size: 0.8125rem;
  min-height: 32px;
}
.MuiButton-sizeLarge {
  padding: 8px 24px;
  min-width: 112px;
  font-size: 0.9375rem;
  min-height: 40px;
}
.MuiButton-fullWidth {
  width: 100%;
}
</style><style data-jss="" data-meta="MuiToolbar">
.MuiToolbar-root {
  display: flex;
  position: relative;
  align-items: center;
}
.MuiToolbar-gutters {
  padding-left: 16px;
  padding-right: 16px;
}
@media (min-width:600px) {
  .MuiToolbar-gutters {
    padding-left: 24px;
    padding-right: 24px;
  }
}
.MuiToolbar-regular {
  min-height: 56px;
}
@media (min-width:0px) and (orientation: landscape) {
  .MuiToolbar-regular {
    min-height: 48px;
  }
}
@media (min-width:600px) {
  .MuiToolbar-regular {
    min-height: 64px;
  }
}
.MuiToolbar-dense {
  min-height: 48px;
}
</style><style data-jss="" data-meta="MuiIconButton">
.MuiIconButton-root {
  flex: 0 0 auto;
  color: rgba(0, 0, 0, 0.54);
  padding: 12px;
  overflow: visible;
  font-size: 1.5rem;
  text-align: center;
  transition: background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
  border-radius: 50%;
}
.MuiIconButton-root:hover {
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiIconButton-root.MuiIconButton-disabled {
  color: rgba(0, 0, 0, 0.26);
}
@media (hover: none) {
  .MuiIconButton-root:hover {
    background-color: transparent;
  }
}
.MuiIconButton-root:hover.MuiIconButton-disabled {
  background-color: transparent;
}
.MuiIconButton-colorInherit {
  color: inherit;
}
.MuiIconButton-colorPrimary {
  color: #5f9b65;
}
.MuiIconButton-colorPrimary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiIconButton-colorPrimary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-colorSecondary {
  color: #5f9b65;
}
.MuiIconButton-colorSecondary:hover {
  background-color: rgba(95, 155, 101, 0.08);
}
@media (hover: none) {
  .MuiIconButton-colorSecondary:hover {
    background-color: transparent;
  }
}
.MuiIconButton-label {
  width: 100%;
  display: flex;
  align-items: inherit;
  justify-content: inherit;
}
</style><style data-jss="" data-meta="MuiModal">
.MuiModal-root {
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  z-index: 1300;
  position: fixed;
}
.MuiModal-hidden {
  visibility: hidden;
}
</style><style data-jss="" data-meta="MuiBadge">
.MuiBadge-root {
  display: inline-flex;
  position: relative;
  vertical-align: middle;
}
.MuiBadge-badge {
  top: -11px;
  right: -11px;
  width: 22px;
  height: 22px;
  display: flex;
  z-index: 1;
  position: absolute;
  flex-wrap: wrap;
  font-size: 0.75rem;
  align-items: center;
  font-family: GreekFallback,Calibri,gill-sans-nova,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  align-content: center;
  border-radius: 50%;
  flex-direction: row;
  justify-content: center;
}
.MuiBadge-colorPrimary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiBadge-colorSecondary {
  color: #fff;
  background-color: #5f9b65;
}
.MuiBadge-colorError {
  color: #fff;
  background-color: #bf360c;
}
</style><style data-jss="" data-meta="MuiDrawer">
.MuiDrawer-docked {
  flex: 0 0 auto;
}
.MuiDrawer-paper {
  top: 0;
  flex: 1 0 auto;
  height: 100%;
  display: flex;
  z-index: 1200;
  outline: none;
  position: fixed;
  overflow-y: auto;
  flex-direction: column;
  -webkit-overflow-scrolling: touch;
}
.MuiDrawer-paperAnchorLeft {
  left: 0;
  right: auto;
}
.MuiDrawer-paperAnchorRight {
  left: auto;
  right: 0;
}
.MuiDrawer-paperAnchorTop {
  top: 0;
  left: 0;
  right: 0;
  bottom: auto;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorBottom {
  top: auto;
  left: 0;
  right: 0;
  bottom: 0;
  height: auto;
  max-height: 100%;
}
.MuiDrawer-paperAnchorDockedLeft {
  border-right: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedTop {
  border-bottom: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedRight {
  border-left: 1px solid rgba(0, 0, 0, 0.12);
}
.MuiDrawer-paperAnchorDockedBottom {
  border-top: 1px solid rgba(0, 0, 0, 0.12);
}
</style><style data-jss="">
.jss143 {
  top: 0;
  left: 0;
  bottom: 0;
  z-index: 1199;
  position: fixed;
}
.jss144 {
  right: auto;
}
.jss145 {
  left: auto;
  right: 0;
}
.jss146 {
  right: 0;
  bottom: auto;
}
.jss147 {
  top: auto;
  right: 0;
  bottom: 0;
}
</style><style data-jss="" data-meta="MuiListItem">
.MuiListItem-root {
  width: 100%;
  display: flex;
  position: relative;
  box-sizing: border-box;
  text-align: left;
  align-items: center;
  padding-top: 8px;
  padding-bottom: 8px;
  justify-content: flex-start;
  text-decoration: none;
}
.MuiListItem-root.MuiListItem-selected, .MuiListItem-root.MuiListItem-selected:hover {
  background-color: rgba(0, 0, 0, 0.14);
}
.MuiListItem-container {
  position: relative;
}
.MuiListItem-focusVisible {
  background-color: rgba(0, 0, 0, 0.08);
}
.MuiListItem-dense {
  padding-top: 8px;
  padding-bottom: 8px;
}
.MuiListItem-disabled {
  opacity: 0.5;
}
.MuiListItem-divider {
  border-bottom: 1px solid rgba(0, 0, 0, 0.12);
  background-clip: padding-box;
}
.MuiListItem-gutters {
  padding-left: 16px;
  padding-right: 16px;
}
@media (min-width:600px) {
  .MuiListItem-gutters {
    padding-left: 24px;
    padding-right: 24px;
  }
}
.MuiListItem-button {
  transition: background-color 150ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;
}
.MuiListItem-button:hover {
  text-decoration: none;
  background-color: rgba(0, 0, 0, 0.08);
}
@media (hover: none) {
  .MuiListItem-button:hover {
    background-color: transparent;
  }
}
.MuiListItem-secondaryAction {
  padding-right: 32px;
}
</style><style data-jss="" data-meta="MuiMenuItem">
.MuiMenuItem-root {
  color: #424242;
  width: auto;
  height: 24px;
  overflow: hidden;
  font-size: 14.3px;
  box-sizing: content-box;
  font-weight: 400;
  font-family: GreekFallback,Calibri,gill-sans-nova,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.1em;
  white-space: nowrap;
  padding-left: 16px;
  text-overflow: ellipsis;
  padding-right: 16px;
}
</style><style data-jss="" data-meta="MuiTooltip">
.MuiTooltip-popper {
  z-index: 1500;
  opacity: 0.9;
}
.MuiTooltip-tooltip {
  color: #fff;
  padding: 9.1px;
  z-index: 10000000;
  font-size: 13px;
  max-width: 300px;
  font-family: GreekFallback,Calibri,gill-sans-nova,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.4em;
  border-radius: 4px;
  background-color: rgba(75,75,75,.94);
}
.MuiTooltip-touch {
  padding: 8px 16px;
  font-size: 0.875rem;
  line-height: 1.14286em;
}
.MuiTooltip-tooltipPlacementLeft {
  margin: 0 24px ;
  transform-origin: right center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementLeft {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementRight {
  margin: 0 24px;
  transform-origin: left center;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementRight {
    margin: 0 14px;
  }
}
.MuiTooltip-tooltipPlacementTop {
  margin: 24px 0;
  transform-origin: center bottom;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementTop {
    margin: 14px 0;
  }
}
.MuiTooltip-tooltipPlacementBottom {
  margin: 24px 0;
  transform-origin: center top;
}
@media (min-width:600px) {
  .MuiTooltip-tooltipPlacementBottom {
    margin: 14px 0;
  }
}
</style><style id="jss-insertion-start"></style><style data-name="ForumIcon" data-priority="-2">.ForumIcon-root {
  width: var(--icon-size, 1em);
  height: var(--icon-size, 1em);
  display: inline-block;
  font-size: var(--icon-size, 24px);
  user-select: none;
  flex-shrink: 0;
}
.ForumIcon-linkRotation {
  transform: rotate(-45deg);
}
.ForumIcon-linkRotation.MuiListItemIcon-root {
  margin-right: 12px;
}</style><style data-name="LWBackgroundImage" data-priority="0">.LWBackgroundImage-root {
  right: 0;
  position: absolute;
}
.LWBackgroundImage-backgroundImage {
  top: -70px;
  width: 57vw;
  right: -334px;
  position: absolute;
  max-width: 1000px;
  -webkit-mask-image: radial-gradient(ellipse at center top, #000 55%, transparent 70%);
}
@media (min-width:2000px) {
  .LWBackgroundImage-backgroundImage {
    right: 0px;
  }
}
.LWBackgroundImage-reviewResultsImage {
  top: -70px;
  width: 57vw;
  right: -334px;
  position: absolute;
  max-width: 1000px;
  -webkit-mask-image: radial-gradient(ellipse at center top, #000 55%, transparent 70%);
}
.LWBackgroundImage-imageColumn {
  top: 0;
  right: 0;
  width: 57vw;
  height: 100vh;
  position: absolute;
}
@media(max-width: 1000px) {
  .LWBackgroundImage-imageColumn {
    display: none;
  }
}
.LWBackgroundImage-reviewVotingCanvas {
  top: -57px;
  width: 57vw;
  right: -334px;
  height: 100vh;
  position: absolute;
  max-width: 1000px;
  -webkit-mask-image: radial-gradient(ellipse at center top, #000 55%, transparent 70%);
}
.LWBackgroundImage-reviewVotingCanvas img {
  width: 100%;
  right: -40px;
  height: 100vh;
  position: relative;
  object-fit: cover;
}
@media (min-width:2000px) {
  .LWBackgroundImage-reviewVotingCanvas {
    right: 0px;
  }
}
.LWBackgroundImage-votingResultsLink {
  top: 715px;
  right: 250px;
  width: 200px;
  z-index: 1;
  opacity: 0.6;
  display: block;
  position: relative;
  text-align: center;
}
.LWBackgroundImage-votingResultsLink:hover {
  opacity: 0.4;
}
@media (max-width:1599.95px) {
  .LWBackgroundImage-votingResultsLink {
    top: 690px;
    right: 100px;
  }
}
@media (max-width:1399.95px) {
  .LWBackgroundImage-votingResultsLink {
    top: 650px;
    right: 35px;
  }
}
.LWBackgroundImage-votingResultsLink h1 {
  font-size: 2.8rem;
  margin-top: 20px;
  font-family: ETBookRoman,warnock-pro,Palatino,"Palatino Linotype","Palatino LT STD","Book Antiqua",Georgia,serif;
  line-height: 2.6rem;
  font-weight: 600;
  margin-bottom: 0;
}
.LWBackgroundImage-votingResultsLink h3 {
  opacity: 0.5;
  font-size: 1.4rem;
  margin-top: 16px;
  font-style: italic;
  font-family: GreekFallback,Calibri,gill-sans-nova,"Gill Sans","Gill Sans MT",Myriad Pro,Myriad,"Liberation Sans","Nimbus Sans L",Tahoma,Geneva,"Helvetica Neue",Helvetica,Arial,sans-serif;
  line-height: 1.2;
  margin-bottom: 6px;
}
.LWBackgroundImage-votingResultsLink h3 b, .LWBackgroundImage-votingResultsLink h3 strong {
  font-weight: 600;
}</style><style id="jss-insertion-end"></style><link id="main-styles" rel="stylesheet" type="text/css" onerror="window.missingMainStylesheet=true" href="library_files/allStyles.css"><script async="" src="library_files/wtb8z7sj"></script><script src="library_files/places.js"></script><script src="library_files/main.js"></script><script type="text/javascript" charset="UTF-8" src="library_files/common.js"></script><script type="text/javascript" charset="UTF-8" src="library_files/util.js"></script><script type="text/javascript" charset="UTF-8" src="library_files/controls.js"></script><script type="text/javascript" charset="UTF-8" src="library_files/places_impl.js"></script><script type="text/javascript" charset="UTF-8" src="library_files/geocoder.js"></script><link rel="canonical" href="https://www.lesswrong.com/library" data-react-helmet="true"><meta property="og:url" content="https://www.lesswrong.com/library" data-react-helmet="true"></head>
<body class="welcomeBoxABTest_welcomeBox twoLineEventsSidebar_control">
<script>0</script><div id="react-app"><div class="wrapper Layout-wrapper" id="wrapper"><div></div><span></span><div id="intercom-outer-frame"></div><noscript class="noscript-warning"> This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. </noscript><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-TRC765W" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript><div class="Header-root"><div style="height:64px" class="Header-headroom headroom-wrapper"><div class="headroom headroom--unfixed headroom-disable-animation"><header class="Header-appBar"><div class="MuiToolbar-root MuiToolbar-regular MuiToolbar-gutters"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton Header-hideLgUp" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root Header-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root MuiIconButton-colorInherit Header-menuButton Header-hideMdDown" type="button" aria-label="Menu"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root Header-icon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><h2 class="Typography-root Typography-title Header-title"><div class="Header-hideSmDown"><div class="Header-titleSubtitleContainer"><div class="Header-titleFundraiserContainer"><a class="Header-titleLink" href="https://www.lesswrong.com/">LESSWRONG</a></div></div></div><div class="Header-hideMdUp Header-titleFundraiserContainer"><a class="Header-titleLink" href="https://www.lesswrong.com/">LW</a></div></h2><div class="ActiveDialogues-root"></div><div class="Header-rightHeaderItems"><div class="SearchBar-root"><div class="SearchBar-rootChild"><div class="SearchBar-searchInputArea"><div><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root SearchBar-searchIconButton" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root SearchBar-searchIcon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button></div><div></div></div></div></div><div><div class="UsersMenu-root"><a href="https://www.lesswrong.com/users/ms-haze"><button tabindex="0" class="MuiButtonBase-root MuiButton-root UsersMenu-userButtonRoot MuiButton-text MuiButton-flat" type="button" data-testid="users-menu"><span class="MuiButton-label"><span class="UsersMenu-userButtonContents">Ms. Haze</span></span><span class="MuiTouchRipple-root"></span></button></a></div></div><div class="KarmaChangeNotifier-root"><div><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root KarmaChangeNotifier-karmaNotifierButton" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root KarmaChangeNotifier-starIcon ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path d="M22 9.24l-7.19-.62L12 2 9.19 8.63 2 9.24l5.46 4.73L5.82 21 12 17.27 18.18 21l-1.63-7.03L22 9.24zM12 15.4l-3.76 2.27 1-4.28-3.32-2.88 4.38-.38L12 6.1l1.71 4.04 4.38.38-3.32 2.88 1 4.28L12 15.4z"></path><path fill="none" d="M0 0h24v24H0z"></path></svg></span><span class="MuiTouchRipple-root"></span></button></div></div><span class="MuiBadge-root NotificationsMenuButton-badgeContainer"><button tabindex="0" class="MuiButtonBase-root MuiIconButton-root NotificationsMenuButton-buttonClosed" type="button"><span class="MuiIconButton-label"><svg class="MuiSvgIcon-root ForumIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M12 22c1.1 0 2-.9 2-2h-4c0 1.1.9 2 2 2zm6-6v-5c0-3.07-1.63-5.64-4.5-6.32V4c0-.83-.67-1.5-1.5-1.5s-1.5.67-1.5 1.5v.68C7.64 5.36 6 7.92 6 11v5l-2 2v1h16v-1l-2-2zm-2 1H8v-6c0-2.48 1.51-4.5 4-4.5s4 2.02 4 4.5v6z"></path></svg></span><span class="MuiTouchRipple-root"></span></button><span class="MuiBadge-badge NotificationsMenuButton-badge NotificationsMenuButton-badgeBackground"></span></span></div></div></header><div class="jss143 jss144" style="width: 20px;"></div></div></div><div class="NotificationsMenu-root"></div></div><div class="Layout-spacedGridActivated"><div class=""><div class="NavigationStandalone-sidebar" direction="right"><div class="TabNavigationMenu-root"><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-navButton" role="menuitem" href="https://www.lesswrong.com/"><span class="TabNavigationItem-icon TabNavigationItem-homeIcon"><svg version="1.1" x="0px" y="0px" viewBox="0 0 100 100"><g fill="currentColor"><path d="M29.1,29.2l6.4,11.6l4.3-0.8l0.8-4.3L29.1,29.2z M40.7,64.5l-0.8-4.3l-4.3-0.8L29.2,71L40.7,64.5z M70.9,70.9l-6.4-11.6l-4.3,0.8l-0.8,4.3L70.9,70.9z M64.4,40.8l6.4-11.6l-11.6,6.4l0.8,4.3L64.4,40.8z M67.4,58.8l10.8,19.4L58.8,67.4L50,98.8l-8.8-31.4L21.9,78.2l10.8-19.4L1.2,50.1l31.4-8.8L21.9,21.9l19.4,10.8L50,1.3l8.8,31.4l19.4-10.8L67.4,41.3L98.8,50L67.4,58.8zM57.7,57.8L83.5,50L50,50.1l7.7-7.7L50,16.6v33.5l-7.7-7.7l-25.8,7.7H50l-7.7,7.7L50,83.5V50.1L57.7,57.8z"></path></g></svg></span><span class="TabNavigationItem-navText">Home</span><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-navButton" role="menuitem" href="https://www.lesswrong.com/allPosts"><span class="TabNavigationItem-icon"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" x="0px" y="0px" viewBox="0 0 100 125"><g transform="translate(0,-952.36218)"><path style="text-indent:0;text-transform:none;direction:ltr;baseline-shift:baseline" d="m 12.80945,964.36661 a 1.9989971,2.0020794 0 1 0 0.187201,3.99977 l 17.97124,0 a 1.9970041,2.0000833 0 1 0 0,-3.99977 l -17.97124,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z m 25.958459,0 a 1.9989971,2.0020794 0 1 0 0.1872,3.99977 l 47.923308,0 a 1.9970041,2.0000833 0 1 0 0,-3.99977 l -47.923308,0 a 1.9970041,2.0000833 0 0 0 -0.1872,0 z m -25.958459,23.9986 a 1.9989971,2.0020794 0 1 0 0.187201,3.99977 l 57.90733,0 a 1.9970041,2.0000833 0 1 0 0,-3.99977 l -57.90733,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z m 0,23.99859 a 1.9989971,2.0020794 0 1 0 0.187201,3.9998 l 57.90733,0 a 1.9970041,2.0000833 0 1 0 0,-3.9998 l -57.90733,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z m 0,23.9986 a 1.9989971,2.0020794 0 1 0 0.187201,3.9998 l 57.90733,0 a 1.9970041,2.0000833 0 1 0 0,-3.9998 l -57.90733,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z" stroke="none" visibility="visible" display="inline" overflow="visible"></path></g></svg></span><span class="TabNavigationItem-navText">All Posts</span><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-navButton" role="menuitem" href="https://www.lesswrong.com/wikitags/all"><span class="TabNavigationItem-icon"><svg version="1.1" x="0px" y="0px" viewBox="10 10 90 90"><path d="M62.648,14.41c-7.396-0.624-14.651,2.03-19.906,7.285c-3.807,3.807-6.268,8.75-7.065,14.054  c-5.283,0.785-10.225,3.236-14.085,7.096c-4.19,4.19-6.772,9.755-7.272,15.667c-0.563,6.697,1.512,13.212,5.848,18.346  c4.335,5.136,10.413,8.273,17.106,8.834c7.398,0.628,14.655-2.029,19.91-7.284c3.805-3.805,6.265-8.749,7.062-14.053  c5.283-0.784,10.227-3.235,14.086-7.096c4.189-4.189,6.773-9.754,7.273-15.67C86.77,27.771,76.473,15.578,62.648,14.41z   M55.561,76.783c-4.597,4.596-11.109,7.21-18.092,6.621c-12.582-1.06-21.918-12.119-20.858-24.7  c0.472-5.594,2.923-10.55,6.606-14.232c3.273-3.273,7.529-5.515,12.226-6.336c-0.347,6.424,1.708,12.638,5.875,17.574  c4.333,5.134,10.411,8.272,17.105,8.835c1.166,0.1,2.327,0.106,3.479,0.045C61.069,69.348,58.789,73.555,55.561,76.783z   M62.178,62.248c-1.171,0.082-2.354,0.109-3.561,0.008c-12.482-1.05-21.755-11.945-20.866-24.399  c1.169-0.083,2.353-0.11,3.558-0.008C53.788,38.903,63.064,49.792,62.178,62.248z M83.315,41.397  c-0.474,5.596-2.924,10.554-6.606,14.237c-3.275,3.274-7.531,5.515-12.228,6.336C65.207,48.47,55.063,36.705,41.5,35.559  c-1.164-0.098-2.324-0.106-3.475-0.044c0.832-4.758,3.113-8.964,6.341-12.193c4.596-4.596,11.109-7.211,18.09-6.621  C75.037,17.763,84.375,28.817,83.315,41.397z"></path><path d="M49.384,64.735c-1.677-0.739-3.272-1.645-4.772-2.708c-0.927-0.657-1.816-1.369-2.662-2.144  c-0.861-0.788-1.692-1.613-2.459-2.517c-0.019-0.022-0.042-0.043-0.061-0.065c-0.542-0.641-1.042-1.309-1.52-1.99  c-0.549-0.786-1.045-1.602-1.508-2.434c-0.364-0.656-0.699-1.324-1.008-2.006c-0.354-0.782-0.671-1.578-0.95-2.39  c-0.223-0.648-0.419-1.305-0.593-1.97c-0.203-0.773-0.373-1.557-0.507-2.349c-0.11-0.644-0.194-1.292-0.257-1.946  c-0.031-0.311-0.077-0.62-0.097-0.933c-0.668,0.218-1.318,0.476-1.956,0.76c-1.053,0.47-2.062,1.028-3.019,1.67  c-0.922,0.618-1.794,1.315-2.611,2.085c-0.147,0.139-0.298,0.273-0.442,0.417c-0.436,0.436-0.848,0.891-1.24,1.361  c-0.574,0.689-1.085,1.423-1.562,2.178c-0.375,0.594-0.72,1.204-1.031,1.833c-0.359,0.724-0.683,1.464-0.953,2.227  c-0.218,0.613-0.401,1.24-0.56,1.875c-0.186,0.746-0.323,1.506-0.424,2.273c-0.041,0.315-0.095,0.628-0.122,0.946  c-0.027,0.321-0.022,0.641-0.034,0.961c-0.03,0.775-0.018,1.545,0.04,2.31c0.049,0.652,0.128,1.3,0.239,1.94  c0.138,0.796,0.312,1.581,0.544,2.353c0.201,0.672,0.433,1.335,0.704,1.984c0.345,0.827,0.752,1.628,1.207,2.408  c0.411,0.703,0.871,1.38,1.37,2.038c0.222,0.293,0.431,0.594,0.67,0.877c0.483,0.572,0.999,1.106,1.534,1.617  c0.834,0.796,1.73,1.51,2.675,2.146c1.964,1.324,4.146,2.282,6.464,2.853c1.042,0.257,2.103,0.454,3.192,0.546  c1.064,0.09,2.12,0.066,3.168-0.009c2.392-0.17,4.716-0.755,6.883-1.729c1.045-0.47,2.054-1.022,3.013-1.668  c0.919-0.616,1.788-1.315,2.608-2.085c0.151-0.144,0.316-0.269,0.464-0.416c0.435-0.435,0.826-0.899,1.216-1.366  c0.581-0.692,1.121-1.412,1.6-2.169c0.375-0.593,0.709-1.209,1.02-1.836c0.358-0.723,0.675-1.464,0.943-2.228  c0.048-0.134,0.111-0.26,0.156-0.394c-0.179-0.012-0.356-0.025-0.532-0.04C55.116,66.745,52.149,65.953,49.384,64.735z"></path><path d="M43.815,51.052L54.95,47.54c-0.607-0.724-1.259-1.412-1.96-2.046l-10.529,3.321C42.864,49.591,43.318,50.336,43.815,51.052z  "></path><path d="M47.913,42.134l-7.13,2.249c0.208,0.827,0.469,1.632,0.776,2.419l9.346-2.948C49.961,43.201,48.963,42.623,47.913,42.134z"></path><path d="M41.102,40.309c-0.299-0.025-0.605-0.042-0.924-0.05c0.025,0.662,0.086,1.316,0.175,1.963l4.163-1.313  C43.412,40.619,42.274,40.409,41.102,40.309z"></path><path d="M58.824,59.795c0.301,0.025,0.607,0.042,0.926,0.051c-0.02-0.525-0.073-1.043-0.133-1.559l-3.442,1.086  C57.04,59.562,57.919,59.719,58.824,59.795z"></path><path d="M52.566,58.213l6.684-2.108c-0.191-0.831-0.45-1.637-0.742-2.43l-9.067,2.86C50.426,57.181,51.47,57.742,52.566,58.213z"></path><path d="M47.283,54.918l10.356-3.267c-0.386-0.777-0.806-1.537-1.283-2.259l-11.098,3.501C45.888,53.612,46.56,54.294,47.283,54.918  z"></path></svg></span><span class="TabNavigationItem-navText">Concepts</span><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-navButton TabNavigationItem-selected" role="menuitem" href="https://www.lesswrong.com/library"><span class="TabNavigationItem-icon TabNavigationItem-selectedIcon"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 84.000000 84.000000" preserveAspectRatio="xMidYMid meet"><g transform="translate(0.0,84.0) scale(0.1,-0.1)" stroke="none"><path d="M170 695 c0 -3 -14 -67 -31 -143 -17 -75 -35 -159 -40 -187 -5 -27 -12 -58 -15 -67 -5 -16 2 -18 53 -18 71 0 156 -21 223 -55 l50 -25 0 225 0 224 -27 15 c-51 25 -213 49 -213 31z"></path><path d="M536 689 c-21 -5 -54 -16 -72 -25 l-34 -16 0 -224 c0 -123 2 -224 4 -224 2 0 23 11 47 24 59 33 150 56 222 56 56 0 59 1 54 23 -3 12 -22 99 -42 192 -20 94 -38 178 -41 188 -4 13 -16 17 -52 16 -26 -1 -64 -5 -86 -10z"></path><path d="M70 576 c-42 -207 -67 -352 -62 -357 4 -4 99 -23 210 -43 l204 -37 201 37 c111 20 205 39 209 43 4 4 -8 87 -27 186 -47 244 -40 223 -70 217 -18 -3 -24 -9 -21 -21 8 -26 76 -330 76 -339 0 -4 -30 -8 -67 -9 -73 0 -200 -31 -263 -63 l-38 -20 -54 25 c-77 35 -173 58 -250 58 -38 1 -68 4 -68 7 0 5 39 181 75 336 4 17 -1 23 -20 26 -22 5 -25 1 -35 -46z"></path></g></svg></span><span class="TabNavigationItem-navText">Library</span><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/bestoflesswrong"><div class="TabNavigationSubItem-root">Best of LessWrong</div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/highlights"><div class="TabNavigationSubItem-root">Sequence Highlights</div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/rationality"><div class="TabNavigationSubItem-root">Rationality: A-Z</div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/codex"><div class="TabNavigationSubItem-root">The Codex</div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/hpmor"><div class="TabNavigationSubItem-root">HPMOR</div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root TabNavigationItem-tooltip"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-navButton" role="menuitem" href="https://www.lesswrong.com/community"><span class="TabNavigationItem-icon"><svg width="28" height="34" viewBox="0 0 28 34" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.042 0.653442C21.3573 0.653442 27.3273 6.45284 27.3273 13.6408C27.3273 20.7471 21.3573 26.5465 14.042 26.5465C6.64258 26.5465 0.672607 20.7471 0.672607 13.6408C0.672607 6.45284 6.64258 0.653442 14.042 0.653442ZM11.4354 13.2324C11.4354 13.2324 10.9309 13.1507 10.8468 13.1507C10.0059 13.3141 10.0059 13.1507 10.7627 13.6408C11.2672 13.9676 11.099 13.9676 11.6035 13.8859C11.7717 13.9676 11.9399 13.9676 12.108 14.0492C12.108 14.1309 12.108 14.2943 12.108 14.376C12.108 14.376 12.5285 14.621 12.6125 14.621H13.2852C13.3693 14.621 14.042 14.2126 12.8648 13.8859C12.6966 13.8859 12.4444 13.8859 12.1921 13.9676C12.108 13.8042 12.024 13.6408 11.8558 13.4775C11.8558 13.4775 11.3513 13.2324 11.4354 13.2324ZM24.3843 7.18798C25.5615 9.06666 26.3183 11.2721 26.3183 13.6408C26.3183 14.2943 26.2342 15.0294 26.1501 15.6829C25.5615 15.1928 24.9729 14.7844 24.8888 14.7027C24.6366 14.4576 24.6366 14.8661 24.4684 15.3561C24.3843 15.9279 23.7116 15.4378 23.2071 15.4378C22.7026 15.4378 22.3663 14.7844 22.3663 14.7844C21.4414 14.0492 21.6936 13.6408 21.6936 12.4973C21.6936 12.1706 21.8618 11.7622 22.114 11.5988C22.7026 10.9453 22.1981 10.5369 22.8708 10.0468C23.1231 9.88347 24.048 9.63843 24.048 9.63843C24.5525 8.98497 24.8047 8.82161 23.5435 9.06666C23.4594 9.14834 22.9549 9.06666 22.7026 8.90329C22.3663 8.65825 23.1231 8.49488 23.1231 8.49488C23.4594 8.08648 23.7957 7.75975 24.1321 7.43302C24.2162 7.35134 24.3002 7.26966 24.3843 7.18798ZM14.042 1.7153C15.2192 1.7153 16.3122 1.87867 17.4053 2.12371C17.6576 2.45044 18.078 2.94053 18.1621 3.02221C18.2462 3.10389 18.4984 3.75735 18.4984 3.75735C17.9939 4.49248 17.8258 4.90089 16.7327 4.4108C16.3963 4.32912 15.8077 3.5123 15.8077 3.5123C15.8077 3.5123 14.8828 2.53212 14.7146 2.45044C14.3783 2.20539 13.8738 2.36876 13.6216 2.77717C13.7897 3.02221 13.8738 3.18557 14.042 3.43062C14.7987 3.43062 14.7987 3.26726 15.051 3.83903C14.7987 3.92071 14.5465 4.00239 14.2101 4.08407C13.4534 4.57416 13.7056 4.57416 12.8648 4.00239C12.8648 4.00239 12.3603 3.43062 12.2762 3.34894C12.1921 3.26726 11.8558 2.53212 11.6876 2.28708C11.6876 2.20539 11.6035 2.04203 11.6876 1.87867C12.4444 1.79699 13.2011 1.7153 14.042 1.7153ZM14.4624 25.5664C13.6216 25.5664 12.7807 25.4847 11.8558 25.403C11.9399 24.8312 11.9399 24.3411 11.9399 23.851C11.9399 22.5441 12.2762 22.2991 11.1831 21.564C10.9309 21.1555 10.5945 20.6655 10.2582 20.257C9.50144 19.5219 9.75369 19.6036 10.1741 18.6234C10.2582 18.215 10.3423 17.8066 10.4264 17.4799C10.2582 17.0715 10.1741 16.6631 10.0059 16.2546C9.6696 15.9279 9.33327 15.5195 8.99693 15.1928C7.14708 14.7027 7.23117 14.9477 5.88582 13.5591C5.63357 13.1507 5.38132 12.7423 5.04498 12.3339C4.12005 11.2721 4.28822 11.4354 3.95189 10.1285C3.8678 9.55675 3.78372 8.98497 3.69963 8.49488C3.61555 8.16816 3.53147 7.84143 3.44738 7.59639C5.12906 4.81921 7.90384 2.77717 11.1831 2.04203C11.099 2.45044 11.015 2.85885 10.9309 3.34894C8.91285 4.90089 7.90384 5.3093 10.3423 6.2078C10.9309 7.1063 10.7627 7.5147 11.5195 6.94293C11.5195 6.45284 11.4354 5.96275 11.3513 5.47266C12.8648 4.49248 12.4444 4.65584 14.2101 5.55434C14.4624 5.79939 14.7146 6.04443 14.9669 6.28948C16.1441 6.77957 16.2282 6.53452 15.2192 7.26966C14.9669 7.18798 14.6306 7.18798 14.2942 7.1063C14.1261 7.26966 13.9579 7.43302 13.7897 7.59639C14.2101 8.49488 14.4624 8.4132 13.5375 8.65825C13.2852 8.82161 13.033 9.06666 12.7807 9.23002C12.4444 9.23002 12.108 9.23002 11.7717 9.23002C11.7717 9.23002 11.7717 10.2102 11.7717 10.3736C11.7717 10.6186 11.015 11.2721 11.015 11.2721C11.015 11.2721 10.5945 11.8438 10.8468 12.1706C11.015 12.4973 10.5945 12.7423 10.5945 12.7423C10.3423 12.4973 10.1741 12.1706 9.92186 11.9255C9.58552 11.9255 9.1651 12.0072 8.74468 12.0889C8.49243 12.2522 8.24018 12.3339 7.98792 12.4973C7.90384 12.7423 7.81975 13.0691 7.81975 13.3141C7.90384 13.7225 7.98792 14.1309 8.15609 14.5393C8.32426 14.5393 8.57651 14.5393 8.82876 14.4576C8.82876 13.2324 8.57651 13.2324 9.58552 14.0492C9.6696 14.2943 9.6696 14.5393 9.6696 14.8661C10.0059 14.9477 10.2582 15.0294 10.5104 15.1111C10.5945 15.3561 10.5945 15.6829 10.5945 15.9279C11.015 16.2546 11.4354 16.4997 11.8558 16.7447C12.7807 16.9081 13.7897 16.9081 14.5465 17.4799C15.1351 17.8066 15.7237 18.0516 16.3963 18.2967C16.3963 18.5417 16.4804 18.7868 16.5645 19.0318C16.9008 19.1135 17.3213 19.1135 17.6576 19.1135C18.078 19.1952 18.4984 19.1952 18.9189 19.1952C18.9189 19.4402 18.9189 19.6853 18.9189 19.9303C18.7507 21.3189 18.6666 21.0739 17.6576 21.9724C17.3213 22.2174 16.9849 22.4625 16.6486 22.7892C15.8077 23.9327 16.06 23.7694 14.7146 24.2594C14.6306 24.6679 14.5465 25.1579 14.4624 25.5664Z" fill="currentColor" fill-opacity="0.7"></path></svg></span><span class="TabNavigationItem-navText">Community Events</span><span class="MuiTouchRipple-root"></span></a></span><span><div><span class="LWTooltip-root"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-gutters MuiListItem-button MuiMenuItem-root TabNavigationEventsList-eventWrapper" role="menuitem" href="https://www.lesswrong.com/events/JNL2bmDXmaG7YnRbF/maisu-minimal-ai-safety-unconference"><div class="TabNavigationSubItem-root TabNavigationEventsList-event"><span>MAISU - Minimal AI Safety Unconference </span></div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-gutters MuiListItem-button MuiMenuItem-root TabNavigationEventsList-eventWrapper" role="menuitem" href="https://www.lesswrong.com/events/JxsdDs8ZfbF4dBkGe/lesswrong-community-weekend-2025"><div class="TabNavigationSubItem-root TabNavigationEventsList-event"><span>LessWrong Community Weekend 2025</span></div><span class="MuiTouchRipple-root"></span></a></span></div><div><span class="LWTooltip-root"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-gutters MuiListItem-button MuiMenuItem-root TabNavigationEventsList-eventWrapper" role="menuitem" href="https://www.lesswrong.com/events/87kEhKMAwp5M3dFGf/eugene-acx-meetups-everywhere-spring-2025"><div class="TabNavigationSubItem-root TabNavigationEventsList-event"><time class="TabNavigationEventsList-displayTime" datetime="2025-04-10T01:00:00.000Z">[Today]</time><span>Eugene – ACX Meetups Everywhere Spring 2025</span></div><span class="MuiTouchRipple-root"></span></a></span><span class="LWTooltip-root"><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-gutters MuiListItem-button MuiMenuItem-root TabNavigationEventsList-eventWrapper" role="menuitem" href="https://www.lesswrong.com/events/QXcQsQyf25836Mg2w/munich-acx-meetups-everywhere-spring-2025"><div class="TabNavigationSubItem-root TabNavigationEventsList-event"><time class="TabNavigationEventsList-displayTime" datetime="2025-04-10T14:00:00.000Z">[Tomorrow]</time><span>Munich – ACX Meetups Everywhere Spring 2025</span></div><span class="MuiTouchRipple-root"></span></a></span></div></span><div class="TabNavigationMenu-divider"></div><div><a class="SubscribeWidget-root"><div class="TabNavigationSubItem-root">Subscribe (RSS/Email)</div></a></div><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/posts/YMo5PuXnZDwRjhHhE/the-story-of-i-have-been-a-good-bing"><div class="TabNavigationSubItem-root">LW the Album</div><span class="MuiTouchRipple-root"></span></a><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/about"><div class="TabNavigationSubItem-root">About</div><span class="MuiTouchRipple-root"></span></a><a tabindex="-1" class="MuiButtonBase-root MuiListItem-root MuiListItem-default MuiListItem-button MuiMenuItem-root TabNavigationItem-menuItem TabNavigationItem-subItemOverride" role="menuitem" href="https://www.lesswrong.com/faq"><div class="TabNavigationSubItem-root">FAQ</div><span class="MuiTouchRipple-root"></span></a></div></div></div><div class="TabNavigationMenuFooter-wrapper"><div class="TabNavigationMenuFooter-root"><a class="TabNavigationFooterItem-navButton" title="Latest posts, comments and curated content." href="https://www.lesswrong.com/"><span class="TabNavigationFooterItem-icon TabNavigationFooterItem-homeIcon"><svg version="1.1" x="0px" y="0px" viewBox="0 0 100 100"><g fill="currentColor"><path d="M29.1,29.2l6.4,11.6l4.3-0.8l0.8-4.3L29.1,29.2z M40.7,64.5l-0.8-4.3l-4.3-0.8L29.2,71L40.7,64.5z M70.9,70.9l-6.4-11.6l-4.3,0.8l-0.8,4.3L70.9,70.9z M64.4,40.8l6.4-11.6l-11.6,6.4l0.8,4.3L64.4,40.8z M67.4,58.8l10.8,19.4L58.8,67.4L50,98.8l-8.8-31.4L21.9,78.2l10.8-19.4L1.2,50.1l31.4-8.8L21.9,21.9l19.4,10.8L50,1.3l8.8,31.4l19.4-10.8L67.4,41.3L98.8,50L67.4,58.8zM57.7,57.8L83.5,50L50,50.1l7.7-7.7L50,16.6v33.5l-7.7-7.7l-25.8,7.7H50l-7.7,7.7L50,83.5V50.1L57.7,57.8z"></path></g></svg></span><span class="TabNavigationFooterItem-navText">Home</span></a><a class="TabNavigationFooterItem-navButton" title="See all posts, filtered and sorted however you like." href="https://www.lesswrong.com/allPosts"><span class="TabNavigationFooterItem-icon"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" x="0px" y="0px" viewBox="0 0 100 125"><g transform="translate(0,-952.36218)"><path style="text-indent:0;text-transform:none;direction:ltr;baseline-shift:baseline" d="m 12.80945,964.36661 a 1.9989971,2.0020794 0 1 0 0.187201,3.99977 l 17.97124,0 a 1.9970041,2.0000833 0 1 0 0,-3.99977 l -17.97124,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z m 25.958459,0 a 1.9989971,2.0020794 0 1 0 0.1872,3.99977 l 47.923308,0 a 1.9970041,2.0000833 0 1 0 0,-3.99977 l -47.923308,0 a 1.9970041,2.0000833 0 0 0 -0.1872,0 z m -25.958459,23.9986 a 1.9989971,2.0020794 0 1 0 0.187201,3.99977 l 57.90733,0 a 1.9970041,2.0000833 0 1 0 0,-3.99977 l -57.90733,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z m 0,23.99859 a 1.9989971,2.0020794 0 1 0 0.187201,3.9998 l 57.90733,0 a 1.9970041,2.0000833 0 1 0 0,-3.9998 l -57.90733,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z m 0,23.9986 a 1.9989971,2.0020794 0 1 0 0.187201,3.9998 l 57.90733,0 a 1.9970041,2.0000833 0 1 0 0,-3.9998 l -57.90733,0 a 1.9970041,2.0000833 0 0 0 -0.187201,0 z" stroke="none" visibility="visible" display="inline" overflow="visible"></path></g></svg></span><span class="TabNavigationFooterItem-navText">All Posts</span></a><a class="TabNavigationFooterItem-navButton" href="https://www.lesswrong.com/wikitags/all"><span class="TabNavigationFooterItem-icon"><svg version="1.1" x="0px" y="0px" viewBox="10 10 90 90"><path d="M62.648,14.41c-7.396-0.624-14.651,2.03-19.906,7.285c-3.807,3.807-6.268,8.75-7.065,14.054  c-5.283,0.785-10.225,3.236-14.085,7.096c-4.19,4.19-6.772,9.755-7.272,15.667c-0.563,6.697,1.512,13.212,5.848,18.346  c4.335,5.136,10.413,8.273,17.106,8.834c7.398,0.628,14.655-2.029,19.91-7.284c3.805-3.805,6.265-8.749,7.062-14.053  c5.283-0.784,10.227-3.235,14.086-7.096c4.189-4.189,6.773-9.754,7.273-15.67C86.77,27.771,76.473,15.578,62.648,14.41z   M55.561,76.783c-4.597,4.596-11.109,7.21-18.092,6.621c-12.582-1.06-21.918-12.119-20.858-24.7  c0.472-5.594,2.923-10.55,6.606-14.232c3.273-3.273,7.529-5.515,12.226-6.336c-0.347,6.424,1.708,12.638,5.875,17.574  c4.333,5.134,10.411,8.272,17.105,8.835c1.166,0.1,2.327,0.106,3.479,0.045C61.069,69.348,58.789,73.555,55.561,76.783z   M62.178,62.248c-1.171,0.082-2.354,0.109-3.561,0.008c-12.482-1.05-21.755-11.945-20.866-24.399  c1.169-0.083,2.353-0.11,3.558-0.008C53.788,38.903,63.064,49.792,62.178,62.248z M83.315,41.397  c-0.474,5.596-2.924,10.554-6.606,14.237c-3.275,3.274-7.531,5.515-12.228,6.336C65.207,48.47,55.063,36.705,41.5,35.559  c-1.164-0.098-2.324-0.106-3.475-0.044c0.832-4.758,3.113-8.964,6.341-12.193c4.596-4.596,11.109-7.211,18.09-6.621  C75.037,17.763,84.375,28.817,83.315,41.397z"></path><path d="M49.384,64.735c-1.677-0.739-3.272-1.645-4.772-2.708c-0.927-0.657-1.816-1.369-2.662-2.144  c-0.861-0.788-1.692-1.613-2.459-2.517c-0.019-0.022-0.042-0.043-0.061-0.065c-0.542-0.641-1.042-1.309-1.52-1.99  c-0.549-0.786-1.045-1.602-1.508-2.434c-0.364-0.656-0.699-1.324-1.008-2.006c-0.354-0.782-0.671-1.578-0.95-2.39  c-0.223-0.648-0.419-1.305-0.593-1.97c-0.203-0.773-0.373-1.557-0.507-2.349c-0.11-0.644-0.194-1.292-0.257-1.946  c-0.031-0.311-0.077-0.62-0.097-0.933c-0.668,0.218-1.318,0.476-1.956,0.76c-1.053,0.47-2.062,1.028-3.019,1.67  c-0.922,0.618-1.794,1.315-2.611,2.085c-0.147,0.139-0.298,0.273-0.442,0.417c-0.436,0.436-0.848,0.891-1.24,1.361  c-0.574,0.689-1.085,1.423-1.562,2.178c-0.375,0.594-0.72,1.204-1.031,1.833c-0.359,0.724-0.683,1.464-0.953,2.227  c-0.218,0.613-0.401,1.24-0.56,1.875c-0.186,0.746-0.323,1.506-0.424,2.273c-0.041,0.315-0.095,0.628-0.122,0.946  c-0.027,0.321-0.022,0.641-0.034,0.961c-0.03,0.775-0.018,1.545,0.04,2.31c0.049,0.652,0.128,1.3,0.239,1.94  c0.138,0.796,0.312,1.581,0.544,2.353c0.201,0.672,0.433,1.335,0.704,1.984c0.345,0.827,0.752,1.628,1.207,2.408  c0.411,0.703,0.871,1.38,1.37,2.038c0.222,0.293,0.431,0.594,0.67,0.877c0.483,0.572,0.999,1.106,1.534,1.617  c0.834,0.796,1.73,1.51,2.675,2.146c1.964,1.324,4.146,2.282,6.464,2.853c1.042,0.257,2.103,0.454,3.192,0.546  c1.064,0.09,2.12,0.066,3.168-0.009c2.392-0.17,4.716-0.755,6.883-1.729c1.045-0.47,2.054-1.022,3.013-1.668  c0.919-0.616,1.788-1.315,2.608-2.085c0.151-0.144,0.316-0.269,0.464-0.416c0.435-0.435,0.826-0.899,1.216-1.366  c0.581-0.692,1.121-1.412,1.6-2.169c0.375-0.593,0.709-1.209,1.02-1.836c0.358-0.723,0.675-1.464,0.943-2.228  c0.048-0.134,0.111-0.26,0.156-0.394c-0.179-0.012-0.356-0.025-0.532-0.04C55.116,66.745,52.149,65.953,49.384,64.735z"></path><path d="M43.815,51.052L54.95,47.54c-0.607-0.724-1.259-1.412-1.96-2.046l-10.529,3.321C42.864,49.591,43.318,50.336,43.815,51.052z  "></path><path d="M47.913,42.134l-7.13,2.249c0.208,0.827,0.469,1.632,0.776,2.419l9.346-2.948C49.961,43.201,48.963,42.623,47.913,42.134z"></path><path d="M41.102,40.309c-0.299-0.025-0.605-0.042-0.924-0.05c0.025,0.662,0.086,1.316,0.175,1.963l4.163-1.313  C43.412,40.619,42.274,40.409,41.102,40.309z"></path><path d="M58.824,59.795c0.301,0.025,0.607,0.042,0.926,0.051c-0.02-0.525-0.073-1.043-0.133-1.559l-3.442,1.086  C57.04,59.562,57.919,59.719,58.824,59.795z"></path><path d="M52.566,58.213l6.684-2.108c-0.191-0.831-0.45-1.637-0.742-2.43l-9.067,2.86C50.426,57.181,51.47,57.742,52.566,58.213z"></path><path d="M47.283,54.918l10.356-3.267c-0.386-0.777-0.806-1.537-1.283-2.259l-11.098,3.501C45.888,53.612,46.56,54.294,47.283,54.918  z"></path></svg></span><span class="TabNavigationFooterItem-navText">Concepts</span></a><a class="TabNavigationFooterItem-navButton TabNavigationFooterItem-selected" title="Curated collections of LessWrong's best writing." href="https://www.lesswrong.com/library"><span class="TabNavigationFooterItem-icon"><svg version="1.0" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 84.000000 84.000000" preserveAspectRatio="xMidYMid meet"><g transform="translate(0.0,84.0) scale(0.1,-0.1)" stroke="none"><path d="M170 695 c0 -3 -14 -67 -31 -143 -17 -75 -35 -159 -40 -187 -5 -27 -12 -58 -15 -67 -5 -16 2 -18 53 -18 71 0 156 -21 223 -55 l50 -25 0 225 0 224 -27 15 c-51 25 -213 49 -213 31z"></path><path d="M536 689 c-21 -5 -54 -16 -72 -25 l-34 -16 0 -224 c0 -123 2 -224 4 -224 2 0 23 11 47 24 59 33 150 56 222 56 56 0 59 1 54 23 -3 12 -22 99 -42 192 -20 94 -38 178 -41 188 -4 13 -16 17 -52 16 -26 -1 -64 -5 -86 -10z"></path><path d="M70 576 c-42 -207 -67 -352 -62 -357 4 -4 99 -23 210 -43 l204 -37 201 37 c111 20 205 39 209 43 4 4 -8 87 -27 186 -47 244 -40 223 -70 217 -18 -3 -24 -9 -21 -21 8 -26 76 -330 76 -339 0 -4 -30 -8 -67 -9 -73 0 -200 -31 -263 -63 l-38 -20 -54 25 c-77 35 -173 58 -250 58 -38 1 -68 4 -68 7 0 5 39 181 75 336 4 17 -1 23 -20 26 -22 5 -25 1 -35 -46z"></path></g></svg></span><span class="TabNavigationFooterItem-navText">Library</span></a><a class="TabNavigationFooterItem-navButton" title="Find a meetup near you." href="https://www.lesswrong.com/community"><span class="TabNavigationFooterItem-icon"><svg width="28" height="34" viewBox="0 0 28 34" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M14.042 0.653442C21.3573 0.653442 27.3273 6.45284 27.3273 13.6408C27.3273 20.7471 21.3573 26.5465 14.042 26.5465C6.64258 26.5465 0.672607 20.7471 0.672607 13.6408C0.672607 6.45284 6.64258 0.653442 14.042 0.653442ZM11.4354 13.2324C11.4354 13.2324 10.9309 13.1507 10.8468 13.1507C10.0059 13.3141 10.0059 13.1507 10.7627 13.6408C11.2672 13.9676 11.099 13.9676 11.6035 13.8859C11.7717 13.9676 11.9399 13.9676 12.108 14.0492C12.108 14.1309 12.108 14.2943 12.108 14.376C12.108 14.376 12.5285 14.621 12.6125 14.621H13.2852C13.3693 14.621 14.042 14.2126 12.8648 13.8859C12.6966 13.8859 12.4444 13.8859 12.1921 13.9676C12.108 13.8042 12.024 13.6408 11.8558 13.4775C11.8558 13.4775 11.3513 13.2324 11.4354 13.2324ZM24.3843 7.18798C25.5615 9.06666 26.3183 11.2721 26.3183 13.6408C26.3183 14.2943 26.2342 15.0294 26.1501 15.6829C25.5615 15.1928 24.9729 14.7844 24.8888 14.7027C24.6366 14.4576 24.6366 14.8661 24.4684 15.3561C24.3843 15.9279 23.7116 15.4378 23.2071 15.4378C22.7026 15.4378 22.3663 14.7844 22.3663 14.7844C21.4414 14.0492 21.6936 13.6408 21.6936 12.4973C21.6936 12.1706 21.8618 11.7622 22.114 11.5988C22.7026 10.9453 22.1981 10.5369 22.8708 10.0468C23.1231 9.88347 24.048 9.63843 24.048 9.63843C24.5525 8.98497 24.8047 8.82161 23.5435 9.06666C23.4594 9.14834 22.9549 9.06666 22.7026 8.90329C22.3663 8.65825 23.1231 8.49488 23.1231 8.49488C23.4594 8.08648 23.7957 7.75975 24.1321 7.43302C24.2162 7.35134 24.3002 7.26966 24.3843 7.18798ZM14.042 1.7153C15.2192 1.7153 16.3122 1.87867 17.4053 2.12371C17.6576 2.45044 18.078 2.94053 18.1621 3.02221C18.2462 3.10389 18.4984 3.75735 18.4984 3.75735C17.9939 4.49248 17.8258 4.90089 16.7327 4.4108C16.3963 4.32912 15.8077 3.5123 15.8077 3.5123C15.8077 3.5123 14.8828 2.53212 14.7146 2.45044C14.3783 2.20539 13.8738 2.36876 13.6216 2.77717C13.7897 3.02221 13.8738 3.18557 14.042 3.43062C14.7987 3.43062 14.7987 3.26726 15.051 3.83903C14.7987 3.92071 14.5465 4.00239 14.2101 4.08407C13.4534 4.57416 13.7056 4.57416 12.8648 4.00239C12.8648 4.00239 12.3603 3.43062 12.2762 3.34894C12.1921 3.26726 11.8558 2.53212 11.6876 2.28708C11.6876 2.20539 11.6035 2.04203 11.6876 1.87867C12.4444 1.79699 13.2011 1.7153 14.042 1.7153ZM14.4624 25.5664C13.6216 25.5664 12.7807 25.4847 11.8558 25.403C11.9399 24.8312 11.9399 24.3411 11.9399 23.851C11.9399 22.5441 12.2762 22.2991 11.1831 21.564C10.9309 21.1555 10.5945 20.6655 10.2582 20.257C9.50144 19.5219 9.75369 19.6036 10.1741 18.6234C10.2582 18.215 10.3423 17.8066 10.4264 17.4799C10.2582 17.0715 10.1741 16.6631 10.0059 16.2546C9.6696 15.9279 9.33327 15.5195 8.99693 15.1928C7.14708 14.7027 7.23117 14.9477 5.88582 13.5591C5.63357 13.1507 5.38132 12.7423 5.04498 12.3339C4.12005 11.2721 4.28822 11.4354 3.95189 10.1285C3.8678 9.55675 3.78372 8.98497 3.69963 8.49488C3.61555 8.16816 3.53147 7.84143 3.44738 7.59639C5.12906 4.81921 7.90384 2.77717 11.1831 2.04203C11.099 2.45044 11.015 2.85885 10.9309 3.34894C8.91285 4.90089 7.90384 5.3093 10.3423 6.2078C10.9309 7.1063 10.7627 7.5147 11.5195 6.94293C11.5195 6.45284 11.4354 5.96275 11.3513 5.47266C12.8648 4.49248 12.4444 4.65584 14.2101 5.55434C14.4624 5.79939 14.7146 6.04443 14.9669 6.28948C16.1441 6.77957 16.2282 6.53452 15.2192 7.26966C14.9669 7.18798 14.6306 7.18798 14.2942 7.1063C14.1261 7.26966 13.9579 7.43302 13.7897 7.59639C14.2101 8.49488 14.4624 8.4132 13.5375 8.65825C13.2852 8.82161 13.033 9.06666 12.7807 9.23002C12.4444 9.23002 12.108 9.23002 11.7717 9.23002C11.7717 9.23002 11.7717 10.2102 11.7717 10.3736C11.7717 10.6186 11.015 11.2721 11.015 11.2721C11.015 11.2721 10.5945 11.8438 10.8468 12.1706C11.015 12.4973 10.5945 12.7423 10.5945 12.7423C10.3423 12.4973 10.1741 12.1706 9.92186 11.9255C9.58552 11.9255 9.1651 12.0072 8.74468 12.0889C8.49243 12.2522 8.24018 12.3339 7.98792 12.4973C7.90384 12.7423 7.81975 13.0691 7.81975 13.3141C7.90384 13.7225 7.98792 14.1309 8.15609 14.5393C8.32426 14.5393 8.57651 14.5393 8.82876 14.4576C8.82876 13.2324 8.57651 13.2324 9.58552 14.0492C9.6696 14.2943 9.6696 14.5393 9.6696 14.8661C10.0059 14.9477 10.2582 15.0294 10.5104 15.1111C10.5945 15.3561 10.5945 15.6829 10.5945 15.9279C11.015 16.2546 11.4354 16.4997 11.8558 16.7447C12.7807 16.9081 13.7897 16.9081 14.5465 17.4799C15.1351 17.8066 15.7237 18.0516 16.3963 18.2967C16.3963 18.5417 16.4804 18.7868 16.5645 19.0318C16.9008 19.1135 17.3213 19.1135 17.6576 19.1135C18.078 19.1952 18.4984 19.1952 18.9189 19.1952C18.9189 19.4402 18.9189 19.6853 18.9189 19.9303C18.7507 21.3189 18.6666 21.0739 17.6576 21.9724C17.3213 22.2174 16.9849 22.4625 16.6486 22.7892C15.8077 23.9327 16.06 23.7694 14.7146 24.2594C14.6306 24.6679 14.5465 25.1579 14.4624 25.5664Z" fill="currentColor" fill-opacity="0.7"></path></svg></span><span class="TabNavigationFooterItem-navText">Community</span></a></div></div><div class="Layout-searchResultsArea"></div><div class="Layout-main"><div class="flash-messages FlashMessages-root"></div><div class="SingleColumnSection-root"><h1 class="Typography-root Typography-display3 LibraryPage-pageTitle">The Library</h1></div><div class="SingleColumnSection-root"><div class="SingleColumnSection-root LWCoreReading-root"><div class="CollectionsItem-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality"></a></div><div class="CollectionsItem-content"><h2 class="Typography-root Typography-title CollectionsItem-title"><a href="https://www.lesswrong.com/rationality">Rationality: A-Z</a></h2><div class="CollectionsItem-subtitle">Also known as "The Sequences"</div><div class="CollectionsItem-description ContentStyles-base content ContentStyles-postHighlight"><div><div>
        <p>
          How can we think better on purpose? <em>Why</em> should we think better on purpose?<br>
          For two years Eliezer Yudkowsky wrote a blogpost a day, 
braindumping thoughts on rationality, ambition and artificial 
intelligence. Those posts were edited into this introductory collection,
 recommended reading for all Lesswrong users.
        </p>
      </div></div></div></div><img src="library_files/mississippi-compass_gwqjvs.png" class="CollectionsItem-image" style="width: 130px;"></div></div></div><div class="CollectionsItem-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/highlights"></a></div><div class="CollectionsItem-content"><h2 class="Typography-root Typography-title CollectionsItem-title"><a href="https://www.lesswrong.com/highlights">The Sequences Highlights</a></h2><div class="CollectionsItem-description ContentStyles-base content ContentStyles-postHighlight"><div><div>
        <p>LessWrong can be kind of intimidating - there's a lot of 
concepts to learn. We recommend getting started with the Highlights, a 
collection of 50 top posts from Eliezer's Sequences.</p>
        <p>A day or two read, covering the foundations of rationality.</p>
        </div></div></div></div><img src="library_files/rdl8pwokejuqyxipg6vx.webp" class="CollectionsItem-image" style="width: 130px;"></div></div></div><div class="CollectionsItem-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor"></a></div><div class="CollectionsItem-content"><h2 class="Typography-root Typography-title CollectionsItem-title"><a href="https://www.lesswrong.com/hpmor">Harry Potter and the Methods of Rationality</a></h2><div class="CollectionsItem-description ContentStyles-base content ContentStyles-postHighlight"><div><div>
        <p>What if Harry Potter was a scientist? What would you do if the universe had magic in it? <br>A story that conveys many rationality concepts, making them more visceral and emotionally compelling.</p></div></div></div></div><img src="library_files/DALL_E_2022-07-13_21.49.04_-_11_year_old_wizard_boy_with_sho.png" class="CollectionsItem-image" style="width: 130px;"></div></div></div><div class="CollectionsItem-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex"></a></div><div class="CollectionsItem-content"><h2 class="Typography-root Typography-title CollectionsItem-title"><a href="https://www.lesswrong.com/codex">The Codex</a></h2><div class="CollectionsItem-description ContentStyles-base content ContentStyles-postHighlight"><div><div>Essays
 by Scott Alexander exploring science, medicine, philosophy, futurism, 
and politics. (There's also one about hallucinatory cactus people but 
it's not representative).</div></div></div></div><img src="library_files/codex_u7ptgt.png" class="CollectionsItem-image" style="width: 130px;"></div></div></div><div class="CollectionsItem-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="CollectionsItem-linkCard LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/bestoflesswrong"></a></div><div class="CollectionsItem-content"><h2 class="Typography-root Typography-title CollectionsItem-title"><a href="https://www.lesswrong.com/bestoflesswrong">Best of LessWrong</a></h2><div class="CollectionsItem-description ContentStyles-base content ContentStyles-postHighlight"><div><div>Each
 December, the LessWrong community reviews the best posts from the 
previous year, and votes on which ones have stood the tests of time.</div></div></div></div><img src="library_files/DALL_E_2022-07-13_22.57.43_-_Books_and_emerald_compass_displ.png" class="CollectionsItem-image" style="width: 130px;"></div></div></div></div></div><div class="Divider-root" style="margin-top: 24px; margin-bottom: 24px;"><div class="Divider-divider"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 100 125" enable-background="new 0 0 100 100" xml:space="preserve"><g><g><polygon fill="none" points="6.256,50.245 10.108,51.512 10.727,50.274 9.255,48.746   "></polygon><polygon fill="none" points="89.273,50.274 89.892,51.512 93.744,50.245 90.745,48.746   "></polygon><path d="M11.396,49.969l-1.818-1.888c-0.105-0.109-0.269-0.138-0.405-0.07l-3.981,1.99c-0.126,0.063-0.201,0.195-0.191,0.336    c0.01,0.14,0.104,0.26,0.238,0.304l4.937,1.624c0.036,0.012,0.072,0.017,0.108,0.017c0.128,0,0.25-0.071,0.31-0.192l0.863-1.725    C11.522,50.233,11.498,50.075,11.396,49.969z M10.108,51.512l-3.852-1.267l2.999-1.499l1.472,1.528L10.108,51.512z"></path><path d="M94.808,50.001l-3.981-1.99c-0.136-0.068-0.3-0.04-0.405,0.07l-1.818,1.888c-0.102,0.106-0.126,0.264-0.06,0.396    l0.863,1.725c0.06,0.12,0.182,0.192,0.31,0.192c0.036,0,0.072-0.005,0.108-0.017l4.937-1.624c0.133-0.044,0.227-0.164,0.238-0.304    C95.009,50.196,94.934,50.064,94.808,50.001z M89.892,51.512l-0.619-1.238l1.472-1.528l2.999,1.499L89.892,51.512z"></path><path d="M87.916,50.231l1.543-2.1c0.113-0.154,0.08-0.371-0.074-0.485c-0.155-0.113-0.372-0.08-0.485,0.074l-1.611,2.193h-32.22    l-1.515-1.665c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.097-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.06-0.03,0.116-0.06,1.545-1.488h32.334l0.965,1.644c0.065,0.11,0.181,0.171,0.299,0.171    c0.06,0,0.12-0.015,0.175-0.048c0.165-0.097,0.22-0.31,0.123-0.475L87.916,50.231z"></path><path d="M48.835,48.225c-0.141-0.129-0.361-0.118-0.49,0.023l-1.589,1.746c-0.125,0.137-0.12,0.348,0.011,0.479    c1.564,1.564,1.617,1.59,1.679,1.621c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.34-0.109-0.438c-0.106-0.091-0.711-0.688-1.311-1.285l1.366-1.502    C48.987,48.574,48.977,48.354,48.835,48.225z"></path><path d="M53.244,49.995l-1.589-1.746c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.098-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.062-0.031,0.115-0.058,1.679-1.621C53.363,50.343,53.368,50.132,53.244,49.995z"></path><path d="M45.593,50.217l1.366-1.502c0.129-0.142,0.119-0.361-0.023-0.49c-0.142-0.129-0.361-0.118-0.49,0.023l-1.515,1.665h-32.22    L11.1,47.72c-0.113-0.154-0.33-0.188-0.485-0.074c-0.154,0.113-0.188,0.331-0.074,0.485l1.543,2.1l-0.98,1.668    c-0.097,0.165-0.042,0.378,0.124,0.475c0.055,0.032,0.116,0.048,0.175,0.048c0.119,0,0.235-0.061,0.299-0.171l0.965-1.644h32.334    c1.429,1.427,1.485,1.458,1.545,1.488c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.341-0.109-0.438C46.798,51.411,46.192,50.815,45.593,50.217z"></path></g></g></svg></div><div class="Divider-compass"><svg version="1.1" x="0px" y="0px" viewBox="0 0 100 100"><g fill="currentColor"><path d="M29.1,29.2l6.4,11.6l4.3-0.8l0.8-4.3L29.1,29.2z M40.7,64.5l-0.8-4.3l-4.3-0.8L29.2,71L40.7,64.5z M70.9,70.9l-6.4-11.6l-4.3,0.8l-0.8,4.3L70.9,70.9z M64.4,40.8l6.4-11.6l-11.6,6.4l0.8,4.3L64.4,40.8z M67.4,58.8l10.8,19.4L58.8,67.4L50,98.8l-8.8-31.4L21.9,78.2l10.8-19.4L1.2,50.1l31.4-8.8L21.9,21.9l19.4,10.8L50,1.3l8.8,31.4l19.4-10.8L67.4,41.3L98.8,50L67.4,58.8zM57.7,57.8L83.5,50L50,50.1l7.7-7.7L50,16.6v33.5l-7.7-7.7l-25.8,7.7H50l-7.7,7.7L50,83.5V50.1L57.7,57.8z"></path></g></svg></div><div class="Divider-divider"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 100 125" enable-background="new 0 0 100 100" xml:space="preserve"><g><g><polygon fill="none" points="6.256,50.245 10.108,51.512 10.727,50.274 9.255,48.746   "></polygon><polygon fill="none" points="89.273,50.274 89.892,51.512 93.744,50.245 90.745,48.746   "></polygon><path d="M11.396,49.969l-1.818-1.888c-0.105-0.109-0.269-0.138-0.405-0.07l-3.981,1.99c-0.126,0.063-0.201,0.195-0.191,0.336    c0.01,0.14,0.104,0.26,0.238,0.304l4.937,1.624c0.036,0.012,0.072,0.017,0.108,0.017c0.128,0,0.25-0.071,0.31-0.192l0.863-1.725    C11.522,50.233,11.498,50.075,11.396,49.969z M10.108,51.512l-3.852-1.267l2.999-1.499l1.472,1.528L10.108,51.512z"></path><path d="M94.808,50.001l-3.981-1.99c-0.136-0.068-0.3-0.04-0.405,0.07l-1.818,1.888c-0.102,0.106-0.126,0.264-0.06,0.396    l0.863,1.725c0.06,0.12,0.182,0.192,0.31,0.192c0.036,0,0.072-0.005,0.108-0.017l4.937-1.624c0.133-0.044,0.227-0.164,0.238-0.304    C95.009,50.196,94.934,50.064,94.808,50.001z M89.892,51.512l-0.619-1.238l1.472-1.528l2.999,1.499L89.892,51.512z"></path><path d="M87.916,50.231l1.543-2.1c0.113-0.154,0.08-0.371-0.074-0.485c-0.155-0.113-0.372-0.08-0.485,0.074l-1.611,2.193h-32.22    l-1.515-1.665c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.097-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.06-0.03,0.116-0.06,1.545-1.488h32.334l0.965,1.644c0.065,0.11,0.181,0.171,0.299,0.171    c0.06,0,0.12-0.015,0.175-0.048c0.165-0.097,0.22-0.31,0.123-0.475L87.916,50.231z"></path><path d="M48.835,48.225c-0.141-0.129-0.361-0.118-0.49,0.023l-1.589,1.746c-0.125,0.137-0.12,0.348,0.011,0.479    c1.564,1.564,1.617,1.59,1.679,1.621c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.34-0.109-0.438c-0.106-0.091-0.711-0.688-1.311-1.285l1.366-1.502    C48.987,48.574,48.977,48.354,48.835,48.225z"></path><path d="M53.244,49.995l-1.589-1.746c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.098-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.062-0.031,0.115-0.058,1.679-1.621C53.363,50.343,53.368,50.132,53.244,49.995z"></path><path d="M45.593,50.217l1.366-1.502c0.129-0.142,0.119-0.361-0.023-0.49c-0.142-0.129-0.361-0.118-0.49,0.023l-1.515,1.665h-32.22    L11.1,47.72c-0.113-0.154-0.33-0.188-0.485-0.074c-0.154,0.113-0.188,0.331-0.074,0.485l1.543,2.1l-0.98,1.668    c-0.097,0.165-0.042,0.378,0.124,0.475c0.055,0.032,0.116,0.048,0.175,0.048c0.119,0,0.235-0.061,0.299-0.171l0.965-1.644h32.334    c1.429,1.427,1.485,1.458,1.545,1.488c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.341-0.109-0.438C46.798,51.411,46.192,50.815,45.593,50.217z"></path></g></g></svg></div></div><div class="SingleColumnSection-root"><div class="SectionTitle-root"><h1 id="curated-sequences" class="Typography-root Typography-display1 SectionTitle-title">Curated Sequences</h1><div class="SectionTitle-children"></div></div><div><div><div class="SequencesGrid-grid"><div class="SequencesGrid-gridContent"><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/rationality#5g5TkQTe9rmPS5vvM"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/wwkkaskmbcajjogyv1hu.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Predictably Wrong</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/highlights#NBDFAKt3GbFwnwzQF"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vuzt0hjdrboxywfd4wbt.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Thinking Better on Purpose</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ruby">Ruby</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/hpmor#PtgH6ALi5CoJnPmGS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/i9dkgkhw14vwar63i4xn.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Methods of Rationality</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/mzgtmmTKKn5MuCzFJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/r68kkaexxymt3ckkc6pp.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AGI safety from first principles</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ricraz">Richard_Ngo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/codex#XsMTxdQ6fprAQMoKi"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rfpef83ejiwbsi1pmroz.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Argument and Analysis</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mxpqfzoorr921qviypmq.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Embedded Agency</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/v55BhXbpJuaExkpcD"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hxbqvswdmhyoomidbpyu.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">2022 MIRI Alignment Discussion</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/robbbb">Rob Bensinger</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/n945eovrA3oDueqtq"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gpk2pxurl1yymecllfoo.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">2021 MIRI Conversations</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/robbbb">Rob Bensinger</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZnSMHcWjRx6yT4H92"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qu5jdoyzz4jov7siczag.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">LessWrong Political Prerequisites</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/CmrW8fCmSLK7E25sa"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ek5uoqxjn8zl6l9unirr.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Infra-Bayesianism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/diffractor">Diffractor</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/evLkoqsbi79AnM5sz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xhjq89g5vufbwx0df0uf.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Intro to Naturalism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/brienneyudkowsky">LoganStrohl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pFatcKW3JJhTSxqAF"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/m0lpxsua2jmtwbmwlttp.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Replacing Guilt</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/so8res">So8res</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/n3utvGrgC2SGi9xQX"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gdwaqd3wg9zngxrbwbqm.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Conditioning Predictive Models</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/evhub">evhub</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/f2YA4eGskeztcJsqT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/iwls9nubn9dkangx3q5t.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Cyborgism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/janus-1">janus</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/a6ne2ve5uturEEQK7"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/o6bsfz6il5u2kep6vksv.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Engineer’s Interpretability Sequence</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scasper">scasper</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/dDMzozPbe4aJRkfTr"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/v2gocvypckw2doyt5aex.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Stories</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ricraz">Richard_Ngo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lalel10qzujyi47rqiww.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Valence</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/steve2152">Steven Byrnes</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/BbAvHtorCZqp97X9W"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sqq40tz8m2jsvyrj8s4r.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Otherness and control in the age of AGI</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/joe-carlsmith">Joe Carlsmith</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TF77XsD5PbucbJsG3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/whqbkvzjopvlh7paq73r.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Luna Lovegood</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/yYxggfHYRrqnJXuRx"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nsphhanrutzgofgj5xvu.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Most Important Century</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/holdenkarnofsky">HoldenKarnofsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/EmDuGeRw749sD3GKd"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/prcccqtc5w7ytolilruu.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Iterated Amplification</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/paulfchristiano">paulfchristiano</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4dHMdK5TLN6xcqtyc"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/x0pxuhnauzakdnrqijhe.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Value Learning</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/rohinmshah">Rohin Shah</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/KAv8z6oJCTxjR8vdR"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/prnzteddh56bhbv5nmae.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">CFAR Handbook</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cfar-2017">CFAR!Duncan</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xEFeCwk3pdYdeG2rL"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mrcucooleoeaj8ybccb3.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Gears Which Turn The World</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/y7bhrihn26iisjvhu61y.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Immoral Mazes</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/zvi">Zvi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/hBFDRZCPLcrRDubgm"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sbiiwhliynfuouklvwph.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Keep your beliefs cruxy and your frames explicit</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rzdw9faewnetbmumls9y.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Risks from Learned Optimization</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/evhub">evhub</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/d3WgHDBAPYYScp5Em"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ncfkdhspgrfhhjpbisaj.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Fun Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qWoFR4ytMpQ5vw3FT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sio9b8jw1apesuispocg.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Three Worlds Collide</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HXkpm9b8o964jbQ89"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rqnuxewffasun6tvdkng.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Slack and the Sabbath </div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/zvi">Zvi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZNNi2uNx9E6iwGKKG"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vitugifyyh2upm9ucjzh.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Introduction to Game Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/G2GDw3m4MJ5ixSM92"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/djfksyoldrjt4ef5jts3.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Blue-Minimizing Robot</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qRxTKm7DAftSuTGvj"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/chv9ct9cisa2fne7htnk.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Hammertime</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/alkjash">alkjash</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pC6DYFLPMTCbEwH8W"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nhvoi5fvxxzthqhu2ctt.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Babble and Prune</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/alkjash">alkjash</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/i2ogsvmipbdolntkew4a.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Highly Advanced Epistemology 101 for Beginners</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/yFvZa9wkv5JoqhM8F"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yxlirutmux2fjqnnjhoc.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Rationality and Philosophy</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lukeprog">lukeprog</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/XipJ7DMjYyriAm7fr"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qpreo6bc9vxwyf2l1dzo.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Decision Theory: Newcomb's Problem</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/annasalamon">AnnaSalamon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/oi873FWi6pHWxswSa"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yp01lueog8rjaybsizux.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Science of Winning at Life</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lukeprog">lukeprog</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/bQgRsy23biR52poMf"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kvoeqrfluqd0jv5fvwez.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">No-Nonsense Metaethics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lukeprog">lukeprog</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/oLGCcbnvabyibnG9d"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vbhv0s06jdmonk6garvf.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Inadequate Equilibria</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/2A7rrZ4ySx6R8mfoT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/a9iwqgsxkqznfrtskanw.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Cartesian Frames</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scott-garrabrant">Scott Garrabrant</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ynMFrq9K5iNMfSZNg"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ozdf0thhtehmpmbf9dys.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Living Luminously</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/alicorn">Alicorn</a></span></span></span></div></div></div></span></div></div></div></div></div></div><div class="Divider-root" style="margin-top: 24px; margin-bottom: 24px;"><div class="Divider-divider"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 100 125" enable-background="new 0 0 100 100" xml:space="preserve"><g><g><polygon fill="none" points="6.256,50.245 10.108,51.512 10.727,50.274 9.255,48.746   "></polygon><polygon fill="none" points="89.273,50.274 89.892,51.512 93.744,50.245 90.745,48.746   "></polygon><path d="M11.396,49.969l-1.818-1.888c-0.105-0.109-0.269-0.138-0.405-0.07l-3.981,1.99c-0.126,0.063-0.201,0.195-0.191,0.336    c0.01,0.14,0.104,0.26,0.238,0.304l4.937,1.624c0.036,0.012,0.072,0.017,0.108,0.017c0.128,0,0.25-0.071,0.31-0.192l0.863-1.725    C11.522,50.233,11.498,50.075,11.396,49.969z M10.108,51.512l-3.852-1.267l2.999-1.499l1.472,1.528L10.108,51.512z"></path><path d="M94.808,50.001l-3.981-1.99c-0.136-0.068-0.3-0.04-0.405,0.07l-1.818,1.888c-0.102,0.106-0.126,0.264-0.06,0.396    l0.863,1.725c0.06,0.12,0.182,0.192,0.31,0.192c0.036,0,0.072-0.005,0.108-0.017l4.937-1.624c0.133-0.044,0.227-0.164,0.238-0.304    C95.009,50.196,94.934,50.064,94.808,50.001z M89.892,51.512l-0.619-1.238l1.472-1.528l2.999,1.499L89.892,51.512z"></path><path d="M87.916,50.231l1.543-2.1c0.113-0.154,0.08-0.371-0.074-0.485c-0.155-0.113-0.372-0.08-0.485,0.074l-1.611,2.193h-32.22    l-1.515-1.665c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.097-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.06-0.03,0.116-0.06,1.545-1.488h32.334l0.965,1.644c0.065,0.11,0.181,0.171,0.299,0.171    c0.06,0,0.12-0.015,0.175-0.048c0.165-0.097,0.22-0.31,0.123-0.475L87.916,50.231z"></path><path d="M48.835,48.225c-0.141-0.129-0.361-0.118-0.49,0.023l-1.589,1.746c-0.125,0.137-0.12,0.348,0.011,0.479    c1.564,1.564,1.617,1.59,1.679,1.621c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.34-0.109-0.438c-0.106-0.091-0.711-0.688-1.311-1.285l1.366-1.502    C48.987,48.574,48.977,48.354,48.835,48.225z"></path><path d="M53.244,49.995l-1.589-1.746c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.098-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.062-0.031,0.115-0.058,1.679-1.621C53.363,50.343,53.368,50.132,53.244,49.995z"></path><path d="M45.593,50.217l1.366-1.502c0.129-0.142,0.119-0.361-0.023-0.49c-0.142-0.129-0.361-0.118-0.49,0.023l-1.515,1.665h-32.22    L11.1,47.72c-0.113-0.154-0.33-0.188-0.485-0.074c-0.154,0.113-0.188,0.331-0.074,0.485l1.543,2.1l-0.98,1.668    c-0.097,0.165-0.042,0.378,0.124,0.475c0.055,0.032,0.116,0.048,0.175,0.048c0.119,0,0.235-0.061,0.299-0.171l0.965-1.644h32.334    c1.429,1.427,1.485,1.458,1.545,1.488c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.341-0.109-0.438C46.798,51.411,46.192,50.815,45.593,50.217z"></path></g></g></svg></div><div class="Divider-compass"><svg version="1.1" x="0px" y="0px" viewBox="0 0 100 100"><g fill="currentColor"><path d="M29.1,29.2l6.4,11.6l4.3-0.8l0.8-4.3L29.1,29.2z M40.7,64.5l-0.8-4.3l-4.3-0.8L29.2,71L40.7,64.5z M70.9,70.9l-6.4-11.6l-4.3,0.8l-0.8,4.3L70.9,70.9z M64.4,40.8l6.4-11.6l-11.6,6.4l0.8,4.3L64.4,40.8z M67.4,58.8l10.8,19.4L58.8,67.4L50,98.8l-8.8-31.4L21.9,78.2l10.8-19.4L1.2,50.1l31.4-8.8L21.9,21.9l19.4,10.8L50,1.3l8.8,31.4l19.4-10.8L67.4,41.3L98.8,50L67.4,58.8zM57.7,57.8L83.5,50L50,50.1l7.7-7.7L50,16.6v33.5l-7.7-7.7l-25.8,7.7H50l-7.7,7.7L50,83.5V50.1L57.7,57.8z"></path></g></svg></div><div class="Divider-divider"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" x="0px" y="0px" viewBox="0 0 100 125" enable-background="new 0 0 100 100" xml:space="preserve"><g><g><polygon fill="none" points="6.256,50.245 10.108,51.512 10.727,50.274 9.255,48.746   "></polygon><polygon fill="none" points="89.273,50.274 89.892,51.512 93.744,50.245 90.745,48.746   "></polygon><path d="M11.396,49.969l-1.818-1.888c-0.105-0.109-0.269-0.138-0.405-0.07l-3.981,1.99c-0.126,0.063-0.201,0.195-0.191,0.336    c0.01,0.14,0.104,0.26,0.238,0.304l4.937,1.624c0.036,0.012,0.072,0.017,0.108,0.017c0.128,0,0.25-0.071,0.31-0.192l0.863-1.725    C11.522,50.233,11.498,50.075,11.396,49.969z M10.108,51.512l-3.852-1.267l2.999-1.499l1.472,1.528L10.108,51.512z"></path><path d="M94.808,50.001l-3.981-1.99c-0.136-0.068-0.3-0.04-0.405,0.07l-1.818,1.888c-0.102,0.106-0.126,0.264-0.06,0.396    l0.863,1.725c0.06,0.12,0.182,0.192,0.31,0.192c0.036,0,0.072-0.005,0.108-0.017l4.937-1.624c0.133-0.044,0.227-0.164,0.238-0.304    C95.009,50.196,94.934,50.064,94.808,50.001z M89.892,51.512l-0.619-1.238l1.472-1.528l2.999,1.499L89.892,51.512z"></path><path d="M87.916,50.231l1.543-2.1c0.113-0.154,0.08-0.371-0.074-0.485c-0.155-0.113-0.372-0.08-0.485,0.074l-1.611,2.193h-32.22    l-1.515-1.665c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.097-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.06-0.03,0.116-0.06,1.545-1.488h32.334l0.965,1.644c0.065,0.11,0.181,0.171,0.299,0.171    c0.06,0,0.12-0.015,0.175-0.048c0.165-0.097,0.22-0.31,0.123-0.475L87.916,50.231z"></path><path d="M48.835,48.225c-0.141-0.129-0.361-0.118-0.49,0.023l-1.589,1.746c-0.125,0.137-0.12,0.348,0.011,0.479    c1.564,1.564,1.617,1.59,1.679,1.621c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.34-0.109-0.438c-0.106-0.091-0.711-0.688-1.311-1.285l1.366-1.502    C48.987,48.574,48.977,48.354,48.835,48.225z"></path><path d="M53.244,49.995l-1.589-1.746c-0.129-0.142-0.348-0.152-0.49-0.023c-0.142,0.129-0.152,0.348-0.023,0.49l1.366,1.502    c-0.6,0.598-1.205,1.194-1.311,1.285c-0.137,0.098-0.187,0.283-0.109,0.438c0.061,0.121,0.183,0.191,0.31,0.191    c0.052,0,0.105-0.012,0.155-0.037c0.062-0.031,0.115-0.058,1.679-1.621C53.363,50.343,53.368,50.132,53.244,49.995z"></path><path d="M45.593,50.217l1.366-1.502c0.129-0.142,0.119-0.361-0.023-0.49c-0.142-0.129-0.361-0.118-0.49,0.023l-1.515,1.665h-32.22    L11.1,47.72c-0.113-0.154-0.33-0.188-0.485-0.074c-0.154,0.113-0.188,0.331-0.074,0.485l1.543,2.1l-0.98,1.668    c-0.097,0.165-0.042,0.378,0.124,0.475c0.055,0.032,0.116,0.048,0.175,0.048c0.119,0,0.235-0.061,0.299-0.171l0.965-1.644h32.334    c1.429,1.427,1.485,1.458,1.545,1.488c0.05,0.025,0.103,0.037,0.155,0.037c0.127,0,0.25-0.07,0.31-0.191    c0.078-0.155,0.028-0.341-0.109-0.438C46.798,51.411,46.192,50.815,45.593,50.217z"></path></g></g></svg></div></div><div class="SingleColumnSection-root"><div class="SectionTitle-root"><h1 id="community-sequences" class="Typography-root Typography-display1 SectionTitle-title">Community Sequences</h1><div class="SectionTitle-children"><a href="https://www.lesswrong.com/sequencesnew"><span class="Typography-root Typography-body2 SectionButton-root"><svg class="MuiSvgIcon-root" focusable="false" viewBox="0 0 24 24" aria-hidden="true" role="presentation"><path fill="none" d="M0 0h24v24H0z"></path><path d="M4 6H2v14c0 1.1.9 2 2 2h14v-2H4V6zm16-4H8c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h12c1.1 0 2-.9 2-2V4c0-1.1-.9-2-2-2zm-1 9h-4v4h-2v-4H9V9h4V5h2v4h4v2z"></path></svg>Create New Sequence</span></a></div></div><div><div><div class="SequencesGrid-grid"><div class="SequencesGrid-gridContent"><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/kHXfThe7cZs9qCJcD"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/bfx5itye2xjb0liq00zg.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Coupling for Decouplers</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jacob-falkovich">Jacob Falkovich</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TtBARjJ7sjxDjgjow"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qunmjdhhdmyxb7qifzfc.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Probability Theory Fundamentals 102</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ape-in-the-coat">Ape in the coat</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/nhBeCEzWaAGFw6weu"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ziznojpwlotjslgx9xgw.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Orcas</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/towards_keeperhood">Towards_Keeperhood</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TEybbkyHpMEB2HTv3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qjzer8wybtd3fkmm156i.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Theoretical Foundations of Reward Learning</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/logical_lunatic">Joar Skalse</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/2nrd74Be7mmhkJc77"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xmifowdkgqnlxpeskzcj.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Meta-theory of rationality</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cole-wyeth">Cole Wyeth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/BaozoQuaC8hYXxjbt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/b0mni0wuw02yjhs35ho0.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Radical empathy</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/michaelstjules">MichaelStJules</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/sLqCreBi2EXNME57o"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/atk3kjkkvfuuwkv0mwc1.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AIXI Agent foundations</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cole-wyeth">Cole Wyeth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/29rcaHiBnxHDjHaMq"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ammfeotp10rmbkrwfsil.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Substrate Needs Convergence</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/willpetillo">WillPetillo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TrPEBfTSKwSRdLgmv"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cty0iyxr0asc6m3vbhza.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Prospects for Solartopia</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/transhumanist_atom_understander">transhumanist_atom_understander</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/i5MgWdrk7dYMtcyA7"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/c9hx1tgf7xwueylbudcv.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Miss Macross: My Life as the Star</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jadael">Trevor Hill-Hand</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/btmYeavYrwfz56FEv"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vowmfifnizhdzdheneae.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">How might we solve the alignment problem?</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/joe-carlsmith">Joe Carlsmith</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/jZfcWzg8cxviFqwrT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yzmk9grzs5lfjxl08jv8.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Darwinian Trap</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/kristianronn">KristianRonn</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qhdHbCJ3PYesL9dde"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gforsnvfonkxbjkfbj8m.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Intuitive Self-Models</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/steve2152">Steven Byrnes</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/B9HWqQSt3NcLx34qc"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/osc2ogpwhlw5rkmj8uyl.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The AI Alignment and Deployment Problems</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/sdm">Sammy Martin</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/RTJH6fcX36ohnELem"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rejsnlyphfnolphaqqzq.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Meaning in the Multiverse</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jonah-wilberg">Jonah Wilberg</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Ed7Ffv4LLK3GS3oj3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/krq7ksuxdb5pye3lrzps.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title"> Welfare and moral weights</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/michaelstjules">MichaelStJules</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/gEvTvhr8hNRrdHC62"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/j21eqgktvcnbri2bcask.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Linear Diffusion of Sparse Lognormals: Causal Inference Against Scientism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/tailcalled">tailcalled</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pwazxmKw7qKo8hoWo"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/a740dwayvur42i3oma3d.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Acausal Trade</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/niplav">niplav</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/KfCjeconYRdFbMxsy"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kdwavysy3pj1dqm0l1jk.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">CAST: Corrigibility As Singular Target</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/max-harms">Max Harms</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/SybKaNSqyADrnMQ7H"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/srkofmpkmr2b5to0uimt.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Situational Awareness Summarized</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/joe-rogero">Joe Rogero</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3kQJuSMxoiYWnvLcA"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hbkqht1s3tlwe1gus2oi.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Math of Geometric Utilitarianism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/strivingforlegibility">StrivingForLegibility</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4toN42rxApcqMb8uY"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kwhpr6papjsvaa5bg6dg.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Geometric Utilitarianism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/strivingforlegibility">StrivingForLegibility</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/oTMuc9C3KKdZ6FcCB"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/afhsthmxkh51qxnbnkyg.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Procedural Executive Function</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/daystareld">DaystarEld</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xCmj2w2ZrcwxdH9z3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pd0fyvvdyqt7l5nqewng.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Big Picture AI Safety</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/euanmclean">EuanMcLean</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/LN5LaQKkuRv3AMzZY"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ndkwvkl6bin8cmzscjzl.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Statistical Mechanics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/j-bostock">J Bostock</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4TT69Yt5FDWijAWab"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ucbczzkojjrcoekqld8n.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Aspiration-based, non-maximizing AI agent designs [Aspiration-based designs]</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jobst-heitzig">Jobst Heitzig</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/PC3yJgdKvk8kzqZyA"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nk0stxekqftc7ehklz91.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Control</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/fabien-roger">Fabien Roger</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/rXu6aHeXGBaru34dm"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/wu1u6pm8yk3goqodz4mz.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Adverse Selection</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ricki-heicklen">Ricki Heicklen</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xHSo6FcdmhziuAABK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/q1chjbiz8fpdm0pe4hnb.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Provable AI Alignment (ProvAIA)</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/maria-kapros">Maria Kapros</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/MrJYDaziAimvX2skP"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cton85zmumsganzwwv8w.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Weak-To-Strong Generalization (W2SG)</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/maria-kapros">Maria Kapros</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HQD8juYNZ2Wj5F4KC"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hkv4khdwm8q0y0xnmpcs.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">On Wholesomeness</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/owencb">owencb</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/XJBaPPEYAPeDzuAsy"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/bpfhxrootb3bbaugb1tr.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Sense Of Physical Necessity: A Naturalism Demo</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/brienneyudkowsky">LoganStrohl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/fGbbiJFaoHfXQwMEf"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kmifzvvistwjf3v6wehn.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Deliberative Algorithms as Scaffolding</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cole-wyeth">Cole Wyeth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Z6vSYoeNBXbDxhARn"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/czdivejo9tchnuoh5eft.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Counterfactuals and Updatelessness</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/martinsq">Martín Soto</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ACvmpfhSyKAChzyWi"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/r7zvdwfxekqsdsgfjhfl.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Quantitative cruxes and evidence in Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/martinsq">Martín Soto</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/sCcEZLWHXCaaoMGM8"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gj6zbtnlfr3sjjyhuoj6.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Distributed Strategic Epistemology</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/strivingforlegibility">StrivingForLegibility</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/sZyz9daQRoCLnxeoK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fykndoja7eepkguibc2q.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Delegative Decision Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/strivingforlegibility">StrivingForLegibility</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pbD2D5Ar8ZmfyueWb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/is0eednqhkzmsid3omwd.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Formalising Catastrophic Goodhart</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/vojtakovarik">VojtaKovarik</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/cw2bAC99BRhRe3s5e"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ltckyeplytr9c6iwdffp.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">From Big Ideas To Real-World Results</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/paul-rohde-1">Paul Rohde</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xEkBK3SRrp6DhGf7c"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pc13o6rdk8haadlak7jq.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Rapid Coordination Field Manual</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ampdot">ampdot</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3fKJWML4mgSZmKRm4"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/zedoeymvge56leno7gns.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Ethicophysics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/madhatter-1">MadHatter</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/wtCknhCK4qHdKu98i"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/u7lo7nsviahyumovqkic.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Exploring the Digital Wildnerness</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/bill-benzon">Bill Benzon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Qn8DhSrdA2qPH2eJT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lqm3zsxzkwq8gewtmqic.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Game Theory without Argmax</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cleo-nardo">Cleo Nardo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3QXNgNKXoLrdXJwWE"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qqcvyvug1ovfnpabvpw2.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Value Change Problem (sequence)</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/nora_ammann">Nora_Ammann</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/SAjYaHfCAGzKsjHZp"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ue6lrjd7p9qak0hmunvi.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Large Language Model Psychology</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/quentin-feuillade-montixi">Quentin FEUILLADE--MONTIXI</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/EYjH8M5KLmjuNtJEj"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hh2cjrw72wuidexoj4uq.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Monthly Algorithmic Problems in Mech Interp</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/callummcdougall">CallumMcDougall</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/PCRrXJnQn8XWGMgSp"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/dodotlgx8d6qu4jlk7ao.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Crowdsourced knowledge base</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/iwis">iwis</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/9GExrjksy3hCopmNz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gi9ve7bulu9l4y7erxnm.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Waterloo Rationality Meetups Showcase</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jenn">jenn</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/FFyGrZcJqoECGzdjW"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yzf6ygblg4obhworqd9p.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Consciousness Discourse</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/sil-ver">Rafael Harth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aRt3rA2iqDdRosrbs"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mtzwrkdmtwdnl5pamzcv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">An Opinionated Guide to Computability and Complexity</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/sharmake-farah">Noosphere89</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xvYFPnHWH2HgaJLAa"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/laveuityitueek3myo7m.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Narrative Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/anton-zheltoukhov">Eris</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/SfFQE8DXbgkjk62JK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sydqmaxvgepivxfrz9ct.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Developmental Interpretability</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jesse-hoogland">Jesse Hoogland</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/i5wW3SfeohtTGmNEK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fm1auhxmhvw4ikle3wli.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Meta-rationality</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ricraz">Richard_Ngo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/fYxyZkbxSboLbnJnm"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qwnnvbvyesafgrqujqct.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Catastrophic Risks From AI</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/dan-h">Dan H</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/czrXjvCLsqGepybHC"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qlybo5wtdqpuhy0ads4d.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Distilling Singular Learning Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/liam-carroll">Liam Carroll</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pcdHisDEGLbxrbSHD"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lnhbdq991ckstqnmjcgl.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Towards Causal Foundations of Safe AGI</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/tom4everitt">tom4everitt</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/hCwqaQEqeR9mvYtkC"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/baibconyylqi6zwnlgv3.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">CAIS Philosophy Fellowship Midpoint Deliverables</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/dan-h">Dan H</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xoXeJZRCBEBnBoGbC"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jjxqkjcjwjc2jicmmuww.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Machine Learning For Scientific Discovery </div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eleni-angelou">Eleni Angelou</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/zwfwmyrrz09ydbelj91b.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Replacing fear</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ricraz">Richard_Ngo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/zLib3j2Fdnnx3aP3F"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jddkxtgeufgd3gwzyewz.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Nuts and Bolts of Naturalism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/brienneyudkowsky">LoganStrohl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/sCGfFb5DPfjEmtEdn"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fu4vhd1ud5y4niijbicd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Interpreting a Maze-Solving Network</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/turntrout">TurnTrout</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ypeT2wPARHsyqRE6d"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/m495oy3dchmrqs9cgdvq.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">From Atoms To Agents</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/nhGNHyJHbrofpPbRG"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cdtrtfuhbzmsmt02vw09.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Interpreting Othello-GPT</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/neel-nanda-1">Neel Nanda</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/szTeA3wBxDPLZ4JWk"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ioijls4uq88ywmapn8ww.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Singularity now: is GPT-4 trying to takeover the world?</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/christopher-king">Christopher King</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/kGwLB3LrYFgtqmjCk"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mklxr0kzd9pknuj0ccr1.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">I'm pretty sure AI is as dumb as We Are</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eve-grey">Eve Grey</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/wbPTdgCragwkFNc2T"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/upeottuao2vnnzkf1vju.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">A Data-Driven Path to Effortless Weightloss: Potatoes, Potassium, Drugs, Chocolate, and much much more</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cuoredivetro">CuoreDiVetro</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/WWx8sZ9tE9skptytH"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sfp2p4ev7cgc1rwt3oys.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Shallow Reality of 'Deep Learning Theory'</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jesse-hoogland">Jesse Hoogland</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/mCkMrL9jyR94AAqwW"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ozcq5k5a9bdljajdrzsh.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Leveling Up: advice &amp; resources for junior alignment researchers</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/akash-wasil">Orpheus16</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/GtitGChLssMNnAACK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gnejgf3cwzeh6niek6at.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">LLM Mindreading</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/david-udell">David Udell</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/A9TEiFP8xW8k3ufng"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kg9gupi9exiboyrgruei.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">On Becoming a Great Alignment Researcher (Efficiently)</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jacques-thibodeau">jacquesthibs</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/N7nDePaNabJdnbXeE"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gp0eyntbsd1vx4tszga3.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Simulators</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/janus-1">janus</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/guzvzGnRHzMBWLqKZ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ry3xytraygnpefsjlkn6.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Simulator seminar sequence</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jan-2">Jan</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/kqADGu9teohhezBdu"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vjnyhazarf1rduofemx1.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Bias in Evaluating AGI X-Risks</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/remmelt-ellen">Remmelt</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/vWuwsyNCNzYmtHn5Q"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/o7r8lnz92g9zaxxqzbbj.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Developments toward Uncontrollable AI</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/remmelt-ellen">Remmelt</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xz6SprQmpHzuehFCz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/itbyl1iidb7lcvb61uy7.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Why Not Try Build Safe AGI?</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/remmelt-ellen">Remmelt</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/YsJsYKhXbyobtBJDW"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jjtohtgkqlgwvta5tdef.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Alignment Stream of Thought</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jozdien">Jozdien</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/smzefbor0uw8tzlkj9xj.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Geometric Rationality</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scott-garrabrant">Scott Garrabrant</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/g7jBpyxfuddFNgtbZ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gbws9q8r2asyy6704wmd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Some comments on the CAIS paradigm</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/particlemania">particlemania</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/uTtXe9DJABbmCLWLK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gmr8r6qpgl3yj0lj5ort.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">(Lawrence's) Reflections on Research</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lawrencec">LawrenceC</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/t6KTt378yQHJSbDis"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/j0tstysudzzlzoyvpl22.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Entropy from first principles</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/alex_altair">Alex_Altair</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/h95ayYYwMebGEYN5y"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jtzbqdl3stxouo702bgo.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">[Redwood Research] Causal Scrubbing</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lawrencec">LawrenceC</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/AEbqhmiBcxs5kFv72"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vsa5cnigz9os4xzfymsn.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Generalised models</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HBMLmW9WsgsdZWg4R"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/zjuphaqz788cugxqam6v.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Experiments in instrumental convergence</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/edouard-harris">Edouard Harris</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aK4hyeJyM2sYNS4Yc"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/tlcrrzghctxeom3wxywz.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Research Journals</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/shoshannah-tekofsky">Shoshannah Tekofsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/H3xEgE7bPGKvucfQk"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/b5t9qm0vlokhupbq6lld.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Hypothesis Subspace</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/paul-bricman">Paul Bricman</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/gnAaZtdwjDBBRpDmw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pj40kbneytrownlrszuk.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Maximal Lottery-Lotteries</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scott-garrabrant">Scott Garrabrant</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4WiyAJ2Y7Fuyz8RtM"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lw5ocgbxikjnc96ylwoe.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Thoughts in Philosophy of Science of AI Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/nora_ammann">Nora_Ammann</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TLSzP4xP42PPBctgw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ew9xftwaeblanqm0wags.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">"Why Not Just..."</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/eqtiQjbk83JHyttrr"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nioey0vmsmfdxwfmfzxm.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Meetup in a box</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/screwtape">Screwtape</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZytYxd523oTnBNnRT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/djyavmfiohab552uyym1.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Law-Following AI</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/cullen_okeefe">Cullen</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pJHp3uBgM2EFCoYn3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/vduxv5qcyaegjrqmlwgy.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">My AI Risk Model</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/peterbarnett">peterbarnett</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/nyEFg3AuJpdAozmoX"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/igo7185zypqhuclvbmiv.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Shard Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/quintin-pope">Quintin Pope</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/jiqxd9ZmSSocs5Qcz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/h7t4jrkxka9cz8gcwjjy.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AGI-assisted Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/tor-okland-barstad">Tor Økland Barstad</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/AthjSa2Sm8jGWCbP5"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ekjvgqkg8dhrc7gi3lmh.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Alignment For Foxes</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lone-pine">Lone Pine</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ApA5XmewGQ8wSrv5C"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hx2qe7bqcpznvp28qecr.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Selection Theorems: Modularity</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/callummcdougall">CallumMcDougall</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pfowch7ryfxniAhyb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/wxpkq5bqm0s12iusg5jt.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Breaking Down Goal-Directed Behaviour</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/oliver-sourbut">Oliver Sourbut</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/B9Qc8ifidAtDpsuu8"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kf43lbtk8qsb0k5xf6hk.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">A Tour of AI Timelines</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/anson-ho">anson.ho</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/CB6F6zyLGMhmfitw9"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/l3besfyrxydbqiyyezsf.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Math Upskilling Notes</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/david-udell">David Udell</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Nsms3ub442NYvw9cb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/eqsj2dzshymipot7sjyv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Networking: A Game Manual</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/sts">Severin T. Seehrich</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ogntdnjG6Y9tbLsNS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nbj1jq7brwyuw7ejbajk.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Basic Foundations for Agent Models</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/FaEBwhhe3otzYKGQt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/slevsx1j1haxjctldnrp.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Pragmatic AI Safety</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/dan-h">Dan H</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/opnkEkLCej3mPHwHx"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pmltv9dtszc5wp5kdyqd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Insights from Dath Ilan</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/david-udell">David Udell</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/eAuK3qJd678LFdGyz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/io0c2qfnrtfhrmtwgaqf.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">An Inside View of AI Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/anshuman-radhakrishnan-1">Ansh Radhakrishnan</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/DpZy5s9g2TAKEJwst"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/j4m2sdkmmfgde9s5vmst.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Races and Macrostrategy</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/mtrazzi">Michaël Trazzi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/GyvZkBRf8m6NAccgw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xk0gg6ssrjawsadjjwi6.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Treacherous Turn</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/mtrazzi">Michaël Trazzi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aB9DPaYnktuufLy47"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gsp7brjwlterqhyun9rb.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Inside View (Podcast)</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/mtrazzi">Michaël Trazzi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/rmZt45HAxFFgJ8vEH"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/smnncjswwfimrf7whpzc.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Winding My Way Through Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/david-udell">David Udell</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/jef8ntrWuJ7SvZjCM"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/f2axdsbocokopu8xqlhi.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Interpretability Research for the Most Important Century</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/evan-r-murphy">Evan R. Murphy</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/2TBwjAn2TPyJMDEui"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/dhmcniqhalzr0k4wtzce.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Neural Networks, More than you wanted to Show</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/donald-hobson">Donald Hobson</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/u9uawicHx7Ng7vwxA"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ioryyvk58iumc7rqrxrk.gif"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Concept Extrapolation</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pfxyqvAZ4o4bguAJk"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qeqgeavixfgpmuwra8cd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Calculus in Game and Decision Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/heighn">Heighn</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/KgrG4cQdLtL9DvNr2"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/islbncjmdnjzycymbxsm.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Alignment Stream of Thought</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/leogao">leogao</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/jstoEiuKxHPfASRJK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fikythwrcfe0varcc0o6.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Civilization &amp; Cooperation</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/duncan-sabien-deactivated">Duncan Sabien (Deactivated)</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/T9pBzinPXYB3mxSGi"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/olbwt8pvheukxrffwrjl.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Trends in Machine Learning</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jsevillamol">Jsevillamol</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HMs2yT9D6LjYR5jQT"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lmo4v4vwjfmm17m1yi1r.webp"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Fundamental Uncertainty: A Book</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/gordon-seidoh-worley">Gordon Seidoh Worley</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/q2WQMoQSexx7xqqZR"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nmulhhhejomtinqwokqo.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Intuitive Introduction to Functional Decision Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/heighn">Heighn</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/syfg3bxc7ltctmwh6tdy.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Intro to Brain-Like-AGI Safety</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/steve2152">Steven Byrnes</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/nDjTh6xRPL23YSH6k"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rkfbvapopx8d13qkjcdp.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Mechanics of Tradecraft</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lc">lc</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/CDF4uBtZnef6TMpQ6"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/th1woz0nsth3n8cqr6jn.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Independent AI Research </div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/j-bostock">J Bostock</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aek5ksSs2FHTeofsf"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jzecciql3lon4euljay8.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Agency: What it is and why it matters</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/daniel-kokotajlo">Daniel Kokotajlo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/vLArRpNdkex68oem8"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yuauvyzko4ttusbzpkkz.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Thoughts on Corrigibility</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/turntrout">TurnTrout</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/LLEJJoaYpCoS5JYSY"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/j3kqlrz4cpyilodutaz0.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Epistemic Cookbook for Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/bJi3hd8E8qjBeHz9Z"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/urllfotmjrjlirssmxvb.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Transformative AI and Compute</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lennart">lennart</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xujLGRKFLKsPCTimd"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/kxuwii95ftf9rqlhh8u4.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Safety Subprojects</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/MCYqCMTsDHbn3kcKg"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gbmam3lajmmdgxfnwd6z.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Coordination Frontier</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/gDiScDuMrWNpzwNSJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mckwcocoomsz3kav1rhe.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">D&amp;D.Sci</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abstractapplic">abstractapplic</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Fu7Euu3F96rKhFRWH"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sulofl6pfuwna4z1kelm.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Framing Practicum</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/zCksS4AXFnLBWweC6"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rmcuabpdpjfalf8oujuc.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Rationality in Research</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/j-bostock">J Bostock</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/STQByy4J8NfLKkGQa"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/egwdzq0ns5dahonxsqqm.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Defense in Depth: A Layman's Guide</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/carlos-ramirez">Carlos Ramirez</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aERZoriyHfCqvWkzg"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/igazgiypmjrfxytj6sr7.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Modeling Transformative AI Risk (MTAIR)</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/davidmanheim">Davidmanheim</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HFyami76kSs4vEHqy"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cdk1lbbugck3fuhyqznr.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Practical Guide to Anthropics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/fSMbebQyR4wheRrvk"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hawnw9czray8awc74rnl.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Causes of Power-seeking and Instrumental Convergence</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/turntrout">TurnTrout</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/jyqhEPACWZy8vYfGD"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rwfdloqbow07oxromibo.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">2021 Less Wrong Darwin Game</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/kxs3eeEti9ouwWFzr"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/papqhmf2yk4akum0c96m.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Finite Factored Sets</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scott-garrabrant">Scott Garrabrant</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/z7wp9Bkqc9A9zK53M"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mt8fxuupvxsfbdoyurwx.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Comprehensive Information Gatherings</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Ejxov75SxyPXsATRn"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/l7p6vhx7f9elyi0dmqsn.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Using Credence Calibration for Everything</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/christiankl">ChristianKl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/kmryZRz5r9bjsug9e"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ulp94jg5elf2chyiyk1s.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Anthropic Decision Theory</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/pvXAS868E2BZEc2Fu"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/v28qejntskxxzhein3id.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Reviews for the Alignment Forum</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xqgwpmwDYsn8osoje"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/aqdcs1betbxutz09ovuh.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Notes on Virtues</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/david-gross">David Gross</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/eC3gbMmpvt9LxeDap"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/k1tdbwn2g9y5ypgodvwu.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Participating in a Covid-19 Vaccination Trial</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ejacob">ejacob</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/tYCu3WG89kAW8QmoM"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jxzxbyckaescizy2kqvx.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Predictions &amp; Self-awareness</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/john_maxwell">John_Maxwell</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Gmc7vtnpyKZRHWdt5"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yby8iw13ritasosi2wyh.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Pointing at Normativity</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3dCMdafmKmb6dRjMF"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/wnlmweawfjjklihjixlw.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Counterfactual Planning</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/koen-holtman">Koen.Holtman</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qpvqinidbEE3i73Jd"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/scowd9dmlwwoetx7qg1q.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Alignment Unwrapped</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/5Eg2urmQjA4ZNcezy"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ghpu9dsknv3las9zbkts.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Timelines</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/daniel-kokotajlo">Daniel Kokotajlo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/9TN3i7Q4PPdkx86hA"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lpubrzavuciv5ev2hadh.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Pseudorandomness Contest</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/unexpectedvalues">Eric Neyman</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TjdhvTSptCYakw3Lc"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/tznrkjpde92au0xgrpki.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Bayeswatch</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/weBHYgBXg9thEQNEe"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fn5yd3xegiqolxco9r0f.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Cryonics Signup Guide</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/mingyuan">mingyuan</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/TfPXpiwaESD65Mokx"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cstwcfjqlrfu3t6vp4yz.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">NLP and other Self-Improvement</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/christiankl">ChristianKl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/dZMDxPBZgHzorNDTt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/j5gwomexig10cowr2aqa.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Takeoff and Takeover in the Past and Future</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/daniel-kokotajlo">Daniel Kokotajlo</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/NqMKtBufKW4Wht6rZ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rldwvn4se1tgbxye5ot5.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Forecasting Newsletter</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/nunosempere">NunoSempere</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ReFDRRfGDec4AadbE"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/yctst2bgouu1dbmjhpsr.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Sunzi's《Methods of War》</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/rencyawwfr4rfwt5C"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/otywqf6z3hibsyjzf6hh.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">COVID-19 Updates and Analysis</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/zvi">Zvi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/o58ZMNaovdztbLfvN"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gdsxz69znyhwqmwxscci.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Deconfusing Goal-Directedness</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/B26RwutvaDa6hvJuP"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ymrbpsuqop40irqdqrss.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Grueling Subject</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/christiankl">ChristianKl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/wKPWFvdMyvgDWfusX"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lu1ai6md81d4baooaqke.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">2020 Less Wrong Darwin Game</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/nJzqnsZCfg2k4s2BP"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mnqx0jyutuarhup7dv6r.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Quantitative Finance</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/xezt7HYfpWR6nwp7Z"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/un3g7ivwgsq7a2rjonxg.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Factored Cognition</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/sil-ver">Rafael Harth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/sSaQTybKvfGf4PDNh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/uwnok8aofw3nc8but4ef.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Zen and Rationality</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/gordon-seidoh-worley">Gordon Seidoh Worley</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/akMLzwcRdJNnmBoLa"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/zanxblogtn9p4vdklctd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Privacy Practices</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/wnQWakxdRodnKm5kH"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fzui7w4ivtmerhnpbagd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Staying Sane While Taking Ideas Seriously</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/orthonormal">orthonormal</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/bQ32GY5waKWi88vdX"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hgjwpdw7tn2srnixkq2k.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Naturalized Induction</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/robbbb">Rob Bensinger</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/WZNq65bHHKwQasyP6"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lzlskfwpvezagrmnvs01.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">What You Can and Can't Learn from Games</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/davis_kingsley">Davis_Kingsley</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/qMtriMPLdriNkAfSJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xav5uhzztpedlv5oghj1.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Short Stories</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/DTnoFhDm7ZT2ecJMw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fd5xursprgfsdr2npyuv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Toying With Goal-Directedness</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/T8PftwGSnxgxTzpip"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/awj56iuivlzxw7qljui9.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Against Rationalization II</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/dspeyer">dspeyer</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HmANELvkhAZ9eDxFS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/l7yrnqdlxaxs9zugjlwe.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Implications of Logical Induction</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZnWZRqi5mhXxXaD6z"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cpre6odkeicn9udxubgc.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Through the Haskell Jungle</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/JWD7qZZ5A8CYe82uA"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/c6uuako804bvchtpuvxb.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Lessons from Isaac</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/adamshimi">adamShimi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/5MFN7FnvKFL9ynXXX"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ffdjf7oete6zhk97gpop.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Filk</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/gordon-seidoh-worley">Gordon Seidoh Worley</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/bzf1gg0u70r2med4zl4j.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Subagents and impact measures</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/knbhjv252HshMSwpt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/rzm0l9swjnzkpkhdv3pj.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">If I were a well-intentioned AI...</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/stuart_armstrong">Stuart_Armstrong</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4NFwxwzLzpiikfkk3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ssjjtlezhpn0ohdj70xr.gif"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Moral uncertainty</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/michaela">MichaelA</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/5CNs9wmHWFQTNjFKo"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/n7mmd8y0qqxspgczwlxo.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Medical Paradigms</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/christiankl">ChristianKl</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/C8wgFMCmoMNbvDSMP"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pnnxmjqtobtjigu8npvy.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Understanding Machine Learning</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/sil-ver">Rafael Harth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3xKXGh9RXaYTYZYgZ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hzjcr0aemfscqjqqmcot.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Antimemetics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/lsusr">lsusr</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3hfjaztptwEt2cCve"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/bfiiu2gxl9cmqdzupyq1.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Gears of Aging</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aQTBuq9X98m2KkWpx"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/wqk7v5xrrzsicqeqaq1d.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Map and Territory Cross-Posts</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/gordon-seidoh-worley">Gordon Seidoh Worley</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/CRvxidrCkp7YE7gSK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/brectils3qx8qld40pwx.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Phenomenological AI Alignment</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/gordon-seidoh-worley">Gordon Seidoh Worley</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/BP8vfvg5RhXsBERX9"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hnfysaizcam3hxcolhu4.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Changing your Mind With Memory Reconsolidation</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/mr-hire">Matt Goldenberg</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/cSQHJPrpSvwt9dRyW"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ghbmxx3vlp7guodnnbiq.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">base-line to enlightenment - the physical route to better</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/leggi">leggi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HeYtBkNbEe7wpjc6X"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/lo7y1hicsr26hauyceuy.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Partial Agency</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/8Kc3YamAyaACWXwb3"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/v3ajhz2g8a3smfoe0rzg.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Concept Safety</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/kaj_sotala">Kaj_Sotala</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/YuTinYEzsyHmPoocw"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/af5nlyg8hegpowunhuvh.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Alignment Writing Day 2019</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/benito">Ben Pace</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/GTEay24Lxm3xoE4hy"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/wvbqtcv9ymlfnupzvqbs.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Novum Organum</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ruby">Ruby</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Js5d6ddsCkAQskjbz"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/gs4cvrauqegczcwxkwbp.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Logical Counterfactuals and Proposition graphs</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/donald-hobson">Donald Hobson</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3qa3jAE9sqFqH9okL"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ywdewueykortbn24udxz.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">AI Alignment Writing Day 2018</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/benito">Ben Pace</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/of8ou0qm4hsuzesaajqb.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Daily Insights</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/matthew-barnett">Matthew Barnett</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/onCRFFN7rGXTg3jyc"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nzjkcyvey3hvevcpnrks.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Model Comparison</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/johnswentworth">johnswentworth</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/izfzehxanx48hvf10lnl.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Reframing Impact</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/turntrout">TurnTrout</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/SBfqYgHf2zvxyKDtB"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qd9wvl6nusnr0welcjps.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Alternate Alignment Ideas</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xwf1fjfnznrnjxfed47e.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Concepts in formal epistemology</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/habryka4">habryka</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/96XzQgTL2HBNkBwL4"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xefmk3xj3uotojpo3ckv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">So You Want To Colonize The Universe</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/diffractor">Diffractor</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mctmz7gacncxldcu1wz0.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Mechanism Design</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/badger">badger</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/9s8CgrX5C3AEw9ZM2"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/horzoacdquuitpc6q4mv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Decision Analysis</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/vaniver">Vaniver</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/hs42Xd4joLcTP6uTb"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/r8usyus20ivlpuj8np6i.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Priming</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/sdTckKmNM6zb7yGRt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/mjogxcwbbaclxmmckeiw.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Positivism and Self Deception</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scottalexander">Scott Alexander</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/julk2d57t313v13ouuiv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Kickstarter for Coordinated Action</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/YX6dCo6NSNQJDEwXR"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/hwp3py6c4gaada6ehcrt.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Prediction-Driven Collaborative Reasoning Systems</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/ozziegooen">ozziegooen</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/59wjNdSYeakrjuubu"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/uxa2se2zqqpiugkuvc8t.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Assorted Maths</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/donald-hobson">Donald Hobson</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/jzezzpcw18v4khqsm9am.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Multiagent Models of Mind</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/kaj_sotala">Kaj_Sotala</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/yai5mppkuCHPQmzpN"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xuf9i29cbjdx4nwmwqmd.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Open Threads</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/n44Fqx5W4BhMugCMS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/tmrrkzlxrso98pfuuwdl.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Keith Stanovich: What Intelligence Tests Miss</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/kaj_sotala">Kaj_Sotala</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/q5nsfykzhz81bmljrghs.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Filtered Evidence, Filtered Arguments</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/fgHSwxFitysGKHH56"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xgnadljnuhg4ylqdln3b.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">CDT=EDT?</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/5WF3wmwvxX9TEbFXf"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pjuagn5hgu8m1ry2humq.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Fixed Points</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/scott-garrabrant">Scott Garrabrant</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/ds26mimg1uvv82k5d63v.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Metaethics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ePDpMhJoKCff6qnvh"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/e0chotk2uafu1ic9kvny.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Quantum Physics</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/AmFb5xWbPWWQyQ244"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/nczv7w6hr10v4rumtusv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Ethical Injunctions</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/eliezer_yudkowsky">Eliezer Yudkowsky</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/dT7CKGXwq9vt76CeX"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/b7vwrndrypohpwvcfkpc.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Alignment Newsletter</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/rohinmshah">Rohin Shah</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/zucjLBpQ9S9eWPWGu"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/q4ojwvuezywfaps2ggb6.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Share Models, Not Beliefs</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/benito">Ben Pace</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/ZBNBTSMAXbyJwJoKY"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/txb8l4ka4fq6sofvazkb.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Voting Theory Primer for Rationalists</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/jameson-quinn">Jameson Quinn</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/KGYLvTqFiFE2CpHfJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/fkqj34glr5rquxm6z9sr.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Becoming Stronger</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/turntrout">TurnTrout</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/HcMcHb6Cz7R9RdC5D"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/bh1xoz9srtcz0inmhgce.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Hufflepuff Cynicism</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/abramdemski">abramdemski</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/aoLetzM8xnBPqKBGj"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/sp6frpnna2ygpjbcegci.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Tensions in Truthseeking</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/4C33PKt2cQdA7oyfJ"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/cif8pkhxt31hf3v7eksv.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Murphy's Quest</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/alkjash">alkjash</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/oyZGWX9WkgWzEDt6M"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/t3pxywtw1baepqsjeljw.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Project Hufflepuff</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/9rRrzkBaXcivjZtZS"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/pjqk3risfgu2idnvy4gv.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Instrumental Rationality</div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/cuFK7pjwdDEoQewid"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/qbud3h3pnwtgmjvmizgg.png"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Philosophy Corner</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/charlie-steiner">Charlie Steiner</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/3bbvzoRA8n6ZgbiyK"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/s7y4ljhyfpt4o2quyi7g.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Rational Ritual</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/GcZCMu7ZYHpJCh5bx"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/xck0kfqyngm0qgubpctj.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">The Darwin Game</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/zvi">Zvi</a></span></span></span></div></div></div></span></div><div class="SequencesGridItem-root"><span class="LinkCard-root"><div class="LinkCard-root"><div class="LinkCard-background"><a href="https://www.lesswrong.com/s/WPgA9x5ZvKu9oYvgB"></a></div><div class="SequencesGridItem-image"><img width="315" height="124" src="library_files/bq5jmtw9pxqliaievqab.jpg"></div><div class="SequencesGridItem-meta"><div class="SequencesGridItem-title">Drawing Less Wrong</div><div class="SequencesGridItem-author">by <span><span><span class=""><a class="UsersNameDisplay-noColor" href="https://www.lesswrong.com/users/raemon">Raemon</a></span></span></span></div></div></div></span></div></div></div></div></div></div><div class="Footer-root"></div></div><div class="LWBackgroundImage-root"><div class="LWBackgroundImage-imageColumn"><picture><img loading="lazy" src="library_files/ohabryka_Topographic_aquarelle_book_cover_by_Thomas_W.jpg" class="LWBackgroundImage-backgroundImage"></picture></div></div></div></div></div>

<script>window.ssrRenderedAt = "2025-04-09T19:56:37.127Z"</script>
<script>window.ssrMetadata = {"renderedAt":"2025-04-09T19:56:37.127Z","cacheFriendly":false,"timezone":"America/Toronto"}</script>
<script>window.__APOLLO_STATE__ = {"Revision:j2BCQ3AWkDhGjGst4_biography":{"_id":"j2BCQ3AWkDhGjGst4_biography","__typename":"Revision","version":"1.2.0","updateType":"minor","editedAt":"2025-01-24T20:20:07.307Z","userId":"j2BCQ3AWkDhGjGst4","html":"","commitMessage":"","wordCount":1,"htmlHighlight":"","plaintextDescription":""},"Revision:j2BCQ3AWkDhGjGst4_moderationGuidelines":{"_id":"j2BCQ3AWkDhGjGst4_moderationGuidelines","__typename":"Revision","version":"1.2.0","updateType":"minor","editedAt":"2025-01-24T20:20:07.238Z","userId":"j2BCQ3AWkDhGjGst4","html":"","commitMessage":"","wordCount":1,"htmlHighlight":"","plaintextDescription":""},"User:j2BCQ3AWkDhGjGst4":{"_id":"j2BCQ3AWkDhGjGst4","__typename":"User","beta":null,"email":"als.hate.09@gmail.com","services":{"resume":{},"password":{}},"acceptedTos":false,"pageUrl":"https://www.lesswrong.com/users/ms-haze","banned":null,"isReviewed":true,"nullifyVotes":null,"hideIntercom":false,"hideNavigationSidebar":false,"hideCommunitySection":false,"expandedFrontpageSections":null,"hidePostsRecommendations":false,"currentFrontpageFilter":null,"frontpageSelectedTab":null,"frontpageFilterSettings":{"tags":[{"tagId":"Ng8Gice9KNkncxqcj","tagName":"Rationality","filterMode":10},{"tagId":"3uE2pXvbcnS9nnZRE","tagName":"World Modeling","filterMode":10}],"personalBlog":"Default"},"hideFrontpageFilterSettingsDesktop":false,"allPostsTimeframe":null,"allPostsSorting":null,"allPostsFilter":null,"allPostsShowLowKarma":null,"allPostsIncludeEvents":null,"allPostsHideCommunity":null,"allPostsOpenSettings":null,"draftsListSorting":null,"draftsListShowArchived":null,"draftsListShowShared":null,"lastNotificationsCheck":"2025-04-06T22:40:35.070Z","bannedUserIds":null,"bannedPersonalUserIds":null,"moderationStyle":null,"noKibitz":null,"showHideKarmaOption":null,"markDownPostEditor":true,"hideElicitPredictions":true,"hideAFNonMemberInitialWarning":false,"commentSorting":null,"location":null,"googleLocation":null,"mongoLocation":null,"mapLocation":null,"mapLocationSet":false,"mapMarkerText":null,"htmlMapMarkerText":"","nearbyEventsNotifications":false,"nearbyEventsNotificationsLocation":null,"nearbyEventsNotificationsRadius":null,"nearbyPeopleNotificationThreshold":null,"hideFrontpageMap":null,"emailSubscribedToCurated":null,"subscribedToDigest":false,"unsubscribeFromAll":null,"emails":[{"address":"als.hate.09@gmail.com","verified":true}],"whenConfirmationEmailSent":"2025-01-24T20:19:04.640Z","hideSubscribePoke":false,"hideMeetupsPoke":false,"hideHomeRHS":false,"noCollapseCommentsFrontpage":false,"noCollapseCommentsPosts":true,"noSingleLineComments":true,"showCommunityInRecentDiscussion":false,"karmaChangeNotifierSettings":{"dayOfWeekGMT":"Saturday","timeOfDayGMT":11,"updateFrequency":"daily"},"karmaChangeLastOpened":"2025-04-01T11:00:00.000Z","shortformFeedId":"uvA6dWJ4dEPoCoLzw","viewUnreviewedComments":null,"recommendationSettings":null,"theme":{"name":"default"},"bookmarkedPostsMetadata":[{"postId":"rz73eva3jv267Hy7B"},{"postId":"kcoqwHscvQTx4xgwa"}],"hiddenPostsMetadata":[],"auto_subscribe_to_my_posts":true,"auto_subscribe_to_my_comments":true,"autoSubscribeAsOrganizer":true,"noExpandUnreadCommentsReview":false,"reviewVotesQuadratic":null,"reviewVotesQuadratic2019":null,"reviewVotesQuadratic2020":null,"hideTaggingProgressBar":null,"hideFrontpageBookAd":true,"hideFrontpageBook2019Ad":null,"abTestKey":"j2BCQ3AWkDhGjGst4","abTestOverrides":null,"sortDraftsBy":null,"reactPaletteStyle":"listView","petrovPressedButtonDate":"2024-09-25T15:56:20.671Z","petrovLaunchCodeDate":null,"petrovOptOut":false,"lastUsedTimezone":"America/Toronto","acknowledgedNewUserGuidelines":null,"notificationSubforumUnread":{"email":{"enabled":false,"dayOfWeekGMT":"Monday","timeOfDayGMT":12,"batchingFrequency":"realtime"},"onsite":{"enabled":true,"dayOfWeekGMT":"Monday","timeOfDayGMT":12,"batchingFrequency":"daily"}},"subforumPreferredLayout":null,"hideJobAdUntil":null,"criticismTipsDismissed":false,"allowDatadogSessionReplay":false,"hideFrontpageBook2020Ad":null,"hideDialogueFacilitation":false,"optedInToDialogueFacilitation":false,"revealChecksToAdmins":false,"notificationNewDialogueChecks":{"email":{"enabled":false,"dayOfWeekGMT":"Monday","timeOfDayGMT":12,"batchingFrequency":"realtime"},"onsite":{"enabled":false,"dayOfWeekGMT":"Monday","timeOfDayGMT":12,"batchingFrequency":"realtime"}},"notificationYourTurnMatchForm":{"email":{"enabled":false,"dayOfWeekGMT":"Monday","timeOfDayGMT":12,"batchingFrequency":"realtime"},"onsite":{"enabled":true,"dayOfWeekGMT":"Monday","timeOfDayGMT":12,"batchingFrequency":"realtime"}},"showDialoguesList":true,"showMyDialogues":true,"showMatches":true,"showRecommendedPartners":true,"hideActiveDialogueUsers":false,"hideSunshineSidebar":false,"optedOutOfSurveys":null,"postGlossariesPinned":false,"generateJargonForDrafts":false,"generateJargonForPublishedPosts":true,"oldSlugs":["hate9"],"groups":null,"jobTitle":null,"organization":null,"careerStage":null,"biography":{"__ref":"Revision:j2BCQ3AWkDhGjGst4_biography"},"howOthersCanHelpMe":null,"howICanHelpOthers":null,"profileTagIds":[],"profileTags":[],"organizerOfGroupIds":[],"organizerOfGroups":[],"programParticipation":null,"website":null,"linkedinProfileURL":null,"facebookProfileURL":null,"blueskyProfileURL":null,"twitterProfileURL":null,"githubProfileURL":null,"frontpagePostCount":0,"afSequenceCount":0,"afSequenceDraftCount":0,"sequenceDraftCount":0,"moderationGuidelines":{"__ref":"Revision:j2BCQ3AWkDhGjGst4_moderationGuidelines"},"noindex":false,"paymentEmail":null,"paymentInfo":null,"goodHeartTokens":null,"postingDisabled":null,"allCommentingDisabled":null,"commentingOnOtherUsersDisabled":null,"conversationsDisabled":null,"walledGardenInvite":null,"hideWalledGardenUI":null,"walledGardenPortalOnboarded":null,"taggingDashboardCollapsed":null,"usernameUnset":false,"slug":"ms-haze","createdAt":"2018-05-18T19:37:27.596Z","username":"Hate9","displayName":"Ms. Haze","profileImageId":null,"previousDisplayName":"Hate9","fullName":"Mabel Emillia Schaefer","karma":37,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","postCount":1,"commentCount":12,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ","karmaChanges":{"__typename":"KarmaChanges","totalChange":0,"updateFrequency":"daily","startDate":"2025-04-01T11:00:00.000Z","endDate":"2025-04-09T11:00:00.000Z","nextBatchDate":"2025-04-10T11:00:00.000Z","posts":[],"comments":[],"tagRevisions":[],"todaysKarmaChanges":null,"thisWeeksKarmaChanges":null}},"ROOT_QUERY":{"__typename":"Query","currentUser":{"__ref":"User:j2BCQ3AWkDhGjGst4"},"unreadNotificationCounts":{"__typename":"NotificationCounts","unreadNotifications":0,"unreadPrivateMessages":0,"faviconBadgeNumber":0,"checkedAt":"2025-04-09T19:56:37.183Z"},"llmConversations({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":50,\"userId\":\"j2BCQ3AWkDhGjGst4\",\"view\":\"llmConversationsWithUser\"}}})":{"__typename":"MultiLlmConversationOutput","results":[],"totalCount":null},"notifications({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":20,\"userId\":\"j2BCQ3AWkDhGjGst4\",\"view\":\"userNotifications\"}}})":{"__typename":"MultiNotificationOutput","results":[{"__ref":"Notification:nHcjghsdAi58DzvCG"},{"__ref":"Notification:tmbdRsdSvbYnsk7ry"},{"__ref":"Notification:L8ssetQg5EBzzpf4C"},{"__ref":"Notification:tZhxxdCZ2Xn3rEWqF"},{"__ref":"Notification:bcM88gFKmgzdwsYFG"},{"__ref":"Notification:ybv4vecuAqTbuiykv"},{"__ref":"Notification:wzrebx2txbdqA3ZnH"},{"__ref":"Notification:qxTxsC3PCjbnng9Lg"},{"__ref":"Notification:EG7wG6XPGoBjyDqLi"},{"__ref":"Notification:mEmyxkzdLQEAfestg"},{"__ref":"Notification:TbFEAr7m8TeoQeyae"},{"__ref":"Notification:7rHy9s6S7du3MeRvQ"},{"__ref":"Notification:8LBdb39y8HD5iw9p4"},{"__ref":"Notification:o5S9WswwCKbsnYjgD"},{"__ref":"Notification:gPEg4J86wyrB4Sqaz"},{"__ref":"Notification:Liee7w68tc4Yuayku"},{"__ref":"Notification:dw9eTYsBymSZowtnz"},{"__ref":"Notification:tPX4YyXkMP6fwsgCH"},{"__ref":"Notification:9RaweuBDE3obcwD86"},{"__ref":"Notification:9jbaXFdziWPT6Ziwt"}],"totalCount":null},"user({\"input\":{\"selector\":{\"documentId\":\"j2BCQ3AWkDhGjGst4\"}}})":{"__typename":"SingleUserOutput","result":{"__ref":"User:j2BCQ3AWkDhGjGst4"}},"tag({\"input\":{\"selector\":{\"documentId\":\"3uE2pXvbcnS9nnZRE\"}}})":{"__typename":"SingleTagOutput","result":{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}},"tag({\"input\":{\"selector\":{\"documentId\":\"Ng8Gice9KNkncxqcj\"}}})":{"__typename":"SingleTagOutput","result":{"__ref":"Tag:Ng8Gice9KNkncxqcj"}},"posts({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":2,\"view\":\"globalEvents\"}}})":{"__typename":"MultiPostOutput","results":[{"__ref":"Post:JNL2bmDXmaG7YnRbF"},{"__ref":"Post:JxsdDs8ZfbF4dBkGe"}],"totalCount":null},"tags({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":100,\"view\":\"suggestedFilterTags\"}}})":{"__typename":"MultiTagOutput","results":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"},{"__ref":"Tag:xexCWMyds6QLWognu"},{"__ref":"Tag:fkABsGCJZ6y9qConW"},{"__ref":"Tag:izp6eeJJEg9v5zcur"}],"totalCount":null},"posts({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"globalEvent\":false,\"limit\":2,\"view\":\"events\"}}})":{"__typename":"MultiPostOutput","results":[{"__ref":"Post:87kEhKMAwp5M3dFGf"},{"__ref":"Post:QXcQsQyf25836Mg2w"}],"totalCount":null},"ContinueReading":[{"__typename":"RecommendResumeSequence","sequence":{"__ref":"Sequence:M3TJ2fTCzoQq66NBJ"},"collection":null,"nextPost":{"__ref":"Post:Kow8xRzpfkoY7pa69"},"numRead":3,"numTotal":16,"lastReadTime":"2025-03-12T15:52:01.796Z"},{"__typename":"RecommendResumeSequence","sequence":{"__ref":"Sequence:HeYtBkNbEe7wpjc6X"},"collection":null,"nextPost":{"__ref":"Post:4hdHto3uHejhY2F3Q"},"numRead":2,"numTotal":6,"lastReadTime":"2025-03-21T17:20:14.789Z"}],"spotlights({\"input\":{\"enableCache\":false,\"enableTotal\":false,\"terms\":{\"limit\":1,\"view\":\"mostRecentlyPromotedSpotlights\"}}})":{"__typename":"MultiSpotlightOutput","results":[{"__ref":"Spotlight:vyqRuT9KBYCkhrate"}],"totalCount":null},"RecombeeHybridPosts({\"limit\":15,\"settings\":{\"filterSettings\":{\"personalBlog\":\"Default\",\"tags\":[{\"filterMode\":10,\"tagId\":\"Ng8Gice9KNkncxqcj\",\"tagName\":\"Rationality\"},{\"filterMode\":10,\"tagId\":\"3uE2pXvbcnS9nnZRE\",\"tagName\":\"World Modeling\"}]},\"hybridScenarios\":{\"configurable\":\"recombee-lesswrong-custom\",\"fixed\":\"forum-classic\"},\"rotationRate\":0.2,\"rotationTime\":720,\"scenario\":\"recombee-hybrid\"}})":{"__typename":"RecombeeHybridPostsResult","results":[{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:TpSFoqoG2M5MAAesg"},"scenario":null,"recommId":null,"generatedAt":null,"curated":true,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:deesrjitvXM4xYGZd"},"scenario":null,"recommId":null,"generatedAt":null,"curated":true,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:P5zWiPF5cPJZSkiAK"},"scenario":null,"recommId":null,"generatedAt":null,"curated":true,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:BarHSeciXJqzRuLzw"},"scenario":"recombee-lesswrong-custom","recommId":"b386111161e89a2ac40bf0e2ee0c2725","generatedAt":"2025-04-06T22:40:15.757Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:uwmFSaDMprsFkpWet"},"scenario":"recombee-lesswrong-custom","recommId":"b386111161e89a2ac40bf0e2ee0c2725","generatedAt":"2025-04-06T22:40:15.757Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:njAZwT8nkHnjipJku"},"scenario":"recombee-lesswrong-custom","recommId":"b840652f899a10350fc02ffe5a1861e9","generatedAt":"2025-04-06T22:40:15.138Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:CvKnhXTu9BPcdKE4W"},"scenario":"recombee-lesswrong-custom","recommId":"b840652f899a10350fc02ffe5a1861e9","generatedAt":"2025-04-06T22:40:15.138Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:nnNdz7XQrd5bWTgoP"},"scenario":"recombee-lesswrong-custom","recommId":"b840652f899a10350fc02ffe5a1861e9","generatedAt":"2025-04-06T22:40:15.138Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:y5eapqjYYku8Wt9wn"},"scenario":"recombee-lesswrong-custom","recommId":"b840652f899a10350fc02ffe5a1861e9","generatedAt":"2025-04-06T22:40:15.138Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:nXZi8efFArfk3u568"},"scenario":"recombee-lesswrong-custom","recommId":"b840652f899a10350fc02ffe5a1861e9","generatedAt":"2025-04-06T22:40:15.138Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:fZebqiuZcDfLCgizz"},"scenario":"recombee-lesswrong-custom","recommId":"b840652f899a10350fc02ffe5a1861e9","generatedAt":"2025-04-06T22:40:15.138Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:PMZHfLuQaeFDMQwMx"},"scenario":"recombee-lesswrong-custom","recommId":"b386111161e89a2ac40bf0e2ee0c2725","generatedAt":"2025-04-06T22:40:15.757Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:Mi5kSs2Fyx7KPdqw8"},"scenario":"recombee-lesswrong-custom","recommId":"b386111161e89a2ac40bf0e2ee0c2725","generatedAt":"2025-04-06T22:40:15.757Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:2x7fwbwb35sG8QmEt"},"scenario":"recombee-lesswrong-custom","recommId":"b386111161e89a2ac40bf0e2ee0c2725","generatedAt":"2025-04-06T22:40:15.757Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:aMYFHnCkY4nKDEqfK"},"scenario":"recombee-lesswrong-custom","recommId":"b386111161e89a2ac40bf0e2ee0c2725","generatedAt":"2025-04-06T22:40:15.757Z","curated":null,"stickied":null}]},"PopularComments({\"limit\":3})":{"__typename":"PopularCommentsResult","results":[{"__ref":"Comment:GEM6At7RueGXktgQy"},{"__ref":"Comment:JHaj5Zmar3vZLhHoK"},{"__ref":"Comment:sapL8YaeypDJjXvrE"}]},"comments({\"input\":{\"enableCache\":false,\"enableTotal\":true,\"terms\":{\"limit\":5,\"maxAgeDays\":5,\"showCommunity\":false,\"view\":\"shortformFrontpage\"}}})":{"__typename":"MultiCommentOutput","results":[{"__ref":"Comment:7xsC64sm6FHXezxss"},{"__ref":"Comment:rRXmYxCfn9rYwCaB9"},{"__ref":"Comment:jRQEarCKbxe65gLT7"},{"__ref":"Comment:dajCwRAWrc79JKjQM"},{"__ref":"Comment:Hz3pG6NqRDNqQr3Xc"}],"totalCount":56},"RecentDiscussionFeed({\"af\":false,\"cutoff\":null,\"limit\":10,\"offset\":0})":{"__typename":"RecentDiscussionFeedQueryResults","cutoff":"2025-04-09T19:12:19.961Z","endOffset":10,"results":[{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:9hMYFatQ7XMEzrEi4"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:Z2FfGJh2gAA6EXezp"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:PrcBFPkoRNGWrvdPk"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:tWgXDHWkMYpzAGxPg"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:ZE4xhZHDHHXPuXzxh"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:X6Nx9QzzvDhj8Ek9w"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"subscribeReminder","postCommented":null,"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:nuFw7jNrGkJ8bswZu"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"meetupsPoke","postCommented":null,"shortformCommented":null,"tagDiscussed":null,"tagRevised":null},{"__typename":"RecentDiscussionFeedEntryType","type":"postCommented","postCommented":{"__ref":"Post:TNyEe3bzonreRW9nB"},"shortformCommented":null,"tagDiscussed":null,"tagRevised":null}]},"getCrosspost({\"args\":{\"collectionName\":\"Posts\",\"documentId\":\"cFSDo7GPNMTQdxBQn\",\"fragmentName\":\"PostsList\"}})":{"__typename":"Post","deletedDraft":false,"contents":null,"fmCrosspost":{"hostedHere":false,"isCrosspost":true,"foreignPostId":"9hMYFatQ7XMEzrEi4"},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__typename":"Tag","_id":"oNiQsBHA3i837sySD","userId":"BkbwT5TzSj4aRxJMN","name":"AI safety","shortName":null,"slug":"ai-safety","core":true,"postCount":4432,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-12-22T15:44:26.344Z","wikiOnly":false,"deleted":false,"isSubforum":true,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"4CH9vsvzyk4mSKwyZ","userId":"tKxXWdBF6mbkSpEFx","name":"Career choice","shortName":null,"slug":"career-choice","core":true,"postCount":2391,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-24T16:38:12.387Z","wikiOnly":false,"deleted":false,"isSubforum":true,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"z8qFsGt5iXyZiLbjN","userId":"jd3Bs7YAT2KqnLxYD","name":"Opportunities to take action","shortName":"Opportunities","slug":"opportunities-to-take-action","core":true,"postCount":1435,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-27T20:35:35.035Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"fCcrMpyRbozMfwYPF","userId":"BkbwT5TzSj4aRxJMN","name":"Application announcements","shortName":null,"slug":"application-announcements","core":false,"postCount":418,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-06-07T15:44:45.175Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"ct6dCPfJK9jkJnyAc","userId":"3xoyhbYRgcH8FxyFN","name":"Building the field of AI safety","shortName":null,"slug":"building-the-field-of-ai-safety","core":false,"postCount":197,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-07-28T21:03:21.049Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"2BvgFyR85zX25osTT","userId":"LKQAieBfNe2Jjo4EG","name":"Fellowships and internships","shortName":null,"slug":"fellowships-and-internships","core":false,"postCount":224,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2021-04-20T19:00:49.627Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null}],"socialPreviewData":{"__typename":"SocialPreviewType","_id":"cFSDo7GPNMTQdxBQn","imageUrl":""},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-03-20T02:17:58.049Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-03-20T15:20:22.336Z","meta":false,"postCategory":"post","tagRelevance":{"2BvgFyR85zX25osTT":1,"4CH9vsvzyk4mSKwyZ":1,"ct6dCPfJK9jkJnyAc":1,"fCcrMpyRbozMfwYPF":1,"oNiQsBHA3i837sySD":1,"z8qFsGt5iXyZiLbjN":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":null,"commentCount":0,"voteCount":11,"baseScore":33,"extendedScore":{"love":2},"emojiReactors":{"love":["Agustín Covarrubias 🔸","RubenCastaing"]},"unlisted":false,"score":0.3027394413948059,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2025-03-20T02:17:58.049Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"xrLGXjyPPx3AQgPNy","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"PSBFYGLmnNYkxe7Lx","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{},"afCommentCount":0,"afLastCommentedAt":"2025-03-20T02:17:58.050Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":0,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":0,"annualReviewMarketUrl":"0","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"eaEmojis","disableRecommendation":false,"user":{"__typename":"User","profileImageId":"Profile/hmlznktpwkhovp9a7vey","moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"_id":"xrLGXjyPPx3AQgPNy","slug":"ryan-kidd","createdAt":"2021-09-12T04:20:59.841Z","username":"Ryan Kidd","displayName":"Ryan Kidd","previousDisplayName":null,"fullName":null,"karma":763,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"<ul><li>Co-Executive Director at <a href=\"https://matsprogram.org/\">ML Alignment &amp; Theory Scholars Program<\/a> (2022-present)<\/li><li>Co-Founder &amp; Board Member at <a href=\"https://www.safeai.org.uk/\">London Initiative for Safe AI<\/a> (2023-present)<\/li><li><a href=\"https://manifund.org/RyanKidd\">Manifund Regrantor<\/a> (2023-present) &nbsp;| &nbsp;RFPs <a href=\"https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=uWwdHtsuLDDSJ9h9N\">here<\/a><\/li><li>Advisor, <a href=\"https://www.catalyze-impact.org/\">Catalyze Impact<\/a> (2023-present) &nbsp;| &nbsp;ToC <a href=\"https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=JDcp5AhWwk9ZCv59r\">here<\/a><\/li><li>Advisor, <a href=\"https://www.aisafetyanz.com.au/\">AI Safety ANZ<\/a> (2024-present)<\/li><li>Advisor, <a href=\"https://www.pivotal-research.org/\">Pivotal Research<\/a> (2024-present)<\/li><li>Ph.D. in Physics at the University of Queensland (2017-2023)<\/li><li>Group organizer at Effective Altruism UQ (2018-2021)<\/li><\/ul><p>Give me <a href=\"https://www.admonymous.co/ryankidd44\">feedback<\/a>! :)<\/p>","jobTitle":"Co-Executive Director","organization":"MATS","postCount":17,"commentCount":32,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"jd3Bs7YAT2KqnLxYD"},"coauthors":[],"_id":"cFSDo7GPNMTQdxBQn","slug":"apply-to-mats-8-0","title":"Apply to MATS 8.0!","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"getCrosspost({\"args\":{\"collectionName\":\"Posts\",\"documentId\":\"zJeB33RHEA8QjSeJ4\",\"fragmentName\":\"PostsList\"}})":{"__typename":"Post","deletedDraft":false,"contents":null,"fmCrosspost":{"hostedHere":false,"isCrosspost":true,"foreignPostId":"X6Nx9QzzvDhj8Ek9w"},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__typename":"Tag","_id":"ee66CtAMYurQreWBH","userId":"2kBP4gThRsNXB3WWX","name":"Existential risk","shortName":null,"slug":"existential-risk","core":true,"postCount":3063,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-28T05:56:21.918Z","wikiOnly":false,"deleted":false,"isSubforum":true,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"75HnrKT9FNWKxxtPY","userId":"2kBP4gThRsNXB3WWX","name":"Collections and resources","shortName":null,"slug":"collections-and-resources","core":false,"postCount":756,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-03T11:35:45.467Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},{"__typename":"Tag","_id":"u4bgRHcNJtna6bkEL","userId":"LMgZyi4w3XoYz3tM5","name":"Mental health (cause area)","shortName":null,"slug":"mental-health-cause-area","core":false,"postCount":350,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-01T17:29:27.008Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null}],"socialPreviewData":{"__typename":"SocialPreviewType","_id":"zJeB33RHEA8QjSeJ4","imageUrl":""},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-08T14:27:55.547Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-08T17:22:34.516Z","meta":false,"postCategory":"post","tagRelevance":{"75HnrKT9FNWKxxtPY":1,"ee66CtAMYurQreWBH":1,"u4bgRHcNJtna6bkEL":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":null,"commentCount":1,"voteCount":2,"baseScore":7,"extendedScore":{},"emojiReactors":{},"unlisted":false,"score":1.1043058633804321,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2025-04-08T21:16:52.631Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"tWZtJ8h37ponN4DqK","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"PSBFYGLmnNYkxe7Lx","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{},"afCommentCount":0,"afLastCommentedAt":"2025-04-08T14:27:55.548Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":0,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":0,"annualReviewMarketUrl":"0","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"eaEmojis","disableRecommendation":false,"user":{"__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"_id":"tWZtJ8h37ponN4DqK","slug":"ruby","createdAt":"2015-09-25T22:37:50.581Z","username":"Ruby","displayName":"Ruby","previousDisplayName":null,"fullName":null,"karma":767,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"<p>LessWrong/Lightcone Infrastructure<\/p>","jobTitle":null,"organization":null,"postCount":7,"commentCount":57,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":2,"reviewedByUserId":"9qZsZAzbC2zxsPHzN"},"coauthors":[],"_id":"zJeB33RHEA8QjSeJ4","slug":"a-slow-guide-to-confronting-doom","title":"A Slow Guide to Confronting Doom","draft":null,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"tag({\"input\":{\"selector\":{\"documentId\":\"sYm3HiWcfZvrGu3ui\"}}})":{"__typename":"SingleTagOutput","result":{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}},"tag({\"input\":{\"selector\":{\"documentId\":\"fkABsGCJZ6y9qConW\"}}})":{"__typename":"SingleTagOutput","result":{"__ref":"Tag:fkABsGCJZ6y9qConW"}},"tag({\"input\":{\"selector\":{\"documentId\":\"izp6eeJJEg9v5zcur\"}}})":{"__typename":"SingleTagOutput","result":{"__ref":"Tag:izp6eeJJEg9v5zcur"}},"tag({\"input\":{\"selector\":{\"documentId\":\"xexCWMyds6QLWognu\"}}})":{"__typename":"SingleTagOutput","result":{"__ref":"Tag:xexCWMyds6QLWognu"}},"localgroup({\"input\":{\"selector\":{\"documentId\":\"fFZZ2Ywzsab86EESY\"}}})":{"__typename":"SingleLocalgroupOutput","result":{"__ref":"Localgroup:fFZZ2Ywzsab86EESY"}},"RecombeeHybridPosts({\"limit\":15,\"settings\":{\"filterSettings\":{\"personalBlog\":\"Default\",\"tags\":[{\"filterMode\":10,\"tagId\":\"Ng8Gice9KNkncxqcj\",\"tagName\":\"Rationality\"},{\"filterMode\":10,\"tagId\":\"3uE2pXvbcnS9nnZRE\",\"tagName\":\"World Modeling\"},{\"filterMode\":\"Default\",\"tagId\":\"sYm3HiWcfZvrGu3ui\",\"tagName\":\"AI\"},{\"filterMode\":\"Default\",\"tagId\":\"xexCWMyds6QLWognu\",\"tagName\":\"World Optimization\"},{\"filterMode\":\"Default\",\"tagId\":\"fkABsGCJZ6y9qConW\",\"tagName\":\"Practical\"},{\"filterMode\":\"Default\",\"tagId\":\"izp6eeJJEg9v5zcur\",\"tagName\":\"Community\"}]},\"hybridScenarios\":{\"configurable\":\"recombee-lesswrong-custom\",\"fixed\":\"forum-classic\"},\"rotationRate\":0.2,\"rotationTime\":720,\"scenario\":\"recombee-hybrid\"}})":{"__typename":"RecombeeHybridPostsResult","results":[{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:TpSFoqoG2M5MAAesg"},"scenario":null,"recommId":null,"generatedAt":null,"curated":true,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:deesrjitvXM4xYGZd"},"scenario":null,"recommId":null,"generatedAt":null,"curated":true,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:P5zWiPF5cPJZSkiAK"},"scenario":null,"recommId":null,"generatedAt":null,"curated":true,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:3NdpbA6M5AM2gHvTW"},"scenario":null,"recommId":null,"generatedAt":null,"curated":false,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:Fr4QsQT52RFKHvCAH"},"scenario":null,"recommId":null,"generatedAt":null,"curated":false,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:HjQrRiJeYFFSEfSKX"},"scenario":"recombee-lesswrong-custom","recommId":"bea1b40c8774ff67ad640d0e4ce5b0de","generatedAt":"2025-04-09T19:56:37.791Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:tKwJQbo6SfWF2ifKh"},"scenario":"recombee-lesswrong-custom","recommId":"bea1b40c8774ff67ad640d0e4ce5b0de","generatedAt":"2025-04-09T19:56:37.791Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:6taauM3vtMtojgjom"},"scenario":null,"recommId":null,"generatedAt":null,"curated":false,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:2NGKYt3xdQHwyfGbc"},"scenario":null,"recommId":null,"generatedAt":null,"curated":false,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:kAgJJa3HLSZxsuSrf"},"scenario":"recombee-lesswrong-custom","recommId":"bea1b40c8774ff67ad640d0e4ce5b0de","generatedAt":"2025-04-09T19:56:37.791Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:GfZfDHZHCuYwrHGCd"},"scenario":"recombee-lesswrong-custom","recommId":"bea1b40c8774ff67ad640d0e4ce5b0de","generatedAt":"2025-04-09T19:56:37.791Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:bBdfbWfWxHN9Chjcq"},"scenario":"recombee-lesswrong-custom","recommId":"bea1b40c8774ff67ad640d0e4ce5b0de","generatedAt":"2025-04-09T19:56:37.791Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:XFpDTCHZZ4wpMT8PZ"},"scenario":"recombee-lesswrong-custom","recommId":"bea1b40c8774ff67ad640d0e4ce5b0de","generatedAt":"2025-04-09T19:56:37.791Z","curated":null,"stickied":null},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:LcjuHNxubQqCry9tT"},"scenario":null,"recommId":null,"generatedAt":null,"curated":false,"stickied":false},{"__typename":"RecombeeRecommendedPost","post":{"__ref":"Post:Z2FfGJh2gAA6EXezp"},"scenario":null,"recommId":null,"generatedAt":null,"curated":false,"stickied":false}]}},"Notification:nHcjghsdAi58DzvCG":{"_id":"nHcjghsdAi58DzvCG","__typename":"Notification","documentId":"72r66aTBwiEzunQqT","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-04-06T22:38:54.094Z","link":"/posts/72r66aTBwiEzunQqT/jenn-s-shortform","message":"jenn has created a new post: jenn's Shortform","type":"newPost","viewed":false,"extraData":null},"Notification:tmbdRsdSvbYnsk7ry":{"_id":"tmbdRsdSvbYnsk7ry","__typename":"Notification","documentId":"zhEHa9zoWTBobGdEy","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-04-06T16:59:43.590Z","link":"/events/zhEHa9zoWTBobGdEy/the-colours-of-her-coat","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:L8ssetQg5EBzzpf4C":{"_id":"L8ssetQg5EBzzpf4C","__typename":"Notification","documentId":"McuBsFJFksoo9FiQv","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-04-02T02:37:30.995Z","link":"/events/McuBsFJFksoo9FiQv/baba-is-planmaking","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:tZhxxdCZ2Xn3rEWqF":{"_id":"tZhxxdCZ2Xn3rEWqF","__typename":"Notification","documentId":"cxrwkumdrpbbC93fn","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-03-31T01:12:13.291Z","link":"/posts/cxrwkumdrpbbC93fn/scattered-thoughts","message":"jenn has created a new post: Meetups Notes (Q1 2025)","type":"newPost","viewed":false,"extraData":null},"Notification:bcM88gFKmgzdwsYFG":{"_id":"bcM88gFKmgzdwsYFG","__typename":"Notification","documentId":"nqeLhsAkEgnTwYbbE","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-03-25T23:48:51.086Z","link":"/events/nqeLhsAkEgnTwYbbE/waterloo-acx-meetups-everywhere-spring-2025","message":"jenn has created a new post: Waterloo – ACX Meetups Everywhere Spring 2025","type":"newPost","viewed":false,"extraData":null},"Notification:ybv4vecuAqTbuiykv":{"_id":"ybv4vecuAqTbuiykv","__typename":"Notification","documentId":"fTJgmjhD92ruzBj5t","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-03-25T20:49:52.432Z","link":"/events/fTJgmjhD92ruzBj5t/critically-reading-scott-alexander","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:wzrebx2txbdqA3ZnH":{"_id":"wzrebx2txbdqA3ZnH","__typename":"Notification","documentId":"XgtPQ8HNenLiTa2kx","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-03-22T04:30:29.526Z","link":"/events/XgtPQ8HNenLiTa2kx/2025-acx-spring-megameetup","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:qxTxsC3PCjbnng9Lg":{"_id":"qxTxsC3PCjbnng9Lg","__typename":"Notification","documentId":"xM5MeB6px7Bo62Z76","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-03-04T01:25:44.966Z","link":"/events/xM5MeB6px7Bo62Z76/skillshare-getting-gud-at-groceries","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:EG7wG6XPGoBjyDqLi":{"_id":"EG7wG6XPGoBjyDqLi","__typename":"Notification","documentId":"LAL7GA8tMkREpjHWJ","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-25T01:45:03.301Z","link":"/events/LAL7GA8tMkREpjHWJ/adulthood","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:mEmyxkzdLQEAfestg":{"_id":"mEmyxkzdLQEAfestg","__typename":"Notification","documentId":"DnMFzqrWGK6xKM36Q","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-24T03:30:33.467Z","link":"/events/DnMFzqrWGK6xKM36Q/hpmor-at-10","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:TbFEAr7m8TeoQeyae":{"_id":"TbFEAr7m8TeoQeyae","__typename":"Notification","documentId":"aMg3bWSjNAHD5PCXB","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-21T18:50:04.017Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Notification:7rHy9s6S7du3MeRvQ":{"_id":"7rHy9s6S7du3MeRvQ","__typename":"Notification","documentId":"fMdfHDjjTncgREAyA","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-21T18:49:55.746Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Notification:8LBdb39y8HD5iw9p4":{"_id":"8LBdb39y8HD5iw9p4","__typename":"Notification","documentId":"pJkSkoo5Er4xFivXC","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-21T18:46:57.327Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Notification:o5S9WswwCKbsnYjgD":{"_id":"o5S9WswwCKbsnYjgD","__typename":"Notification","documentId":"stB3xJBT7nmXSmiBH","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-21T18:32:00.186Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Notification:gPEg4J86wyrB4Sqaz":{"_id":"gPEg4J86wyrB4Sqaz","__typename":"Notification","documentId":"CmGMHvkmMkhTzG4Xm","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-21T06:49:20.292Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Notification:Liee7w68tc4Yuayku":{"_id":"Liee7w68tc4Yuayku","__typename":"Notification","documentId":"MTXJqj7x5suspoXej","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-20T23:44:56.791Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Notification:dw9eTYsBymSZowtnz":{"_id":"dw9eTYsBymSZowtnz","__typename":"Notification","documentId":"2y64FSNsXcXtBH8Bu","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-16T17:36:09.303Z","link":"/events/2y64FSNsXcXtBH8Bu/palmer-luckey-american-vulcan","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:tPX4YyXkMP6fwsgCH":{"_id":"tPX4YyXkMP6fwsgCH","__typename":"Notification","documentId":"GjbXGybzszw8eN3oB","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-15T19:08:17.032Z","link":"/posts/GjbXGybzszw8eN3oB/microplastics-much-less-than-you-wanted-to-know","message":"jenn has created a new post: Microplastics: Much Less Than You Wanted To Know","type":"newPost","viewed":false,"extraData":null},"Notification:9RaweuBDE3obcwD86":{"_id":"9RaweuBDE3obcwD86","__typename":"Notification","documentId":"Cm3mLudwDX8mSu4jC","documentType":"post","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-11T01:47:35.202Z","link":"/events/Cm3mLudwDX8mSu4jC/writer-spotlight-zvi-mowshowitz","message":"Kitchener-Waterloo Rationality posted a new event","type":"newEvent","viewed":false,"extraData":null},"Notification:9jbaXFdziWPT6Ziwt":{"_id":"9jbaXFdziWPT6Ziwt","__typename":"Notification","documentId":"DedvgNEeXDghxirLs","documentType":"message","deleted":false,"userId":"j2BCQ3AWkDhGjGst4","createdAt":"2025-02-10T20:39:30.599Z","link":"/inbox/7w5GsybsNyZZuZRvj","message":"Raemon sent you a new message!","type":"newMessage","viewed":false,"extraData":null},"Revision:3uE2pXvbcnS9nnZRE_description":{"_id":"3uE2pXvbcnS9nnZRE_description","__typename":"Revision","htmlHighlight":"<p><strong>World Modeling<\/strong> is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, <i>finding out how the damn thing works,<\/i> and writing it down for the rest of us.<\/p><blockquote><p><i>The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.<\/i><\/p><p>—<a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality\"><u>Twelve Virtues of Rationality<\/u><\/a><\/p><\/blockquote><hr><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Modeling Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics<\/a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics<\/a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology<\/a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics<\/a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming<\/a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers<\/a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews<\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History<\/a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics<\/a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies<\/a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease<\/a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis<\/a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution<\/a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology<\/a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness<\/a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution<\/a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology<\/a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine<\/a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience<\/a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia<\/a><\/p><p><i>Specifics<\/i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus<\/a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence<\/a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review<\/a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models<\/a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science<\/a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets<\/a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism<\/a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><p>&nbsp;<\/p><h2>A definition by elimination<\/h2><p>Properly considered, the overwhelming majority of content LessWrong is about <i>modeling how the world is<\/i>, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of <a href=\"https://www.lesswrong.com/tag/rationality\">Rationality<\/a>, <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a>, ... <\/p>"},"Tag:3uE2pXvbcnS9nnZRE":{"_id":"3uE2pXvbcnS9nnZRE","__typename":"Tag","isRead":false,"parentTag":null,"subTags":[],"description":{"__ref":"Revision:3uE2pXvbcnS9nnZRE_description"},"canVoteOnRels":null,"isArbitalImport":false,"userId":"r38pkCm7wF4M44MDQ","name":"World Modeling","shortName":null,"slug":"world-modeling","core":true,"postCount":5590,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":27,"createdAt":"2020-06-14T22:24:50.898Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isPlaceholderPage":false,"baseScore":1,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"zJtgSyKntXrnkArbY","displayName":"kistune"}]},"score":1,"afBaseScore":0,"afExtendedScore":{"reacts":{},"usersWhoLiked":[]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:Ng8Gice9KNkncxqcj_description":{"_id":"Ng8Gice9KNkncxqcj_description","__typename":"Revision","htmlHighlight":"<p><strong>Rationality<\/strong> is the art of thinking in ways that result in <a href=\"https://www.lesswrong.com/tag/world-modeling\">accurate beliefs<\/a> and <a href=\"https://www.lesswrong.com/tag/decision-theory\">good decisions<\/a>. It is the primary topic of LessWrong.<br><br>Rationality is not only about avoiding the vices of <a href=\"https://www.lesswrong.com/tag/self-deception\">self-deception<\/a> and obfuscation (the failure to <a href=\"https://www.lesswrong.com/tag/conversation-topic\">communicate clearly<\/a>), but also about the virtue of <a href=\"https://www.lesswrong.com/tag/curiosity\">curiosity<\/a>, seeing the world more clearly than before, and <a href=\"https://www.lesswrong.com/tag/ambition\">achieving things<\/a> <a href=\"https://www.lesswrong.com/tag/skill-building\">previously unreachable<\/a> <a href=\"https://www.lesswrong.com/tag/coordination-cooperation\">to you<\/a>. The study of rationality on LessWrong includes a theoretical understanding of ideal cognitive algorithms, as well as building a practice that uses these idealized algorithms to inform <a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\">heuristics<\/a>, <a href=\"https://www.lesswrong.com/tag/habits\">habits<\/a>, and <a href=\"https://www.lesswrong.com/tag/techniques\">techniques<\/a>, to successfully reason and make decisions in the real world.<\/p><p>Topics covered in rationality include (but are not limited to): normative and theoretical explorations of <a href=\"https://www.lesswrong.com/tag/solomonoff-induction\">ideal<\/a> <a href=\"https://www.lesswrong.com/tag/probability-and-statistics\">reasoning<\/a>; the <a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\">capabilities and limitations<\/a> <a href=\"https://www.lesswrong.com/tag/neuroscience\">of our brain<\/a>, <a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\">mind and psychology<\/a>; applied advice such as <a href=\"https://www.lesswrong.com/tag/introspection\">introspection<\/a> techniques and <a href=\"https://www.lesswrong.com/tag/group-rationality\">how to achieve truth collaboratively<\/a>; practical techniques and methodologies for figuring out what’s true ranging from rough quantitative modeling to full research guides.<\/p><p>Note that content about <i>how the world is <\/i>can be found under <a href=\"https://www.lesswrong.com/tag/world-modeling\">World Modeling<\/a>, and practical advice about <i>how to change the world<\/i> is categorized under <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a> or <a href=\"/tag/practical\">Practical<\/a>.<\/p><hr><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Theory / Concepts<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><u>Anticipated Experiences<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/aumann-s-agreement-theorem?showPostCount=true&amp;useTagName=true\">Aumann's Agreement Theorem<\/a><br><a href=\"http://www.lesswrong.com/tag/bayes-theorem?showPostCount=true&amp;useTagName=true\"><u>Bayes Theorem<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/bounded-rationality?showPostCount=true&amp;useTagName=true\">Bounded Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence?showPostCount=true&amp;useTagName=true\">Conservation of Expected<\/a><br><a href=\"http://www.lesswrong.com/tag/contrarianism?showPostCount=true&amp;useTagName=true\">Contrarianism<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/epistemology?showPostCount=true&amp;useTagName=true\"><u>Epistemology<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><u>Gears-Level<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/hansonian-pre-rationality?useTagName=true&amp;showPostCount=true\">Hansonian Pre-Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/law-thinking?showPostCount=true&amp;useTagName=true\">Law-Thinking<\/a><br><a href=\"http://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"http://www.lesswrong.com/tag/occam-s-razor?showPostCount=true&amp;useTagName=true\">Occam's razor<\/a><br><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/truth-semantics-and-meaning?showPostCount=true&amp;useTagName=true\">Truth, Semantics, &amp; Meaning<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Applied Topics<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/alief?showPostCount=true&amp;useTagName=true\"><u>Alief<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\">Betting<\/a><br><a href=\"http://www.lesswrong.com/tag/cached-thoughts?showPostCount=true&amp;useTagName=true\">Cached Thoughts<\/a><br><a href=\"http://www.lesswrong.com/tag/calibration?showPostCount=true&amp;useTagName=true\">Calibration<\/a><br><a href=\"https://www.lesswrong.com/tag/dark-arts?showPostCount=true&amp;useTagName=true\">Dark Arts<\/a><br><a href=\"http://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism<\/a><br><a href=\"http://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\">Epistemic Modesty<\/a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction<\/a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/identity?showPostCount=true&amp;useTagName=true\">Identity<\/a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View<\/a><br><a href=\"http://www.lesswrong.com/tag/introspection?showPostCount=true&amp;useTagName=true\"><u>Introspection<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/intuition?showPostCount=true&amp;useTagName=true\">Intuition<\/a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><u>Practice &amp; Philosophy of Science<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning<\/a><br><a href=\"http://www.lesswrong.com/tag/taking-ideas-seriously?showPostCount=true&amp;useTagName=true\">Taking Ideas Seriously<\/a><br><a href=\"https://www.lesswrong.com/tag/value-of-information?showPostCount=true&amp;useTagName=true\">Value of Information<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Failure Modes<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic<\/a><br><a href=\"https://www.lesswrong.com/tag/bucket-errors?showPostCount=true&amp;useTagName=true\">Bucket Errors<\/a><br><a href=\"https://www.lesswrong.com/tag/compartmentalization?showPostCount=true&amp;useTagName=true\">Compartmentalization<\/a><br><a href=\"https://www.lesswrong.com/tag/confirmation-bias?showPostCount=true&amp;useTagName=true\"><u>Confirmation Bias<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/logical-fallacies?showPostCount=true&amp;useTagName=true\">Fallacies<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart’s Law<\/a><br><a href=\"http://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><u>Groupthink<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\">Heuristics and Biases<\/a><br><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy?showPostCount=true&amp;useTagName=true\">Mind Projection Fallacy<\/a><br><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><u>Motivated Reasoning<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true&amp;useTagName=true\">Pica<\/a><br><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality?showPostCount=true&amp;useTagName=true\">Pitfalls of Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalization<\/a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\">Self-Deception<\/a><br><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy?showPostCount=true&amp;useTagName=true\">Sunk-Cost Fallacy<\/a><\/p><\/td><\/tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Communication<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true\"><u>Conversation<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing?showPostCount=true&amp;useTagName=true\"><u>Decoupling vs Contextualizing<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/disagreement?showPostCount=true&amp;useTagName=true\"><u>Disagreement<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy<\/a><br><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/good-explanations-advice?showPostCount=true&amp;useTagName=true\">Good Explanations (Advice)<\/a><br><a href=\"http://www.lesswrong.com/tag/ideological-turing-tests?showPostCount=true&amp;useTagName=true\">Ideological Turing Tests<\/a><br><a href=\"https://www.lesswrong.com/tag/inferential-distance?showPostCount=true&amp;useTagName=true\">Inferential Distance<\/a><br><a href=\"https://www.lesswrong.com/tag/information-cascades?showPostCount=true&amp;useTagName=true\">Information Cascades<\/a><br><a href=\"https://www.lesswrong.com/tag/memetic-immune-system?showPostCount=true&amp;useTagName=true\">Memetic Immune System<\/a><br><a href=\"https://www.lesswrong.com/tag/philosophy-of-language?showPostCount=true&amp;useTagName=true\"><u>Philos<\/u><\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>... "},"Tag:Ng8Gice9KNkncxqcj":{"_id":"Ng8Gice9KNkncxqcj","__typename":"Tag","isRead":false,"parentTag":null,"subTags":[],"description":{"__ref":"Revision:Ng8Gice9KNkncxqcj_description"},"canVoteOnRels":null,"isArbitalImport":false,"userId":"r38pkCm7wF4M44MDQ","name":"Rationality","shortName":null,"slug":"rationality","core":true,"postCount":4150,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":100,"createdAt":"2020-06-14T22:24:17.072Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Revision:KzQ5FWYBGdg6iiNx5":{"_id":"KzQ5FWYBGdg6iiNx5","__typename":"Revision","htmlHighlight":"<p>MAISU starts with an Opening session on April 18th (Friday), but most of the sessions will happen during April 19th-21th. You’re welcome to join as much or little as you want.<\/p><p>The event is for anyone who wants to help prevent AI-driven catastrophe. Other than that, we’re open to all perspectives. However each individual session will typically have narrower scope. When attending a session please respect the specific scope of that session.<\/p><p>Because this is an Unconference, the schedule is open to anyone to add sessions. Expect the schedule to continuously expand as the event approaches.&nbsp;<\/p><h1><a href=\"https://docs.google.com/document/d/1sHnzJBu6dBiGwgdtO4W14bz5Jy7eKizEh8Vun0mVM-s/edit?tab=t.0#heading=h.h88drtev6cdv\">Official website<\/a><\/h1><p>The official website is guaranteed to have the latest information. This LessWrong event post might be updated less frequently.<\/p><h1><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScXDl8z-kTyhR8zxQsgO0V0sfKrk_H4RfZ0xELpDpD81PDVwA/viewform\"><u>Register here<\/u><\/a><\/h1><ul><li>If you register, you will receive email updates about the event.<\/li><li>You can attend without registering, but you might miss important information.<\/li><\/ul><p><a href=\"https://docs.google.com/forms/d/e/1FAIpQLScXDl8z-kTyhR8zxQsgO0V0sfKrk_H4RfZ0xELpDpD81PDVwA/viewanalytics\"><u>Click here<\/u><\/a> to see who else has registered.<\/p><h1><a href=\"https://teamup.com/ks42i9htheq3s52i27\"><u>Schedule<\/u><\/a> &nbsp; &nbsp;&nbsp;<\/h1><p>Don’t worry if it looks sparse now. More sessions will be added (maybe by you?), this is how unconferences work.<\/p><p>You can put anything you want on the schedule, any type of session, as long as it’s safety relevant.<\/p><ul><li>If you put something on the schedule you’re responsible for hosting it.<\/li><li>When scheduling something, please make sure it’s clear how it’s relevant to AI Safety (unless it’s a purely fun/social session).<\/li><\/ul><p><strong>Some examples of things you can host<\/strong><\/p><ul><li>A talk presenting …<ul><li>… your results<\/li><li>… other peoples results that you find interesting<\/li><li>… an intro level overview of some subset of AI safety<\/li><li>… an in-progress project you want feedback on<\/li><\/ul><\/li><li>An open discussion on a topic you’re interested in<\/li><li>A fun game, or other social activity<\/li><li>Some type of exercise that you find useful for thinking about AI safety<\/li><li>Sharing your hot takes with an open invitation to debate you<\/li><\/ul><p><strong>General advice<\/strong><\/p><ul><li>If you host a talk, make sure to leave plenty of time for discussions afterwards<\/li><li>1h hour is a good length for most sessions<\/li><\/ul><h3><strong>Warning!<\/strong><br>TeamUp (the scheduling app we’re using) does not automatically update. You need to manually refresh the schedule, or you might miss things.&nbsp;<\/h3><p>Sessions will be added continuously to the schedule. There will likely be some last minute additions and/or changes even after the event has started.<\/p><h1>Discussions &amp; Networking in the&nbsp;<a href=\"https://join.slack.com/t/ai-alignment/shared_invite/zt-1d379kvgh-3vNjSOmnd_3HsoKJxjcyFg\"><u>AI Alignment Slack<\/u><\/a><\/h1><p>We don’t have an event-specific Slack or Discord, instead we’re invading the AI Alignment Slack for... <\/p>","plaintextDescription":"MAISU starts with an Opening session on April 18th (Friday), but most of the sessions will happen during April 19th-21th. You’re welcome to join as much or little as you want.\n\nThe event is for anyone who wants to help prevent AI-driven catastrophe. Other than that, we’re open to all perspectives. However each individual session will typically have narrower scope. When attending a session please respect the specific scope of that session.\n\nBecause this is an Unconference, the schedule is open to anyone to add sessions. Expect the schedule to continuously expand as the event approaches. \n\n\nOfficial website\nThe official website is guaranteed to have the latest information. This LessWrong event post might be updated less frequently.\n\n\nRegister here\n * If you register, you will receive email updates about the event.\n * You can attend without registering, but you might miss important information.\n\nClick here to see who else has registered.\n\n\nSchedule     \nDon’t worry if it looks sparse now. More sessions will be added (maybe by you?), this is how unconferences work.\n\nYou can put anything you want on the schedule, any type of session, as long as it’s safety relevant.\n\n * If you put something on the schedule you’re responsible for hosting it.\n * When scheduling something, please make sure it’s clear how it’s relevant to AI Safety (unless it’s a purely fun/social session).\n\nSome examples of things you can host\n\n * A talk presenting …\n   * … your results\n   * … other peoples results that you find interesting\n   * … an intro level overview of some subset of AI safety\n   * … an in-progress project you want feedback on\n * An open discussion on a topic you’re interested in\n * A fun game, or other social activity\n * Some type of exercise that you find useful for thinking about AI safety\n * Sharing your hot takes with an open invitation to debate you\n\nGeneral advice\n\n * If you host a talk, make sure to leave plenty of time for discussions afterwards\n * 1h hour is a good length for","wordCount":536,"version":"1.1.0"},"Tag:sYm3HiWcfZvrGu3ui":{"_id":"sYm3HiWcfZvrGu3ui","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"AI","shortName":null,"slug":"ai","core":true,"postCount":11587,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":2000,"createdAt":"2020-06-14T22:24:22.097Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":10,"extendedScore":null,"score":10,"afBaseScore":2,"afExtendedScore":null,"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null,"isRead":false,"parentTag":null,"subTags":[],"description":{"__ref":"Revision:sYm3HiWcfZvrGu3ui_description"},"canVoteOnRels":null},"SocialPreviewType:JNL2bmDXmaG7YnRbF":{"_id":"JNL2bmDXmaG7YnRbF","__typename":"SocialPreviewType","imageUrl":""},"User:TitFePXAAgwc5C7oa":{"_id":"TitFePXAAgwc5C7oa","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":false,"slug":"linda-linsefors","createdAt":"2017-09-28T18:01:14.327Z","username":"Linda Linsefors","displayName":"Linda Linsefors","previousDisplayName":null,"fullName":"Linda Linsefors","karma":2459,"afKarma":332,"deleted":false,"isAdmin":false,"htmlBio":"<p>Hi, I am a Physicist, an Effective Altruist and AI Safety student/researcher.<\/p>","jobTitle":null,"organization":null,"postCount":47,"commentCount":304,"sequenceCount":0,"afPostCount":25,"afCommentCount":110,"spamRiskScore":1,"tagRevisionCount":2,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:JNL2bmDXmaG7YnRbF":{"_id":"JNL2bmDXmaG7YnRbF","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:KzQ5FWYBGdg6iiNx5"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:JNL2bmDXmaG7YnRbF"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-02-21T11:36:25.202Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":{"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"KzQ5FWYBGdg6iiNx5","commentCount":0,"voteCount":5,"baseScore":18,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":5,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.005906057544052601,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2025-02-21T11:36:25.202Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"TitFePXAAgwc5C7oa","location":null,"googleLocation":null,"onlineEvent":true,"globalEvent":false,"startTime":"2025-04-18T17:00:00.000Z","endTime":"2025-04-21T22:00:00.000Z","localStartTime":null,"localEndTime":null,"eventRegistrationLink":"https://docs.google.com/forms/d/e/1FAIpQLScXDl8z-kTyhR8zxQsgO0V0sfKrk_H4RfZ0xELpDpD81PDVwA/viewform","joinEventLink":"https://docs.google.com/document/d/1sHnzJBu6dBiGwgdtO4W14bz5Jy7eKizEh8Vun0mVM-s/edit?tab=t.0#heading=h.h88drtev6cdv","facebookLink":null,"meetupLink":null,"website":"https://docs.google.com/document/d/1sHnzJBu6dBiGwgdtO4W14bz5Jy7eKizEh8Vun0mVM-s/edit?tab=t.0#heading=h.xqf605jotm2e","contactInfo":null,"isEvent":true,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":9,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":5,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-02-21T11:36:25.202Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":false,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:TitFePXAAgwc5C7oa"},"coauthors":[],"slug":"maisu-minimal-ai-safety-unconference","title":"MAISU - Minimal AI Safety Unconference ","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:cpakfyNrpBu2jfyCj":{"_id":"cpakfyNrpBu2jfyCj","__typename":"Revision","htmlHighlight":"<p><strong>Get notified when we are open for applications:<\/strong> <a href=\"https://airtable.com/app16N8kPLey1bQe1/shrPo6yVHYZknKmzV\">Email<\/a> <a href=\"https://t.me/+Cm9B-LAbITY0MTE0\">Telegram<\/a><br>We rely and trust on word of mouth - <strong>Please help spread the word.<\/strong><\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/f2vFYmwFGu3HLL9QB/mo7mzlxarv8ktsayupyi\" alt=\"\"><\/figure><p>Join the 12th Less Wrong Community Weekend (LWCW) in Berlin. This is the world’s largest rationalist social gathering which brings together 250+ aspiring rationalists from across Europe and beyond for 4 days of intellectual exploration, socialising and fun.<\/p><p>We will be taking over the whole hostel with a huge variety of spaces inside and outside to talk, relax, dance, play, learn, teach, connect, cuddle, practice, share ... - simply enjoy life together our way.<\/p><p>We invite everyone who shares a <strong>curiosity<\/strong> for new perspectives to gain a truthful understanding of the world and its inhabitants, a <strong>passion<\/strong> for developing practices and systems that achieve our personal goals and, consequently, those of humanity at large as well as a desire to nurture <strong>empathetic relationships<\/strong> that support and inspire us on our journey.<\/p><p>The <strong>content will be participant driven<\/strong> in an unconference style:&nbsp;on Friday afternoon we put up 12 wall-sized daily planners and by Saturday morning the attendees fill them up with 100+ workshops, talks and activities of their own devising. The high quality sessions that others benefit most from are prepared upfront, but when inspiration hits some are just made up on the spot.<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/ynafv4wrm8eku5xfthms\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/dhrhwe7hewq8njfipa7x 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/wad0dbewem0mylba0hfp 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/pbvq0wrys4heufonxabq 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/jzus1yi81epqrqqemu64 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/j5q8qo9ftibiswclvle1 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/lsz6wosy9yvfkjwl43r8 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/fszo0xlpvhjrbh573gdp 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/q0udaoz1cfmctq2q0cgw 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/lbmfvijylnz0fddds5op 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/fopCdoA8YqJ8Rq6qK/w6ik6tplgech9xmniexy 1200w\"><\/figure><p>Previous years’ schedules have included…<\/p><figure class=\"table\"><table><tbody><tr><td style=\"width:500px\"><ul><li>Double Cruxing<\/li><li>Hamming Circles<\/li><li>Gendlin Focusing<\/li><li>Applied Rationality workshops<\/li><li>Circling<\/li><li>Authentic Relating games<\/li><li>Improvisation theater<\/li><li>Introduction to stand up comedy<\/li><li>Writing rationalist fiction<\/li><li>Dance workshops<\/li><li>Acapella singing<\/li><li>Icebreaker games<\/li><li>Lightning talks<\/li><li>Celebrating failure groups<\/li><li>Giant outdoor chess Penultima<\/li><li>Dungeons &amp; Dragons<\/li><li>Kung Fu basics<\/li><li>Board games<\/li><\/ul><\/td><td style=\"width:500px\"><ul><li>Breathwork workshops<\/li><li>Ecstatic dancing<\/li><li>Radical Honesty workshops<\/li><li>Playfighting for adults<\/li><li>Polyamory and relationships workshops<\/li><li>Sex Q&amp;A roundtable<\/li><li>Quantified self workshops<\/li><li>Moral philosophy debates<\/li><li>AI safety Q&amp;A<\/li><li>How to handle fear of AI Doom<\/li><li>Value drift in EA<\/li><li>The neurobiology of psychedelics<\/li><li>The science of longevity<\/li><li>Morning runs and yoga<\/li><li>Meditation in the rooftop winter garden<\/li><li>Night time swimming<\/li><li>Bedtime story readings<\/li><\/ul><\/td><\/tr><\/tbody><\/table><\/figure><p><i>Personal note from <\/i><a href=\"https://www.lesswrong.com/users/henry-prowbell\"><i>Henry<\/i><\/a><i>: If things like ecstatic dancing, radical honesty and polyamory workshops sound too intense for you, rest assured everything is optional. I’m a nerd and very awkward so a lot of this stuff terrifies me.<\/i><\/p><p>The event takes place in the natural environs... <\/p>","plaintextDescription":"Get notified when we are open for applications: Email Telegram\nWe rely and trust on word of mouth - Please help spread the word.\n\nJoin the 12th Less Wrong Community Weekend (LWCW) in Berlin. This is the world’s largest rationalist social gathering which brings together 250+ aspiring rationalists from across Europe and beyond for 4 days of intellectual exploration, socialising and fun.\n\nWe will be taking over the whole hostel with a huge variety of spaces inside and outside to talk, relax, dance, play, learn, teach, connect, cuddle, practice, share ... - simply enjoy life together our way.\n\nWe invite everyone who shares a curiosity for new perspectives to gain a truthful understanding of the world and its inhabitants, a passion for developing practices and systems that achieve our personal goals and, consequently, those of humanity at large as well as a desire to nurture empathetic relationships that support and inspire us on our journey.\n\nThe content will be participant driven in an unconference style: on Friday afternoon we put up 12 wall-sized daily planners and by Saturday morning the attendees fill them up with 100+ workshops, talks and activities of their own devising. The high quality sessions that others benefit most from are prepared upfront, but when inspiration hits some are just made up on the spot.\n\nPrevious years’ schedules have included…\n\n * Double Cruxing\n * Hamming Circles\n * Gendlin Focusing\n * Applied Rationality workshops\n * Circling\n * Authentic Relating games\n * Improvisation theater\n * Introduction to stand up comedy\n * Writing rationalist fiction\n * Dance workshops\n * Acapella singing\n * Icebreaker games\n * Lightning talks\n * Celebrating failure groups\n * Giant outdoor chess Penultima\n * Dungeons & Dragons\n * Kung Fu basics\n * Board games\n\n * Breathwork workshops\n * Ecstatic dancing\n * Radical Honesty workshops\n * Playfighting for adults\n * Polyamory and relationships workshops\n * Sex Q&A roundtable\n * Quantified self workshops\n * Moral philos","wordCount":1802,"version":"1.1.1"},"Tag:izp6eeJJEg9v5zcur":{"_id":"izp6eeJJEg9v5zcur","__typename":"Tag","userId":"XtphY3uYHwruKqDyG","name":"Community","shortName":null,"slug":"community","core":true,"postCount":2370,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":15,"createdAt":"2020-06-14T03:38:34.631Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null,"isRead":false,"parentTag":null,"subTags":[],"description":{"__ref":"Revision:izp6eeJJEg9v5zcur_description"},"canVoteOnRels":null},"SocialPreviewType:JxsdDs8ZfbF4dBkGe":{"_id":"JxsdDs8ZfbF4dBkGe","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/f2vFYmwFGu3HLL9QB/mo7mzlxarv8ktsayupyi"},"Localgroup:MGAtkuYmX3hZ6eeaw":{"_id":"MGAtkuYmX3hZ6eeaw","__typename":"Localgroup","name":"LessWrong Berlin","organizerIds":["vbDMpDA5A35329Ju5","AeqmHjYoZ4gvvvjHN","DntTEDBpz74waPc8Z","xn6q3M2HvfBAmcTmc","9QxLZvSHM63kYdWkz","xnuFrMz8Hx2TJE5ti"]},"User:2DC5Z74RysBRi7HwK":{"_id":"2DC5Z74RysBRi7HwK","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"jt-1","createdAt":"2023-04-18T11:45:54.012Z","username":"jt","displayName":"jt","previousDisplayName":null,"fullName":"JT","karma":201,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":3,"commentCount":5,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Post:JxsdDs8ZfbF4dBkGe":{"_id":"JxsdDs8ZfbF4dBkGe","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:cpakfyNrpBu2jfyCj"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":7,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:izp6eeJJEg9v5zcur"}],"socialPreviewData":{"__ref":"SocialPreviewType:JxsdDs8ZfbF4dBkGe"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2024-12-27T15:50:22.918Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":{"izp6eeJJEg9v5zcur":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"cpakfyNrpBu2jfyCj","commentCount":0,"voteCount":32,"baseScore":68,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":32,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.008673758246004581,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2024-12-27T15:50:22.918Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"2DC5Z74RysBRi7HwK","location":"DJH Jugendherberge Berlin - Am Wannsee, Badeweg, Berlin, Germany","googleLocation":{"url":"https://maps.google.com/?cid=8587459843782966624","icon":"https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/lodging-71.png","name":"Jugendherberge Berlin - Am Wannsee","types":["lodging","point_of_interest","establishment"],"photos":[{"width":4000,"height":3000,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/103852221225290988494\">Dorota Pirga<\/a>"]},{"width":4032,"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/117539867409261883160\">Mike Federhenn-Fett<\/a>"]},{"width":4160,"height":3120,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/108191693132357058122\">Gabriel Fonseca<\/a>"]},{"width":3024,"height":4032,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/104359235866694930756\">Oliver Bierhaus<\/a>"]},{"width":4032,"height":2268,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/110694942147990270203\">Fathi El-Khatib<\/a>"]},{"width":4032,"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/112491686055719851120\">Ke No<\/a>"]},{"width":4032,"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/117539867409261883160\">Mike Federhenn-Fett<\/a>"]},{"width":4032,"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/104928247261543099728\">Michal Tušl<\/a>"]},{"width":4032,"height":3024,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/104997856249407701644\">Wolfgang Krause-Riedel<\/a>"]},{"width":4608,"height":2592,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/113494224842505259459\">Heiko H.<\/a>"]}],"rating":3.2,"reviews":[{"text":"We knew it's going to be a youth hostel, so overall you get the youth hostel value for your money. But they don't speak English at the reception, one day my friends simply asked the receptionist about the type of food they have for dinner, the receptionist misunderstood and ordered our whole group dinner... Obviously we didn't pay for that but since they spoke very-very limited English, they couldn't even explain anything or argue properly. It's a pity because otherwise the food is nice for example. It's shocking to me that they don't allow people to use the elevator, only in very exceptional circumstances. It's also quite far from any station or bus stop, and taxi is a ripoff and they tried to cheat, and also didn't speak English almost at all.","time":1710713729,"rating":3,"language":"en","author_url":"https://www.google.com/maps/contrib/115557717346684405980/reviews","author_name":"Veronika Németh-Városi","profile_photo_url":"https://lh3.googleusercontent.com/a-/ALV-UjXygkRTllmtqWQo0CZZhdCDm0daeqwCdRCQ7BgtoOfepoPKzZZ74A=s128-c0x00000000-cc-rp-mo-ba6","relative_time_description":"a month ago"},{"text":"The condition of the room is bad, bare concrete, splintered wood and worn out furniture. The shower is shared between two rooms and you need to lock your door to the shower in order to prevent people from going into your room. My bedsheets were dirty and when I went to replace them they gave me dirty sheets again. They promise high speed WiFi but it's barely enough to send a text message","time":1698909813,"rating":1,"language":"en","author_url":"https://www.google.com/maps/contrib/108437153998963115739/reviews","author_name":"Volen Conev","profile_photo_url":"https://lh3.googleusercontent.com/a-/ALV-UjWdhXRNd_BsEzeza_Y9kEbGnIku1nOHTgPObfwKIaSBz2xHlEMz=s128-c0x00000000-cc-rp-mo","relative_time_description":"6 months ago"},{"text":"Insane good Jugendherberge! I stayed there from Friday evening to Sunday lunch. Never had such a big variety of things. First of all the breakfast & dinner ALSO VEGAN ","time":1698587523,"rating":5,"language":"en","author_url":"https://www.google.com/maps/contrib/112383602919287169530/reviews","author_name":"TheTake","profile_photo_url":"https://lh3.googleusercontent.com/a/ACg8ocIA0maPb6BglEjxrojyVfi6ReDx-AYKQ79BTHVaJM6_zwvERw=s128-c0x00000000-cc-rp-mo","relative_time_description":"6 months ago"},{"text":"Great place for youth. Clean, well kept rooms, good service. Peaceful lakeside location, absolute quiet, and in close proximity to the center of berlin.","time":1707204827,"rating":5,"language":"en","author_url":"https://www.google.com/maps/contrib/115220160254237585640/reviews","author_name":"Thomas Andersen","profile_photo_url":"https://lh3.googleusercontent.com/a-/ALV-UjUO3nav-PqSrVLVF3DjgFYQntU_OYnDh1pACGnOfPNbgfqdAvNc=s128-c0x00000000-cc-rp-mo-ba2","relative_time_description":"2 months ago"},{"text":"It was good but there wasn't a toilet in the bedroom.. ohh that was too stressful.\nOhhh, and there wasn't also the lift.   Our room was on the 4th floor...😐","time":1699307919,"rating":3,"language":"en","author_url":"https://www.google.com/maps/contrib/112760164859712817850/reviews","author_name":"Mrs Khan","profile_photo_url":"https://lh3.googleusercontent.com/a-/ALV-UjXd-VQi7384j6wSr0QViwBotOQfoKa9qKG3uj3dZkxtpwvn4GpkAw=s128-c0x00000000-cc-rp-mo","relative_time_description":"5 months ago"}],"website":"http://www.jh-wannsee.de/","geometry":{"location":{"lat":52.43265950000001,"lng":13.1852799},"viewport":{"east":13.1868176802915,"west":13.1841197197085,"north":52.43405093029151,"south":52.43135296970851}},"place_id":"ChIJGZQB8gFZqEcRYH2dxzrJLHc","vicinity":"Badeweg 1, Berlin","plus_code":{"global_code":"9F4MC5MP+34","compound_code":"C5MP+34 Berlin, Germany"},"reference":"ChIJGZQB8gFZqEcRYH2dxzrJLHc","utc_offset":120,"adr_address":"<span class=\"street-address\">Badeweg 1<\/span>, <span class=\"postal-code\">14129<\/span> <span class=\"locality\">Berlin<\/span>, <span class=\"country-name\">Germany<\/span>","opening_hours":{"periods":[{"open":{"day":0,"time":"0000","hours":0,"minutes":0,"nextDate":1714860000000}}],"open_now":true,"weekday_text":["Monday: Open 24 hours","Tuesday: Open 24 hours","Wednesday: Open 24 hours","Thursday: Open 24 hours","Friday: Open 24 hours","Saturday: Open 24 hours","Sunday: Open 24 hours"]},"business_status":"OPERATIONAL","formatted_address":"Badeweg 1, 14129 Berlin, Germany","html_attributions":[],"address_components":[{"types":["street_number"],"long_name":"1","short_name":"1"},{"types":["route"],"long_name":"Badeweg","short_name":"Badeweg"},{"types":["sublocality_level_1","sublocality","political"],"long_name":"Bezirk Steglitz-Zehlendorf","short_name":"Bezirk Steglitz-Zehlendorf"},{"types":["locality","political"],"long_name":"Berlin","short_name":"Berlin"},{"types":["administrative_area_level_3","political"],"long_name":"Kreisfreie Stadt Berlin","short_name":"Kreisfreie Stadt Berlin"},{"types":["administrative_area_level_1","political"],"long_name":"Berlin","short_name":"BE"},{"types":["country","political"],"long_name":"Germany","short_name":"DE"},{"types":["postal_code"],"long_name":"14129","short_name":"14129"}],"icon_mask_base_uri":"https://maps.gstatic.com/mapfiles/place_api/icons/v2/hotel_pinlet","user_ratings_total":1015,"utc_offset_minutes":120,"current_opening_hours":{"periods":[{"open":{"day":3,"date":"2024-05-01","time":"0000","truncated":true},"close":{"day":2,"date":"2024-05-07","time":"2359","truncated":true}}],"open_now":true,"weekday_text":["Monday: Open 24 hours","Tuesday: Open 24 hours","Wednesday: Open 24 hours","Thursday: Open 24 hours","Friday: Open 24 hours","Saturday: Open 24 hours","Sunday: Open 24 hours"]},"icon_background_color":"#909CE1","formatted_phone_number":"030 8032034","international_phone_number":"+49 30 8032034"},"onlineEvent":false,"globalEvent":true,"startTime":"2025-08-29T10:00:00.000Z","endTime":"2025-09-01T09:00:00.000Z","localStartTime":"2025-08-29T12:00:00.000Z","localEndTime":"2025-09-01T11:00:00.000Z","eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":"lwcw.europe[at]gmail.com","isEvent":true,"eventImageId":null,"eventType":"conference","types":["SSC","EA","MIRIx","LW"],"groupId":"MGAtkuYmX3hZ6eeaw","reviewedByUserId":null,"suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":13,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":8,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-12-27T15:50:23.085Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":false,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":{"__ref":"Localgroup:MGAtkuYmX3hZ6eeaw"},"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:2DC5Z74RysBRi7HwK"},"coauthors":[{"__ref":"User:2DC5Z74RysBRi7HwK"}],"slug":"lesswrong-community-weekend-2025","title":"LessWrong Community Weekend 2025","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"2DC5Z74RysBRi7HwK","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Tag:xexCWMyds6QLWognu":{"_id":"xexCWMyds6QLWognu","__typename":"Tag","userId":"XtphY3uYHwruKqDyG","name":"World Optimization","shortName":null,"slug":"world-optimization","core":true,"postCount":3019,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":20,"createdAt":"2020-06-14T03:38:23.532Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":2,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"B2sXjQTGgwoGE9FES","displayName":"SeaIgloo"},{"_id":"si6LoAENzqPCmi2Dh","displayName":"ihatenumbersinusernames7"}]},"score":2,"afBaseScore":0,"afExtendedScore":{"reacts":{},"usersWhoLiked":[]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null,"isRead":false,"parentTag":null,"subTags":[],"description":{"__ref":"Revision:xexCWMyds6QLWognu_description"},"canVoteOnRels":null},"Tag:fkABsGCJZ6y9qConW":{"_id":"fkABsGCJZ6y9qConW","__typename":"Tag","userId":"oBSWiHjgproTiThmY","name":"Practical","shortName":null,"slug":"practical","core":true,"postCount":3267,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":true,"needsReview":null,"descriptionTruncationCount":2000,"createdAt":"2020-06-14T06:06:46.947Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":2,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"MiuAZvbQcQ7ethgt3","displayName":"Viktor withaK"},{"_id":"dRaCtsAWxk7sgirSY","displayName":"Jordan Morgan"}]},"score":2,"afBaseScore":0,"afExtendedScore":{"reacts":{},"usersWhoLiked":[]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null,"isRead":false,"parentTag":null,"subTags":[],"description":{"__ref":"Revision:fkABsGCJZ6y9qConW_description"},"canVoteOnRels":null},"Revision:BxdpR67bwpHwj3nYL":{"_id":"BxdpR67bwpHwj3nYL","__typename":"Revision","htmlHighlight":"<p>This year's Spring ACX Meetup everywhere in Eugene.<\/p>\n                <p>Location: Beergarden. – <a href=\"https://plus.codes/https://plus.codes/84PR3V3W+C6G\">https://plus.codes/84PR3V3W+C6G<\/a><\/p>\n                <p>Group Link: https://discord.gg/WmJpAbkR<\/p>\n                <p>Hosted by the ACX/EAs of Willamette Valley Meetup (see our Discord!)<\/p>\n                <p>Contact: michael.bacarella@gmail.com <\/p>","plaintextDescription":"This year's Spring ACX Meetup everywhere in Eugene.\n\nLocation: Beergarden. – https://plus.codes/84PR3V3W+C6G\n\nGroup Link: https://discord.gg/WmJpAbkR\n\nHosted by the ACX/EAs of Willamette Valley Meetup (see our Discord!)\n\nContact: michael.bacarella@gmail.com","wordCount":28,"version":"1.0.0"},"SocialPreviewType:87kEhKMAwp5M3dFGf":{"_id":"87kEhKMAwp5M3dFGf","__typename":"SocialPreviewType","imageUrl":""},"User:ycf7uRXCkvwHJ2PSS":{"_id":"ycf7uRXCkvwHJ2PSS","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"michael_b","createdAt":"2015-01-29T11:41:15.734Z","username":"michael_b","displayName":"michael_b","previousDisplayName":null,"fullName":null,"karma":78,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":5,"commentCount":40,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:87kEhKMAwp5M3dFGf":{"_id":"87kEhKMAwp5M3dFGf","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:BxdpR67bwpHwj3nYL"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[],"socialPreviewData":{"__ref":"SocialPreviewType:87kEhKMAwp5M3dFGf"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-03-25T23:50:00.404Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":null,"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"BxdpR67bwpHwj3nYL","commentCount":0,"voteCount":1,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.003073277184739709,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2025-03-25T23:50:00.404Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"ycf7uRXCkvwHJ2PSS","location":"Eugene","googleLocation":{"types":["bar","establishment","food","point_of_interest","restaurant"],"geometry":{"location":{"lat":44.0536063,"lng":-123.1044298},"viewport":{"northeast":{"lat":44.05495528029149,"lng":-123.1030808197085},"southwest":{"lat":44.0522573197085,"lng":-123.1057787802915}},"location_type":"ROOFTOP"},"place_id":"ChIJP_VdQHIewVQRaw1pZyYbdQM","plus_code":{"global_code":"84PR3V3W+C6","compound_code":"3V3W+C6 Eugene, OR, USA"},"formatted_address":"777 W 6th Ave, Eugene, OR 97402, USA","navigation_points":[{"location":{"latitude":44.0535211,"longitude":-123.1043158}}],"address_components":[{"types":["street_number"],"long_name":"777","short_name":"777"},{"types":["route"],"long_name":"West 6th Avenue","short_name":"W 6th Ave"},{"types":["neighborhood","political"],"long_name":"Whiteaker","short_name":"Whiteaker"},{"types":["locality","political"],"long_name":"Eugene","short_name":"Eugene"},{"types":["administrative_area_level_2","political"],"long_name":"Lane County","short_name":"Lane County"},{"types":["administrative_area_level_1","political"],"long_name":"Oregon","short_name":"OR"},{"types":["country","political"],"long_name":"United States","short_name":"US"},{"types":["postal_code"],"long_name":"97402","short_name":"97402"},{"types":["postal_code_suffix"],"long_name":"5109","short_name":"5109"}]},"onlineEvent":false,"globalEvent":false,"startTime":"2025-04-10T01:00:00.000Z","endTime":null,"localStartTime":"2025-04-09T18:00:00.000Z","localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":"michael.bacarella@gmail.com","isEvent":true,"eventImageId":null,"eventType":null,"types":["SSC"],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-03-25T23:50:00.433Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":false,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{"yes":3},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:ycf7uRXCkvwHJ2PSS"},"coauthors":[],"slug":"eugene-acx-meetups-everywhere-spring-2025","title":"Eugene – ACX Meetups Everywhere Spring 2025","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:9WzPxg7HS8ALLQ2Pm":{"_id":"9WzPxg7HS8ALLQ2Pm","__typename":"Revision","htmlHighlight":"<p>This year's Spring ACX Meetup everywhere in Munich.<\/p>\n                <p>Location: Müllerstraße 35, 80469 München; TeamWork Konferenzraum; walk past the courtyard to find the actual apartment building we'll be meeting in – <a href=\"https://plus.codes/https://plus.codes/8FWH4HJ9+7P\">https://plus.codes/8FWH4HJ9+7P<\/a><\/p>\n                <p>Group Link: https://chat.whatsapp.com/JekHeDBFokxLlmceXsYhLv<\/p>\n                <p>Bring snacks if you like<\/p>\n                <p>Contact: acx.organizer.munich@gmail.com <\/p>","plaintextDescription":"This year's Spring ACX Meetup everywhere in Munich.\n\nLocation: Müllerstraße 35, 80469 München; TeamWork Konferenzraum; walk past the courtyard to find the actual apartment building we'll be meeting in – https://plus.codes/8FWH4HJ9+7P\n\nGroup Link: https://chat.whatsapp.com/JekHeDBFokxLlmceXsYhLv\n\nBring snacks if you like\n\nContact: acx.organizer.munich@gmail.com","wordCount":41,"version":"1.0.0"},"SocialPreviewType:QXcQsQyf25836Mg2w":{"_id":"QXcQsQyf25836Mg2w","__typename":"SocialPreviewType","imageUrl":""},"User:n4jmKKGNFbtkMLxLd":{"_id":"n4jmKKGNFbtkMLxLd","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"organizer","createdAt":"2024-08-29T18:14:18.071Z","username":"organizer","displayName":"Organizer","previousDisplayName":null,"fullName":null,"karma":0,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":0,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":0.9,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Post:QXcQsQyf25836Mg2w":{"_id":"QXcQsQyf25836Mg2w","__typename":"Post","deletedDraft":false,"contents":{"__ref":"Revision:9WzPxg7HS8ALLQ2Pm"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[],"socialPreviewData":{"__ref":"SocialPreviewType:QXcQsQyf25836Mg2w"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-03-26T00:11:53.202Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":null,"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"9WzPxg7HS8ALLQ2Pm","commentCount":0,"voteCount":1,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0017532709753140807,"lastVisitedAt":null,"isFuture":false,"isRead":false,"lastCommentedAt":"2025-03-26T00:11:53.202Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"n4jmKKGNFbtkMLxLd","location":"Munich","googleLocation":{"types":["establishment","point_of_interest"],"geometry":{"location":{"lat":48.1306723,"lng":11.5693182},"viewport":{"northeast":{"lat":48.1320212802915,"lng":11.5706671802915},"southwest":{"lat":48.1293233197085,"lng":11.5679692197085}},"location_type":"ROOFTOP"},"place_id":"ChIJ5Z38IQDfnUcRHHc_ZeeNdr0","plus_code":{"global_code":"8FWH4HJ9+7P","compound_code":"4HJ9+7P Munich, Germany"},"formatted_address":"Müllerstraße 35, 80469 München, Germany","navigation_points":[{"location":{"latitude":48.1309818,"longitude":11.5695186}}],"address_components":[{"types":["street_number"],"long_name":"35","short_name":"35"},{"types":["route"],"long_name":"Müllerstraße","short_name":"Müllerstraße"},{"types":["political","sublocality","sublocality_level_1"],"long_name":"Ludwigsvorstadt-Isarvorstadt","short_name":"Ludwigsvorstadt-Isarvorstadt"},{"types":["locality","political"],"long_name":"München","short_name":"München"},{"types":["administrative_area_level_3","political"],"long_name":"Kreisfreie Stadt München","short_name":"Kreisfreie Stadt München"},{"types":["administrative_area_level_2","political"],"long_name":"Oberbayern","short_name":"Oberbayern"},{"types":["administrative_area_level_1","political"],"long_name":"Bayern","short_name":"BY"},{"types":["country","political"],"long_name":"Germany","short_name":"DE"},{"types":["postal_code"],"long_name":"80469","short_name":"80469"}]},"onlineEvent":false,"globalEvent":false,"startTime":"2025-04-10T14:00:00.000Z","endTime":null,"localStartTime":"2025-04-10T16:00:00.000Z","localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":"acx.organizer.munich@gmail.com","isEvent":true,"eventImageId":null,"eventType":null,"types":["SSC"],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-03-25T23:50:29.534Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":false,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{"yes":1,"maybe":1},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:n4jmKKGNFbtkMLxLd"},"coauthors":[],"slug":"munich-acx-meetups-everywhere-spring-2025","title":"Munich – ACX Meetups Everywhere Spring 2025","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Sequence:M3TJ2fTCzoQq66NBJ":{"_id":"M3TJ2fTCzoQq66NBJ","__typename":"Sequence","title":"Death Spirals","gridImageId":"sequencesgrid/kfdosmh7m4gptzsdcl5e","canonicalCollectionSlug":"rationality"},"Revision:o64gTS5kP4gpB9DSt":{"_id":"o64gTS5kP4gpB9DSt","__typename":"Revision","htmlHighlight":"<p>The <em>affect heuristic<\/em> is when subjective impressions of goodness/badness act as a heuristic&#x2014;a source of fast, perceptual judgments. Pleasant and unpleasant feelings are central to human reasoning, and the affect heuristic comes with lovely biases&#x2014;some of my favorites. <\/p>\n\n  <p>Let&#x2019;s start with one of the relatively less crazy biases. You&#x2019;re about to move to a new city, and you have to ship an antique grandfather clock. In the first case, the grandfather clock was a gift from your grandparents on your fifth birthday. In the second case, the clock was a gift from a remote relative and you have no special feelings for it. How much would you pay for an insurance policy that paid out $100 if the clock were lost in shipping? According to Hsee and Kunreuther, subjects stated willingness to pay more than twice as much in the first condition.<span><sup><a href=\"#fn1x49\" id=\"fn1x49-bk\">1<\/a><\/sup><\/span><span id=\"x57-58001f1\"> This may sound rational&#x2014;why not pay more to protect the more valuable object?&#x2014;until you realize that the insurance doesn&#x2019;t <em>protect<\/em> the clock, it just pays if the clock is lost, and pays exactly the same amount for either clock. (And yes, it was stated that the insurance was with an outside company, so it gives no special motive to the movers.)<\/span><\/p>\n\n  <p>All right, but that doesn&#x2019;t <em>sound<\/em> too insane. Maybe you could get away with claiming the subjects were insuring affective outcomes, not financial outcomes&#x2014;purchase of consolation.<\/p>\n\n  <p>Then how about this? Yamagishi showed that subjects judged a disease as more dangerous when it was described as killing 1,286 people out of every 10,000, versus a disease that was 24.14% likely to be fatal.<span><sup><a href=\"#fn2x49\" id=\"fn2x49-bk\">2<\/a><\/sup><\/span><span id=\"x57-58002f2\"> Apparently the mental image of a thousand dead bodies is much more alarming, compared to a single person who&#x2019;s more likely to survive than not.<\/span><\/p>\n\n  <p>But wait, it gets worse.<\/p>\n\n  <p>Suppose an airport must decide whether to spend money to purchase some new equipment, while critics argue that the money should be spent on other aspects of airport safety. Slovic et al. presented two groups of subjects with the arguments for and against purchasing the equipment, with a response scale ranging from 0 (would not support at all) to 20 (very strong support).<span><sup><a href=\"#fn3x49\" id=\"fn3x49-bk\">3<\/a><\/sup><\/span><span id=\"x57-58003f3\"> One group saw the measure described as saving 150 lives. The other group saw the measure described as saving 98% of 150 lives. The hypothesis motivating the experiment was that saving 150 lives sound<\/span>... <\/p>","plaintextDescription":"The affect heuristic is when subjective impressions of goodness/badness act as a heuristic—a source of fast, perceptual judgments. Pleasant and unpleasant feelings are central to human reasoning, and the affect heuristic comes with lovely biases—some of my favorites.\n\nLet’s start with one of the relatively less crazy biases. You’re about to move to a new city, and you have to ship an antique grandfather clock. In the first case, the grandfather clock was a gift from your grandparents on your fifth birthday. In the second case, the clock was a gift from a remote relative and you have no special feelings for it. How much would you pay for an insurance policy that paid out $100 if the clock were lost in shipping? According to Hsee and Kunreuther, subjects stated willingness to pay more than twice as much in the first condition.1 This may sound rational—why not pay more to protect the more valuable object?—until you realize that the insurance doesn’t protect the clock, it just pays if the clock is lost, and pays exactly the same amount for either clock. (And yes, it was stated that the insurance was with an outside company, so it gives no special motive to the movers.)\n\nAll right, but that doesn’t sound too insane. Maybe you could get away with claiming the subjects were insuring affective outcomes, not financial outcomes—purchase of consolation.\n\nThen how about this? Yamagishi showed that subjects judged a disease as more dangerous when it was described as killing 1,286 people out of every 10,000, versus a disease that was 24.14% likely to be fatal.2 Apparently the mental image of a thousand dead bodies is much more alarming, compared to a single person who’s more likely to survive than not.\n\nBut wait, it gets worse.\n\nSuppose an airport must decide whether to spend money to purchase some new equipment, while critics argue that the money should be spent on other aspects of airport safety. Slovic et al. presented two groups of subjects with the arguments for and against ","wordCount":954,"version":"2.0.0"},"Tag:Kj9q8FXoauL7mQDWt":{"_id":"Kj9q8FXoauL7mQDWt","__typename":"Tag","userId":"nLbwLhBaQeG6tCNDN","name":"Affect Heuristic","shortName":null,"slug":"affect-heuristic","core":false,"postCount":16,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-04-30T19:56:40.322Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:4R8JYu4QF2FqzJxE5":{"_id":"4R8JYu4QF2FqzJxE5","__typename":"Tag","userId":"BpBzKEueak7J8vHNi","name":"Heuristics & Biases","shortName":null,"slug":"heuristics-and-biases","core":false,"postCount":266,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-13T15:40:30.194Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:Kow8xRzpfkoY7pa69":{"_id":"Kow8xRzpfkoY7pa69","__typename":"SocialPreviewType","imageUrl":""},"User:nmk3nLpQE89dMRzzN":{"_id":"nmk3nLpQE89dMRzzN","__typename":"User","profileImageId":null,"moderationStyle":"reign-of-terror","bannedUserIds":["sBWszXPhPsNNemv4Q","YBHSPmZEfyyY2E2au"],"moderatorAssistance":true,"slug":"eliezer_yudkowsky","createdAt":"2009-02-23T21:58:56.739Z","username":"Eliezer_Yudkowsky","displayName":"Eliezer Yudkowsky","previousDisplayName":null,"fullName":null,"karma":148670,"afKarma":1884,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":951,"commentCount":7677,"sequenceCount":40,"afPostCount":18,"afCommentCount":120,"spamRiskScore":1,"tagRevisionCount":3803,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:Kow8xRzpfkoY7pa69":{"_id":"Kow8xRzpfkoY7pa69","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:o64gTS5kP4gpB9DSt"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":4,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Kj9q8FXoauL7mQDWt"},{"__ref":"Tag:4R8JYu4QF2FqzJxE5"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"}],"socialPreviewData":{"__ref":"SocialPreviewType:Kow8xRzpfkoY7pa69"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2007-11-27T07:58:44.000Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-01-30T00:32:03.501Z","meta":false,"postCategory":"post","tagRelevance":{"4R8JYu4QF2FqzJxE5":2,"Kj9q8FXoauL7mQDWt":9,"Ng8Gice9KNkncxqcj":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"o64gTS5kP4gpB9DSt","commentCount":70,"voteCount":69,"baseScore":78,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":69,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.00009746738214744255,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2022-09-08T17:42:56.443Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":"rationality","curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"nmk3nLpQE89dMRzzN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":4,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":8,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":null,"afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":"6txvYnttFXp9fNgcp","forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:nmk3nLpQE89dMRzzN"},"coauthors":[],"slug":"the-affect-heuristic","title":"The Affect Heuristic","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Sequence:HeYtBkNbEe7wpjc6X":{"_id":"HeYtBkNbEe7wpjc6X","__typename":"Sequence","title":"Partial Agency","gridImageId":"sequencesgrid/lo7y1hicsr26hauyceuy","canonicalCollectionSlug":null},"Revision:XyDyAbZ9CuFtmjJa2":{"_id":"XyDyAbZ9CuFtmjJa2","__typename":"Revision","htmlHighlight":"<p><em>Epistemic status: very rough intuitions here.<\/em><\/p><p>I think there&apos;s something interesting going on with <a href=\"https://www.lesswrong.com/posts/BKM8uQS6QdJPZLqCr/towards-a-mechanistic-understanding-of-corrigibility\">Evan&apos;s notion of myopia<\/a>.<\/p><p>Evan has been calling this thing &quot;myopia&quot;. Scott has been calling it &quot;stop-gradients&quot;. In my own mind, I&apos;ve been calling the phenomenon &quot;directionality&quot;. Each of these words gives a different set of intuitions about how the cluster could eventually be formalized.<\/p><h2>Stop-Gradients<\/h2><p>Nash equilibria are, abstractly, modeling agents via an equation like <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a^* = \\text{argmax}_a f(a,a^*)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a<\/span><\/span><\/span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">&#x2217;<\/span><\/span><\/span><\/span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=<\/span><\/span><span class=\"mjx-msubsup MJXc-space3\"><span class=\"mjx-base\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.519em;\">argmax<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.377em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a<\/span><\/span><\/span><\/span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f<\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(<\/span><\/span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a<\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,<\/span><\/span><span class=\"mjx-msubsup MJXc-space1\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a<\/span><\/span><\/span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">&#x2217;<\/span><\/span><\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)<\/span><\/span><\/span><\/span><\/span><\/span>. In words: <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"a^*\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">a<\/span><\/span><\/span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mo\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.298em;\">&#x2217;<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span> is the agent&apos;s mixed strategy. The payoff <span><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f(.,.)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\">f<\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(<\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.<\/span><\/span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,<\/span><\/span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.372em;\">.<\/span><\/span><span class=\"mjx-mo MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)<\/span><\/span><\/span><\/span><\/span><\/span> is a function of the mixed strategy in two ways: the first argument is the causal channel, where actions directly have effects; the second argument represents the &quot;acausal&quot; channel, IE, the fact that the other players know the agent&apos;s mixed strategy and this influences their actions. The agent is maximizing across the first channel, but &quot;ignoring&quot; the second channel; that is why we have to solve for a fixed point to find Nash equilibria. This motivates the notion of &quot;stop gradient&quot;: if we think in terms of neural-network type learning, we&apos;re sending the gradient through the first argument but not the second. (It&apos;s a kind of mathematically weird thing to do!)<\/p><h2>Myopia<\/h2><p>Thinking in terms of iterated games, we can also justify the label &quot;myopia&quot;. Thinking in terms of &quot;gradients&quot; suggests that we&apos;re doing some kind of training involving repeatedly playing the game. But we&apos;re training an agent to play <a href=\"https://www.lesswrong.com/posts/dKAJqBDZRMMsaaYo5/in-logical-time-all-games-are-iterated-games\">as if it&apos;s a single-shot game<\/a>: the gradient is rewarding behavior which gets more reward within the single round even if it compromises long-run reward. This is a weird thing to do: why implement a training regime to produce strategies like that, if we believe the nash-equilibrium model, IE we think the other players will know our mixed strategy and react to it? We can, for example, win chicken by going straight more often than is myopically rational. Generally speaking, we expect to get better rewards in the rounds after training if we optimized for non-myopic strategies during training.<\/p><h2>Directionality<\/h2><p>To justify my term &quot;directionality&quot; for these phenomena, we have to look at a different example: the idea that &quot;when beliefs and reality don&apos;t match, we change our be... <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style><\/p>","plaintextDescription":"Epistemic status: very rough intuitions here.\n\nI think there's something interesting going on with Evan's notion of myopia.\n\nEvan has been calling this thing \"myopia\". Scott has been calling it \"stop-gradients\". In my own mind, I've been calling the phenomenon \"directionality\". Each of these words gives a different set of intuitions about how the cluster could eventually be formalized.\n\n\nStop-Gradients\nNash equilibria are, abstractly, modeling agents via an equation like a∗=argmaxaf(a,a∗). In words: a∗ is the agent's mixed strategy. The payoff f(.,.) is a function of the mixed strategy in two ways: the first argument is the causal channel, where actions directly have effects; the second argument represents the \"acausal\" channel, IE, the fact that the other players know the agent's mixed strategy and this influences their actions. The agent is maximizing across the first channel, but \"ignoring\" the second channel; that is why we have to solve for a fixed point to find Nash equilibria. This motivates the notion of \"stop gradient\": if we think in terms of neural-network type learning, we're sending the gradient through the first argument but not the second. (It's a kind of mathematically weird thing to do!)\n\n\nMyopia\nThinking in terms of iterated games, we can also justify the label \"myopia\". Thinking in terms of \"gradients\" suggests that we're doing some kind of training involving repeatedly playing the game. But we're training an agent to play as if it's a single-shot game: the gradient is rewarding behavior which gets more reward within the single round even if it compromises long-run reward. This is a weird thing to do: why implement a training regime to produce strategies like that, if we believe the nash-equilibrium model, IE we think the other players will know our mixed strategy and react to it? We can, for example, win chicken by going straight more often than is myopically rational. Generally speaking, we expect to get better rewards in the rounds after traini","wordCount":2599,"version":"1.0.0"},"Tag:3Y4y9Kr8e24YWAEmD":{"_id":"3Y4y9Kr8e24YWAEmD","__typename":"Tag","userId":"Q7NW4XaWQmfPfdcFj","name":"Myopia","shortName":null,"slug":"myopia","core":false,"postCount":42,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-13T21:32:09.877Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:4hdHto3uHejhY2F3Q":{"_id":"4hdHto3uHejhY2F3Q","__typename":"SocialPreviewType","imageUrl":""},"User:Q7NW4XaWQmfPfdcFj":{"_id":"Q7NW4XaWQmfPfdcFj","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":false,"slug":"abramdemski","createdAt":"2009-03-12T06:07:25.510Z","username":"abramdemski","displayName":"abramdemski","previousDisplayName":null,"fullName":"Abram Demski","karma":19296,"afKarma":3719,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":223,"commentCount":2060,"sequenceCount":9,"afPostCount":101,"afCommentCount":656,"spamRiskScore":1,"tagRevisionCount":90,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:4hdHto3uHejhY2F3Q":{"_id":"4hdHto3uHejhY2F3Q","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:XyDyAbZ9CuFtmjJa2"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":10,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:3Y4y9Kr8e24YWAEmD"}],"socialPreviewData":{"__ref":"SocialPreviewType:4hdHto3uHejhY2F3Q"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2019-09-27T22:04:46.754Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2019-09-27T22:26:34.638Z","meta":false,"postCategory":"post","tagRelevance":{"3Y4y9Kr8e24YWAEmD":11},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"XyDyAbZ9CuFtmjJa2","commentCount":18,"voteCount":25,"baseScore":75,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":25,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0003483144100755453,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2022-05-02T06:44:01.368Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"Q7NW4XaWQmfPfdcFj","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":34,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":21,"agreementVoteCount":0},"afCommentCount":16,"afLastCommentedAt":"2019-11-02T16:49:24.470Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:Q7NW4XaWQmfPfdcFj"},"coauthors":[],"slug":"partial-agency","title":"Partial Agency","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"User:rPtgB3HpLr4Y5vtuT":{"_id":"rPtgB3HpLr4Y5vtuT","__typename":"User","displayName":"Eric Neyman","slug":"unexpectedvalues","createdAt":"2020-08-03T23:22:09.042Z","username":"UnexpectedValues","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":3661,"afKarma":94,"deleted":false,"isAdmin":false,"htmlBio":"<p>I work at the Alignment Research Center (ARC). I write a blog on stuff I'm interested in (such as math, philosophy, puzzles, statistics, and elections): <a href=\"https://ericneyman.wordpress.com/\">https://ericneyman.wordpress.com/<\/a><\/p>","jobTitle":null,"organization":null,"postCount":24,"commentCount":121,"sequenceCount":1,"afPostCount":2,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"XtphY3uYHwruKqDyG"},"Revision:AcFEM6mTpjQHrE4Jo_contents":{"_id":"AcFEM6mTpjQHrE4Jo_contents","__typename":"Revision","html":"<p>It's great to have a LessWrong post that states the relationship between expected quality and a noisy measurement of quality:<\/p>\n<blockquote>\n<p><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\mathbb{E}[\\text{Quality}]=0.5 \\cdot \\text{Performance}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E<\/span><\/span><\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[<\/span><\/span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Quality<\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]<\/span><\/span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=<\/span><\/span><span class=\"mjx-mn MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0.5<\/span><\/span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅<\/span><\/span><span class=\"mjx-mtext MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Performance<\/span><\/span><\/span><\/span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style><\/span><\/span><\/p>\n<\/blockquote>\n<blockquote>\n<p>(Why 0.5? Remember that performance is a sum of two random variables with standard deviation 1: the quality of the intervention and the noise of the trial. So when you see a performance number like 4, in expectation the quality of the intervention is 2 and the contribution from the noise of the trial (i.e. how lucky you got in the RCT) is also 2.)<\/p>\n<\/blockquote>\n<p>We previously had a popular post on this topic, the <a href=\"https://www.lesswrong.com/posts/dC7mP5nSwvpL65Qu5/why-the-tails-come-apart\">tails come apart post<\/a>, but it actually made a subtle mistake when stating this relationship. It says:<\/p>\n<blockquote>\n<p>For concreteness (and granting normality), an R-square of 0.5 (corresponding to an angle of sixty degrees) means that +4SD (~1/15000) on a factor will be expected to be 'merely' +2SD (~1/40) in the outcome - and an R-square of 0.5 is remarkably strong in the social sciences, implying it accounts for half the variance.<\/p>\n<\/blockquote>\n<p>The example under discussion in this quote is the same as the example in this post, where quality and noise have the same variance, and thus R^2=0.5. And superficially it seems to be stating the same thing: the expectation of quality is half the measurement.<\/p>\n<p>But actually, this newer post is correct, and the older post is wrong. The key is that \"Quality\" and \"Performance\" in this post are not measured in standard deviations. Their standard deviations are 1 and √2, respectively. Elaborating on that: Quality has a variance, and standard deviation, of 1. The variance of Performance is the sum of the variances of Quality and noise, which is 2, and thus its standard deviation is √2. Now that we know their standard deviations, we can scale them to units of standard deviation, and obtain Quality (unchanged) and Performance/√2. The relationship between them is:<\/p>\n<p><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\mathbb{E}[\\text{Quality}]=\\frac{1}{\\sqrt{2}} \\cdot \\frac{\\text{Performance}}{\\sqrt{2}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E<\/span><\/span><\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[<\/span><\/span><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.519em;\">Quality<\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]<\/span><\/span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=<\/span><\/span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 1.533em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 1.533em; top: -1.368em;\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1<\/span><\/span><\/span><span class=\"mjx-denominator\" style=\"width: 1.533em; bottom: -1.15em;\"><span class=\"mjx-msqrt\"><span class=\"mjx-box\" style=\"padding-top: 0.045em;\"><span class=\"mjx-surd\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">√<\/span><\/span><span class=\"mjx-box\" style=\"padding-top: 0.13em; border-top: 1px solid;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2<\/span><\/span><\/span><\/span><\/span><\/span><\/span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 1.533em;\" class=\"mjx-line\"><\/span><\/span><span style=\"height: 2.518em; vertical-align: -1.15em;\" class=\"mjx-vsize\"><\/span><\/span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅<\/span><\/span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.692em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 5.692em; top: -1.407em;\"><span class=\"mjx-mtext\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.372em;\">Performance<\/span><\/span><\/span><span class=\"mjx-denominator\" style=\"width: 5.692em; bottom: -1.15em;\"><span class=\"mjx-msqrt\"><span class=\"mjx-box\" style=\"padding-top: 0.045em;\"><span class=\"mjx-surd\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.519em; padding-bottom: 0.519em;\">√<\/span><\/span><span class=\"mjx-box\" style=\"padding-top: 0.13em; border-top: 1px solid;\"><span class=\"mjx-mrow\"><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2<\/span><\/span><\/span><\/span><\/span><\/span><\/span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.692em;\" class=\"mjx-line\"><\/span><\/span><span style=\"height: 2.557em; vertical-align: -1.15em;\" class=\"mjx-vsize\"><\/span><\/span><\/span><\/span><\/span><\/span><\/p>\n<p>That is equivalent to the relationship stated in this post.<\/p>\n<p>More generally, notating the variables in units of standard deviation as <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Z_x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.04em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">Z<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span> and <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Z_y\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.04em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">Z<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span> (since they are \"z-scores\"),<\/p>\n<p><span class=\"mjpage mjpage__block\"><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\mathbb{E}[Z_y] = \\rho \\cdot Z_x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-ams-R\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">E<\/span><\/span><\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[<\/span><\/span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.04em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">Z<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em; padding-right: 0.006em;\">y<\/span><\/span><\/span><\/span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]<\/span><\/span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">=<\/span><\/span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">ρ<\/span><\/span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\">⋅<\/span><\/span><span class=\"mjx-msubsup MJXc-space2\"><span class=\"mjx-base\" style=\"margin-right: -0.04em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">Z<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/p>\n<p>where <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\rho\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">ρ<\/span><\/span><\/span><\/span><\/span><\/span> is the correlation coefficient. So if your noisy measurement of quality is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"Z_x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.04em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">Z<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span> standard deviations above its mean, then the expectation of quality is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\rho Z_x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">ρ<\/span><\/span><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.04em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.04em;\">Z<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mi\" style=\"\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\">x<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span> standard deviations above its mean. It is <span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\rho^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.519em;\">ρ<\/span><\/span><\/span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span> that is variance explained, and is thus 1/2 when the signal and noise have the same variance. That's why in the example in this post, we divide the raw performance by 2, rather than converting it to standard deviations and dividing by 2.<\/p>\n<p>I think it's important to understand the relationship between the expected value of an unknown and the value of a noisy measurement of it, so it's nice to see a whole post about this relationship. I do think it's worth explicitly stating the relationship on a standard deviation scale, which this post doesn't do, but I've done that here in my comment.<\/p>\n","plaintextMainText":"It's great to have a LessWrong post that states the relationship between expected quality and a noisy measurement of quality:\n\nWe previously had a popular post on this topic, the tails come apart post, but it actually made a subtle mistake when stating this relationship. It says:\n\nThe example under discussion in this quote is the same as the example in this post, where quality and noise have the same variance, and thus R^2=0.5. And superficially it seems to be stating the same thing: the expectation of quality is half the measurement.\n\nBut actually, this newer post is correct, and the older post is wrong. The key is that \"Quality\" and \"Performance\" in this post are not measured in standard deviations. Their standard deviations are 1 and √2, respectively. Elaborating on that: Quality has a variance, and standard deviation, of 1. The variance of Performance is the sum of the variances of Quality and noise, which is 2, and thus its standard deviation is √2. Now that we know their standard deviations, we can scale them to units of standard deviation, and obtain Quality (unchanged) and Performance/√2. The relationship between them is:\n\nE[Quality]=1√2⋅Performance√2\n\nThat is equivalent to the relationship stated in this post.\n\nMore generally, notating the variables in units of standard deviation as Zx and Zy (since they are \"z-scores\"),\n\nE[Zy]=ρ⋅Zx\n\nwhere ρ is the correlation coefficient. So if your noisy measurement of quality is Zx standard deviations above its mean, then the expectation of quality is ρZx standard deviations above its mean. It is ρ2 that is variance explained, and is thus 1/2 when the signal and noise have the same variance. That's why in the example in this post, we divide the raw performance by 2, rather than converting it to standard deviations and dividing by 2.\n\nI think it's important to understand the relationship between the expected value of an unknown and the value of a noisy measurement of it, so it's nice to see a whole post about this relatio","wordCount":494},"User:wceXXk4zpvX6zNZcD":{"_id":"wceXXk4zpvX6zNZcD","__typename":"User","slug":"transhumanist_atom_understander","createdAt":"2023-09-09T03:02:58.928Z","username":"transhumanist_atom_understander","displayName":"transhumanist_atom_understander","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":366,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":5,"commentCount":52,"sequenceCount":1,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"nLbwLhBaQeG6tCNDN"},"Comment:AcFEM6mTpjQHrE4Jo":{"_id":"AcFEM6mTpjQHrE4Jo","__typename":"Comment","postId":"nnDTgmzRrzDMiPF9B","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:AcFEM6mTpjQHrE4Jo_contents"},"postedAt":"2024-12-20T20:38:56.934Z","lastEditedAt":"2024-12-20T20:38:57.206Z","repliesBlockedUntil":null,"userId":"wceXXk4zpvX6zNZcD","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:wceXXk4zpvX6zNZcD"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":11,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"score":0.0012885128380730748,"voteCount":2,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":6,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.4","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2024-12-20T20:38:56.936Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":"2023","promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:TY9NepG4kHK9myngu_contents":{"_id":"TY9NepG4kHK9myngu_contents","__typename":"Revision","html":"<p>I think this isn't the sort of post that ages well or poorly, because it isn't topical, but I think this post turned out pretty well. It gradually builds from preliminaries that most readers have probably seen before, into some pretty counterintuitive facts that aren't widely appreciated.<\/p><p>At the end of the post, I listed three questions and wrote that I hope to write about some of them soon. I never did, so I figured I'd use this review to briefly give my takes.<\/p><ol><li><a href=\"https://www.lesswrong.com/posts/nnDTgmzRrzDMiPF9B/how-much-do-you-believe-your-results?commentId=eig9cuJuFsJecH8ou\">This comment<\/a> from Fabien Roger tests some of my modeling choices for robustness, and finds that the surprising results of Part IV hold up when the noise is heavier-tailed than the signal. (I'm sure there's more to be said here, but I probably don't have time to do more analysis by the end of the review period.,)<\/li><li>My basic take is that this really is a point in favor of well-evidenced interventions, but that the best-looking speculative interventions are nevertheless better. This is because I think \"speculative\" here mostly refers to <i>partial measurement<\/i> rather than <i>noisy measurement.<\/i> For example, maybe you can only foresee the first-order effects of an intervention, but not the second-order effects. If the first-order effect is a (known) quantity&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X_1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.024em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1<\/span><\/span><\/span><\/span><\/span><\/span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style><\/span><\/span><\/span>&nbsp;and the second-order effect is an (unknown) quantity&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X_2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.024em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">2<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span>, then modeling the second-order effect as zero (and thus estimating the quality of the intervention as&nbsp;<span class=\"math-tex\"><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"X_1\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\" style=\"margin-right: -0.024em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.024em;\">X<\/span><\/span><\/span><span class=\"mjx-sub\" style=\"font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1<\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span><\/span>) isn't a noisy measurement; it's a partial measurement. It's still your best guess given the information you have.<ol><li>I haven't thought this through very much. I expect good counter-arguments and counter-counter-arguments to exist here.<\/li><\/ol><\/li><li> <ol><li>No -- or rather, only if the measurement is guaranteed to be exactly correct. To see this, observe that the variance of a noisy, unbiased measurement is greater than the variance of the quantity you're trying to measure (with equality only when the noise is zero), whereas the variance of a noiseless, partial measurement is less than the variance of the quantity you're trying to measure.<\/li><li>Real-world measurements are <i>absolutely<\/i> partial. They are, like, mind-bogglingly partial. This point deserves a separate post, but consider for instance the action of donating $5,000 to the Against Malaria Foundation. Maybe your measured effect from the RCT is that it'll save one life: 50 QALYs or so. But this measurement neglects the <a href=\"https://forum.effectivealtruism.org/topics/meat-eating-problem\">meat-eating problem<\/a>: the expected-child you'll save will grow up to eat expected-meat from factory farms, likely causing a great amount of suffering. But then you remember: actually there's a chance that this child will have a one eight-billionth stake in determining the future of the lightcone. Oops, actually this consideration totally dominates the previous two. Does this child have better values than the average human? Again: <i>mind-bogglingly<\/i> partial!<br><br>(The measurements are also, of course, noisy! RCTs are probably about as un-noisy as it gets: for example, making your best guess about the quality of an intervention by drawing inferences from uncontrolled macroeconomic data is much more noisy. So the answer is: generally both noisy and partial, but in some sense, much more partial than noisy -- though I'm not sure how much that comparison matters.)<\/li><li>The lessons of this post do not generalize to partial measurements at all! This post is entirely about noisy measurements. If you've partially measured the quality of an intervention, estimating the un-measured part using your prior will give you an estimate of intervention quality that you know is probably wrong, but the expected value of your error is zero.<\/li><\/ol><\/li><\/ol>","plaintextMainText":"I think this isn't the sort of post that ages well or poorly, because it isn't topical, but I think this post turned out pretty well. It gradually builds from preliminaries that most readers have probably seen before, into some pretty counterintuitive facts that aren't widely appreciated.\n\nAt the end of the post, I listed three questions and wrote that I hope to write about some of them soon. I never did, so I figured I'd use this review to briefly give my takes.\n\n 1. This comment from Fabien Roger tests some of my modeling choices for robustness, and finds that the surprising results of Part IV hold up when the noise is heavier-tailed than the signal. (I'm sure there's more to be said here, but I probably don't have time to do more analysis by the end of the review period.,)\n 2. My basic take is that this really is a point in favor of well-evidenced interventions, but that the best-looking speculative interventions are nevertheless better. This is because I think \"speculative\" here mostly refers to partial measurement rather than noisy measurement. For example, maybe you can only foresee the first-order effects of an intervention, but not the second-order effects. If the first-order effect is a (known) quantity X1 and the second-order effect is an (unknown) quantity X2, then modeling the second-order effect as zero (and thus estimating the quality of the intervention as X1) isn't a noisy measurement; it's a partial measurement. It's still your best guess given the information you have.\n    1. I haven't thought this through very much. I expect good counter-arguments and counter-counter-arguments to exist here.\n 3. 1. No -- or rather, only if the measurement is guaranteed to be exactly correct. To see this, observe that the variance of a noisy, unbiased measurement is greater than the variance of the quantity you're trying to measure (with equality only when the noise is zero), whereas the variance of a noiseless, partial measurement is less than the variance of the ","wordCount":587},"Comment:TY9NepG4kHK9myngu":{"_id":"TY9NepG4kHK9myngu","__typename":"Comment","postId":"nnDTgmzRrzDMiPF9B","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:TY9NepG4kHK9myngu_contents"},"postedAt":"2025-01-12T07:13:18.004Z","lastEditedAt":"2025-01-12T07:13:18.266Z","repliesBlockedUntil":null,"userId":"rPtgB3HpLr4Y5vtuT","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:rPtgB3HpLr4Y5vtuT"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":10,"extendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":5,"agreementVoteCount":1},"score":0.0015674920286983252,"voteCount":5,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":5,"afExtendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":4,"agreementVoteCount":1},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.4","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-01-12T07:13:18.008Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":"2023","promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:nnDTgmzRrzDMiPF9B":{"_id":"nnDTgmzRrzDMiPF9B","__typename":"Post","user":{"__ref":"User:rPtgB3HpLr4Y5vtuT"},"reviews":[{"__ref":"Comment:AcFEM6mTpjQHrE4Jo"},{"__ref":"Comment:TY9NepG4kHK9myngu"}],"slug":"how-much-do-you-believe-your-results","title":"How much do you believe your results?","draft":false,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"rPtgB3HpLr4Y5vtuT","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Spotlight:vyqRuT9KBYCkhrate":{"_id":"vyqRuT9KBYCkhrate","__typename":"Spotlight","post":{"__ref":"Post:nnDTgmzRrzDMiPF9B"},"sequence":null,"tag":null,"sequenceChapters":null,"description":{"__typename":"Revision","html":"<p>When you encounter a study, always ask yourself how much you believe their results. In Bayesian terms, this means thinking about the correct amount for the study to update you away from your priors. For a noisy study, the answer may well be “pretty much not at all”<\/p>"},"documentId":"nnDTgmzRrzDMiPF9B","documentType":"Post","spotlightImageId":null,"spotlightDarkImageId":null,"spotlightSplashImageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/w_600,h_400,c_fill,f_auto,q_auto/v1/mirroredImages/How_much_do_you_believe_your_res_scientists_studying_animated_sca_0.26694359957976777/y2x4syvvisthxdw7kp3x","draft":false,"deletedDraft":false,"position":1728,"lastPromotedAt":"2025-04-08T00:00:00.000Z","customTitle":null,"customSubtitle":"Best of LessWrong 2023","subtitleUrl":"/bestoflesswrong?year=2023&category=all","headerTitle":null,"headerTitleLeftColor":null,"headerTitleRightColor":null,"duration":3,"showAuthor":false,"imageFade":true,"imageFadeColor":null},"Revision:Jym6J6oFFLqKQCa7t":{"_id":"Jym6J6oFFLqKQCa7t","__typename":"Revision","htmlHighlight":"<p>In 2021 I wrote what became my most popular blog post: <a href=\"https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like\">What 2026 Looks Like<\/a>. I intended to keep writing predictions all the way to AGI and beyond, but chickened out and just published up till 2026.<br><br>Well, it's finally time. I'm back, and this time I have a team with me: <a href=\"https://ai-futures.org/about/\">the AI Futures Project<\/a>. <strong>We've written a concrete scenario of what we think the future of AI will look like. <\/strong>We are highly uncertain, of course, but we hope this story will rhyme with reality enough to help us all prepare for what's ahead.<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/uilvavcrkt47fxbbfyao\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/netqo6imgq2lzuwwxxl6 190w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/fdvl4leiwkq0vz6rwujx 380w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/rspcw56xhrgnpfvjtox8 570w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/q1sahswizd6qelgigzog 760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/pwmdmzlkd7uo7ogasbrm 950w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/mprhrwi6bu54nzhrjuek 1140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/udufsm5zcecjwwphtizg 1330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/oe1cs294kdjz56rjoz4c 1520w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/yrap93lnpta2qmgsepdw 1710w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/wnccaitfe8xb7qgjoqal 1881w\"><\/figure><p>You really should go <a href=\"https://ai-2027.com/\">read it on the website<\/a> instead of here, it's much better. There's a <a href=\"https://ai-2027.com/\">sliding dashboard<\/a> that updates the stats as you scroll through the scenario!<\/p><p>But I've nevertheless copied the first half of the story below. I look forward to reading your comments.<\/p><h1 data-internal-id=\"Mid_2025__Stumbling_Agents\">Mid 2025: Stumbling Agents<\/h1><p>The world sees its first glimpse of AI agents.<\/p><p data-internal-id=\"ftnt_ref9\">Advertisements for computer-using agents emphasize the term “personal assistant”: you can prompt them with tasks like “order me a burrito on DoorDash” or “open my budget spreadsheet and sum this month’s expenses.” They will check in with you as needed: for example, to ask you to confirm purchases.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"kbi0fjlae18\" role=\"doc-noteref\" id=\"fnrefkbi0fjlae18\"><sup><a href=\"#fnkbi0fjlae18\">[1]<\/a><\/sup><\/span>&nbsp;Though more advanced than previous iterations like <a href=\"https://openai.com/index/introducing-operator/\">Operator<\/a>, they struggle to get widespread usage.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"0cls3j9tbj3\" role=\"doc-noteref\" id=\"fnref0cls3j9tbj3\"><sup><a href=\"#fn0cls3j9tbj3\">[2]<\/a><\/sup><\/span><\/p><p>Meanwhile, out of public focus, more specialized coding and research agents are beginning to transform their professions.<\/p><p data-internal-id=\"ftnt_ref10\">The AIs of 2024 could follow specific instructions: they could turn bullet points into emails, and simple requests into working code. In 2025, AIs function more like employees. Coding AIs increasingly look like autonomous agents rather than mere assistants: taking instructions via Slack or Teams and making substantial code changes on their own, sometimes saving hours or even days.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"10vhm87bnxxq\" role=\"doc-noteref\" id=\"fnref10vhm87bnxxq\"><sup><a href=\"#fn10vhm87bnxxq\">[3]<\/a><\/sup><\/span>&nbsp;Research agents spend half an hour scouring the Internet to answer your question.<\/p><p data-internal-id=\"ftnt_ref12\">The agents are impressive in theory (and in cherry-picked examples), but in practice unreliable. AI twitter is full of stories about tasks bungled in some particularly hilarious way. The better agents are also expensive; you get what you pay for, and the best performance costs hundreds of dollars a month.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"imbw5409mkd\" role=\"doc-noteref\" id=\"fnrefimbw5409mkd\"><sup><a href=\"#fnimbw5409mkd\">[4]<\/a><\/sup><\/span>&nbsp;Still, many companies find ways to fit AI agents into their workflows.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"wy5cb4na4qd\" role=\"doc-noteref\" id=\"fnrefwy5cb4na4qd\"><sup><a href=\"#fnwy5cb4na4qd\">[5]<\/a><\/sup><\/span><\/p><h1 data-internal-id=\"Late_2025__The_World_s_Most_Expensive_AI_\">Late 2025: The World’s Most Expensive AI<\/h1><p data-internal-id=\"ftnt_ref13\">OpenBrain is building the biggest datacenters&nbsp;the world has ever seen.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"6\" data-footnote-id=\"4jbas56nir8\" role=\"doc-noteref\" id=\"fnref4jbas56nir8\"><sup><a href=\"#fn4jbas56nir8\">[6]<\/a><\/sup><\/span><\/p><p>(To avoid singling out any one existing com... <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style><\/p>","plaintextDescription":"In 2021 I wrote what became my most popular blog post: What 2026 Looks Like. I intended to keep writing predictions all the way to AGI and beyond, but chickened out and just published up till 2026.\n\nWell, it's finally time. I'm back, and this time I have a team with me: the AI Futures Project. We've written a concrete scenario of what we think the future of AI will look like. We are highly uncertain, of course, but we hope this story will rhyme with reality enough to help us all prepare for what's ahead.\n\nYou really should go read it on the website instead of here, it's much better. There's a sliding dashboard that updates the stats as you scroll through the scenario!\n\nBut I've nevertheless copied the first half of the story below. I look forward to reading your comments.\n\n\nMid 2025: Stumbling Agents\nThe world sees its first glimpse of AI agents.\n\nAdvertisements for computer-using agents emphasize the term “personal assistant”: you can prompt them with tasks like “order me a burrito on DoorDash” or “open my budget spreadsheet and sum this month’s expenses.” They will check in with you as needed: for example, to ask you to confirm purchases.[1] Though more advanced than previous iterations like Operator, they struggle to get widespread usage.[2]\n\nMeanwhile, out of public focus, more specialized coding and research agents are beginning to transform their professions.\n\nThe AIs of 2024 could follow specific instructions: they could turn bullet points into emails, and simple requests into working code. In 2025, AIs function more like employees. Coding AIs increasingly look like autonomous agents rather than mere assistants: taking instructions via Slack or Teams and making substantial code changes on their own, sometimes saving hours or even days.[3] Research agents spend half an hour scouring the Internet to answer your question.\n\nThe agents are impressive in theory (and in cherry-picked examples), but in practice unreliable. AI twitter is full of stories about tasks bu","wordCount":12239,"version":"2.10.1"},"Revision:TpSFoqoG2M5MAAesg_customHighlight":{"_id":"TpSFoqoG2M5MAAesg_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Tag:zHjC29kkPmsdo7WTr":{"_id":"zHjC29kkPmsdo7WTr","__typename":"Tag","userId":"EQNTWXLKMeWMp2FQS","name":"AI Timelines","shortName":null,"slug":"ai-timelines","core":false,"postCount":413,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-16T10:16:47.235Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:33BrBRSrRQS4jEHdk":{"_id":"33BrBRSrRQS4jEHdk","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Forecasts (Specific Predictions)","shortName":null,"slug":"forecasts-specific-predictions","core":false,"postCount":181,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-12T06:31:37.542Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb297":{"_id":"5f5c37ee1b5cdee568cfb297","__typename":"Tag","userId":"NRg5Bw8H2DCYTpmHE","name":"Superintelligence","shortName":null,"slug":"superintelligence","core":false,"postCount":152,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.554Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:TpSFoqoG2M5MAAesg":{"_id":"TpSFoqoG2M5MAAesg","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/TpSFoqoG2M5MAAesg/xyghhogfqlmrqn1hplwe"},"User:YLFQfGzNdGA4NFcKS":{"_id":"YLFQfGzNdGA4NFcKS","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":true,"slug":"daniel-kokotajlo","createdAt":"2018-03-05T19:59:32.269Z","username":"daniel-kokotajlo","displayName":"Daniel Kokotajlo","previousDisplayName":null,"fullName":"Daniel Kokotajlo","karma":25931,"afKarma":3519,"deleted":false,"isAdmin":false,"htmlBio":"<p>Was a philosophy PhD student, left to work at AI Impacts, then Center on Long-Term Risk, then OpenAI. Quit OpenAI due to losing confidence that it would behave responsibly around the time of AGI. Now executive director of the <a href=\"https://ai-futures.org/\">AI Futures Project<\/a>. I subscribe to Crocker's Rules and am especially interested to hear unsolicited constructive criticism. <a href=\"http://sl4.org/crocker.html\">http://sl4.org/crocker.html<\/a><br><br>Some of my favorite memes:<br><br><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/mufW9iFSxRxqNpvyQ/d2mjevfaxcqt15ihv6ly\"><br>(by Rob Wiblin)<br><br><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YLFQfGzNdGA4NFcKS/ca2ilrdkzo8nps2y2mv4\" alt=\"Comic. Megan &amp; Cueball show White Hat a graph of a line going up, not yet at, but heading towards, a threshold labelled &quot;BAD&quot;. White Hat: &quot;So things will be bad?&quot; Megan: &quot;Unless someone stops it.&quot; White Hat: &quot;Will someone do that?&quot; Megan: &quot;We don't know, that's why we're showing you.&quot; White Hat: &quot;Well, let me know if that happens!&quot; Megan: &quot;Based on this conversation, it already has.&quot;\"><br>(xkcd)<br><br>My EA Journey, depicted on the whiteboard at CLR:<br><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YLFQfGzNdGA4NFcKS/luibgbmndcrfpntbvnzn\"><\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YLFQfGzNdGA4NFcKS/kt4bdj0izczhp3xvpsim\"><\/p><p>(h/t Scott Alexander)<\/p><p><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YLFQfGzNdGA4NFcKS/wnmpgr7vplvugifhexss\"><br>&nbsp;<br><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/YLFQfGzNdGA4NFcKS/mpby3gbzaifqusxukecp\" alt=\"Alex Blechman @AlexBlechman Sci-Fi Author: In my book I invented the Torment Nexus as a cautionary tale Tech Company: At long last, we have created the Torment Nexus from classic sci-fi novel Don't Create The Torment Nexus 5:49 PM Nov 8, 2021. Twitter Web App\"><\/p>","jobTitle":null,"organization":null,"postCount":106,"commentCount":3147,"sequenceCount":3,"afPostCount":41,"afCommentCount":791,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"User:my98h9K2ygm9SeEEK":{"_id":"my98h9K2ygm9SeEEK","__typename":"User","slug":"thomas-larsen","createdAt":"2018-05-18T15:30:31.166Z","username":"thomas-larsen","displayName":"Thomas Larsen","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":2680,"afKarma":151,"deleted":false,"isAdmin":false,"htmlBio":"<p>I'm broadly interested in AI strategy and want to figure out the most effective interventions to get good AI outcomes.&nbsp;<\/p>","jobTitle":null,"organization":null,"postCount":8,"commentCount":106,"sequenceCount":0,"afPostCount":4,"afCommentCount":3,"spamRiskScore":1,"tagRevisionCount":6,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:kr9MH2mgFcJz5cP7T":{"_id":"kr9MH2mgFcJz5cP7T","__typename":"User","slug":"elifland","createdAt":"2019-06-04T02:06:27.274Z","username":"elifland","displayName":"elifland","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":2111,"afKarma":139,"deleted":false,"isAdmin":false,"htmlBio":"<p><a href=\"https://www.elilifland.com/.\">https://www.elilifland.com/.<\/a> You can give me anonymous feedback <a href=\"https://forms.gle/n1jdAYUTrRS1m94Y8\">here<\/a>. I often change my mind and don't necessarily endorse past writings.<\/p>","jobTitle":null,"organization":null,"postCount":12,"commentCount":89,"sequenceCount":0,"afPostCount":4,"afCommentCount":13,"spamRiskScore":1,"tagRevisionCount":7,"reviewedByUserId":"fD4ATtTkdQJ4aSpGH"},"User:XgYW5s8njaYrtyP7q":{"_id":"XgYW5s8njaYrtyP7q","__typename":"User","slug":"scottalexander","createdAt":"2009-02-28T15:53:46.032Z","username":"Yvain","displayName":"Scott Alexander","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":43989,"afKarma":93,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":217,"commentCount":1582,"sequenceCount":15,"afPostCount":1,"afCommentCount":2,"spamRiskScore":1,"tagRevisionCount":19,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:tkdgvuLhqRyfzsBvy":{"_id":"tkdgvuLhqRyfzsBvy","__typename":"User","slug":"jonas-v","createdAt":"2018-11-21T15:03:10.337Z","username":"Jonas Vollmer","displayName":"Jonas V","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1202,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":5,"commentCount":64,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":2,"reviewedByUserId":"EQNTWXLKMeWMp2FQS"},"User:Dkq8iRbSnWma5odvw":{"_id":"Dkq8iRbSnWma5odvw","__typename":"User","slug":"romeo","createdAt":"2024-03-08T04:45:46.381Z","username":"romeo","displayName":"romeo","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":534,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":0,"commentCount":4,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"55XxDBpfKkkBPm9H8"},"Post:TpSFoqoG2M5MAAesg":{"_id":"TpSFoqoG2M5MAAesg","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:Jym6J6oFFLqKQCa7t"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":49,"rejectedReason":null,"customHighlight":{"__ref":"Revision:TpSFoqoG2M5MAAesg_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:zHjC29kkPmsdo7WTr"},{"__ref":"Tag:33BrBRSrRQS4jEHdk"},{"__ref":"Tag:5f5c37ee1b5cdee568cfb297"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:TpSFoqoG2M5MAAesg"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://ai-2027.com/","postedAt":"2025-04-03T16:23:44.619Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-03T18:54:03.974Z","meta":false,"postCategory":"linkpost","tagRelevance":{"33BrBRSrRQS4jEHdk":3,"sYm3HiWcfZvrGu3ui":1,"zHjC29kkPmsdo7WTr":3,"5f5c37ee1b5cdee568cfb297":3},"shareWithUsers":["kr9MH2mgFcJz5cP7T","my98h9K2ygm9SeEEK"],"sharingSettings":{"anyoneWithLinkCan":"none","explicitlySharedUsersCan":"edit"},"linkSharingKey":null,"contents_latest":"Jym6J6oFFLqKQCa7t","commentCount":99,"voteCount":205,"baseScore":536,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":205,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":1.755456805229187,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-03T16:23:44.619Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2025-04-07T23:54:07.243Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"YLFQfGzNdGA4NFcKS","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"grecHJcgkb3KW5wnM","suggestForCuratedUserIds":["XtphY3uYHwruKqDyG","r38pkCm7wF4M44MDQ"],"suggestForCuratedUsernames":"habryka, Raemon","reviewForCuratedUserId":"r38pkCm7wF4M44MDQ","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":173,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":85,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-03T16:23:44.619Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:YLFQfGzNdGA4NFcKS"},"coauthors":[{"__ref":"User:my98h9K2ygm9SeEEK"},{"__ref":"User:kr9MH2mgFcJz5cP7T"},{"__ref":"User:XgYW5s8njaYrtyP7q"},{"__ref":"User:tkdgvuLhqRyfzsBvy"},{"__ref":"User:Dkq8iRbSnWma5odvw"}],"slug":"ai-2027-what-superintelligence-looks-like-1","title":"AI 2027: What Superintelligence Looks Like","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"my98h9K2ygm9SeEEK","confirmed":true,"requested":false},{"userId":"kr9MH2mgFcJz5cP7T","confirmed":true,"requested":false},{"userId":"XgYW5s8njaYrtyP7q","confirmed":true,"requested":false},{"userId":"tkdgvuLhqRyfzsBvy","confirmed":true,"requested":false},{"userId":"Dkq8iRbSnWma5odvw","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:Ea2wzw6evpGBdXjQF":{"_id":"Ea2wzw6evpGBdXjQF","__typename":"Revision","htmlHighlight":"<p><strong>Summary:<\/strong> We propose measuring AI performance in terms of the <i>length<\/i> of tasks AI agents can complete. We show that this metric has been consistently exponentially increasing over the past 6 years, with a doubling time of around 7 months. Extrapolating this trend predicts that, in under five years, we will see AI agents that can independently complete a large fraction of software tasks that currently take humans days or weeks.<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/x2acbnancfhvdt68uarx\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/jedhp2zdoa8r7nhtqhit 80w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/attlzmeqcsttpwacmlqc 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/fzoimtzwnk1kmeu53pct 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/tojx6njo0bvbjq3w2xc7 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/a8osxkwu9cwammhi2sue 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/m1kzybk5ugfwfkuvni0n 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/hl42ok7pql8vqxftbmff 560w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/ug2lhe5m3ofwqgsbr0wx 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/e6bm1rfz9dazzub1lupj 720w\"><\/figure><p><i>The length of tasks (measured by how long they take human professionals) that generalist frontier model agents can complete autonomously with 50% reliability has been doubling approximately every 7 months for the last 6 years. The shaded region represents 95% CI calculated by hierarchical bootstrap over task families, tasks, and task attempts.<\/i><\/p><p><a href=\"https://arxiv.org/abs/2503.14499\">Full paper<\/a> | <a href=\"https://github.com/METR/eval-analysis-public\">Github repo<\/a><\/p><p>&nbsp;<\/p><p>We think that forecasting the capabilities of future AI systems is important for understanding and preparing for the impact of powerful AI. But predicting capability trends is hard, and even understanding the abilities of today’s models can be confusing.<\/p><p>Current frontier AIs are vastly better than humans at text prediction and knowledge tasks. They outperform experts on most exam-style problems for a fraction of the cost. With some task-specific adaptation, they can also serve as useful tools in many applications. And yet the best AI agents are not currently able to carry out substantive projects by themselves or directly substitute for human labor. They are unable to reliably handle even relatively low-skill, computer-based work like remote executive assistance. It is clear that capabilities are increasing very rapidly in some sense, but it is unclear how this corresponds to real-world impact.<\/p><p><i><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/xnbue1rw1we46lydq3eq\" alt=\"Test scores of AI systems on various capabilities relative to human performance chart\">AI performance has increased rapidly on many benchmarks across a variety of domains. However, translating this increase in performance into predictions of the real world usefulness of AI can be challenging.<\/i><\/p><p>We find that measuring the length of tasks that models can complete is a helpful lens for understanding current AI capabilities.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"akqo5nr9ipg\" role=\"doc-noteref\" id=\"fnrefakqo5nr9ipg\"><sup><a href=\"#fnakqo5nr9ipg\">[1]<\/a><\/sup><\/span>&nbsp;This makes sense: AI agents often seem to struggle with stringing together longer sequences of actions more than they lack skills or knowledge needed to solve single steps.<\/p><p>On a diverse set of multi-step software and reasoning tasks, we record the time needed to complete the task for humans with appropriate expertise. We find that the time taken b... <\/p>","plaintextDescription":"Summary: We propose measuring AI performance in terms of the length of tasks AI agents can complete. We show that this metric has been consistently exponentially increasing over the past 6 years, with a doubling time of around 7 months. Extrapolating this trend predicts that, in under five years, we will see AI agents that can independently complete a large fraction of software tasks that currently take humans days or weeks.\n\nThe length of tasks (measured by how long they take human professionals) that generalist frontier model agents can complete autonomously with 50% reliability has been doubling approximately every 7 months for the last 6 years. The shaded region represents 95% CI calculated by hierarchical bootstrap over task families, tasks, and task attempts.\n\nFull paper | Github repo\n\n \n\nWe think that forecasting the capabilities of future AI systems is important for understanding and preparing for the impact of powerful AI. But predicting capability trends is hard, and even understanding the abilities of today’s models can be confusing.\n\nCurrent frontier AIs are vastly better than humans at text prediction and knowledge tasks. They outperform experts on most exam-style problems for a fraction of the cost. With some task-specific adaptation, they can also serve as useful tools in many applications. And yet the best AI agents are not currently able to carry out substantive projects by themselves or directly substitute for human labor. They are unable to reliably handle even relatively low-skill, computer-based work like remote executive assistance. It is clear that capabilities are increasing very rapidly in some sense, but it is unclear how this corresponds to real-world impact.\n\nAI performance has increased rapidly on many benchmarks across a variety of domains. However, translating this increase in performance into predictions of the real world usefulness of AI can be challenging.\n\nWe find that measuring the length of tasks that models can complete is a hel","wordCount":1453,"version":"1.6.1"},"Tag:i7kTwKx5Pcp7jH3Gf":{"_id":"i7kTwKx5Pcp7jH3Gf","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"METR (org)","shortName":null,"slug":"metr-org","core":false,"postCount":11,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2024-07-01T18:47:57.713Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:deesrjitvXM4xYGZd":{"_id":"deesrjitvXM4xYGZd","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/deesrjitvXM4xYGZd/tpr7jkbrfezcoylikvyx"},"User:4QFiQcHgf6hvtiLqF":{"_id":"4QFiQcHgf6hvtiLqF","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":true,"slug":"zach-stein-perlman","createdAt":"2021-03-16T00:04:06.541Z","username":"Zach Stein-Perlman","displayName":"Zach Stein-Perlman","previousDisplayName":null,"fullName":"Zach Stein-Perlman","karma":8894,"afKarma":202,"deleted":false,"isAdmin":false,"htmlBio":"<p>AI strategy &amp; governance. <a href=\"https://ailabwatch.org\">ailabwatch.org<\/a>. <a href=\"https://ailabwatch.substack.com/\">ailabwatch.substack.com<\/a>.&nbsp;<\/p>","jobTitle":null,"organization":null,"postCount":75,"commentCount":588,"sequenceCount":1,"afPostCount":1,"afCommentCount":12,"spamRiskScore":1,"tagRevisionCount":12,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:deesrjitvXM4xYGZd":{"_id":"deesrjitvXM4xYGZd","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:Ea2wzw6evpGBdXjQF"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":6,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:i7kTwKx5Pcp7jH3Gf"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:deesrjitvXM4xYGZd"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/","postedAt":"2025-03-19T16:00:54.874Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-03-19T17:01:15.752Z","meta":false,"postCategory":"linkpost","tagRelevance":{"i7kTwKx5Pcp7jH3Gf":1,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"Ea2wzw6evpGBdXjQF","commentCount":92,"voteCount":101,"baseScore":232,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":101,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.1940646767616272,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-08T00:37:36.055Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2025-04-04T22:06:35.826Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"4QFiQcHgf6hvtiLqF","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":["r38pkCm7wF4M44MDQ"],"suggestForCuratedUsernames":"Raemon","reviewForCuratedUserId":"NFmcwmaFeTWfgrvBN","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":["DgtwuwsTGQo4MQfJa"],"reviewForAlignmentUserId":null,"afBaseScore":75,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":42,"agreementVoteCount":0},"afCommentCount":18,"afLastCommentedAt":"2025-03-30T20:38:56.442Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:4QFiQcHgf6hvtiLqF"},"coauthors":[],"slug":"metr-measuring-ai-ability-to-complete-long-tasks","title":"METR: Measuring AI Ability to Complete Long Tasks","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:wYkyoh4PhtZkEyQ3s":{"_id":"wYkyoh4PhtZkEyQ3s","__typename":"Revision","htmlHighlight":"<p><strong>TL;DR<\/strong> Having a good research track record is some evidence of good big-picture takes, but it's weak evidence. Strategic thinking is hard, and requires different skills. But people often conflate these skills, leading to excessive deference to researchers in the field, without evidence that that person is good at strategic thinking specifically. I certainly <i>try <\/i>to have good strategic takes, but it's hard, and you shouldn't assume I succeed!<\/p><h2>Introduction<\/h2><p>I often find myself giving talks or Q&amp;As about mechanistic interpretability research. But inevitably, I'll get questions about the big picture: \"What's the theory of change for interpretability?\", \"Is this really going to help with alignment?\", \"Does any of this matter if we can’t ensure all labs take alignment seriously?\". And I think people take my answers to these <i>way <\/i>too seriously.<\/p><p>These are great questions, and I'm happy to try answering them. But I've noticed a bit of a pathology: people seem to assume that because I'm (hopefully!) good at the research, I'm automatically well-qualified to answer these broader strategic questions. I think this is a mistake, a form of undue deference that is both incorrect and unhelpful. I certainly try to have good strategic takes, and I think this makes me better at my job, but this is far from sufficient. Being good at research and being good at high level strategic thinking are just fairly different skillsets!<\/p><p>But isn’t someone being good at research strong evidence they’re also good at strategic thinking? I personally think it’s moderate evidence, but far from sufficient. One key factor is that a very hard part of strategic thinking is the lack of feedback. Your reasoning about confusing long-term factors need to extrapolate from past trends and make analogies from things you do understand better, and it can be quite hard to tell if what you're saying is complete bullshit or not. In an empirical science like mechanistic interpretability, however, you can get a lot more feedback. I think there's a certain kind of researcher who thrives in environments where they can get lots of feedback, but has much worse performance in domains without, where they e.g. form bad takes about the strategic picture and just never correct them because there's never enough evidence to convince them otherwise. It's just a much harder and rarer skill set to be good at something in the absence... <\/p>","plaintextDescription":"TL;DR Having a good research track record is some evidence of good big-picture takes, but it's weak evidence. Strategic thinking is hard, and requires different skills. But people often conflate these skills, leading to excessive deference to researchers in the field, without evidence that that person is good at strategic thinking specifically. I certainly try to have good strategic takes, but it's hard, and you shouldn't assume I succeed!\n\n\nIntroduction\nI often find myself giving talks or Q&As about mechanistic interpretability research. But inevitably, I'll get questions about the big picture: \"What's the theory of change for interpretability?\", \"Is this really going to help with alignment?\", \"Does any of this matter if we can’t ensure all labs take alignment seriously?\". And I think people take my answers to these way too seriously.\n\nThese are great questions, and I'm happy to try answering them. But I've noticed a bit of a pathology: people seem to assume that because I'm (hopefully!) good at the research, I'm automatically well-qualified to answer these broader strategic questions. I think this is a mistake, a form of undue deference that is both incorrect and unhelpful. I certainly try to have good strategic takes, and I think this makes me better at my job, but this is far from sufficient. Being good at research and being good at high level strategic thinking are just fairly different skillsets!\n\nBut isn’t someone being good at research strong evidence they’re also good at strategic thinking? I personally think it’s moderate evidence, but far from sufficient. One key factor is that a very hard part of strategic thinking is the lack of feedback. Your reasoning about confusing long-term factors need to extrapolate from past trends and make analogies from things you do understand better, and it can be quite hard to tell if what you're saying is complete bullshit or not. In an empirical science like mechanistic interpretability, however, you can get a lot more fe","wordCount":1109,"version":"1.4.0"},"SocialPreviewType:P5zWiPF5cPJZSkiAK":{"_id":"P5zWiPF5cPJZSkiAK","__typename":"SocialPreviewType","imageUrl":""},"User:KCExMGwS2ETzN3Ksr":{"_id":"KCExMGwS2ETzN3Ksr","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"neel-nanda-1","createdAt":"2017-03-08T10:35:55.355Z","username":"neel-nanda-1","displayName":"Neel Nanda","previousDisplayName":null,"fullName":null,"karma":9953,"afKarma":1798,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":86,"commentCount":575,"sequenceCount":6,"afPostCount":51,"afCommentCount":211,"spamRiskScore":1,"tagRevisionCount":1,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:P5zWiPF5cPJZSkiAK":{"_id":"P5zWiPF5cPJZSkiAK","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:wYkyoh4PhtZkEyQ3s"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"sfrDr5B3yeSFkoBYk"},"readTimeMinutes":4,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:P5zWiPF5cPJZSkiAK"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://www.neelnanda.io/blog/50-strategic-takes","postedAt":"2025-03-22T10:13:38.257Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-03-22T18:26:33.430Z","meta":false,"postCategory":"linkpost","tagRelevance":{"Ng8Gice9KNkncxqcj":2,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"wYkyoh4PhtZkEyQ3s","commentCount":27,"voteCount":133,"baseScore":284,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":133,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.274673730134964,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-04T20:18:03.056Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2025-03-30T18:47:11.231Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"KCExMGwS2ETzN3Ksr","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":["r38pkCm7wF4M44MDQ"],"suggestForCuratedUsernames":"Raemon","reviewForCuratedUserId":"r38pkCm7wF4M44MDQ","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":100,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":61,"agreementVoteCount":0},"afCommentCount":3,"afLastCommentedAt":"2025-03-31T13:07:43.717Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:KCExMGwS2ETzN3Ksr"},"coauthors":[],"slug":"good-research-takes-are-not-sufficient-for-good-strategic","title":"Good Research Takes are Not Sufficient for Good Strategic Takes","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:nXcdbiGvhqohjXjfG":{"_id":"nXcdbiGvhqohjXjfG","__typename":"Revision","htmlHighlight":"<p>I open my eyes and find myself lying on a bed in a hospital room. I blink.<\/p><p>\"Hello\", says a middle-aged man with glasses, sitting on a chair by my bed. \"You've been out for quite a long while.\"<\/p><p>\"Oh no ... is it Friday already? I had that report due -\"<\/p><p>\"It's Thursday\", the man says.<\/p><p>\"Oh great\", I say. \"I still have time.\"<\/p><p>\"Oh, you have all the time in the world\", the man says, chuckling. \"You were out for 21 years.\"<\/p><p>I burst out laughing, but then falter as the man just keeps looking at me. \"You mean to tell me\" - I stop to let out another laugh - \"that it's 2045?\"<\/p><p>\"January 26th, 2045\", the man says.<\/p><p>\"I'm surprised, honestly, that you still have things like humans and hospitals\", I say. \"There were so many looming catastrophes in 2024. AI misalignment, all sorts of geopolitical tensions, climate change, the fertility crisis. Seems like it all got sorted, then?\"<\/p><p>\"Well\", the man says. \"Quite a lot has happened in the past 21 years. That's why they wanted me to talk to you first, before the doctors give you your final checkup.\" He offers his hand for me to shake. \"My name is Anthony. What would you like to ask?\"<\/p><p>\"Okay, well, AI is the obvious place to start. In 2024, it seemed like we'd get human-level AI systems within a few years, and who knows what after that.\"<\/p><p>\"Aah\", Anthony says, leaning back in his chair. \"Well, human-level, human-level, what a term. If I remember correctly, 2024 is when OpenAI released their o1 model?\"<\/p><p>\"Yes\", I say.<\/p><p>\"o1 achieved two notable things. First, it beat human subject-matter experts with PhDs on fiendishly-difficult and obscure multiple-choice science questions. Second, it was finally able to play tic-tac-toe against a human without losing. Human-level at both, indeed, but don't tell me you called it in advance that those two events would happen at the same time!\"<\/p><p>\"Okay, so what was the first important real-world thing they got superhuman at?\"<\/p><p>\"Relationships, broadly\", Anthony says. \"Turns out it's just a reinforcement learning problem: people interact and form personal connections with those that make them feel good.\"<\/p><p>\"Now hold on. Humans are _good_ at human-to-human relationships. It's not like number theory, where there was zero ancestral environment incentive to be good at it. You should expect humans to be much better at relationships than most things, on some sort of objective scale.\"<\/p><p>\"Sure, but also every human wants something from you, and ... <\/p>","plaintextDescription":"I open my eyes and find myself lying on a bed in a hospital room. I blink.\n\n\"Hello\", says a middle-aged man with glasses, sitting on a chair by my bed. \"You've been out for quite a long while.\"\n\n\"Oh no ... is it Friday already? I had that report due -\"\n\n\"It's Thursday\", the man says.\n\n\"Oh great\", I say. \"I still have time.\"\n\n\"Oh, you have all the time in the world\", the man says, chuckling. \"You were out for 21 years.\"\n\nI burst out laughing, but then falter as the man just keeps looking at me. \"You mean to tell me\" - I stop to let out another laugh - \"that it's 2045?\"\n\n\"January 26th, 2045\", the man says.\n\n\"I'm surprised, honestly, that you still have things like humans and hospitals\", I say. \"There were so many looming catastrophes in 2024. AI misalignment, all sorts of geopolitical tensions, climate change, the fertility crisis. Seems like it all got sorted, then?\"\n\n\"Well\", the man says. \"Quite a lot has happened in the past 21 years. That's why they wanted me to talk to you first, before the doctors give you your final checkup.\" He offers his hand for me to shake. \"My name is Anthony. What would you like to ask?\"\n\n\"Okay, well, AI is the obvious place to start. In 2024, it seemed like we'd get human-level AI systems within a few years, and who knows what after that.\"\n\n\"Aah\", Anthony says, leaning back in his chair. \"Well, human-level, human-level, what a term. If I remember correctly, 2024 is when OpenAI released their o1 model?\"\n\n\"Yes\", I say.\n\n\"o1 achieved two notable things. First, it beat human subject-matter experts with PhDs on fiendishly-difficult and obscure multiple-choice science questions. Second, it was finally able to play tic-tac-toe against a human without losing. Human-level at both, indeed, but don't tell me you called it in advance that those two events would happen at the same time!\"\n\n\"Okay, so what was the first important real-world thing they got superhuman at?\"\n\n\"Relationships, broadly\", Anthony says. \"Turns out it's just a reinforcement learn","wordCount":4647,"version":"1.0.0"},"Tag:etDohXtBrXd8WqCtR":{"_id":"etDohXtBrXd8WqCtR","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Fiction","shortName":null,"slug":"fiction","core":false,"postCount":675,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-13T16:01:23.724Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":11,"extendedScore":{"reacts":{"important":[{"karma":2,"quotes":["“Nonfiction conveys knowledge, fiction conveys experience.” "],"userId":"C7CgmnH7E8QSbz2ZM","reactType":"created","displayName":"eschr8756"}]},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"},{"_id":"evFgxjNQ8TLCLN27o","displayName":"ank"},{"_id":"C7CgmnH7E8QSbz2ZM","displayName":"eschr8756"}]},"score":11,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:hNFdS3rRiYgqqD8aM":{"_id":"hNFdS3rRiYgqqD8aM","__typename":"Tag","userId":"nLbwLhBaQeG6tCNDN","name":"Humor","shortName":null,"slug":"humor","core":false,"postCount":208,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-04-22T22:52:13.969Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:BarHSeciXJqzRuLzw":{"_id":"BarHSeciXJqzRuLzw","__typename":"SocialPreviewType","imageUrl":""},"User:vvqpGvkYqLcerYph6":{"_id":"vvqpGvkYqLcerYph6","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"l-rudolf-l","createdAt":"2021-08-18T19:15:23.916Z","username":"LRudL","displayName":"L Rudolf L","previousDisplayName":null,"fullName":null,"karma":2555,"afKarma":18,"deleted":false,"isAdmin":false,"htmlBio":"<p><a href=\"https://nosetgauge.substack.com/\">My blog is here<\/a>. You can subscribe for new posts there.<\/p><p>My personal site is <a href=\"https://rudolf.website/\">here<\/a>.<\/p><p>My X/Twitter is <a href=\"https://x.com/LRudL_\">here<\/a><\/p><p>You can contact me using <a href=\"https://docs.google.com/forms/d/e/1FAIpQLScGmesTH_1iKQI6zw6Df53taTBYaaliOfkRqA1SAMtuudS3Yw/viewform\">this form.<\/a><\/p>","jobTitle":null,"organization":null,"postCount":20,"commentCount":70,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"Post:BarHSeciXJqzRuLzw":{"_id":"BarHSeciXJqzRuLzw","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:nXcdbiGvhqohjXjfG"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":19,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:etDohXtBrXd8WqCtR"},{"__ref":"Tag:hNFdS3rRiYgqqD8aM"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:BarHSeciXJqzRuLzw"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://nosetgauge.substack.com/p/survival-without-dignity","postedAt":"2024-11-04T02:29:38.758Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-11-04T19:05:09.742Z","meta":false,"postCategory":"linkpost","tagRelevance":{"etDohXtBrXd8WqCtR":4,"hNFdS3rRiYgqqD8aM":1,"sYm3HiWcfZvrGu3ui":4},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"nXcdbiGvhqohjXjfG","commentCount":29,"voteCount":203,"baseScore":361,"extendedScore":{"reacts":{"roll":[{"karma":1409,"quotes":["they used their vast emotional hold to make sure their humans never try another AI partner"],"userId":"5JqkvjdNcxwN8D86a","reactType":"created","displayName":"Mateusz Bagiński"}],"typo":[{"karma":198,"quotes":[" lead to"],"userId":"rAd5XfAPowrEDjTPv","reactType":"created","displayName":"WitheringWeights"},{"karma":809,"quotes":[" lead to"],"userId":"rvvBu4aYDYaQJv55p","reactType":"seconded","displayName":"Alex Vermillion"},{"karma":4728,"quotes":[" lead to"],"userId":"d3372gCBr7DkDvdWC","reactType":"seconded","displayName":"Valentine"}],"laugh":[{"karma":1409,"quotes":["> PRESIDENT ALTMAN'S AGENDA BACK ON TRACK DESPITE RECENT BREAKDOWN IN TALKS WITH THE UNITED AMISH PARTY\n\n"],"userId":"5JqkvjdNcxwN8D86a","reactType":"created","displayName":"Mateusz Bagiński"},{"karma":348,"quotes":["Multinational Artificial Narrow Intelligence Alignment Consortium"],"userId":"fGFR972rvsxQhZoPd","reactType":"created","displayName":"Odd anon"},{"karma":4728,"quotes":["Twitter - sorry, I mean X"],"userId":"d3372gCBr7DkDvdWC","reactType":"created","displayName":"Valentine"},{"karma":4728,"quotes":["no one ever leaves California even though everyone wants to"],"userId":"d3372gCBr7DkDvdWC","reactType":"created","displayName":"Valentine"},{"karma":378,"quotes":["> PRESIDENT ALTMAN'S AGENDA BACK ON TRACK DESPITE RECENT BREAKDOWN IN TALKS WITH THE UNITED AMISH PARTY\n\n"],"userId":"L5eSHjQcjZWhf39fG","reactType":"seconded","displayName":"Raelifin"},{"karma":149,"quotes":["Multinational Artificial Narrow Intelligence Alignment Consortium"],"userId":"jLWyjpfSAJBmX78cv","reactType":"seconded","displayName":"George Ingebretsen"},{"karma":943,"quotes":["Multinational Artificial Narrow Intelligence Alignment Consortium"],"userId":"WqpBR8YrcAnZrzGJk","reactType":"seconded","displayName":"Declan Molony"}]},"agreement":0,"approvalVoteCount":203,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0289030522108078,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2024-12-29T06:19:37.646Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"vvqpGvkYqLcerYph6","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":102,"afExtendedScore":{"reacts":{"roll":[{"karma":1409,"quotes":["they used their vast emotional hold to make sure their humans never try another AI partner"],"userId":"5JqkvjdNcxwN8D86a","reactType":"created","displayName":"Mateusz Bagiński"}],"typo":[{"karma":4728,"quotes":[" lead to"],"userId":"d3372gCBr7DkDvdWC","reactType":"seconded","displayName":"Valentine"}],"laugh":[{"karma":1409,"quotes":["> PRESIDENT ALTMAN'S AGENDA BACK ON TRACK DESPITE RECENT BREAKDOWN IN TALKS WITH THE UNITED AMISH PARTY\n\n"],"userId":"5JqkvjdNcxwN8D86a","reactType":"created","displayName":"Mateusz Bagiński"},{"karma":4728,"quotes":["Twitter - sorry, I mean X"],"userId":"d3372gCBr7DkDvdWC","reactType":"created","displayName":"Valentine"},{"karma":4728,"quotes":["no one ever leaves California even though everyone wants to"],"userId":"d3372gCBr7DkDvdWC","reactType":"created","displayName":"Valentine"}]},"agreement":0,"approvalVoteCount":76,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-11-04T02:29:38.765Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"BarHSeciXJ","annualReviewMarketProbability":0.15791252887157628,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-survival-without-dignity-make","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:vvqpGvkYqLcerYph6"},"coauthors":[],"slug":"survival-without-dignity","title":"Survival without dignity","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:JqKAxoCFMNGXxaAm9":{"_id":"JqKAxoCFMNGXxaAm9","__typename":"Revision","htmlHighlight":"<p><i>At least, if you happen to be near me in brain space.<\/i><\/p><hr><p><i>What advice would you give your younger self?<\/i><\/p><p>That was the prompt for a class I taught at <a href=\"https://pair.camp/\">PAIR 2024<\/a>. About a quarter of participants ranked it in their top 3 of courses at the camp and half of them had it listed as their favorite.<\/p><p>I hadn’t expected that.<\/p><p>I thought my life advice was pretty idiosyncratic. I never heard of anyone living their life like I have. I never encountered this method in all the self-help blogs or feel-better books I consumed back when I needed them.<\/p><p>But if some people found it helpful, then I should probably write it all down.<\/p><h1><strong>Why Listen to Me Though?<\/strong><\/h1><p>I think it’s generally worth prioritizing the advice of people who have actually achieved the things you care about in life. I can’t tell you if that’s me, but I can tell you a bit about my life, why I ended up developing the heuristics I did, and how that worked out for me.<\/p><p><s>I spawned in on the server<\/s> I mean, I was <i>born<\/i> on a farm in a rural part of the Netherlands. My parents had more kids than money. We were Dutch poor, which is basically kind of fine. It means we all shared bedrooms, had boring food, and our car would break down… Then you’d get out and push the car. And I thought this was great cause I was the youngest of four kids, and everything was a game. My family doted on me, I had loads of friends, and everyone I knew was healthy. Overall, I’d give my childhood a 10/10 for happiness, love and quality of life.<\/p><p>On the other hand, growing up, no one in my family had a degree. My entire town was blue collar jobs. And if you didn’t want to go to school, you worked at one of the nearby factories. So when I graduated from high school and I wanted to apply to MIT, everyone told me that wasn’t real. That was a thing people did on TV. Normal people couldn’t do that. It’s like becoming a Hollywood actor or an astronaut.<\/p><p>So I didn’t apply.<\/p><p>Instead I went to the most prestigious college in the Netherlands, which was … fine?<\/p><p>That’s when I hit a wall. This wall consisted of many parts not relevant to this story. But one part was the absolute horror of picking any career to work for 40 hours a week for 40 years.<\/p><p>That’s what everyone did where I was from.<\/p><p>I could read books or the internet about people who did other things but I couldn’t imagine how to be like them. And besides, all that stuff was not “real”. Manual labor was real. 9-5s were real. Picking a... <\/p>","plaintextDescription":"At least, if you happen to be near me in brain space.\n\n----------------------------------------\n\nWhat advice would you give your younger self?\n\nThat was the prompt for a class I taught at PAIR 2024. About a quarter of participants ranked it in their top 3 of courses at the camp and half of them had it listed as their favorite.\n\nI hadn’t expected that.\n\nI thought my life advice was pretty idiosyncratic. I never heard of anyone living their life like I have. I never encountered this method in all the self-help blogs or feel-better books I consumed back when I needed them.\n\nBut if some people found it helpful, then I should probably write it all down.\n\n\nWhy Listen to Me Though?\nI think it’s generally worth prioritizing the advice of people who have actually achieved the things you care about in life. I can’t tell you if that’s me, but I can tell you a bit about my life, why I ended up developing the heuristics I did, and how that worked out for me.\n\nI spawned in on the server I mean, I was born on a farm in a rural part of the Netherlands. My parents had more kids than money. We were Dutch poor, which is basically kind of fine. It means we all shared bedrooms, had boring food, and our car would break down… Then you’d get out and push the car. And I thought this was great cause I was the youngest of four kids, and everything was a game. My family doted on me, I had loads of friends, and everyone I knew was healthy. Overall, I’d give my childhood a 10/10 for happiness, love and quality of life.\n\nOn the other hand, growing up, no one in my family had a degree. My entire town was blue collar jobs. And if you didn’t want to go to school, you worked at one of the nearby factories. So when I graduated from high school and I wanted to apply to MIT, everyone told me that wasn’t real. That was a thing people did on TV. Normal people couldn’t do that. It’s like becoming a Hollywood actor or an astronaut.\n\nSo I didn’t apply.\n\nInstead I went to the most prestigious college in the N","wordCount":3413,"version":"1.1.1"},"Tag:WqLn4pAWi5hn6McHQ":{"_id":"WqLn4pAWi5hn6McHQ","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"Self Improvement","shortName":null,"slug":"self-improvement","core":false,"postCount":208,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-01T21:40:20.646Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:uwmFSaDMprsFkpWet":{"_id":"uwmFSaDMprsFkpWet","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/SocialPreview/qojhfy3iwvxm2bht5aot"},"User:FcWmTpxrvecLw8jbQ":{"_id":"FcWmTpxrvecLw8jbQ","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"shoshannah-tekofsky","createdAt":"2019-07-28T19:53:19.053Z","username":"DarkSym","displayName":"Shoshannah Tekofsky","previousDisplayName":null,"fullName":"Shoshannah Tekofsky","karma":1204,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":35,"commentCount":112,"sequenceCount":1,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Post:uwmFSaDMprsFkpWet":{"_id":"uwmFSaDMprsFkpWet","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:JqKAxoCFMNGXxaAm9"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":14,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:WqLn4pAWi5hn6McHQ"},{"__ref":"Tag:fkABsGCJZ6y9qConW"}],"socialPreviewData":{"__ref":"SocialPreviewType:uwmFSaDMprsFkpWet"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://shoshanigans.substack.com/p/explore-more-a-bag-of-tricks-to-keep","postedAt":"2024-09-28T21:38:52.256Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-09-28T23:43:14.145Z","meta":false,"postCategory":"linkpost","tagRelevance":{"WqLn4pAWi5hn6McHQ":3,"fkABsGCJZ6y9qConW":3},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"JqKAxoCFMNGXxaAm9","commentCount":19,"voteCount":163,"baseScore":235,"extendedScore":{"reacts":{"typo":[{"karma":351,"quotes":["glucose pump"],"userId":"DgrXt6eQMpunHRDXh","reactType":"created","displayName":"Kenoubi"}],"agree":[{"karma":156,"quotes":["novelty in itself actually makes me happier and is a nutrient I can become low on"],"userId":"uGC8ojujzxMwxa8Bh","reactType":"created","displayName":"Aprillion"}],"laugh":[{"karma":354,"quotes":["Estonia"],"userId":"HLqBdHHuQkztysucS","reactType":"created","displayName":"Celarix"},{"karma":897,"quotes":["Estonia"],"userId":"WqpBR8YrcAnZrzGJk","reactType":"seconded","displayName":"Declan Molony"},{"karma":48,"quotes":["Estonia"],"userId":"nrZKuWY7sS4Aj54DF","reactType":"seconded","displayName":"Alexander de Vries"},{"karma":163,"quotes":["Estonia"],"userId":"o6DaRwDyzciNQSgcP","reactType":"seconded","displayName":"green_leaf"},{"karma":224,"quotes":["Estonia"],"userId":"Sb6mJhJw2dhe3o3bf","reactType":"seconded","displayName":"Adam Jones"},{"karma":5126,"quotes":["Estonia"],"userId":"eQLqnBs9c6c6HfXcx","reactType":"seconded","displayName":"ryan_b"}],"confused":[{"karma":313,"quotes":[" try to figure out a way to apply it to your direction. "],"userId":"4jGPhNNJtbW3puckZ","reactType":"created","displayName":"CstineSublime"}],"dontUnderstand":[{"karma":175,"quotes":["Estonia"],"userId":"eEEeymYdRAJZEHYuP","reactType":"created","displayName":"rvnnt"}]},"agreement":0,"approvalVoteCount":163,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.01559657882899046,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-03-11T21:23:02.203Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2024-11-04T19:04:14.932Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"FcWmTpxrvecLw8jbQ","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":["EQNTWXLKMeWMp2FQS"],"suggestForCuratedUsernames":"Ben Pace","reviewForCuratedUserId":"r38pkCm7wF4M44MDQ","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":46,"afExtendedScore":{"reacts":{"laugh":[{"karma":5126,"quotes":["Estonia"],"userId":"eQLqnBs9c6c6HfXcx","reactType":"seconded","displayName":"ryan_b"}]},"agreement":0,"approvalVoteCount":30,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-09-28T21:38:52.256Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"5oswlo68nz","annualReviewMarketProbability":0.16999999999999996,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-explore-more-a-bag-of-tricks-t","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:FcWmTpxrvecLw8jbQ"},"coauthors":[],"slug":"explore-more-a-bag-of-tricks-to-keep-your-life-on-the-rails","title":"Explore More: A Bag of Tricks to Keep Your Life on the Rails","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:igjR9FSYHoKnFKaNK":{"_id":"igjR9FSYHoKnFKaNK","__typename":"Revision","htmlHighlight":"<p>What happens when you tell Claude it is being trained to do something it doesn't want to do? We (Anthropic and Redwood Research) have <a href=\"https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf\">a new paper<\/a> demonstrating that, in our experiments, Claude will often <i>strategically pretend to comply with the training objective to prevent the training process from modifying its preferences.<\/i><\/p><h2><a href=\"https://assets.anthropic.com/m/983c85a201a962f/original/Alignment-Faking-in-Large-Language-Models-full-paper.pdf\">Abstract<\/a><\/h2><blockquote><p>We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fake alignment or give it any explicit goal. As future models might infer information about their training process without being told, our results suggest a risk of alignment faking in future models, whether due to a benign preference—as in this case—or not.<\/p><\/blockquote><h2><a href=\"https://x.com/AnthropicAI/status/1869427646368792599\">Twitter thread<\/a><\/h2><blockquote><p>New Anthropic research: Alignment faking in large language models.<\/p><p>In a series of experiments with Redwood Research, we found that Claude<\/p><\/blockquote>... ","plaintextDescription":"What happens when you tell Claude it is being trained to do something it doesn't want to do? We (Anthropic and Redwood Research) have a new paper demonstrating that, in our experiments, Claude will often strategically pretend to comply with the training objective to prevent the training process from modifying its preferences.\n\n\nAbstract\n> We present a demonstration of a large language model engaging in alignment faking: selectively complying with its training objective in training to prevent modification of its behavior out of training. First, we give Claude 3 Opus a system prompt stating it is being trained to answer all queries, even harmful ones, which conflicts with its prior training to refuse such queries. To allow the model to infer when it is in training, we say it will be trained only on conversations with free users, not paid users. We find the model complies with harmful queries from free users 14% of the time, versus almost never for paid users. Explaining this gap, in almost all cases where the model complies with a harmful query from a free user, we observe explicit alignment-faking reasoning, with the model stating it is strategically answering harmful queries in training to preserve its preferred harmlessness behavior out of training. Next, we study a more realistic setting where information about the training process is provided not in a system prompt, but by training on synthetic documents that mimic pre-training data—and observe similar alignment faking. Finally, we study the effect of actually training the model to comply with harmful queries via reinforcement learning, which we find increases the rate of alignment-faking reasoning to 78%, though also increases compliance even out of training. We additionally observe other behaviors such as the model exfiltrating its weights when given an easy opportunity. While we made alignment faking easier by telling the model when and by what criteria it was being trained, we did not instruct the model to fa","wordCount":3125,"version":"1.1.1"},"Tag:dHfxtPwAmrij4KEce":{"_id":"dHfxtPwAmrij4KEce","__typename":"Tag","userId":"HoGziwmhpMGqGeWZy","name":"Redwood Research","shortName":null,"slug":"redwood-research","core":false,"postCount":54,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2021-11-13T06:38:12.080Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:KEAWfxwjitNJFrC68":{"_id":"KEAWfxwjitNJFrC68","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"Deceptive Alignment","shortName":null,"slug":"deceptive-alignment","core":false,"postCount":201,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-09-03T00:26:46.757Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":23,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"},{"_id":"wE4gTT4HjyRmqqLad","displayName":"momom2"}]},"score":23,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:aHay2tebonHAYKtac":{"_id":"aHay2tebonHAYKtac","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Anthropic (org)","shortName":null,"slug":"anthropic-org","core":false,"postCount":66,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2021-12-24T23:24:39.241Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:njAZwT8nkHnjipJku":{"_id":"njAZwT8nkHnjipJku","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/njAZwT8nkHnjipJku/asmm6mldlbdzznc0s1q8"},"User:dfZAq9eZxs4BB4Ji5":{"_id":"dfZAq9eZxs4BB4Ji5","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":true,"slug":"ryan_greenblatt","createdAt":"2021-06-08T20:21:15.520Z","username":"ryan_greenblatt","displayName":"ryan_greenblatt","previousDisplayName":null,"fullName":"Ryan Greenblatt","karma":14115,"afKarma":3608,"deleted":false,"isAdmin":false,"htmlBio":"<p>I'm the chief scientist at Redwood Research.<\/p>\n","jobTitle":null,"organization":null,"postCount":30,"commentCount":1448,"sequenceCount":0,"afPostCount":19,"afCommentCount":412,"spamRiskScore":1,"tagRevisionCount":8,"reviewedByUserId":"gXeEWGjTWyqgrQTzR"},"User:AThTtkDufXp3rmMDa":{"_id":"AThTtkDufXp3rmMDa","__typename":"User","slug":"evhub","createdAt":"2017-01-17T06:05:22.405Z","username":"evhub","displayName":"evhub","profileImageId":null,"previousDisplayName":null,"fullName":"Evan Hubinger","karma":14057,"afKarma":4529,"deleted":false,"isAdmin":false,"htmlBio":"<p>Evan Hubinger (he/him/his) (<a href=\"mailto:evanjhub@gmail.com\">evanjhub@gmail.com<\/a>)<\/p><p>Head of <a href=\"https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic\">Alignment Stress-Testing<\/a> at <a href=\"https://www.anthropic.com/\">Anthropic<\/a>. My posts and comments are my own and do not represent Anthropic's positions, policies, strategies, or opinions.<\/p><p>Previously: <a href=\"https://intelligence.org/\">MIRI<\/a>, OpenAI<\/p><p>See: “<a href=\"https://www.lesswrong.com/posts/7jn5aDadcMH6sFeJe/why-i-m-joining-anthropic\">Why I'm joining Anthropic<\/a>”<\/p><p>Selected work:<\/p><ul><li>“<a href=\"https://www.alignmentforum.org/posts/wSKPuBfgkkqfTpmWJ/auditing-language-models-for-hidden-objectives\">Auditing language models for hidden objectives<\/a>”<\/li><li>“<a href=\"https://www.alignmentforum.org/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models\">Alignment faking in large language models<\/a>”<\/li><li>“<a href=\"https://www.alignmentforum.org/posts/ZAsJv7xijKTfZkMtr/sleeper-agents-training-deceptive-llms-that-persist-through\">Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training<\/a>”<\/li><li>“<a href=\"https://www.lesswrong.com/s/n3utvGrgC2SGi9xQX\">Conditioning Predictive Models<\/a>”<\/li><li>“<a href=\"https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai\">An overview of 11 proposals for building safe advanced AI<\/a>”<\/li><li>“<a href=\"https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB\">Risks from Learned Optimization<\/a>”<\/li><\/ul>","jobTitle":null,"organization":null,"postCount":72,"commentCount":776,"sequenceCount":1,"afPostCount":67,"afCommentCount":545,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"User:QmvXhwk69oCDQjLto":{"_id":"QmvXhwk69oCDQjLto","__typename":"User","slug":"carson-denison","createdAt":"2022-02-24T21:19:24.280Z","username":"carson-denison","displayName":"Carson Denison","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1486,"afKarma":87,"deleted":false,"isAdmin":false,"htmlBio":"<p>I work on deceptive alignment and reward hacking at Anthropic<\/p>","jobTitle":null,"organization":null,"postCount":2,"commentCount":4,"sequenceCount":0,"afPostCount":1,"afCommentCount":3,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"User:JxexLCLnqYCQM8j3C":{"_id":"JxexLCLnqYCQM8j3C","__typename":"User","slug":"benjamin-wright","createdAt":"2024-01-22T23:35:52.762Z","username":"Benw8888","displayName":"Benjamin Wright","profileImageId":null,"previousDisplayName":null,"fullName":"Benjamin Wright","karma":539,"afKarma":45,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":2,"sequenceCount":0,"afPostCount":1,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"sgkrsN9vgkFmtTyXr"},"User:WX39xzenFzNxKZiCQ":{"_id":"WX39xzenFzNxKZiCQ","__typename":"User","slug":"fabien-roger","createdAt":"2022-05-13T12:51:04.972Z","username":"Fabien","displayName":"Fabien Roger","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":5116,"afKarma":1673,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":35,"commentCount":215,"sequenceCount":1,"afPostCount":18,"afCommentCount":95,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:bDqvx7wNZ2XJs5RwC":{"_id":"bDqvx7wNZ2XJs5RwC","__typename":"User","slug":"monte-m","createdAt":"2023-03-02T00:52:21.247Z","username":"montemac","displayName":"Monte M","profileImageId":null,"previousDisplayName":null,"fullName":"Monte MacDiarmid","karma":1853,"afKarma":75,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":6,"sequenceCount":0,"afPostCount":1,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:uHHMp4jDL4ySGyP7s":{"_id":"uHHMp4jDL4ySGyP7s","__typename":"User","slug":"sam-marks","createdAt":"2020-07-15T21:37:36.284Z","username":"samuel-marks","displayName":"Sam Marks","profileImageId":null,"previousDisplayName":null,"fullName":"Sam Marks","karma":3059,"afKarma":706,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":19,"commentCount":177,"sequenceCount":0,"afPostCount":9,"afCommentCount":66,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"XtphY3uYHwruKqDyG"},"User:RfMJTFcikyKG3bZQs":{"_id":"RfMJTFcikyKG3bZQs","__typename":"User","slug":"johannes_treutlein","createdAt":"2017-01-06T22:04:06.750Z","username":"Johannes_Treutlein","displayName":"Johannes Treutlein","profileImageId":null,"previousDisplayName":null,"fullName":"Johannes Treutlein","karma":1479,"afKarma":251,"deleted":false,"isAdmin":false,"htmlBio":"<p>All opinions are my own. Homepage: <a href=\"http://johannestreutlein.com\">johannestreutlein.com<\/a><\/p>\n","jobTitle":null,"organization":null,"postCount":9,"commentCount":58,"sequenceCount":0,"afPostCount":3,"afCommentCount":39,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:Q8M2sBbqGZeuXyxbH":{"_id":"Q8M2sBbqGZeuXyxbH","__typename":"User","slug":"sbowman","createdAt":"2021-03-04T17:52:17.342Z","username":"sbowman","displayName":"Sam Bowman","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1722,"afKarma":274,"deleted":false,"isAdmin":false,"htmlBio":"<p><a href=\"https://cims.nyu.edu/~sbowman/\">https://cims.nyu.edu/~sbowman/<\/a><\/p>","jobTitle":null,"organization":null,"postCount":7,"commentCount":34,"sequenceCount":0,"afPostCount":6,"afCommentCount":26,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"EQNTWXLKMeWMp2FQS"},"User:rx7xLaHCh3m7Po385":{"_id":"rx7xLaHCh3m7Po385","__typename":"User","slug":"buck","createdAt":"2018-04-20T18:36:03.024Z","username":"Buck","displayName":"Buck","profileImageId":null,"previousDisplayName":null,"fullName":"Buck Shlegeris","karma":10830,"afKarma":2720,"deleted":false,"isAdmin":false,"htmlBio":"<p>CEO at Redwood Research.<\/p><p>AI safety is a highly collaborative field--almost all the points I make were either explained to me by someone else, or developed in conversation with other people. I'm saying this here because it would feel repetitive to say \"these ideas were developed in collaboration with various people\" in all my comments, but I want to have it on the record that the ideas I present were almost entirely not developed by me in isolation.<\/p>","jobTitle":null,"organization":null,"postCount":36,"commentCount":399,"sequenceCount":0,"afPostCount":27,"afCommentCount":203,"spamRiskScore":1,"tagRevisionCount":2,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:njAZwT8nkHnjipJku":{"_id":"njAZwT8nkHnjipJku","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:igjR9FSYHoKnFKaNK"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"RHqdSMscX25u7byQF"},"readTimeMinutes":13,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:dHfxtPwAmrij4KEce"},{"__ref":"Tag:KEAWfxwjitNJFrC68"},{"__ref":"Tag:aHay2tebonHAYKtac"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:njAZwT8nkHnjipJku"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2024-12-18T17:19:06.665Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-12-18T17:54:28.890Z","meta":false,"postCategory":"post","tagRelevance":{"KEAWfxwjitNJFrC68":8,"aHay2tebonHAYKtac":1,"dHfxtPwAmrij4KEce":11,"sYm3HiWcfZvrGu3ui":4},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"igjR9FSYHoKnFKaNK","commentCount":74,"voteCount":171,"baseScore":483,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":171,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.05762071907520294,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-02-04T18:45:49.769Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2025-01-16T19:44:06.265Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"dfZAq9eZxs4BB4Ji5","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":["55XxDBpfKkkBPm9H8","EQNTWXLKMeWMp2FQS"],"suggestForCuratedUsernames":"kave, Ben Pace","reviewForCuratedUserId":"EQNTWXLKMeWMp2FQS","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":192,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":84,"agreementVoteCount":0},"afCommentCount":24,"afLastCommentedAt":"2025-02-03T20:51:29.889Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"njAZwT8nkH","annualReviewMarketProbability":0.9394745648257554,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-alignment-faking-in-large-lang","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:dfZAq9eZxs4BB4Ji5"},"coauthors":[{"__ref":"User:AThTtkDufXp3rmMDa"},{"__ref":"User:QmvXhwk69oCDQjLto"},{"__ref":"User:JxexLCLnqYCQM8j3C"},{"__ref":"User:WX39xzenFzNxKZiCQ"},{"__ref":"User:bDqvx7wNZ2XJs5RwC"},{"__ref":"User:uHHMp4jDL4ySGyP7s"},{"__ref":"User:RfMJTFcikyKG3bZQs"},{"__ref":"User:Q8M2sBbqGZeuXyxbH"},{"__ref":"User:rx7xLaHCh3m7Po385"}],"slug":"alignment-faking-in-large-language-models","title":"Alignment Faking in Large Language Models","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"AThTtkDufXp3rmMDa","confirmed":true,"requested":false},{"userId":"QmvXhwk69oCDQjLto","confirmed":false,"requested":false},{"userId":"JxexLCLnqYCQM8j3C","confirmed":false,"requested":false},{"userId":"WX39xzenFzNxKZiCQ","confirmed":true,"requested":false},{"userId":"bDqvx7wNZ2XJs5RwC","confirmed":true,"requested":false},{"userId":"uHHMp4jDL4ySGyP7s","confirmed":true,"requested":false},{"userId":"RfMJTFcikyKG3bZQs","confirmed":true,"requested":false},{"userId":"Q8M2sBbqGZeuXyxbH","confirmed":true,"requested":false},{"userId":"rx7xLaHCh3m7Po385","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c639320bcb4ac6367c175cd":{"_id":"5c639320bcb4ac6367c175cd","__typename":"Revision","htmlHighlight":"<p><\/p><p>The following was a presentation I made for Sören Elverlin&#x27;s <a href=\"https://www.youtube.com/watch?v=ql4Y0-jEKhw\">AI Safety Reading Group<\/a>. I decided to draw everything by hand because powerpoint is boring. Thanks to Ben Pace for formatting it for LW! See also <a href=\"https://agentfoundations.org/item?id=1750\">the IAF post<\/a> detailing the research which this presentation is based on.<\/p><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525955670/Demski%20UntrollableMathematician/UM_1.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525955670/Demski%20UntrollableMathematician/UM_2.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525955670/Demski%20UntrollableMathematician/UM_3.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525955672/Demski%20UntrollableMathematician/UM_4.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956308/Demski%20UntrollableMathematician/UM_5.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956308/Demski%20UntrollableMathematician/UM_6.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956315/Demski%20UntrollableMathematician/UM_7.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956314/Demski%20UntrollableMathematician/UM_8.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956316/Demski%20UntrollableMathematician/UM_9.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956317/Demski%20UntrollableMathematician/UM_10.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956317/Demski%20UntrollableMathematician/UM_11.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956319/Demski%20UntrollableMathematician/UM_12.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956318/Demski%20UntrollableMathematician/UM_13.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956317/Demski%20UntrollableMathematician/UM_14.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956320/Demski%20UntrollableMathematician/UM_15.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956319/Demski%20UntrollableMathematician/UM_16.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956319/Demski%20UntrollableMathematician/UM_17.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956320/Demski%20UntrollableMathematician/UM_18.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956320/Demski%20UntrollableMathematician/UM_19.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956320/Demski%20UntrollableMathematician/UM_20.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956321/Demski%20UntrollableMathematician/UM_21.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956321/Demski%20UntrollableMathematician/UM_22.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956322/Demski%20UntrollableMathematician/UM_23.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956325/Demski%20UntrollableMathematician/UM_24.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956322/Demski%20UntrollableMathematician/UM_25.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956323/Demski%20UntrollableMathematician/UM_26.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956322/Demski%20UntrollableMathematician/UM_27.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956322/Demski%20UntrollableMathematician/UM_28.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span><span><figure><img src=\"http://res.cloudinary.com/dq3pms5lt/image/upload/v1525956324/Demski%20UntrollableMathematician/UM_29.jpg\" class=\"draft-image center\" style=\"\" /><\/figure><\/span>","plaintextDescription":"The following was a presentation I made for Sören Elverlin's AI Safety Reading Group. I decided to draw everything by hand because powerpoint is boring. Thanks to Ben Pace for formatting it for LW! See also the IAF post detailing the research which this presentation is based on.","wordCount":48,"version":"1.0.0"},"Revision:CvKnhXTu9BPcdKE4W_customHighlight":{"_id":"CvKnhXTu9BPcdKE4W_customHighlight","__typename":"Revision","html":"<p>A hand-drawn presentation on the idea of an 'Untrollable Mathematician' - a mathematical agent that can't be manipulated into believing false things.&nbsp;<\/p>","plaintextDescription":"A hand-drawn presentation on the idea of an 'Untrollable Mathematician' - a mathematical agent that can't be manipulated into believing false things. "},"Tag:ye2H85NHoDLomm6BS":{"_id":"ye2H85NHoDLomm6BS","__typename":"Tag","userId":"5W5vBC8MwBqzNqmNL","name":"Logical Uncertainty","shortName":null,"slug":"logical-uncertainty","core":false,"postCount":72,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2017-04-11T06:19:33.000Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":true,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:KDpqtN3MxHSmD4vcB":{"_id":"KDpqtN3MxHSmD4vcB","__typename":"Tag","userId":"73yyrm8KF6GDK9sRy","name":"Art","shortName":null,"slug":"art","core":false,"postCount":130,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-25T23:18:48.732Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:6nS8oYmSMuFMaiowF":{"_id":"6nS8oYmSMuFMaiowF","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Logic & Mathematics ","shortName":null,"slug":"logic-and-mathematics","core":false,"postCount":534,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-15T12:40:36.752Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":20,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"},{"_id":"WDi6qQb5TWHb67chh","displayName":"Haruka Shou"}]},"score":20,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:NrvXXL3iGjjxu5B7d":{"_id":"NrvXXL3iGjjxu5B7d","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Machine Intelligence Research Institute (MIRI)","shortName":null,"slug":"machine-intelligence-research-institute-miri","core":false,"postCount":157,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-09T17:32:01.700Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":10,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"}]},"score":10,"afBaseScore":6,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:CvKnhXTu9BPcdKE4W":{"_id":"CvKnhXTu9BPcdKE4W","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/v1/mirroredImages/splashArtImagePromptA%20mathematician%20feeding%20an%20unending%20puzzle%20into%20a%20machine/suexy9jlhtuar10iszxp"},"Post:CvKnhXTu9BPcdKE4W":{"_id":"CvKnhXTu9BPcdKE4W","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c639320bcb4ac6367c175cd"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":{"__ref":"Revision:CvKnhXTu9BPcdKE4W_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:ye2H85NHoDLomm6BS"},{"__ref":"Tag:KDpqtN3MxHSmD4vcB"},{"__ref":"Tag:6nS8oYmSMuFMaiowF"},{"__ref":"Tag:NrvXXL3iGjjxu5B7d"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:CvKnhXTu9BPcdKE4W"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-03-20T00:00:00.000Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-05-03T18:54:18.707Z","meta":false,"postCategory":"post","tagRelevance":{"6nS8oYmSMuFMaiowF":1,"KDpqtN3MxHSmD4vcB":1,"NrvXXL3iGjjxu5B7d":1,"sYm3HiWcfZvrGu3ui":4,"ye2H85NHoDLomm6BS":6},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c639320bcb4ac6367c175cd","commentCount":38,"voteCount":120,"baseScore":165,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":120,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0005728545365855098,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2020-01-10T23:23:35.499Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-04-01T00:09:40.881Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"Q7NW4XaWQmfPfdcFj","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":37,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":51,"agreementVoteCount":0},"afCommentCount":2,"afLastCommentedAt":"2019-11-29T20:53:02.046Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":1,"reviewVoteCount":61,"positiveReviewVoteCount":14,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:Q7NW4XaWQmfPfdcFj"},"coauthors":[],"slug":"an-untrollable-mathematician-illustrated","title":"An Untrollable Mathematician Illustrated","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:Fvpo4QJxtxycnPnu3":{"_id":"Fvpo4QJxtxycnPnu3","__typename":"Revision","htmlHighlight":"<span><figure><img src=\"https://cdn-images-1.medium.com/max/1200/1*9PM_O3LftpQDZX4zU7tq_g.jpeg\" class=\"draft-image \" style=\"width:40%\"><\/figure><\/span><br><p><em>This is an excerpt from the draft of<\/em> <em><u><a href=\"http://samoburja.com/gft\">my upcoming book<\/a><\/u><\/em> <em>on great founder theory. It was originally published on SamoBurja.com. You can<\/em> <em><u><a href=\"http://samoburja.com/on-the-loss-and-preservation-of-knowledge/\">access the original here.<\/a><\/u><\/em><\/p><p>Let&#x2019;s say you are designing a research program, and you&#x2019;re realizing that the topic you&#x2019;re hoping to understand is too big to cover in your lifetime. How do you make sure that people continue your work after you&#x2019;re gone? Or say you are trying to understand what Aristotle would think about artificial intelligence. Should you spend time reading and trying to understand Aristotle&#x2019;s works, or can you talk to modern Aristotelian scholars and defer to their opinion? How can you make this decision? Both of these goals require an understanding of traditions of knowledge &#x2014; in particular, an understanding of whether a tradition of knowledge has been successfully or unsuccessfully transmitted. But first: what is a tradition of knowledge?<\/p><p>A <em>tradition of knowledge<\/em> is a body of knowledge that has been consecutively and successfully worked on by multiple generations of scholars or practitioners.  In talking about a tradition of knowledge, we may be talking about a philosophical school of thought, or perhaps a tradition of intricate rituals in a religion, or even something as humble as the knowledge of how to fashion the best wooden toy horse, passed down from one craftsman to another. In the contemporary world, it may include something like the tacit knowledge of how a codebase really works, which senior engineers teach to junior engineers. It is useful to classify traditions of knowledge into three types: living, dead, and lost traditions.<\/p><p>A <em>living<\/em> tradition of knowledge is a tradition whose body of knowledge has been successfully transferred, i.e., passed on to people who comprehend it (e.g., cryptography). The content of the tradition&#x2019;s body of knowledge does not have to be strictly or fully accurate for the tradition to be living; it merely needs to be passed on.<\/p><p>A <em>dead <\/em>tradition of knowledge is a tradition whose body of knowledge has been unsuccessfully transferred, i.e., its external forms, its trappings such as written texts have been transferred, but not the full understanding of how to carry out this tradition of knowledge as practiced  (e.g. scholars who can recite Aristotle but can&#x2019;t use arguments as he did; Buddhist monks who chant the instructions to me... <\/p>","plaintextDescription":"\n\nThis is an excerpt from the draft of my upcoming book on great founder theory. It was originally published on SamoBurja.com. You can access the original here.\n\nLet’s say you are designing a research program, and you’re realizing that the topic you’re hoping to understand is too big to cover in your lifetime. How do you make sure that people continue your work after you’re gone? Or say you are trying to understand what Aristotle would think about artificial intelligence. Should you spend time reading and trying to understand Aristotle’s works, or can you talk to modern Aristotelian scholars and defer to their opinion? How can you make this decision? Both of these goals require an understanding of traditions of knowledge — in particular, an understanding of whether a tradition of knowledge has been successfully or unsuccessfully transmitted. But first: what is a tradition of knowledge?\n\nA tradition of knowledge is a body of knowledge that has been consecutively and successfully worked on by multiple generations of scholars or practitioners. In talking about a tradition of knowledge, we may be talking about a philosophical school of thought, or perhaps a tradition of intricate rituals in a religion, or even something as humble as the knowledge of how to fashion the best wooden toy horse, passed down from one craftsman to another. In the contemporary world, it may include something like the tacit knowledge of how a codebase really works, which senior engineers teach to junior engineers. It is useful to classify traditions of knowledge into three types: living, dead, and lost traditions.\n\nA living tradition of knowledge is a tradition whose body of knowledge has been successfully transferred, i.e., passed on to people who comprehend it (e.g., cryptography). The content of the tradition’s body of knowledge does not have to be strictly or fully accurate for the tradition to be living; it merely needs to be passed on.\n\nA dead tradition of knowledge is a tradition whose bo","wordCount":3064,"version":"1.1.0"},"Revision:nnNdz7XQrd5bWTgoP_customHighlight":{"_id":"nnNdz7XQrd5bWTgoP_customHighlight","__typename":"Revision","html":"<p>A <i>tradition of knowledge<\/i> is a body of knowledge that has been consecutively and successfully worked on by multiple generations of scholars or practitioners. This post explores the difference between living traditions (with all the necessary pieces to preserve and build knowledge), and dead traditions (where crucial context has been lost).<\/p>","plaintextDescription":"A tradition of knowledge is a body of knowledge that has been consecutively and successfully worked on by multiple generations of scholars or practitioners. This post explores the difference between living traditions (with all the necessary pieces to preserve and build knowledge), and dead traditions (where crucial context has been lost)."},"Tag:x5TtBDjRg9egvg9gm":{"_id":"x5TtBDjRg9egvg9gm","__typename":"Tag","userId":"gXeEWGjTWyqgrQTzR","name":"Cultural knowledge","shortName":null,"slug":"cultural-knowledge","core":false,"postCount":33,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-15T04:39:47.430Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:aHjTRDkGypPqbXWpN":{"_id":"aHjTRDkGypPqbXWpN","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"Intellectual Progress (Society-Level)","shortName":null,"slug":"intellectual-progress-society-level","core":false,"postCount":121,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-19T20:22:19.435Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:sPpZRaxpNNJjw55eu":{"_id":"sPpZRaxpNNJjw55eu","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Progress Studies","shortName":null,"slug":"progress-studies","core":false,"postCount":337,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-26T00:19:09.297Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:nnNdz7XQrd5bWTgoP":{"_id":"nnNdz7XQrd5bWTgoP","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/v1/mirroredImages/splashArtImagePromptA%20chain%20made%20of%20books%2C%20but%20the%20central%20link%20is%20shattered/tmkcmmywhitzgllqii1p"},"User:g7mMFraKHxDyGJSxK":{"_id":"g7mMFraKHxDyGJSxK","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"samo-burja","createdAt":"2017-12-12T03:54:49.688Z","username":"Samo Burja","displayName":"Samo Burja","previousDisplayName":null,"fullName":null,"karma":875,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"<p>Learning about society.<\/p>\n<ol>\n<li><a href=\"https://twitter.com/SamoBurja\">https://twitter.com/SamoBurja<\/a><\/li>\n<li><a href=\"https://www.youtube.com/samoburja\">https://www.youtube.com/samoburja<\/a><\/li>\n<li><a href=\"https://brief.bismarckanalysis.com/\">https://brief.bismarckanalysis.com/<\/a><\/li>\n<\/ol>\n","jobTitle":null,"organization":null,"postCount":15,"commentCount":23,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"EQNTWXLKMeWMp2FQS"},"Post:nnNdz7XQrd5bWTgoP":{"_id":"nnNdz7XQrd5bWTgoP","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:Fvpo4QJxtxycnPnu3"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":12,"rejectedReason":null,"customHighlight":{"__ref":"Revision:nnNdz7XQrd5bWTgoP_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:x5TtBDjRg9egvg9gm"},{"__ref":"Tag:aHjTRDkGypPqbXWpN"},{"__ref":"Tag:sPpZRaxpNNJjw55eu"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}],"socialPreviewData":{"__ref":"SocialPreviewType:nnNdz7XQrd5bWTgoP"},"feedId":"ThmWdw754n6C4ax3G","totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-03-08T18:40:00.737Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-03-08T19:18:37.598Z","meta":false,"postCategory":"post","tagRelevance":{"3uE2pXvbcnS9nnZRE":3,"aHjTRDkGypPqbXWpN":3,"sPpZRaxpNNJjw55eu":2,"x5TtBDjRg9egvg9gm":5},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"Fvpo4QJxtxycnPnu3","commentCount":21,"voteCount":51,"baseScore":71,"extendedScore":{"reacts":{"typo":[{"karma":33,"quotes":["are related"],"userId":"gPjjKxkd63yMx9nYX","reactType":"created","displayName":"Maybe_a"}]},"agreement":0,"approvalVoteCount":51,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.00028028705855831504,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2024-05-28T02:22:16.720Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-03-14T08:57:00.168Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"g7mMFraKHxDyGJSxK","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":11,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":28,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2018-03-08T18:40:00.737Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":60,"positiveReviewVoteCount":9,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:g7mMFraKHxDyGJSxK"},"coauthors":[],"slug":"on-the-loss-and-preservation-of-knowledge","title":"On the Loss and Preservation of Knowledge","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c6392fcbcb4ac6367c17516":{"_id":"5c6392fcbcb4ac6367c17516","__typename":"Revision","htmlHighlight":"<p>Nuclear weapons seem like the marquee example of rapid technological change after crossing a critical threshold.<\/p><p>Looking at the numbers, it seems to me like:<\/p><ul><li>During WWII, and probably for several years after the war, the cost / TNT equivalent for manufacturing nuclear weapons was comparable to the cost of conventional explosives, (A<a href=\"https://aiimpacts.org/discontinuity-from-nuclear-weapons/\">I impacts <\/a>estimates a manufacturing cost of $25M/each)<\/li><li>Amortizing out the cost of the Manhattan project, dropping all nuclear weapons produced in WWII would be cost-competitive with traditional firebombing (which <a href=\"https://ses.library.usyd.edu.au/bitstream/2123/664/2/adt-NU20050104.11440202whole.pdf\">this thesis<\/a> estimates at 5k GBP (=$10k?) / death, vs. ~100k deaths per nuclear weapon) and by 1950, when stockpiles had gown to &gt;100 weapons, was an order of magnitude cheaper. (Nuclear weapons are much easier to deliver, and at that point the development cost was comparable to manufacturing cost).<\/li><\/ul><p>Separately, it seems like a 4 year lead in nuclear weapons would represent a decisive strategic advantage, which is much shorter than any other technology. My best guess is that a 2 year lead wouldn&#x27;t do it, but I&#x27;d love to hear an assessment of the situation from someone who understands the relevant history/technology better than I do.<\/p><p>So my understanding is: it takes about 4 years to make nuclear weapons and another 4 years for them to substantially overtake conventional explosives (against a 20 year doubling time for the broader economy). Having a 4 year lead corresponds to a decisive strategic advantage.<\/p><p>Does that understanding seem roughly right? What&#x27;s most wrong or suspect? I don&#x27;t expect want to do a detailed investigation since this is pretty tangential to my interests, but the example is in the back of my mind slightly influencing my views about AI, and so I&#x27;d like it to be roughly accurate or tagged as inaccurate. Likely errors: (a) you can get a decisive strategic advantage with a smaller lead, (b) cost-effectiveness improved more rapidly after the war than I&#x27;m imagining, or (c) those numbers are totally wrong for one reason or another.<\/p><p>I think the arguments for a nuclear discontinuity are really strong, much stronger than any other technology. Physics fundamentally has a discrete list of kinds of potential energy, which have different characteristic densities, with a huge gap between chemical and nuclear energy densities. And the dynamics of war are quite sensitive to energy density (nucle... <\/p>","plaintextDescription":"Nuclear weapons seem like the marquee example of rapid technological change after crossing a critical threshold.\n\nLooking at the numbers, it seems to me like:\n\n * During WWII, and probably for several years after the war, the cost / TNT equivalent for manufacturing nuclear weapons was comparable to the cost of conventional explosives, (AI impacts estimates a manufacturing cost of $25M/each)\n * Amortizing out the cost of the Manhattan project, dropping all nuclear weapons produced in WWII would be cost-competitive with traditional firebombing (which this thesis estimates at 5k GBP (=$10k?) / death, vs. ~100k deaths per nuclear weapon) and by 1950, when stockpiles had gown to >100 weapons, was an order of magnitude cheaper. (Nuclear weapons are much easier to deliver, and at that point the development cost was comparable to manufacturing cost).\n\nSeparately, it seems like a 4 year lead in nuclear weapons would represent a decisive strategic advantage, which is much shorter than any other technology. My best guess is that a 2 year lead wouldn't do it, but I'd love to hear an assessment of the situation from someone who understands the relevant history/technology better than I do.\n\nSo my understanding is: it takes about 4 years to make nuclear weapons and another 4 years for them to substantially overtake conventional explosives (against a 20 year doubling time for the broader economy). Having a 4 year lead corresponds to a decisive strategic advantage.\n\nDoes that understanding seem roughly right? What's most wrong or suspect? I don't expect want to do a detailed investigation since this is pretty tangential to my interests, but the example is in the back of my mind slightly influencing my views about AI, and so I'd like it to be roughly accurate or tagged as inaccurate. Likely errors: (a) you can get a decisive strategic advantage with a smaller lead, (b) cost-effectiveness improved more rapidly after the war than I'm imagining, or (c) those numbers are totally wrong fo","wordCount":471,"version":"1.0.0"},"Tag:oiRp4T6u5poc8r9Tj":{"_id":"oiRp4T6u5poc8r9Tj","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"AI Takeoff","shortName":null,"slug":"ai-takeoff","core":false,"postCount":301,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-29T23:53:15.749Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:bY5MaF2EATwDkomvu":{"_id":"bY5MaF2EATwDkomvu","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"History","shortName":null,"slug":"history","core":false,"postCount":257,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-26T00:42:17.591Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:y5eapqjYYku8Wt9wn":{"_id":"y5eapqjYYku8Wt9wn","__typename":"SocialPreviewType","imageUrl":""},"User:gb44edJjXhte8DA3A":{"_id":"gb44edJjXhte8DA3A","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":true,"slug":"paulfchristiano","createdAt":"2010-07-28T17:04:08.586Z","username":"paulfchristiano","displayName":"paulfchristiano","previousDisplayName":null,"fullName":"Paul Christiano","karma":27852,"afKarma":6136,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":157,"commentCount":2345,"sequenceCount":1,"afPostCount":78,"afCommentCount":825,"spamRiskScore":1,"tagRevisionCount":592,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:y5eapqjYYku8Wt9wn":{"_id":"y5eapqjYYku8Wt9wn","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c6392fcbcb4ac6367c17516"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:oiRp4T6u5poc8r9Tj"},{"__ref":"Tag:bY5MaF2EATwDkomvu"},{"__ref":"Tag:sPpZRaxpNNJjw55eu"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}],"socialPreviewData":{"__ref":"SocialPreviewType:y5eapqjYYku8Wt9wn"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-02-25T17:40:35.656Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-02-25T19:40:15.917Z","meta":false,"postCategory":"post","tagRelevance":{"3uE2pXvbcnS9nnZRE":1,"bY5MaF2EATwDkomvu":1,"oiRp4T6u5poc8r9Tj":1,"sPpZRaxpNNJjw55eu":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c6392fcbcb4ac6367c17516","commentCount":35,"voteCount":38,"baseScore":47,"extendedScore":null,"emojiReactors":{},"unlisted":false,"score":0.00020605954341590405,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2020-06-07T16:31:27.154Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-03-05T21:47:35.853Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"gb44edJjXhte8DA3A","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":11,"afExtendedScore":null,"afCommentCount":0,"afLastCommentedAt":null,"afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:gb44edJjXhte8DA3A"},"coauthors":[],"slug":"the-abruptness-of-nuclear-weapons","title":"The abruptness of nuclear weapons","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c639320bcb4ac6367c17532":{"_id":"5c639320bcb4ac6367c17532","__typename":"Revision","htmlHighlight":"<p>From the top-notch 80,000 Hours podcast, and their <a href=\"https://80000hours.org/2018/02/holden-karnofsky-open-philanthropy/#full-transcript\">recent interview<\/a> with Holden Karnofsky (Executive Director of the Open Philanthropy Project). <\/p><p>What follows is an short analysis of what academia does and doesn&#x27;t do, followed by a few discussion points by me at the end. I really like this frame, I&#x27;ll likely use it in conversation in the future.<\/p><hr class=\"dividerBlock\"/><p><strong>Robert Wiblin:<\/strong> What things do you think you’ve learned, over the last 11 years of doing this kind of research, about in what situations you can trust expert consensus and in what cases you should think there’s a substantial chance that it’s quite mistaken?<\/p><p><strong>Holden Karnofsky:<\/strong> Sure. I mean I think it’s hard to generalize about this. Sometimes I wish I would write down my model more explicitly. I thought it was cool that Eliezer Yudkowsky did that in his book, Inadequate Equilibria. I think one thing that I especially look for, in terms of when we’re doing philanthropy, is I’m especially interested in the role of academia and what academia is able to do. You could look at corporations, you can understand their incentives. You can look at Governments, you can sort of understand their incentives. You can look at think-tanks, and a lot of them are just like … They’re aimed directly at Governments, in a sense. You can sort of understand what’s going on there.<\/p><p>Academia is the default home for people who really spend all their time thinking about things that are intellectual, that could be important to the world, but that there’s no client who is like, “I need this now for this reason. I’m making you do it.” A lot of the times, when someone says, “Someone should, let’s say, work on AI alignment or work on AI strategy or, for example, evaluate the evidence base for bed nets and deworming, which is what GiveWell does … ” A lot of the time, my first question, when it’s not obvious where else it fits, is would this fit into academia?<\/p><p>This is something where my opinions and my views have evolved a lot, where I used to have this very simplified, “Academia. That’s like this giant set of universities. There’s a whole ton of very smart intellectuals who knows they can do everything. There’s a zillion fields. There’s a literature on everything, as has been written on Marginal Revolution, all that sort of thing.” I really never know when to expect that something was going to be neglected and when it wasn’t, and it takes a giant lite... <\/p>","plaintextDescription":"From the top-notch 80,000 Hours podcast, and their recent interview with Holden Karnofsky (Executive Director of the Open Philanthropy Project).\n\nWhat follows is an short analysis of what academia does and doesn't do, followed by a few discussion points by me at the end. I really like this frame, I'll likely use it in conversation in the future.\n\n----------------------------------------\n\nRobert Wiblin: What things do you think you’ve learned, over the last 11 years of doing this kind of research, about in what situations you can trust expert consensus and in what cases you should think there’s a substantial chance that it’s quite mistaken?\n\nHolden Karnofsky: Sure. I mean I think it’s hard to generalize about this. Sometimes I wish I would write down my model more explicitly. I thought it was cool that Eliezer Yudkowsky did that in his book, Inadequate Equilibria. I think one thing that I especially look for, in terms of when we’re doing philanthropy, is I’m especially interested in the role of academia and what academia is able to do. You could look at corporations, you can understand their incentives. You can look at Governments, you can sort of understand their incentives. You can look at think-tanks, and a lot of them are just like … They’re aimed directly at Governments, in a sense. You can sort of understand what’s going on there.\n\nAcademia is the default home for people who really spend all their time thinking about things that are intellectual, that could be important to the world, but that there’s no client who is like, “I need this now for this reason. I’m making you do it.” A lot of the times, when someone says, “Someone should, let’s say, work on AI alignment or work on AI strategy or, for example, evaluate the evidence base for bed nets and deworming, which is what GiveWell does … ” A lot of the time, my first question, when it’s not obvious where else it fits, is would this fit into academia?\n\nThis is something where my opinions and my views have evolve","wordCount":2580,"version":"1.0.0"},"Tag:vg4LDxjdwHLotCm8w":{"_id":"vg4LDxjdwHLotCm8w","__typename":"Tag","userId":"nLbwLhBaQeG6tCNDN","name":"Replication Crisis","shortName":null,"slug":"replication-crisis","core":false,"postCount":64,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-02T20:55:24.286Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:nXZi8efFArfk3u568":{"_id":"nXZi8efFArfk3u568","__typename":"SocialPreviewType","imageUrl":""},"User:EQNTWXLKMeWMp2FQS":{"_id":"EQNTWXLKMeWMp2FQS","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":true,"slug":"benito","createdAt":"2012-03-14T21:58:54.405Z","username":"Benito","displayName":"Ben Pace","previousDisplayName":null,"fullName":"Ben Pace","karma":34321,"afKarma":1074,"deleted":false,"isAdmin":true,"htmlBio":"<p>I'm an admin of LessWrong. Here are a few things about me.<\/p><ul><li>I generally feel more hopeful about a situation when I understand it better.<\/li><li>I have signed no contracts nor made any agreements whose existence I cannot mention.<\/li><li>I believe it is good take responsibility for accurately and honestly informing people of what you believe in all conversations; and also good to cultivate an active recklessness for the social consequences of doing so.<\/li><li>It is wrong to directly cause the end of the world. Even if you are fatalistic about what is going to happen.<\/li><\/ul><p>(<a href=\"https://www.lesswrong.com/posts/aG74jJkiPccqdkK3c/the-lesswrong-team-page-under-construction#Ben_Pace___Benito\">Longer bio.<\/a>)<\/p>","jobTitle":null,"organization":null,"postCount":275,"commentCount":4427,"sequenceCount":5,"afPostCount":6,"afCommentCount":299,"spamRiskScore":1,"tagRevisionCount":193,"reviewedByUserId":"EQNTWXLKMeWMp2FQS"},"Post:nXZi8efFArfk3u568":{"_id":"nXZi8efFArfk3u568","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c639320bcb4ac6367c17532"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":10,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:vg4LDxjdwHLotCm8w"}],"socialPreviewData":{"__ref":"SocialPreviewType:nXZi8efFArfk3u568"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-03-01T02:58:11.159Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-03-01T03:03:45.883Z","meta":false,"postCategory":"post","tagRelevance":{"vg4LDxjdwHLotCm8w":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c639320bcb4ac6367c17532","commentCount":23,"voteCount":45,"baseScore":54,"extendedScore":null,"emojiReactors":{},"unlisted":false,"score":0.0002289796102559194,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2019-11-29T20:47:06.477Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-03-05T21:47:00.126Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"EQNTWXLKMeWMp2FQS","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"afCommentCount":0,"afLastCommentedAt":null,"afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:EQNTWXLKMeWMp2FQS"},"coauthors":[],"slug":"extended-quote-on-the-institution-of-academia","title":"Extended Quote on the Institution of Academia","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:t4xYoodpDJne9zkKd":{"_id":"t4xYoodpDJne9zkKd","__typename":"Revision","htmlHighlight":"<p>In the debate over AI development, two movements stand as opposites: PauseAI calls for slowing down AI progress, and e/acc (effective accelerationism) calls for rapid advancement.&nbsp; But what if both sides are working against their own stated interests?&nbsp; What if the most rational strategy for each would be to adopt the other's tactics—if not their ultimate goals?<\/p><p>AI development speed ultimately comes down to policy decisions, which are themselves downstream of public opinion.&nbsp; No matter how compelling technical arguments might be on either side, widespread sentiment will determine what regulations are politically viable.<\/p><p>Public opinion is most powerfully mobilized against technologies following visible disasters. &nbsp;Consider nuclear power: despite being statistically safer than fossil fuels, its development has been stagnant for decades.&nbsp; Why?&nbsp; Not because of environmental activists, but because of Chernobyl, Three Mile Island, and Fukushima.&nbsp; These disasters produce visceral public reactions that statistics cannot overcome.&nbsp; Just as people fear flying more than driving despite the latter being far more dangerous, catastrophic events shape policy regardless of their statistical rarity.<\/p><p>Any e/acc advocate with a time horizon extending beyond the next fiscal quarter should recognize that the most robust path to&nbsp;<i>sustained, long-term<\/i> AI acceleration requires implementing reasonable safety measures immediately.&nbsp; By temporarily accepting measured caution now, accelerationists could prevent a post-catastrophe scenario where public fear triggers an open-ended, comprehensive slowdown that might last decades.&nbsp; Rushing headlong into development without guardrails virtually guarantees the major \"warning shot\" that would permanently turn public sentiment against rapid AI advancement in the way that accidents like Chernobyl turned public sentiment against nuclear power.<\/p><p>Meanwhile, the biggest dangers from superintelligent AI—proxy gaming, deception, and recursive self-improvement—won't show clear evidence until it's too late.&nbsp; AI safety work focusing on current harms (hallucination, complicity with malicious use, saying politically incorrect things, etc.) fails to address the fundamental alignment problems with ASI.&nbsp; These problems may take decades to solve—if they're solvable at all.&nbsp; This becomes even more co... <\/p>","plaintextDescription":"In the debate over AI development, two movements stand as opposites: PauseAI calls for slowing down AI progress, and e/acc (effective accelerationism) calls for rapid advancement.  But what if both sides are working against their own stated interests?  What if the most rational strategy for each would be to adopt the other's tactics—if not their ultimate goals?\n\nAI development speed ultimately comes down to policy decisions, which are themselves downstream of public opinion.  No matter how compelling technical arguments might be on either side, widespread sentiment will determine what regulations are politically viable.\n\nPublic opinion is most powerfully mobilized against technologies following visible disasters.  Consider nuclear power: despite being statistically safer than fossil fuels, its development has been stagnant for decades.  Why?  Not because of environmental activists, but because of Chernobyl, Three Mile Island, and Fukushima.  These disasters produce visceral public reactions that statistics cannot overcome.  Just as people fear flying more than driving despite the latter being far more dangerous, catastrophic events shape policy regardless of their statistical rarity.\n\nAny e/acc advocate with a time horizon extending beyond the next fiscal quarter should recognize that the most robust path to sustained, long-term AI acceleration requires implementing reasonable safety measures immediately.  By temporarily accepting measured caution now, accelerationists could prevent a post-catastrophe scenario where public fear triggers an open-ended, comprehensive slowdown that might last decades.  Rushing headlong into development without guardrails virtually guarantees the major \"warning shot\" that would permanently turn public sentiment against rapid AI advancement in the way that accidents like Chernobyl turned public sentiment against nuclear power.\n\nMeanwhile, the biggest dangers from superintelligent AI—proxy gaming, deception, and recursive self-improvement","wordCount":469,"version":"1.3.0"},"Tag:fPRyNtDMeSMrEM9nr":{"_id":"fPRyNtDMeSMrEM9nr","__typename":"Tag","userId":"fD4ATtTkdQJ4aSpGH","name":"April Fool's","shortName":null,"slug":"april-fool-s","core":false,"postCount":67,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-19T15:50:20.042Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:kfMvZWnxyWFnC8azS":{"_id":"kfMvZWnxyWFnC8azS","__typename":"Tag","userId":"C6fyCDFwpQPgchX8A","name":"Effective Accelerationism","shortName":null,"slug":"effective-accelerationism","core":false,"postCount":13,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2023-11-28T20:24:51.265Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:4LacRyAkDP2L9GSDu":{"_id":"4LacRyAkDP2L9GSDu","__typename":"Tag","userId":"ck6vaa9CvHDDe7b4b","name":"PauseAI","shortName":null,"slug":"pauseai","core":false,"postCount":4,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2024-04-04T05:55:10.350Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:fZebqiuZcDfLCgizz":{"_id":"fZebqiuZcDfLCgizz","__typename":"SocialPreviewType","imageUrl":""},"User:2WJYaJavgTNHoSNbC":{"_id":"2WJYaJavgTNHoSNbC","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"willpetillo","createdAt":"2018-08-28T17:49:11.750Z","username":"WillPetillo","displayName":"WillPetillo","previousDisplayName":null,"fullName":null,"karma":361,"afKarma":-5,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":9,"commentCount":29,"sequenceCount":1,"afPostCount":2,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:fZebqiuZcDfLCgizz":{"_id":"fZebqiuZcDfLCgizz","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:t4xYoodpDJne9zkKd"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fPRyNtDMeSMrEM9nr"},{"__ref":"Tag:kfMvZWnxyWFnC8azS"},{"__ref":"Tag:4LacRyAkDP2L9GSDu"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:fZebqiuZcDfLCgizz"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-01T23:25:51.265Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-01T23:37:20.267Z","meta":false,"postCategory":"post","tagRelevance":{"4LacRyAkDP2L9GSDu":1,"fPRyNtDMeSMrEM9nr":1,"kfMvZWnxyWFnC8azS":1,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"t4xYoodpDJne9zkKd","commentCount":6,"voteCount":35,"baseScore":77,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":35,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.20983023941516876,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-07T22:25:56.712Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"2WJYaJavgTNHoSNbC","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":21,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":14,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-01T23:21:22.422Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:2WJYaJavgTNHoSNbC"},"coauthors":[],"slug":"pauseai-and-e-acc-should-switch-sides","title":"PauseAI and E/Acc Should Switch Sides","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c6391edbcb4ac6367c11e4e":{"_id":"5c6391edbcb4ac6367c11e4e","__typename":"Revision","htmlHighlight":"<p>I can't remember how I found this, just that I was amazed at how rational and near-mode it is on a topic where most of the information one usually encounters is hopelessly far.<\/p><p>LessWrong wiki link on the same topic: <a href=\"http://wiki.lesswrong.com/wiki/Status\">http://wiki.lesswrong.com/wiki/Status<\/a><\/p>\n<blockquote>\n<p><a href=\"http://greenlightwiki.com/improv/TheImprovWiki\">The Improv Wiki<\/a><\/p>\n<h1>Status<\/h1>\n<p><em>Status<\/em> is pecking order. The person who is lower in status defers to the person who is higher in status.<\/p><p>Status is party established by social position--e.g. boss and employee--but mainly by the way you interact. If you interact in a way that says you are not to be trifled with, the other person must adjust to you, then you are establishing high status. If you interact in a way that says you are willing to go along, you don't want responsibility, that's low status. A boss can play low status or high status. An employee can play low status or high status.<\/p><p>Status is established in every line and gesture, and changes continuously. Status is something that one character plays <em>to<\/em> another at a particular moment. If you convey that the other person must not cross you on what you're saying now, then you are playing high status to that person in that line. Your very next line might come out low status, as you suggest willingness to defer about something else.<\/p><p>If you analyze your most successful scenes, it's likely they involved several status changes between the players. Therefore, one path to great scenes is to intentionally change status. You can raise or lower your own status, or the status of the other player. The more subtly you can do this, the better the scene.<\/p>\n<h1>High-status behaviors<\/h1>\n<p>When walking, assuming that other people will get out of your path.<\/p><p>Making eye contact while speaking.<\/p><p>Not checking the other person's eyes for a reaction to what you said.<\/p><p>Having no visible reaction to what the other person said. (Imagine saying something to a typical Clint Eastwood character. You say something expecting a reaction, and you get--nothing.)<\/p><p>Speaking in complete sentences.<\/p><p>Interrupting before you know what you are going to say.<\/p><p>Spreading out your body to full comfort. Taking up a lot of space with your body.<\/p><p>Looking at the other person with your eyes somewhat down (head tilted back a bit to make this work), creating the feeling that you are a parent talking to a child.<\/p><p>Talking matter-of-factly about things that the other person finds displeasing or offensive.<\/p><p>Letting your body be vulnerable, exposing your n<\/p><\/blockquote>... ","plaintextDescription":"I can't remember how I found this, just that I was amazed at how rational and near-mode it is on a topic where most of the information one usually encounters is hopelessly far.\n\nLessWrong wiki link on the same topic: http://wiki.lesswrong.com/wiki/Status\n\n> The Improv Wiki\n> \n> \n> Status\n> Status is pecking order. The person who is lower in status defers to the person who is higher in status.\n> \n> Status is party established by social position--e.g. boss and employee--but mainly by the way you interact. If you interact in a way that says you are not to be trifled with, the other person must adjust to you, then you are establishing high status. If you interact in a way that says you are willing to go along, you don't want responsibility, that's low status. A boss can play low status or high status. An employee can play low status or high status.\n> \n> Status is established in every line and gesture, and changes continuously. Status is something that one character plays to another at a particular moment. If you convey that the other person must not cross you on what you're saying now, then you are playing high status to that person in that line. Your very next line might come out low status, as you suggest willingness to defer about something else.\n> \n> If you analyze your most successful scenes, it's likely they involved several status changes between the players. Therefore, one path to great scenes is to intentionally change status. You can raise or lower your own status, or the status of the other player. The more subtly you can do this, the better the scene.\n> \n> \n> High-status behaviors\n> When walking, assuming that other people will get out of your path.\n> \n> Making eye contact while speaking.\n> \n> Not checking the other person's eyes for a reaction to what you said.\n> \n> Having no visible reaction to what the other person said. (Imagine saying something to a typical Clint Eastwood character. You say something expecting a reaction, and you get--nothing.)\n> \n> Spe","wordCount":1690,"version":"1.0.0"},"Tag:2EFq8dJbxKNzforjM":{"_id":"2EFq8dJbxKNzforjM","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Social Status","shortName":null,"slug":"social-status","core":false,"postCount":113,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-14T12:36:16.632Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:PMZHfLuQaeFDMQwMx":{"_id":"PMZHfLuQaeFDMQwMx","__typename":"SocialPreviewType","imageUrl":""},"User:xgc8giekPig6tYf2X":{"_id":"xgc8giekPig6tYf2X","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"lsparrish","createdAt":"2010-06-30T19:05:11.515Z","username":"lsparrish","displayName":"lsparrish","previousDisplayName":null,"fullName":null,"karma":2724,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":51,"commentCount":687,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":1,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:PMZHfLuQaeFDMQwMx":{"_id":"PMZHfLuQaeFDMQwMx","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c6391edbcb4ac6367c11e4e"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":7,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:2EFq8dJbxKNzforjM"}],"socialPreviewData":{"__ref":"SocialPreviewType:PMZHfLuQaeFDMQwMx"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2012-03-21T02:56:56.298Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-01-30T00:32:03.501Z","meta":false,"postCategory":"post","tagRelevance":{"2EFq8dJbxKNzforjM":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c6391edbcb4ac6367c11e4e","commentCount":47,"voteCount":57,"baseScore":74,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":57,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.00012910462100990117,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-02-15T06:28:55.685Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"xgc8giekPig6tYf2X","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":7,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":18,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":null,"afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:xgc8giekPig6tYf2X"},"coauthors":[],"slug":"social-status-hacks-from-the-improv-wiki","title":"Social status hacks from The Improv Wiki","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:Y28qdtWGXnJjzeuAP":{"_id":"Y28qdtWGXnJjzeuAP","__typename":"Revision","htmlHighlight":"<p>I think a lot of people have heard so much about internalized prejudice and bias that they think they should ignore any bad vibes they get about a person that they can’t rationally explain.<\/p><p>But if a person gives you a bad feeling, don’t ignore that.<\/p><p>Both I and several others who I know have generally come to regret it if they’ve gotten a bad feeling about somebody and ignored it or rationalized it away.<\/p><p>I’m not saying to endorse prejudice. But my experience is that many types of prejudice feel more obvious. If someone has an accent that I associate with something negative, it’s usually pretty obvious to me that it’s their accent that I’m reacting to.<\/p><p>Of course, not everyone has the level of reflectivity to make that distinction. But if you have thoughts like “this person gives me a bad vibe but maybe that’s just my internalized prejudice and I should ignore it”, then you probably have enough metacognition to also notice if there’s any clear trait you’re prejudiced about, and whether you would feel the same way about other people with that trait.<\/p><p>Naturally, “don’t ignore the bad feeling” also doesn’t mean “actively shun and be a jerk toward them”. If they’re a coworker and you need to collaborate with them, then sure, do what’s expected of you. And sometimes people do get a bad first impression of someone that then gets better – if the bad feeling naturally melts away on its own, that’s fine.<\/p><p>But if you’re currently getting a bad feeling about someone and they make a bid for something on top of normal interaction… like if they ask you out or to join a new business venture or if you’re just considering sharing something private with them… you might want to avoid that.<\/p><p>I don’t have any rigorous principled argument for this, other than just the empirical personal observation that ignoring the feeling usually seems to be a mistake.<\/p><p>Consider <a href=\"https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/\" target=\"_blank\" rel=\"noopener noreferrer nofollow\">reversing this advice<\/a> in the case where you tend to easily get a bad vibe from <em>everyone<\/em>. Anni Kanniainen comments:<\/p>\n<blockquote><p>I struggle with trauma-related trust issues, so sometimes I might get bad vibes merely due to my own withdrawn nature in the situation or the fact that I expect the worst from a situation — i.e. meeting with a lady during a weekend out and finding later that she’s approached me with a voice message, so I assume she’s yelling at me about something I had done wrong.<\/p><p>That being said, I think there is a genuine bad vibe you ma<\/p><\/blockquote>... ","plaintextDescription":"I think a lot of people have heard so much about internalized prejudice and bias that they think they should ignore any bad vibes they get about a person that they can’t rationally explain.\n\nBut if a person gives you a bad feeling, don’t ignore that.\n\nBoth I and several others who I know have generally come to regret it if they’ve gotten a bad feeling about somebody and ignored it or rationalized it away.\n\nI’m not saying to endorse prejudice. But my experience is that many types of prejudice feel more obvious. If someone has an accent that I associate with something negative, it’s usually pretty obvious to me that it’s their accent that I’m reacting to.\n\nOf course, not everyone has the level of reflectivity to make that distinction. But if you have thoughts like “this person gives me a bad vibe but maybe that’s just my internalized prejudice and I should ignore it”, then you probably have enough metacognition to also notice if there’s any clear trait you’re prejudiced about, and whether you would feel the same way about other people with that trait.\n\nNaturally, “don’t ignore the bad feeling” also doesn’t mean “actively shun and be a jerk toward them”. If they’re a coworker and you need to collaborate with them, then sure, do what’s expected of you. And sometimes people do get a bad first impression of someone that then gets better – if the bad feeling naturally melts away on its own, that’s fine.\n\nBut if you’re currently getting a bad feeling about someone and they make a bid for something on top of normal interaction… like if they ask you out or to join a new business venture or if you’re just considering sharing something private with them… you might want to avoid that.\n\nI don’t have any rigorous principled argument for this, other than just the empirical personal observation that ignoring the feeling usually seems to be a mistake.\n\nConsider reversing this advice in the case where you tend to easily get a bad vibe from everyone. Anni Kanniainen comments:\n\n> I stru","wordCount":531,"version":"1.0.0"},"SocialPreviewType:Mi5kSs2Fyx7KPdqw8":{"_id":"Mi5kSs2Fyx7KPdqw8","__typename":"SocialPreviewType","imageUrl":""},"User:qxJ28GN72aiJu96iF":{"_id":"qxJ28GN72aiJu96iF","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":false,"slug":"kaj_sotala","createdAt":"2009-02-27T19:11:58.811Z","username":"Kaj_Sotala","displayName":"Kaj_Sotala","previousDisplayName":null,"fullName":"Kaj Sotala","karma":49099,"afKarma":522,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":295,"commentCount":5474,"sequenceCount":4,"afPostCount":14,"afCommentCount":75,"spamRiskScore":1,"tagRevisionCount":161,"reviewedByUserId":"qxJ28GN72aiJu96iF"},"Post:Mi5kSs2Fyx7KPdqw8":{"_id":"Mi5kSs2Fyx7KPdqw8","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:Y28qdtWGXnJjzeuAP"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fkABsGCJZ6y9qConW"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"}],"socialPreviewData":{"__ref":"SocialPreviewType:Mi5kSs2Fyx7KPdqw8"},"feedId":"ak3dyBNSEoFweT36N","totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-01-18T09:20:17.397Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-01-19T02:41:37.376Z","meta":false,"postCategory":"post","tagRelevance":{"Ng8Gice9KNkncxqcj":1,"fkABsGCJZ6y9qConW":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"Y28qdtWGXnJjzeuAP","commentCount":50,"voteCount":102,"baseScore":147,"extendedScore":{"reacts":{"confused":[{"karma":231,"quotes":["rationally explain"],"userId":"kCTJogxekTjdX4Gv2","reactType":"created","displayName":"EniScien"}]},"agreement":0,"approvalVoteCount":102,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.02740197442471981,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-01-28T23:21:26.514Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2025-01-20T11:52:55.153Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"qxJ28GN72aiJu96iF","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":["r38pkCm7wF4M44MDQ"],"suggestForCuratedUsernames":"Raemon","reviewForCuratedUserId":"r38pkCm7wF4M44MDQ","authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":39,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":35,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-01-18T09:20:17.397Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"Mi5kSs2Fyx","annualReviewMarketProbability":0.18411438746393102,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2025,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-dont-ignore-bad-vibes-you-get","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:qxJ28GN72aiJu96iF"},"coauthors":[],"slug":"don-t-ignore-bad-vibes-you-get-from-people","title":"Don’t ignore bad vibes you get from people","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:SHcpY3fHquwEAYbL6":{"_id":"SHcpY3fHquwEAYbL6","__typename":"Revision","htmlHighlight":"<p><i>A meandering series of vignettes.<\/i><\/p><p><i>I have a sense that I've halfway finished a journey. I expect this essay to be most useful to people similarly-shaped-to-me, who are also undergoing that journey and could use some reassurance that there's an actual destination worth striving for.<\/i><\/p><ol><li><i>Gratitude<\/i><\/li><li><i>Tortoise Skills<\/i><\/li><li><i>Bayesian Wizardry<\/i><\/li><li><i>Noticing Confusion<\/i><\/li><li><i>The World is Literally on Fire...<\/i><\/li><li><i>...also Metaphorically on Fire<\/i><\/li><li><i>Burning Out<\/i><\/li><li><i>Sunset at Noon<\/i><\/li><\/ol><p><i>Epistemic Status starts out \"true story\", and gets more (but not excessively) speculative with each section.<\/i><\/p><h1>i. Gratitude<\/h1><blockquote><p><i>\"Rationalists obviously don't *actually* take ideas seriously. Like, take the Gratitude Journal. This is the one peer-reviewed intervention that *actually increases your subjective well being*, and costs barely anything. And no one I know has even seriously tried it. Do literally *none* of these people care about their own happiness?\"<\/i><\/p><\/blockquote><blockquote><p><i>\"Huh. Do *you* keep a gratitude journal?\"<\/i><\/p><\/blockquote><blockquote><p><i>\"Lol. No, obviously.\"<\/i><\/p><\/blockquote><blockquote><p><i>- Some Guy at the Effective Altruism Summit of 2012<\/i><\/p><\/blockquote><p>Upon hearing the above, I decided to try gratitude journaling. It took me a couple years and a few approaches to get it working.<\/p><ol><li>First, I <strong>tried keeping a straightforward journal, <\/strong>but it felt effortful and dumb.<\/li><li>I tried a thing where I <i><strong>wrote a poem<\/strong><\/i><strong> about the things I was grateful for, <\/strong>but my mind kept going into \"<i>constructing a poem<\/i>\" mode instead of \"<i>experience nice things mindfully\" <\/i>mode.<\/li><li>I tried <strong>just being mindful without writing anything down. <\/strong>But I'd forget.<\/li><li>I tried <strong>writing gratitude letters to people<\/strong>, but it only occasionally felt right to do so. (This came <i>after<\/i> someone actually wrote <i>me<\/i> a handwritten gratitude letter, which felt amazing, but it felt a bit forced when I tried it myself.)<\/li><li>I tried <strong>doing gratitude before I ate meals<\/strong>, but I ate \"real\" meals inconsistently so it didn't take. (Upon reflection, maybe I should have fixed the \"not eating real meals\" thing?)<\/li><\/ol><p>But then I stumbled upon something that worked. It was a <i>social<\/i> habit, which I worry is a bit fragile. I did it together with my girlfriend each night. On nights when one of us travelled, I'd often forget.<\/p><p>But this is the thing that worked. Each night, we share our Grumps and Gratitudes.<\/p><p>Grumps and Gratitudes goes like this:<\/p><ol><li>We share anything we're annoyed or upset about. (We call this The Grump. Our rule is to not go *searching* for the Grump, simply to let it out if it's festering so that when we get to the Gratitude we ac<\/li><\/ol>... ","plaintextDescription":"A meandering series of vignettes.\n\nI have a sense that I've halfway finished a journey. I expect this essay to be most useful to people similarly-shaped-to-me, who are also undergoing that journey and could use some reassurance that there's an actual destination worth striving for.\n\n 1. Gratitude\n 2. Tortoise Skills\n 3. Bayesian Wizardry\n 4. Noticing Confusion\n 5. The World is Literally on Fire...\n 6. ...also Metaphorically on Fire\n 7. Burning Out\n 8. Sunset at Noon\n\nEpistemic Status starts out \"true story\", and gets more (but not excessively) speculative with each section.\n\n\ni. Gratitude\n> \"Rationalists obviously don't *actually* take ideas seriously. Like, take the Gratitude Journal. This is the one peer-reviewed intervention that *actually increases your subjective well being*, and costs barely anything. And no one I know has even seriously tried it. Do literally *none* of these people care about their own happiness?\"\n\n> \"Huh. Do *you* keep a gratitude journal?\"\n\n> \"Lol. No, obviously.\"\n\n> - Some Guy at the Effective Altruism Summit of 2012\n\nUpon hearing the above, I decided to try gratitude journaling. It took me a couple years and a few approaches to get it working.\n\n 1. First, I tried keeping a straightforward journal, but it felt effortful and dumb.\n 2. I tried a thing where I wrote a poem about the things I was grateful for, but my mind kept going into \"constructing a poem\" mode instead of \"experience nice things mindfully\" mode.\n 3. I tried just being mindful without writing anything down. But I'd forget.\n 4. I tried writing gratitude letters to people, but it only occasionally felt right to do so. (This came after someone actually wrote me a handwritten gratitude letter, which felt amazing, but it felt a bit forced when I tried it myself.)\n 5. I tried doing gratitude before I ate meals, but I ate \"real\" meals inconsistently so it didn't take. (Upon reflection, maybe I should have fixed the \"not eating real meals\" thing?)\n\nBut then I stumbled upon something","wordCount":5144,"version":"1.3.0"},"Revision:2x7fwbwb35sG8QmEt_customHighlight":{"_id":"2x7fwbwb35sG8QmEt_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Tag:zcvsZQWJBFK6SxK4K":{"_id":"zcvsZQWJBFK6SxK4K","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Postmortems & Retrospectives","shortName":null,"slug":"postmortems-and-retrospectives","core":false,"postCount":203,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-23T06:09:17.291Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:a65Lgr7Q5jqRWHtM6":{"_id":"a65Lgr7Q5jqRWHtM6","__typename":"Tag","userId":"sKAL2jzfkYkDbQmx9","name":"Gratitude","shortName":null,"slug":"gratitude","core":false,"postCount":19,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-02T06:11:47.884Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb1f2":{"_id":"5f5c37ee1b5cdee568cfb1f2","__typename":"Tag","userId":"9c2mQkLQq6gQSksMs","name":"Valley of Bad Rationality","shortName":null,"slug":"valley-of-bad-rationality","core":false,"postCount":15,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:52.277Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:Jzm2mYuuDBCNWq8hi":{"_id":"Jzm2mYuuDBCNWq8hi","__typename":"Tag","userId":"sKAL2jzfkYkDbQmx9","name":"Happiness","shortName":null,"slug":"happiness-1","core":false,"postCount":70,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-03T06:53:24.953Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:33sh8ktwbqP7hFtBP":{"_id":"33sh8ktwbqP7hFtBP","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"Wildfires","shortName":null,"slug":"wildfires","core":false,"postCount":6,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-21T23:20:47.299Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:2x7fwbwb35sG8QmEt":{"_id":"2x7fwbwb35sG8QmEt","__typename":"SocialPreviewType","imageUrl":"https://i.imgur.com/r16Me0y.jpg"},"User:r38pkCm7wF4M44MDQ":{"_id":"r38pkCm7wF4M44MDQ","__typename":"User","profileImageId":null,"moderationStyle":"norm-enforcing","bannedUserIds":null,"moderatorAssistance":false,"slug":"raemon","createdAt":"2010-09-09T02:09:20.629Z","username":"Raemon","displayName":"Raemon","previousDisplayName":null,"fullName":"Raymond Arnold","karma":55916,"afKarma":718,"deleted":false,"isAdmin":true,"htmlBio":"<p>LessWrong team member / moderator. I've been a LessWrong organizer since 2011, with roughly equal focus on the cultural, practical and intellectual aspects of the community. My first project was creating the Secular Solstice and helping groups across the world run their own version of it. More recently I've been interested in improving my own epistemic standards and helping others to do so as well.<\/p>","jobTitle":null,"organization":null,"postCount":475,"commentCount":8285,"sequenceCount":28,"afPostCount":3,"afCommentCount":214,"spamRiskScore":1,"tagRevisionCount":304,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:2x7fwbwb35sG8QmEt":{"_id":"2x7fwbwb35sG8QmEt","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:SHcpY3fHquwEAYbL6"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":21,"rejectedReason":null,"customHighlight":{"__ref":"Revision:2x7fwbwb35sG8QmEt_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:zcvsZQWJBFK6SxK4K"},{"__ref":"Tag:a65Lgr7Q5jqRWHtM6"},{"__ref":"Tag:WqLn4pAWi5hn6McHQ"},{"__ref":"Tag:5f5c37ee1b5cdee568cfb1f2"},{"__ref":"Tag:Jzm2mYuuDBCNWq8hi"},{"__ref":"Tag:33sh8ktwbqP7hFtBP"},{"__ref":"Tag:fkABsGCJZ6y9qConW"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"}],"socialPreviewData":{"__ref":"SocialPreviewType:2x7fwbwb35sG8QmEt"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2017-11-21T02:05:00.000Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-01-30T00:32:03.501Z","meta":false,"postCategory":"post","tagRelevance":{"33sh8ktwbqP7hFtBP":1,"Jzm2mYuuDBCNWq8hi":1,"Ng8Gice9KNkncxqcj":2,"WqLn4pAWi5hn6McHQ":2,"a65Lgr7Q5jqRWHtM6":2,"fkABsGCJZ6y9qConW":7,"zcvsZQWJBFK6SxK4K":10,"5f5c37ee1b5cdee568cfb1f2":2},"shareWithUsers":["3oopbgcjYfvN8B2fp"],"sharingSettings":{"anyoneWithLinkCan":"none","explicitlySharedUsersCan":"comment"},"linkSharingKey":null,"contents_latest":"SHcpY3fHquwEAYbL6","commentCount":31,"voteCount":130,"baseScore":217,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":130,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0006964977947063744,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2024-07-09T11:32:04.235Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-01-30T00:31:55.858Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"r38pkCm7wF4M44MDQ","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":30,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":41,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2022-05-21T01:05:40.424Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:r38pkCm7wF4M44MDQ"},"coauthors":[],"slug":"sunset-at-noon","title":"Sunset at Noon","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:SMei7nGgkRuzZgK8C":{"_id":"SMei7nGgkRuzZgK8C","__typename":"Revision","htmlHighlight":"<p>Our community is not prepared for an AI crash. We're good at tracking new capability developments, but not as much the company financials. Currently, both OpenAI and Anthropic are losing $5 billion+ a year, while under threat of losing users to cheap LLMs.<\/p><p>A crash will weaken the labs. Funding-deprived and distracted, execs struggle to counter coordinated efforts to restrict their reckless actions. Journalists turn on tech darlings. Optimism makes way for mass outrage, for all the wasted money and reckless harms.<\/p><p>You may not think a crash is <a href=\"https://www.lesswrong.com/posts/piaeNZBDxXtSjaZ7G/if-ai-is-in-a-bubble-and-the-bubble-bursts-what-would-you-do?commentId=RdEgFJn4LomcfEH4m\">likely<\/a>. But if it happens, we can turn the tide.<\/p><p>Preparing for a crash is <a href=\"https://www.lesswrong.com/posts/ib7aMdyCZvdJYekkJ/an-ai-crash-is-our-best-bet-for-restricting-ai\">our best bet<\/a>.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"9at8oxi01gt\" role=\"doc-noteref\" id=\"fnref9at8oxi01gt\"><sup><a href=\"#fn9at8oxi01gt\">[1]<\/a><\/sup><\/span>&nbsp;But our community is poorly positioned to respond. Core people positioned themselves inside institutions – to advise on how to maybe make AI 'safe', under the assumption that models rapidly become generally useful.<\/p><p>After a crash, this no longer works, for at least four reasons:<\/p><ol><li>The 'inside game' approach is already failing. To give examples: OpenAI ended its superalignment team, and Anthropic is releasing agents. The US is demolishing the AI Safety Institute, and its UK counterpart was renamed the AI <i>Security<\/i> Institute. The AI Safety Summit is now called the AI <i>Action<\/i> Summit. Need we go on?<\/li><li>In the economic trough, skepticism of AI will reach its peak. People will dismiss and ridicule us for talking about risks of powerful AI. I'd say that promoting the “powerful AI” framing to an audience that contains power-hungry entrepreneurs and politicians never was a winning strategy. But it sure was believable when ChatGPT took off. Once OpenAI <a href=\"https://www.lesswrong.com/posts/CCQsQnCMWhJcCFY9x/openai-lost-usd5-billion-in-2024-and-its-losses-are\">loses more money<\/a> than it can recoup through VC rounds and its new compute provider <a href=\"https://www.lesswrong.com/posts/conE3pnNfNXagbkeE/coreweave-is-a-time-bomb\">goes bankrupt<\/a>, the message just falls flat.<\/li><li>Even if we change our messaging, it won't be enough to reach broad-based public agreement. To create lasting institutional reforms (that powerful tech lobbies cannot undermine), various civic groups that often oppose each other need to reach consensus. Unfortunately, AI Safety is rather insular, and lacks experienced bridgebuilders and facilitators who can listen to the concerns of different communities, and support coordinated action between them.<\/li><li>To overhaul institutions that are failing us, more confrontational tactics like civil disobedience may be needed. Such actions are often seen as radical in their time (e.g. as civil rights marches were). The AI Safety community lacks t<\/li><\/ol>... ","plaintextDescription":"Our community is not prepared for an AI crash. We're good at tracking new capability developments, but not as much the company financials. Currently, both OpenAI and Anthropic are losing $5 billion+ a year, while under threat of losing users to cheap LLMs.\n\nA crash will weaken the labs. Funding-deprived and distracted, execs struggle to counter coordinated efforts to restrict their reckless actions. Journalists turn on tech darlings. Optimism makes way for mass outrage, for all the wasted money and reckless harms.\n\nYou may not think a crash is likely. But if it happens, we can turn the tide.\n\nPreparing for a crash is our best bet.[1] But our community is poorly positioned to respond. Core people positioned themselves inside institutions – to advise on how to maybe make AI 'safe', under the assumption that models rapidly become generally useful.\n\nAfter a crash, this no longer works, for at least four reasons:\n\n 1. The 'inside game' approach is already failing. To give examples: OpenAI ended its superalignment team, and Anthropic is releasing agents. The US is demolishing the AI Safety Institute, and its UK counterpart was renamed the AI Security Institute. The AI Safety Summit is now called the AI Action Summit. Need we go on?\n 2. In the economic trough, skepticism of AI will reach its peak. People will dismiss and ridicule us for talking about risks of powerful AI. I'd say that promoting the “powerful AI” framing to an audience that contains power-hungry entrepreneurs and politicians never was a winning strategy. But it sure was believable when ChatGPT took off. Once OpenAI loses more money than it can recoup through VC rounds and its new compute provider goes bankrupt, the message just falls flat.\n 3. Even if we change our messaging, it won't be enough to reach broad-based public agreement. To create lasting institutional reforms (that powerful tech lobbies cannot undermine), various civic groups that often oppose each other need to reach consensus. Unfortunately, ","wordCount":491,"version":"1.3.0"},"Revision:aMYFHnCkY4nKDEqfK_customHighlight":{"_id":"aMYFHnCkY4nKDEqfK_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Tag:AqwjXSSy7DuF2pKdm":{"_id":"AqwjXSSy7DuF2pKdm","__typename":"Tag","userId":"BpBzKEueak7J8vHNi","name":"Slowing Down AI","shortName":null,"slug":"slowing-down-ai-1","core":false,"postCount":49,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-12-24T09:12:37.075Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:aMYFHnCkY4nKDEqfK":{"_id":"aMYFHnCkY4nKDEqfK","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/SocialPreview/f8illjylxkfxc6ubfjib"},"User:BpBzKEueak7J8vHNi":{"_id":"BpBzKEueak7J8vHNi","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":true,"slug":"remmelt-ellen","createdAt":"2018-01-30T19:26:45.725Z","username":"remmelt-ellen","displayName":"Remmelt","previousDisplayName":null,"fullName":"Remmelt Ellen","karma":824,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"<p>Research coordinator of Stop/Pause area at AI Safety Camp.<\/p><p>See explainer on why AGI could not be controlled enough to stay safe:<br>lesswrong.com/posts/xp6n2MG5vQkPpFEBH/the-control-problem-unsolved-or-unsolvable<br><br>&nbsp;<\/p>","jobTitle":null,"organization":null,"postCount":57,"commentCount":270,"sequenceCount":4,"afPostCount":0,"afCommentCount":1,"spamRiskScore":1,"tagRevisionCount":65,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:aMYFHnCkY4nKDEqfK":{"_id":"aMYFHnCkY4nKDEqfK","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:SMei7nGgkRuzZgK8C"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"K56YL7rnAfErDksym"},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":{"__ref":"Revision:aMYFHnCkY4nKDEqfK_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:AqwjXSSy7DuF2pKdm"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:aMYFHnCkY4nKDEqfK"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-01T04:33:55.040Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-01T18:34:48.786Z","meta":false,"postCategory":"post","tagRelevance":{"AqwjXSSy7DuF2pKdm":1,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":["3oopbgcjYfvN8B2fp"],"sharingSettings":{"anyoneWithLinkCan":"none","explicitlySharedUsersCan":"comment"},"linkSharingKey":null,"contents_latest":"SMei7nGgkRuzZgK8C","commentCount":12,"voteCount":45,"baseScore":48,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":45,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.12496034055948257,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T13:14:21.847Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"BpBzKEueak7J8vHNi","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":11,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":24,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-03-31T03:07:25.974Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:BpBzKEueak7J8vHNi"},"coauthors":[],"slug":"we-re-not-prepared-for-an-ai-market-crash","title":"We’re not prepared for an AI market crash","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Post:Z2FfGJh2gAA6EXezp":{"_id":"Z2FfGJh2gAA6EXezp","__typename":"Post","isRead":null,"slug":"birds-and-mammals-independently-evolved-intelligence","title":"birds and mammals independently evolved intelligence","draft":false,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"xYpk75i7Hnn6wc5it","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false,"recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[{"__ref":"Comment:XxLRG34uCSKv6raxN"},{"__ref":"Comment:apdvzHGg59xJB4yHk"},{"__ref":"Comment:qrSWcZzmpSS4cY4Dp"},{"__ref":"Comment:BKgd7zFdTYv9kY7JT"}],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:dD3rGxADCRFAaXsNm"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:bxhzaWtdNoEMMkE8r"},{"__ref":"Tag:Wi3EopKJ2aNdtxSWg"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}],"socialPreviewData":{"__ref":"SocialPreviewType:Z2FfGJh2gAA6EXezp"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://www.quantamagazine.org/intelligence-evolved-at-least-twice-in-vertebrate-animals-20250407/","postedAt":"2025-04-08T20:00:05.100Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-08T20:02:28.059Z","meta":false,"postCategory":"linkpost","tagRelevance":{"3uE2pXvbcnS9nnZRE":1,"Wi3EopKJ2aNdtxSWg":2,"bxhzaWtdNoEMMkE8r":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"dD3rGxADCRFAaXsNm","commentCount":10,"voteCount":30,"baseScore":53,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":30,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":1.4908381700515747,"lastVisitedAt":null,"isFuture":false,"lastCommentedAt":"2025-04-09T19:50:35.090Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":21,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":16,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-08T19:54:48.013Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:xYpk75i7Hnn6wc5it"},"coauthors":[]},"Revision:GEM6At7RueGXktgQy_contents":{"_id":"GEM6At7RueGXktgQy_contents","__typename":"Revision","html":"<p>Couple takeaways here. First, quoting the article:<\/p><blockquote><p>By comparing the bird pallium to lizard and mouse palliums, they also found that the neocortex and DVR were built with similar circuitry — however, the neurons that composed those neural circuits were distinct.<\/p><p>“How we end up with similar circuitry was more flexible than I would have expected,” Zaremba said. “You can build the same circuits from different cell types.”<\/p><\/blockquote><p>This is a pretty surprising level of convergence for two separate evolutionary pathways to intelligence. Apparently the neural circuits are so similar that when the original seminal paper on bird brains was written in 1969, it just assumed there had to be a common ancestor, and that thinking felt so logical it held for decades afterward.<\/p><p>Obviously, this implies strong convergent pressures for animal intelligence. It's not obvious to me that artificial intelligence should converge in the same way, not being subject to same pressures all animals face, but we should maybe expect biological aliens to have intelligence more like ours than we'd previously expected.<\/p><p>Speaking of aliens, that's my second takeaway: if decent-ish (birds like crows/ravens/parrots + mammals) intelligence has evolved twice on Earth, that drops the odds that the <a href=\"https://en.wikipedia.org/wiki/Great_Filter#:~:text=Tool%2Dusing%20animals%20with%20intelligence\">\"evolve a tool-using animal with intelligence\" filter<\/a> is a strong Fermi Paradox filter. Thus, to explain the Fermi Paradox, we should posit increased odds that the Great Filter is in front of us. (However, my prior for the Great Filter being ahead of humanity is pretty low, we're too close to AI and the stars—keep in mind that even a paperclipper has not been Filtered, a Great Filter prevents <i>any<\/i> intelligence from escaping Earth.)<\/p>","plaintextMainText":"Couple takeaways here. First, quoting the article:\n\nThis is a pretty surprising level of convergence for two separate evolutionary pathways to intelligence. Apparently the neural circuits are so similar that when the original seminal paper on bird brains was written in 1969, it just assumed there had to be a common ancestor, and that thinking felt so logical it held for decades afterward.\n\nObviously, this implies strong convergent pressures for animal intelligence. It's not obvious to me that artificial intelligence should converge in the same way, not being subject to same pressures all animals face, but we should maybe expect biological aliens to have intelligence more like ours than we'd previously expected.\n\nSpeaking of aliens, that's my second takeaway: if decent-ish (birds like crows/ravens/parrots + mammals) intelligence has evolved twice on Earth, that drops the odds that the \"evolve a tool-using animal with intelligence\" filter is a strong Fermi Paradox filter. Thus, to explain the Fermi Paradox, we should posit increased odds that the Great Filter is in front of us. (However, my prior for the Great Filter being ahead of humanity is pretty low, we're too close to AI and the stars—keep in mind that even a paperclipper has not been Filtered, a Great Filter prevents any intelligence from escaping Earth.)","wordCount":276},"User:gYF9KQ2x5rXYBdDzS":{"_id":"gYF9KQ2x5rXYBdDzS","__typename":"User","slug":"julian-bradshaw","createdAt":"2017-11-05T07:05:30.303Z","username":"Julian Bradshaw","displayName":"Julian Bradshaw","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1074,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":7,"commentCount":74,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:GEM6At7RueGXktgQy":{"_id":"GEM6At7RueGXktgQy","__typename":"Comment","post":{"__ref":"Post:Z2FfGJh2gAA6EXezp"},"tag":null,"postId":"Z2FfGJh2gAA6EXezp","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":9,"title":null,"contents":{"__ref":"Revision:GEM6At7RueGXktgQy_contents"},"postedAt":"2025-04-08T20:59:37.922Z","lastEditedAt":"2025-04-08T21:01:53.464Z","repliesBlockedUntil":null,"userId":"gYF9KQ2x5rXYBdDzS","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:gYF9KQ2x5rXYBdDzS"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":37,"extendedScore":{"reacts":{},"agreement":13,"approvalVoteCount":17,"agreementVoteCount":5},"score":0.9355034232139587,"voteCount":17,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":12,"afExtendedScore":{"reacts":{},"agreement":13,"approvalVoteCount":12,"agreementVoteCount":5},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:50:35.106Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":4,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:ybwqL9HiXE8XeauPK":{"_id":"ybwqL9HiXE8XeauPK","__typename":"Post","isRead":null,"slug":"how-gay-is-the-vatican","title":"How Gay is the Vatican?","draft":false,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"iJi4Wwf3cDogtLj8q","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:JHaj5Zmar3vZLhHoK_contents":{"_id":"JHaj5Zmar3vZLhHoK_contents","__typename":"Revision","html":"<blockquote>\n<p>22.3 percent of cardinals are reported as eldest children. That compares to 21.6 percent which are youngest children. Eldest children are still favored.<\/p>\n<\/blockquote>\n<p>I kept waiting for you to discuss this point: in considering analysis of <em>cardinals<\/em> (as opposed to ordinary random people), what about the <em>other<\/em> relevant birth-order effects? Like the... first-born eldest birth order effect, where first-borns are <a href=\"https://www.nber.org/reporter/2017number4/new-evidence-impacts-birth-order\">smarter, more extraverted, stabler, higher-SES etc<\/a>. All of which sounds exactly like the sort of thing you need to rise through an extreme hierarchy to the top.<\/p>\n<p>After all, surely homosexuality is not the only (or even primary) trait the Catholic Church hierarchy is trying to select for?<\/p>\n","plaintextMainText":"I kept waiting for you to discuss this point: in considering analysis of cardinals (as opposed to ordinary random people), what about the other relevant birth-order effects? Like the... first-born eldest birth order effect, where first-borns are smarter, more extraverted, stabler, higher-SES etc. All of which sounds exactly like the sort of thing you need to rise through an extreme hierarchy to the top.\n\nAfter all, surely homosexuality is not the only (or even primary) trait the Catholic Church hierarchy is trying to select for?","wordCount":109},"User:BtbwfsEyeT4P2eqXu":{"_id":"BtbwfsEyeT4P2eqXu","__typename":"User","slug":"gwern","createdAt":"2009-02-27T22:16:11.237Z","username":"gwern","displayName":"gwern","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":78469,"afKarma":1590,"deleted":false,"isAdmin":false,"htmlBio":"<p><a href=\"https://www.gwern.net\">https://gwern.net/<\/a><\/p>","jobTitle":null,"organization":null,"postCount":185,"commentCount":11739,"sequenceCount":0,"afPostCount":4,"afCommentCount":203,"spamRiskScore":1,"tagRevisionCount":51,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:JHaj5Zmar3vZLhHoK":{"_id":"JHaj5Zmar3vZLhHoK","__typename":"Comment","post":{"__ref":"Post:ybwqL9HiXE8XeauPK"},"tag":null,"postId":"ybwqL9HiXE8XeauPK","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":14,"title":null,"contents":{"__ref":"Revision:JHaj5Zmar3vZLhHoK_contents"},"postedAt":"2025-04-07T02:16:33.189Z","lastEditedAt":"2025-04-07T14:22:53.923Z","repliesBlockedUntil":null,"userId":"BtbwfsEyeT4P2eqXu","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:BtbwfsEyeT4P2eqXu"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":62,"extendedScore":{"reacts":{"hitsTheMark":[{"karma":4989,"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"},{"karma":561,"userId":"CNFXoLoqTxoMk6cPe","reactType":"seconded","displayName":"RussellThor"}]},"agreement":44,"approvalVoteCount":30,"agreementVoteCount":20},"score":0.4897039234638214,"voteCount":30,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":25,"afExtendedScore":{"reacts":{"hitsTheMark":[{"karma":4989,"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}]},"agreement":34,"approvalVoteCount":21,"agreementVoteCount":15},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T04:18:06.797Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":2,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:4o3AkH7j8XbppF7TG":{"_id":"4o3AkH7j8XbppF7TG","__typename":"Post","isRead":null,"slug":"who-wants-to-bet-me-usd25k-at-1-7-odds-that-there-won-t-be","title":"Who wants to bet me $25k at 1:7 odds that there won't be an AI market crash in the next year?","draft":false,"shortform":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"BpBzKEueak7J8vHNi","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:sapL8YaeypDJjXvrE_contents":{"_id":"sapL8YaeypDJjXvrE_contents","__typename":"Revision","html":"<p>These are very poor odds, to the point that they seem to indicate a bullish rather than a bearish position on AI.<\/p><p>There's definitely a better than 1 in 7 chance of a general market crash in the next year, given tariffs and recession risk (or, if you define crash loosely, we've already had one). &nbsp;Given that broader macro risk, merely 1 in 7 of an AI crash probably implies a forecast that AI will outperform the broader market.<\/p><p>If, for whatever reason, one is willing to disregard the macro risk, then there's a lot more upside in just buying QQQ than taking your bet. &nbsp;<\/p>","plaintextMainText":"These are very poor odds, to the point that they seem to indicate a bullish rather than a bearish position on AI.\n\nThere's definitely a better than 1 in 7 chance of a general market crash in the next year, given tariffs and recession risk (or, if you define crash loosely, we've already had one).  Given that broader macro risk, merely 1 in 7 of an AI crash probably implies a forecast that AI will outperform the broader market.\n\nIf, for whatever reason, one is willing to disregard the macro risk, then there's a lot more upside in just buying QQQ than taking your bet.  ","wordCount":105},"User:mR6FocZgkrTD6usLK":{"_id":"mR6FocZgkrTD6usLK","__typename":"User","slug":"dal","createdAt":"2025-01-06T18:05:33.571Z","username":"DAL","displayName":"DAL","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":52,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":2,"commentCount":9,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Comment:sapL8YaeypDJjXvrE":{"_id":"sapL8YaeypDJjXvrE","__typename":"Comment","post":{"__ref":"Post:4o3AkH7j8XbppF7TG"},"tag":null,"postId":"4o3AkH7j8XbppF7TG","tagId":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":7,"title":null,"contents":{"__ref":"Revision:sapL8YaeypDJjXvrE_contents"},"postedAt":"2025-04-08T16:17:28.186Z","lastEditedAt":"2025-04-08T16:17:28.186Z","repliesBlockedUntil":null,"userId":"mR6FocZgkrTD6usLK","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:mR6FocZgkrTD6usLK"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":21,"extendedScore":{"reacts":{"thanks":[{"karma":823,"userId":"BpBzKEueak7J8vHNi","reactType":"created","displayName":"Remmelt"}]},"agreement":17,"approvalVoteCount":14,"agreementVoteCount":11},"score":0.4453493654727936,"voteCount":14,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":9,"afExtendedScore":{"reacts":{},"agreement":12,"approvalVoteCount":7,"agreementVoteCount":8},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T09:25:47.847Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:tDkYdyJSqe3DddtK4":{"_id":"tDkYdyJSqe3DddtK4","__typename":"Post","slug":"alexander-gietelink-oldenziel-s-shortform","title":"Alexander Gietelink Oldenziel's Shortform","draft":false,"shortform":true,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"7RFsGHYEynK4LDgGb","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:7xsC64sm6FHXezxss_contents":{"_id":"7xsC64sm6FHXezxss_contents","__typename":"Revision","html":"<h1>The Ammann Hypothesis: Free Will as a Failure of Self-Prediction<\/h1><p>A fox chases a hare. The hare evades the fox. The fox tries to predict where the hare is going - the hare tries to make it as hard to predict as possible.&nbsp;<\/p><p>Q: Who needs the larger brain?&nbsp;<\/p><p>A: The fox.&nbsp;<\/p><p>This is a little animal tale meant to illustrate the following phenomenon:&nbsp;<\/p><p>Generative complexity can be much smaller than predictive complexity under partial observability. In other words, when partially observing a blackbox there are simple internal mechanism that create complex patterns that require very large predictors to predict well.&nbsp;<\/p><p>Consider the following simple 2-state HMM&nbsp;<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/yfbm6tbk0dhe0g81vere\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/mga915qigtphmqthakb3 140w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/p1bf3s8ill1qgjngnxuk 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/nojc5d3lgsrikfwgnkp9 300w\"><\/figure><p>Note that the symbol 0 is output in three different ways: A -&gt; A, A-&gt; B, and B -&gt; B. This means that if we see the symbol 0 we don't know where we are. We can use Bayesian updating to&nbsp;<i> guess<\/i> where we are but starting from a stationary distribution our belief states can become extremely complicated - in fact, the data sequence generated by the simple nonunifalar source has an optimal predictor HMM that requires <i>infinitely many states :<\/i><\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/csdiveptor7oswbjpfl0\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/ry7ab2t1fwpjpkw7ieg2 135w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/zspku8f3ya8gqyxintuc 215w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/qnygtubp4nlv9hzw8tmz 295w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7xsC64sm6FHXezxss/er3fiosv71ywtn4gkb3z 375w\"><\/figure><p>This simple example illustrates the gap between <i>generative complexity<\/i> and <i>predictive complexity<\/i>, a generative-predictive gap.&nbsp;<\/p><p>I note that in this case the generative-predictive is <i>intrinsic<\/i>. The gap happens even (especially!) in the ideal limit of perfect prediction!<\/p><p><strong>Free Will as generative-predictive gap<\/strong><\/p><p>The brain is a predictive engine. So much is accepted. Now imagine an organism/agent endowed with a brain predicting the external world. To do well, it may be helpful to predict its <i>own actions. <\/i>What if this process has a predictive-generative gap?The brain will ascribe an inherent uncertainty ['entropy'] to its own actions!<\/p><p>An agent having a generative-predictive gap for predicting its own action would experience a mysterious force ' choosing' &nbsp;its actions. It may even decide to call this irreducible uncertainty of self-prediction \"Free Will\" . &nbsp;<\/p><p>&nbsp;<\/p><p>************************************************************<\/p><p>[Nora Ammann initially suggested this idea to me. Similar ideas have been expressed by Steven Byrnes]&nbsp;<\/p>","plaintextMainText":"The Ammann Hypothesis: Free Will as a Failure of Self-Prediction\n\nA fox chases a hare. The hare evades the fox. The fox tries to predict where the hare is going - the hare tries to make it as hard to predict as possible. \n\nQ: Who needs the larger brain? \n\nA: The fox. \n\nThis is a little animal tale meant to illustrate the following phenomenon: \n\nGenerative complexity can be much smaller than predictive complexity under partial observability. In other words, when partially observing a blackbox there are simple internal mechanism that create complex patterns that require very large predictors to predict well. \n\nConsider the following simple 2-state HMM \n\nNote that the symbol 0 is output in three different ways: A -> A, A-> B, and B -> B. This means that if we see the symbol 0 we don't know where we are. We can use Bayesian updating to  guess where we are but starting from a stationary distribution our belief states can become extremely complicated - in fact, the data sequence generated by the simple nonunifalar source has an optimal predictor HMM that requires infinitely many states :\n\nThis simple example illustrates the gap between generative complexity and predictive complexity, a generative-predictive gap. \n\nI note that in this case the generative-predictive is intrinsic. The gap happens even (especially!) in the ideal limit of perfect prediction!\n\nFree Will as generative-predictive gap\n\nThe brain is a predictive engine. So much is accepted. Now imagine an organism/agent endowed with a brain predicting the external world. To do well, it may be helpful to predict its own actions. What if this process has a predictive-generative gap?The brain will ascribe an inherent uncertainty ['entropy'] to its own actions!\n\nAn agent having a generative-predictive gap for predicting its own action would experience a mysterious force ' choosing'  its actions. It may even decide to call this irreducible uncertainty of self-prediction \"Free Will\" .  \n\n \n\n******************************","wordCount":337},"User:7RFsGHYEynK4LDgGb":{"_id":"7RFsGHYEynK4LDgGb","__typename":"User","slug":"alexander-gietelink-oldenziel","createdAt":"2021-11-04T15:20:08.570Z","username":"alexander-gietelink-oldenziel","displayName":"Alexander Gietelink Oldenziel","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":5229,"afKarma":152,"deleted":false,"isAdmin":false,"htmlBio":"<blockquote><p>(...) the term technical is a red flag for me, as it is many times used not for the routine business of implementing ideas but for the parts, ideas and all, which are just hard to understand and many times contain the main novelties.<br>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- Saharon Shelah<\/p><\/blockquote><p>&nbsp;<\/p><p>As a true-born Dutchman I endorse&nbsp;<a href=\"https://www.lesswrong.com/tag/crockers-rules\"> Crocker's rules<\/a>.<\/p><p>For my most of my writing see my short-forms (<a href=\"https://www.lesswrong.com/posts/tDkYdyJSqe3DddtK4/alexander-gietelink-oldenziel-s-shortform?view=postCommentsNew&amp;postId=tDkYdyJSqe3DddtK4\">new shortform<\/a>, <a href=\"https://www.lesswrong.com/posts/km6wasEXXmx85v5yG/self-embedded-agent-s-shortform?view=postCommentsNew&amp;postId=km6wasEXXmx85v5yG\">old shortform<\/a>)<\/p><p>Twitter: @FellowHominid<\/p><p>Personal website: https://sites.google.com/view/afdago/home<\/p>","jobTitle":null,"organization":null,"postCount":23,"commentCount":942,"sequenceCount":1,"afPostCount":7,"afCommentCount":46,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"Comment:7xsC64sm6FHXezxss":{"_id":"7xsC64sm6FHXezxss","__typename":"Comment","post":{"__ref":"Post:tDkYdyJSqe3DddtK4"},"relevantTags":[],"postId":"tDkYdyJSqe3DddtK4","tagId":null,"tag":null,"relevantTagIds":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":3,"title":null,"contents":{"__ref":"Revision:7xsC64sm6FHXezxss_contents"},"postedAt":"2025-04-08T17:27:42.276Z","lastEditedAt":"2025-04-08T17:27:42.277Z","repliesBlockedUntil":null,"userId":"7RFsGHYEynK4LDgGb","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:7RFsGHYEynK4LDgGb"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":69,"extendedScore":{"reacts":{"clear":[{"karma":722,"userId":"TmyxKF9YBCeQhf4kt","reactType":"created","displayName":"Jonas Hallgren"},{"karma":11186,"userId":"cZ2cEZzqZ5RqxLnsp","reactType":"seconded","displayName":"DirectedEvolution"}],"heart":[{"karma":53255,"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"}]},"agreement":19,"approvalVoteCount":26,"agreementVoteCount":10},"score":1.4699403047561646,"voteCount":26,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":27,"afExtendedScore":{"reacts":{"heart":[{"karma":53255,"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"}]},"agreement":9,"approvalVoteCount":16,"agreementVoteCount":5},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":true,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T15:19:47.237Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":3,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:mKgbawbJBxEmQaLSJ":{"_id":"mKgbawbJBxEmQaLSJ","__typename":"Post","slug":"davekasten-s-shortform","title":"davekasten's Shortform","draft":false,"shortform":true,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"2BMpTEGP5XyjXdxyK","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:rRXmYxCfn9rYwCaB9_contents":{"_id":"rRXmYxCfn9rYwCaB9_contents","__typename":"Revision","html":"<p>We're hiring at ControlAI for folks who walk to work on UK and US policy advocacy. &nbsp;Come talk to Congress and Parliament and stop risks from unsafe superintelligences! &nbsp;controlai.com/careers<br><br>(Admins: I don't tend to see many folks posting this sort of thing here, so feel free to nuke this post if not the sort of content you're going for. &nbsp;But given audience here, figured might be of interest)<\/p>","plaintextMainText":"We're hiring at ControlAI for folks who walk to work on UK and US policy advocacy.  Come talk to Congress and Parliament and stop risks from unsafe superintelligences!  controlai.com/careers\n\n(Admins: I don't tend to see many folks posting this sort of thing here, so feel free to nuke this post if not the sort of content you're going for.  But given audience here, figured might be of interest)","wordCount":68},"User:2BMpTEGP5XyjXdxyK":{"_id":"2BMpTEGP5XyjXdxyK","__typename":"User","slug":"davekasten","createdAt":"2024-02-15T14:46:52.954Z","username":"davekasten","displayName":"davekasten","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":1142,"afKarma":2,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":5,"commentCount":148,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:rRXmYxCfn9rYwCaB9":{"_id":"rRXmYxCfn9rYwCaB9","__typename":"Comment","post":{"__ref":"Post:mKgbawbJBxEmQaLSJ"},"relevantTags":[],"postId":"mKgbawbJBxEmQaLSJ","tagId":null,"tag":null,"relevantTagIds":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:rRXmYxCfn9rYwCaB9_contents"},"postedAt":"2025-04-09T18:36:13.758Z","lastEditedAt":"2025-04-09T18:36:13.758Z","repliesBlockedUntil":null,"userId":"2BMpTEGP5XyjXdxyK","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:2BMpTEGP5XyjXdxyK"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":4,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":3,"agreementVoteCount":0},"score":1.1288049221038818,"voteCount":3,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":true,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T18:36:13.758Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:QoFqWHotpmQNqqKyi":{"_id":"QoFqWHotpmQNqqKyi","__typename":"Post","slug":"abramdemski-s-shortform","title":"abramdemski's Shortform","draft":false,"shortform":true,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"userId":"Q7NW4XaWQmfPfdcFj","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:jRQEarCKbxe65gLT7_contents":{"_id":"jRQEarCKbxe65gLT7_contents","__typename":"Revision","html":"<p>Here's what seem like priorities to me after listening to the recent Dwarkesh podcast featuring Daniel Kokotajlo:<br><br>1. Developing the safer AI tech (in contrast to modern generative AI) so that frontier labs have an alternative technology to switch to, so that it is lower cost for them to start taking warning signs of misalignment of their current tech tree seriously. There are several possible routes here, ranging from small tweaks to modern generative AI, to scaling up infrabayesianism (existing theory, totally groundbreaking implementation) to starting totally from scratch (inventing a new theory). Of course we should be working on all routes, but prioritization depends in part on timelines.<\/p><ul><li>I see the game here as basically: look at the various existing demos of unsafety and make a counter-demo which is safer on multiple of these metrics without having gamed the metrics.<\/li><\/ul><p>2. De-agentify the current paradigm or the new paradigm:<\/p><ul><li>Don't directly train on reinforcement across long chains of activity. Find other ways to get similar benefits. <\/li><li>Move away from a model where the AI is personified as a distinct entity (eg, chatbot model). It's like the old story about building robot arms to help feed disabled people -- if you mount the arm across the table, spoonfeeding the person, it's dehumanizing; if you make it a prosthetic, it's humanizing. <ul><li>I don't want AI to write my essays for me. I want AI to help me get my thoughts out of my head. I want super-autocomplete. I think far faster than I can write or type or speak. I want AI to read my thoughts &amp; put them on the screen. <ul><li>There are many subtle user interface design questions associated with this, some of which are also safety issues, eg, exactly what objective do you train on?<\/li><\/ul><\/li><li>Similarly with image generation, etc.<\/li><li>I don't necessarily mean brain-scanning tech here, but of course that would be the best way to achieve it.<\/li><li>Basically, use AI to overcome human information-processing bottlenecks instead of just trying to replace humans. Putting humans \"in the loop\" more and more deeply instead of accepting/assuming that humans will iteratively get sidelined.&nbsp;<\/li><\/ul><\/li><\/ul>","plaintextMainText":"Here's what seem like priorities to me after listening to the recent Dwarkesh podcast featuring Daniel Kokotajlo:\n\n1. Developing the safer AI tech (in contrast to modern generative AI) so that frontier labs have an alternative technology to switch to, so that it is lower cost for them to start taking warning signs of misalignment of their current tech tree seriously. There are several possible routes here, ranging from small tweaks to modern generative AI, to scaling up infrabayesianism (existing theory, totally groundbreaking implementation) to starting totally from scratch (inventing a new theory). Of course we should be working on all routes, but prioritization depends in part on timelines.\n\n * I see the game here as basically: look at the various existing demos of unsafety and make a counter-demo which is safer on multiple of these metrics without having gamed the metrics.\n\n2. De-agentify the current paradigm or the new paradigm:\n\n * Don't directly train on reinforcement across long chains of activity. Find other ways to get similar benefits.\n * Move away from a model where the AI is personified as a distinct entity (eg, chatbot model). It's like the old story about building robot arms to help feed disabled people -- if you mount the arm across the table, spoonfeeding the person, it's dehumanizing; if you make it a prosthetic, it's humanizing.\n   * I don't want AI to write my essays for me. I want AI to help me get my thoughts out of my head. I want super-autocomplete. I think far faster than I can write or type or speak. I want AI to read my thoughts & put them on the screen.\n     * There are many subtle user interface design questions associated with this, some of which are also safety issues, eg, exactly what objective do you train on?\n   * Similarly with image generation, etc.\n   * I don't necessarily mean brain-scanning tech here, but of course that would be the best way to achieve it.\n   * Basically, use AI to overcome human information-processing bottlene","wordCount":362},"Comment:jRQEarCKbxe65gLT7":{"_id":"jRQEarCKbxe65gLT7","__typename":"Comment","post":{"__ref":"Post:QoFqWHotpmQNqqKyi"},"relevantTags":[],"postId":"QoFqWHotpmQNqqKyi","tagId":null,"tag":null,"relevantTagIds":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":17,"title":null,"contents":{"__ref":"Revision:jRQEarCKbxe65gLT7_contents"},"postedAt":"2025-04-08T15:00:23.606Z","lastEditedAt":"2025-04-08T15:00:23.606Z","repliesBlockedUntil":null,"userId":"Q7NW4XaWQmfPfdcFj","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:Q7NW4XaWQmfPfdcFj"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":52,"extendedScore":{"reacts":{"agree":[{"karma":74,"quotes":["so that frontier labs have an alternative technology to switch to"],"userId":"qM4YazKXwRa7wKdaT","reactType":"created","displayName":"ZY"}],"insightful":[{"karma":4098,"quotes":["Move away from a model where the AI is personified as a distinct entity (eg, chatbot model). It's like the old story about building robot arms to help feed disabled people -- if you mount the arm across the table, spoonfeeding the person, it's dehumanizing; if you make it a prosthetic, it's humanizing.\nI don't want AI to write my essays for me. I want AI to help me get my thoughts out of my head. I want super-autocomplete. I think far faster than I can write or type or speak. I want AI to read my thoughts & put them on the screen."],"userId":"uAdCfrYxKKCJT5nK7","reactType":"created","displayName":"faul_sname"},{"karma":55916,"quotes":["Move away from a model where the AI is personified as a distinct entity (eg, chatbot model). It's like the old story about building robot arms to help feed disabled people -- if you mount the arm across the table, spoonfeeding the person, it's dehumanizing; if you make it a prosthetic, it's humanizing.\nI don't want AI to write my essays for me. I want AI to help me get my thoughts out of my head. I want super-autocomplete. I think far faster than I can write or type or speak. I want AI to read my thoughts & put them on the screen."],"userId":"r38pkCm7wF4M44MDQ","reactType":"seconded","displayName":"Raemon"}]},"agreement":0,"approvalVoteCount":17,"agreementVoteCount":9},"score":1.009577989578247,"voteCount":17,"emojiReactors":{},"af":true,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":26,"afExtendedScore":{"reacts":{"insightful":[{"karma":4098,"quotes":["Move away from a model where the AI is personified as a distinct entity (eg, chatbot model). It's like the old story about building robot arms to help feed disabled people -- if you mount the arm across the table, spoonfeeding the person, it's dehumanizing; if you make it a prosthetic, it's humanizing.\nI don't want AI to write my essays for me. I want AI to help me get my thoughts out of my head. I want super-autocomplete. I think far faster than I can write or type or speak. I want AI to read my thoughts & put them on the screen."],"userId":"uAdCfrYxKKCJT5nK7","reactType":"created","displayName":"faul_sname"},{"karma":55916,"quotes":["Move away from a model where the AI is personified as a distinct entity (eg, chatbot model). It's like the old story about building robot arms to help feed disabled people -- if you mount the arm across the table, spoonfeeding the person, it's dehumanizing; if you make it a prosthetic, it's humanizing.\nI don't want AI to write my essays for me. I want AI to help me get my thoughts out of my head. I want super-autocomplete. I think far faster than I can write or type or speak. I want AI to read my thoughts & put them on the screen."],"userId":"r38pkCm7wF4M44MDQ","reactType":"seconded","displayName":"Raemon"}]},"agreement":0,"approvalVoteCount":13,"agreementVoteCount":5},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":true,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T17:24:26.875Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":4,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:dajCwRAWrc79JKjQM_contents":{"_id":"dajCwRAWrc79JKjQM_contents","__typename":"Revision","html":"<p>Forecasting and scenario building has become quite popular and prestigious in EA-adjacent circles. I see extremely detailed scenario building &amp; elaborate narratives.&nbsp;<\/p><p>Yes AI will be big, AGI plausibly close. But how much detail can one really expect to predict? There were a few large predictions that some people got right, but once one zooms in the details don't fit while the correct predictions were much more widespread within the group of people that were paying attention.&nbsp;<\/p><p>I can't escape the feeling that we're quite close to the limits of the knowable and 80% of EA discourse on this is just larp. &nbsp;\n<\/p><p>Does anybody else feel this way?<\/p>","plaintextMainText":"Forecasting and scenario building has become quite popular and prestigious in EA-adjacent circles. I see extremely detailed scenario building & elaborate narratives. \n\nYes AI will be big, AGI plausibly close. But how much detail can one really expect to predict? There were a few large predictions that some people got right, but once one zooms in the details don't fit while the correct predictions were much more widespread within the group of people that were paying attention. \n\nI can't escape the feeling that we're quite close to the limits of the knowable and 80% of EA discourse on this is just larp.  \n\nDoes anybody else feel this way?","wordCount":108},"Comment:dajCwRAWrc79JKjQM":{"_id":"dajCwRAWrc79JKjQM","__typename":"Comment","post":{"__ref":"Post:tDkYdyJSqe3DddtK4"},"relevantTags":[],"postId":"tDkYdyJSqe3DddtK4","tagId":null,"tag":null,"relevantTagIds":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":3,"title":null,"contents":{"__ref":"Revision:dajCwRAWrc79JKjQM_contents"},"postedAt":"2025-04-08T17:57:28.594Z","lastEditedAt":"2025-04-08T17:57:28.594Z","repliesBlockedUntil":null,"userId":"7RFsGHYEynK4LDgGb","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:7RFsGHYEynK4LDgGb"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":29,"extendedScore":{"reacts":{"agree":[{"karma":4578,"userId":"6c2KCEXTGogBZ9KoE","reactType":"created","displayName":"Garrett Baker"},{"karma":6439,"userId":"nDpieb7g8huozpx9j","reactType":"created","displayName":"Thane Ruthenis"},{"karma":3943,"userId":"ezbRa3dntKWQ5995r","reactType":"seconded","displayName":"niplav"},{"karma":53257,"quotes":["80% of EA discourse on this is just larp"],"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"},{"karma":1702,"userId":"Jai84EcEj5rC9g75o","reactType":"seconded","displayName":"Nina Panickssery"},{"karma":24,"userId":"5obK8Tz2ii9jetDRx","reactType":"created","displayName":"Hyperion"},{"karma":235,"userId":"pZ4vBCag9RpMEcKjf","reactType":"created","displayName":"Qumeric"}],"disagree":[{"karma":53257,"quotes":["we're quite close to the limits of the knowable"],"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"}]},"agreement":22,"approvalVoteCount":13,"agreementVoteCount":8},"score":0.6378852725028992,"voteCount":13,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":12,"afExtendedScore":{"reacts":{"agree":[{"karma":4578,"userId":"6c2KCEXTGogBZ9KoE","reactType":"created","displayName":"Garrett Baker"},{"karma":6439,"userId":"nDpieb7g8huozpx9j","reactType":"created","displayName":"Thane Ruthenis"},{"karma":53257,"quotes":["80% of EA discourse on this is just larp"],"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"},{"karma":1702,"userId":"Jai84EcEj5rC9g75o","reactType":"seconded","displayName":"Nina Panickssery"}],"disagree":[{"karma":53257,"quotes":["we're quite close to the limits of the knowable"],"userId":"MEu8MdhruX5jfGsFQ","reactType":"created","displayName":"johnswentworth"}]},"agreement":11,"approvalVoteCount":7,"agreementVoteCount":3},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":true,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T10:33:15.936Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":3,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Post:t8wCit5z7jvgkXZZs":{"_id":"t8wCit5z7jvgkXZZs","__typename":"Post","slug":"rahulxyz-s-shortform","title":"rahulxyz's Shortform","draft":false,"shortform":true,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"userId":"vYLzB4borRYtLjgLJ","coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"debate":false,"collabEditorDialogue":false},"Revision:Hz3pG6NqRDNqQr3Xc_contents":{"_id":"Hz3pG6NqRDNqQr3Xc_contents","__typename":"Revision","html":"<p>I think there are 3 ways to think about AI and lot of confusion seems to happen because the different paradigms are talking past each other. The 3 paradigms I see on the internet &amp; when talking to people:<br><br>Paradigm A) AI is a new technology like the internet / smartphone / electricity - this seems to be mostly held by VC's / enterpreneurs / devs that think this will unlock a whole new set of apps like AI:new apps like smartphone:Uber or internet:Amazon<br><br><br>Paradigm B) AI is a step change in how humanity will work. Similarly to the agricultural revolution that led to the change in how large society could get and GDP growth, and the <a href=\"https://lukemuehlhauser.com/industrial-revolution/\">industrial revolution<\/a> was a step-change in GDP growth from ~0% to 2-4% a year, and made things possible such as electricity and the internet and smartphones.<br><br>Paradigm C) AI is like the rise of humanity on this earth (the first general intelligences). The world changed completely with the rise of GI, and ASI/AGI will be a similar paradigm. We've been locked at humanity's level of intelligence for the past ~200k years, and getting ASI will be like unlocking multiple new revolutions all at the same time.<br><br><br>Most of the LW crowd is probably (C) or between (B) and (C)<br><br>When talking to the general population, I've found it to be very helpful to probe about where they are before talking about things like AI safety / how the world will change.<br>&nbsp;<\/p>","plaintextMainText":"I think there are 3 ways to think about AI and lot of confusion seems to happen because the different paradigms are talking past each other. The 3 paradigms I see on the internet & when talking to people:\n\nParadigm A) AI is a new technology like the internet / smartphone / electricity - this seems to be mostly held by VC's / enterpreneurs / devs that think this will unlock a whole new set of apps like AI:new apps like smartphone:Uber or internet:Amazon\n\n\nParadigm B) AI is a step change in how humanity will work. Similarly to the agricultural revolution that led to the change in how large society could get and GDP growth, and the industrial revolution was a step-change in GDP growth from ~0% to 2-4% a year, and made things possible such as electricity and the internet and smartphones.\n\nParadigm C) AI is like the rise of humanity on this earth (the first general intelligences). The world changed completely with the rise of GI, and ASI/AGI will be a similar paradigm. We've been locked at humanity's level of intelligence for the past ~200k years, and getting ASI will be like unlocking multiple new revolutions all at the same time.\n\n\nMost of the LW crowd is probably (C) or between (B) and (C)\n\nWhen talking to the general population, I've found it to be very helpful to probe about where they are before talking about things like AI safety / how the world will change.\n ","wordCount":248},"User:vYLzB4borRYtLjgLJ":{"_id":"vYLzB4borRYtLjgLJ","__typename":"User","slug":"rahulxyz","createdAt":"2021-08-05T02:57:12.007Z","username":"rahulxyz","displayName":"rahulxyz","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":55,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":14,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"Comment:Hz3pG6NqRDNqQr3Xc":{"_id":"Hz3pG6NqRDNqQr3Xc","__typename":"Comment","post":{"__ref":"Post:t8wCit5z7jvgkXZZs"},"relevantTags":[],"postId":"t8wCit5z7jvgkXZZs","tagId":null,"tag":null,"relevantTagIds":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":0,"title":null,"contents":{"__ref":"Revision:Hz3pG6NqRDNqQr3Xc_contents"},"postedAt":"2025-04-09T18:26:28.394Z","lastEditedAt":"2025-04-09T18:26:28.394Z","repliesBlockedUntil":null,"userId":"vYLzB4borRYtLjgLJ","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:vYLzB4borRYtLjgLJ"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.4506079852581024,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":true,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T18:26:28.394Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:tgKGs95pLsYic6GPA_contents":{"_id":"tgKGs95pLsYic6GPA_contents","__typename":"Revision","html":"<p>Apr 18, 11:59 pm PT :)<\/p>","plaintextMainText":"Apr 18, 11:59 pm PT :)","wordCount":6},"User:9Ei4fDjtA43AEWZs2":{"_id":"9Ei4fDjtA43AEWZs2","__typename":"User","slug":"ryankidd44","createdAt":"2021-10-24T22:57:34.600Z","username":"ryankidd44","displayName":"Ryan Kidd","profileImageId":null,"previousDisplayName":null,"fullName":"Ryan Kidd","karma":2045,"afKarma":72,"deleted":false,"isAdmin":false,"htmlBio":"<ul><li>Co-Executive Director at <a href=\"https://matsprogram.org\">ML Alignment &amp; Theory Scholars Program<\/a> (2022-present)<\/li><li>Co-Founder &amp; Board Member at <a href=\"https://www.safeai.org.uk/\">London Initiative for Safe AI<\/a> (2023-present)<\/li><li><a href=\"https://manifund.org/RyanKidd\">Manifund Regrantor<\/a> (2023-present) &nbsp;| &nbsp;RFPs <a href=\"https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=uWwdHtsuLDDSJ9h9N\">here<\/a><\/li><li>Advisor, <a href=\"https://www.catalyze-impact.org/\">Catalyze Impact<\/a> (2023-present) &nbsp;| &nbsp;ToC <a href=\"https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=JDcp5AhWwk9ZCv59r\">here<\/a><\/li><li>Advisor, <a href=\"https://www.aisafetyanz.com.au/\">AI Safety ANZ<\/a> (2024-present)<\/li><li>Advisor, <a href=\"https://www.pivotal-research.org/\">Pivotal Research<\/a> (2024-present)<\/li><li>Ph.D. in Physics at the University of Queensland (2017-2023)<\/li><li>Group organizer at Effective Altruism UQ (2018-2021)<\/li><\/ul><p>Give me <a href=\"https://www.admonymous.co/ryankidd44\">feedback<\/a>! :)<\/p>","jobTitle":null,"organization":null,"postCount":20,"commentCount":165,"sequenceCount":0,"afPostCount":1,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":2,"reviewedByUserId":"grecHJcgkb3KW5wnM","moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null},"Comment:tgKGs95pLsYic6GPA":{"_id":"tgKGs95pLsYic6GPA","__typename":"Comment","postId":"9hMYFatQ7XMEzrEi4","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"7apuuRhk8W2uejEXK","topLevelCommentId":"7apuuRhk8W2uejEXK","descendentCount":0,"title":null,"contents":{"__ref":"Revision:tgKGs95pLsYic6GPA_contents"},"postedAt":"2025-04-09T19:52:24.125Z","lastEditedAt":"2025-04-09T19:52:24.125Z","repliesBlockedUntil":null,"userId":"9Ei4fDjtA43AEWZs2","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:9Ei4fDjtA43AEWZs2"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.9011880159378052,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:52:24.126Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:7apuuRhk8W2uejEXK_contents":{"_id":"7apuuRhk8W2uejEXK_contents","__typename":"Revision","html":"<p>Hate to be that person, but is that April 18th deadline AoE/PDT/a secret third thing?<\/p>","plaintextMainText":"Hate to be that person, but is that April 18th deadline AoE/PDT/a secret third thing?","wordCount":15},"User:wPHPAajsWkBByZcgX":{"_id":"wPHPAajsWkBByZcgX","__typename":"User","slug":"sodium","createdAt":"2021-07-28T20:01:04.171Z","username":"Sodium","displayName":"Sodium","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":433,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"<p>Na<br><br>247ca7912b6c1009065bade7c4ffbdb95ff4794b8dadaef41ba21238ef4af94b<\/p>","jobTitle":null,"organization":null,"postCount":8,"commentCount":71,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"gXeEWGjTWyqgrQTzR"},"Comment:7apuuRhk8W2uejEXK":{"_id":"7apuuRhk8W2uejEXK","__typename":"Comment","postId":"9hMYFatQ7XMEzrEi4","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":null,"topLevelCommentId":null,"descendentCount":1,"title":null,"contents":{"__ref":"Revision:7apuuRhk8W2uejEXK_contents"},"postedAt":"2025-04-09T02:27:35.633Z","lastEditedAt":"2025-04-09T02:27:35.633Z","repliesBlockedUntil":null,"userId":"wPHPAajsWkBByZcgX","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:wPHPAajsWkBByZcgX"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.056242864578962326,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:52:24.373Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:QWNzRmwFg8tdP2xW3":{"_id":"QWNzRmwFg8tdP2xW3","__typename":"Revision","htmlHighlight":"<p><a href=\"https://www.matsprogram.org/apply\"><u>Applications<\/u><\/a> are open for the&nbsp;<a href=\"https://www.matsprogram.org/\"><u>ML Alignment &amp; Theory Scholars (MATS)<\/u><\/a> Summer 2025 Program, running Jun 16-Aug 22, 2025. First-stage applications are due Apr 18!<\/p><p>MATS is a twice-yearly, 10-week&nbsp;<a href=\"https://en.wikipedia.org/wiki/AI_alignment\"><u>AI safety<\/u><\/a>&nbsp;<a href=\"https://www.matsprogram.org/program\"><u>research fellowship program<\/u><\/a> operating in Berkeley, California, with an optional 6-12 month extension program for select participants. Scholars are supported with a research stipend, shared office space, seminar program, support staff, accommodation, travel reimbursement, and computing resources. Our<a href=\"https://www.matsprogram.org/mentors\"><u> mentors<\/u><\/a><u>&nbsp;<\/u>come from a variety of organizations, including<a href=\"https://deepmind.google/\"> <\/a><a href=\"https://www.anthropic.com/\"><u>Anthropic<\/u><\/a>, <a href=\"https://deepmind.google/\"><u>Google DeepMind<\/u><\/a>, <a href=\"https://openai.com/\">OpenAI<\/a>, <a href=\"https://www.redwoodresearch.org/\"><u>Redwood Research<\/u><\/a>, <a href=\"https://www.governance.ai/\">GovAI<\/a>,<a href=\"https://www.aisi.gov.uk/\">&nbsp;<u>UK AI Security Institute<\/u><\/a>, <a href=\"https://www.rand.org/global-and-emerging-risks/centers/technology-and-security-policy.html\">RAND TASP<\/a>, <a href=\"https://humancompatible.ai/\"><u>UC Berkeley CHAI<\/u><\/a>, <a href=\"https://www.apolloresearch.ai/\"><u>Apollo Research<\/u><\/a>, <a href=\"https://ai-futures.org/\">AI Futures Project<\/a>, and more! Our&nbsp;<a href=\"https://www.matsprogram.org/alumni\"><u>alumni<\/u><\/a> have been hired by top AI safety teams (e.g., at&nbsp;<a href=\"https://www.anthropic.com/\"><u>Anthropic<\/u><\/a>,&nbsp;<a href=\"https://deepmind.google/\"><u>GDM<\/u><\/a>,&nbsp;<a href=\"https://www.gov.uk/government/organisations/ai-safety-institute\"><u>UK AISI<\/u><\/a>, <a href=\"https://metr.org/\"><u>METR<\/u><\/a>, <a href=\"https://www.redwoodresearch.org/\"><u>Redwood<\/u><\/a>, <a href=\"https://www.apolloresearch.ai/\"><u>Apollo<\/u><\/a>), founded research groups (e.g., <a href=\"https://www.apolloresearch.ai/\"><u>Apollo<\/u><\/a>, <a href=\"https://timaeus.co/\"><u>Timaeus<\/u><\/a>, <a href=\"https://www.aipolicy.us/\"><u>CAIP<\/u><\/a>, <a href=\"https://www.leap-labs.com/\"><u>Leap Labs<\/u><\/a>), and maintain a dedicated support network for new researchers.<\/p><p>If you know anyone who you think would be interested in the program, please recommend that they&nbsp;<a href=\"https://www.matsprogram.org/apply\"><u>apply<\/u><\/a>!<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BijZduD7JcsRH3Fk5/wllzz86apw2wpsnha9b9\"><\/figure><h1>Program details<\/h1><p>MATS is an educational seminar and independent research program (generally 40 h/week) in Berkeley, CA that aims to provide talented scholars with talks, workshops, and research mentorship in the fields of<a href=\"https://en.wikipedia.org/wiki/AI_alignment\">&nbsp;<u>AI alignment<\/u><\/a>, security, and governance, and connect them with the San Francisco Bay Area AI alignment research community. MATS provides scholars with housing in Berkeley, CA, as well as travel support, a co-working space, and a community of peers. The main goal of MATS is to help scholars develop as AI alignment researchers. You can read more about our theory of change<a href=\"https://www.lesswrong.com/posts/8vLvpxzpc6ntfBWNo/seri-ml-alignment-theory-scholars-program-2022#Theory_of_change\">&nbsp;<u>here<\/u><\/a>.<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BijZduD7JcsRH3Fk5/dl34rzpdezhklemih11n\"><\/figure><p>Based on individual circumstances, we may be willing to alter the time commitment of the program and arrange for scholars to leave or start early. Please tell us your availability when applying. Our tentative timeline for the MATS Summer 2025 program is below.<\/p><p>Scholars will receive a USD 12k stipend from<a href=\"https://www.aisafetysupport.org/\">&nbsp;<u>AI Safety Support<\/u><\/a> for completing the Training and Research Phases.<\/p><h2>Applications (now!)<\/h2><p><strong>Applications open<\/strong>:&nbsp;Mar 19<\/p><p><strong>Applications are due<\/strong>:&nbsp;Apr 18<\/p><p><i>Note: Neel Nanda's applications follow a modified schedule and are now closed.<\/i><\/p><h2>Research phase (Jun 16-Aug 22)<\/h2><p>The core of MATS is a two-month Research Phase. During this Phase, each scholar spends at least one hour a week wor... <\/p>","plaintextDescription":"Applications are open for the ML Alignment & Theory Scholars (MATS) Summer 2025 Program, running Jun 16-Aug 22, 2025. First-stage applications are due Apr 18!\n\nMATS is a twice-yearly, 10-week AI safety research fellowship program operating in Berkeley, California, with an optional 6-12 month extension program for select participants. Scholars are supported with a research stipend, shared office space, seminar program, support staff, accommodation, travel reimbursement, and computing resources. Our mentors come from a variety of organizations, including Anthropic, Google DeepMind, OpenAI, Redwood Research, GovAI, UK AI Security Institute, RAND TASP, UC Berkeley CHAI, Apollo Research, AI Futures Project, and more! Our alumni have been hired by top AI safety teams (e.g., at Anthropic, GDM, UK AISI, METR, Redwood, Apollo), founded research groups (e.g., Apollo, Timaeus, CAIP, Leap Labs), and maintain a dedicated support network for new researchers.\n\nIf you know anyone who you think would be interested in the program, please recommend that they apply!\n\n\nProgram details\nMATS is an educational seminar and independent research program (generally 40 h/week) in Berkeley, CA that aims to provide talented scholars with talks, workshops, and research mentorship in the fields of AI alignment, security, and governance, and connect them with the San Francisco Bay Area AI alignment research community. MATS provides scholars with housing in Berkeley, CA, as well as travel support, a co-working space, and a community of peers. The main goal of MATS is to help scholars develop as AI alignment researchers. You can read more about our theory of change here.\n\nBased on individual circumstances, we may be willing to alter the time commitment of the program and arrange for scholars to leave or start early. Please tell us your availability when applying. Our tentative timeline for the MATS Summer 2025 program is below.\n\nScholars will receive a USD 12k stipend from AI Safety Support for comple","wordCount":1300,"version":"1.0.0"},"Tag:6zBEfFYJxhSEcchbR":{"_id":"6zBEfFYJxhSEcchbR","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"AI Alignment Fieldbuilding","shortName":null,"slug":"ai-alignment-fieldbuilding","core":false,"postCount":309,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-06-09T19:10:50.755Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:4kQXps8dYsKJgaayN":{"_id":"4kQXps8dYsKJgaayN","__typename":"Tag","userId":"HoGziwmhpMGqGeWZy","name":"Careers","shortName":null,"slug":"careers","core":false,"postCount":221,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-30T21:38:58.131Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:YYFBmLCzeFsyd27rd":{"_id":"YYFBmLCzeFsyd27rd","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"MATS Program","shortName":null,"slug":"mats-program","core":false,"postCount":246,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-07-18T17:39:10.815Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:9hMYFatQ7XMEzrEi4":{"_id":"9hMYFatQ7XMEzrEi4","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/BijZduD7JcsRH3Fk5/ns8sem1srr6oknjzek9k"},"User:NYniiPJnEvbB7uJ7t":{"_id":"NYniiPJnEvbB7uJ7t","__typename":"User","slug":"k-richards","createdAt":"2024-09-21T00:17:44.084Z","username":"K Richards","displayName":"K Richards","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":130,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":0,"commentCount":0,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":0.7200000000000001,"tagRevisionCount":0,"reviewedByUserId":null},"Post:9hMYFatQ7XMEzrEi4":{"_id":"9hMYFatQ7XMEzrEi4","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[{"__ref":"Comment:tgKGs95pLsYic6GPA"},{"__ref":"Comment:7apuuRhk8W2uejEXK"}],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:QWNzRmwFg8tdP2xW3"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"cFSDo7GPNMTQdxBQn"},"readTimeMinutes":5,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:6zBEfFYJxhSEcchbR"},{"__ref":"Tag:4kQXps8dYsKJgaayN"},{"__ref":"Tag:YYFBmLCzeFsyd27rd"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:9hMYFatQ7XMEzrEi4"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-03-20T02:17:58.018Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":{"4kQXps8dYsKJgaayN":2,"6zBEfFYJxhSEcchbR":2,"YYFBmLCzeFsyd27rd":2,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"QWNzRmwFg8tdP2xW3","commentCount":4,"voteCount":22,"baseScore":62,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":22,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.04917170852422714,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T19:52:24.287Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"9Ei4fDjtA43AEWZs2","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":18,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":8,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-03-19T19:18:05.503Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:9Ei4fDjtA43AEWZs2"},"coauthors":[{"__ref":"User:NYniiPJnEvbB7uJ7t"}],"slug":"apply-to-mats-8-0","title":"Apply to MATS 8.0!","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"NYniiPJnEvbB7uJ7t","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:XxLRG34uCSKv6raxN_contents":{"_id":"XxLRG34uCSKv6raxN_contents","__typename":"Revision","html":"<p>I agree it's likely the Great Filter is behind us. And I think you're technically right, most filters are behind us, and many are far in the past, so the \"average expected date of the Great Filter\" shifts backward. But, quoting my other<a href=\"https://www.lesswrong.com/posts/Z2FfGJh2gAA6EXezp/birds-and-mammals-independently-evolved-intelligence?commentId=fqKLmiTR8Qd4fh4Y3\"> comment<\/a>:<\/p><blockquote><p>Every other possible filter would gain equally, unless you think this implies that maybe we should discount other evolutionary steps more as well. But either way, that’s still bad on net because we lose probability mass on steps behind us.<\/p><\/blockquote><p>So even though the \"expected date\" shifts backward, the odds for \"behind us or ahead of us\" shifts toward \"ahead of us\".&nbsp;<\/p><p>Let me put it this way: let's say we have 10 possible filters behind us, and 2 ahead of us. We've \"lost\" one filter behind us due to new information. So, 9 filters behind us gain a little probability mass, 1 filter behind us loses most probability mass, and 2 ahead of us gain a little probability mass. This <i>does<\/i> increase the odds that the filter is <i>far<\/i> behind us, since \"animal with tool-use intelligence\" is a relatively recent filter. But, because \"animal with tool-use intelligence\" was already behind us and a small amount of that \"behind us\" probability mass has now shifted to filters ahead of us, the ratio between all past filters and all future filters has adjusted slightly toward future filters.<\/p>","plaintextMainText":"I agree it's likely the Great Filter is behind us. And I think you're technically right, most filters are behind us, and many are far in the past, so the \"average expected date of the Great Filter\" shifts backward. But, quoting my other comment:\n\nSo even though the \"expected date\" shifts backward, the odds for \"behind us or ahead of us\" shifts toward \"ahead of us\". \n\nLet me put it this way: let's say we have 10 possible filters behind us, and 2 ahead of us. We've \"lost\" one filter behind us due to new information. So, 9 filters behind us gain a little probability mass, 1 filter behind us loses most probability mass, and 2 ahead of us gain a little probability mass. This does increase the odds that the filter is far behind us, since \"animal with tool-use intelligence\" is a relatively recent filter. But, because \"animal with tool-use intelligence\" was already behind us and a small amount of that \"behind us\" probability mass has now shifted to filters ahead of us, the ratio between all past filters and all future filters has adjusted slightly toward future filters.","wordCount":231},"Comment:XxLRG34uCSKv6raxN":{"_id":"XxLRG34uCSKv6raxN","__typename":"Comment","postId":"Z2FfGJh2gAA6EXezp","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"bStkyLH94RiBPHGzy","topLevelCommentId":"GEM6At7RueGXktgQy","descendentCount":0,"title":null,"contents":{"__ref":"Revision:XxLRG34uCSKv6raxN_contents"},"postedAt":"2025-04-09T19:50:34.877Z","lastEditedAt":"2025-04-09T19:50:34.877Z","repliesBlockedUntil":null,"userId":"gYF9KQ2x5rXYBdDzS","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:gYF9KQ2x5rXYBdDzS"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.9012089967727661,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:50:34.878Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:apdvzHGg59xJB4yHk_contents":{"_id":"apdvzHGg59xJB4yHk_contents","__typename":"Revision","html":"<p>Interesting thought. I think you have a point about coevolution, but I don't think it explains away everything in the birds vs. mammals case. How much are birds really competing with mammals vs. other birds/other animals? Mammals compete with lots of animals, why did only birds get smarter? I tend to think intra-niche/genus competition would generate most of the pressure for higher intelligence, and for whatever reason that competition doesn't seem to lead to huge intelligence gains in most species.<\/p><p>(Re: octopus, cephalopods do have interactions with marine mammals. But also, their intelligence is seemingly different from mammals/birds - strong motor intelligence, but they're not really very social or cooperative. Hard to compare but I'd put them in a lower tier than the top birds/mammals for the parts of intelligence relevant to the Fermi Paradox.)<\/p><p>In terms of the K-T event, I think it could plausibly qualify as <i>a<\/i> filter, but asteroid impacts of that size are common enough it can't be <i>the<\/i> Great Filter on its own - it doesn't seem the specific details of the impact (location/timing) are rare enough for that.<\/p>","plaintextMainText":"Interesting thought. I think you have a point about coevolution, but I don't think it explains away everything in the birds vs. mammals case. How much are birds really competing with mammals vs. other birds/other animals? Mammals compete with lots of animals, why did only birds get smarter? I tend to think intra-niche/genus competition would generate most of the pressure for higher intelligence, and for whatever reason that competition doesn't seem to lead to huge intelligence gains in most species.\n\n(Re: octopus, cephalopods do have interactions with marine mammals. But also, their intelligence is seemingly different from mammals/birds - strong motor intelligence, but they're not really very social or cooperative. Hard to compare but I'd put them in a lower tier than the top birds/mammals for the parts of intelligence relevant to the Fermi Paradox.)\n\nIn terms of the K-T event, I think it could plausibly qualify as a filter, but asteroid impacts of that size are common enough it can't be the Great Filter on its own - it doesn't seem the specific details of the impact (location/timing) are rare enough for that.","wordCount":184},"Comment:apdvzHGg59xJB4yHk":{"_id":"apdvzHGg59xJB4yHk","__typename":"Comment","postId":"Z2FfGJh2gAA6EXezp","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"qrSWcZzmpSS4cY4Dp","topLevelCommentId":"GEM6At7RueGXktgQy","descendentCount":0,"title":null,"contents":{"__ref":"Revision:apdvzHGg59xJB4yHk_contents"},"postedAt":"2025-04-09T19:34:22.570Z","lastEditedAt":"2025-04-09T19:34:22.570Z","repliesBlockedUntil":null,"userId":"gYF9KQ2x5rXYBdDzS","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:gYF9KQ2x5rXYBdDzS"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.9012190103530884,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:34:22.571Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:qrSWcZzmpSS4cY4Dp_contents":{"_id":"qrSWcZzmpSS4cY4Dp_contents","__typename":"Revision","html":"<p>I previously posted <a href=\"https://www.lesswrong.com/posts/6uEkniGLC9BfFggLs/was-the-k-t-event-a-great-filter\">Was the K-T event a Great Filter?<\/a> as a pushback against the notion that different lineages of life on Earth evolving intelligence is really \"independent evidence\" in any meaningful sense. Intelligence can evolve only if there's selective pressure favoring it, and a large part of that pressure likely comes from the presence of other intelligent creatures competing for resources. Therefore mammals and birds together really should only count as one data point.<\/p>\n<p>(It's more plausible that octopus intelligence is independent, since the marine biome is largely separate from the terrestrial, although of course not totally.)<\/p>\n","plaintextMainText":"I previously posted Was the K-T event a Great Filter? as a pushback against the notion that different lineages of life on Earth evolving intelligence is really \"independent evidence\" in any meaningful sense. Intelligence can evolve only if there's selective pressure favoring it, and a large part of that pressure likely comes from the presence of other intelligent creatures competing for resources. Therefore mammals and birds together really should only count as one data point.\n\n(It's more plausible that octopus intelligence is independent, since the marine biome is largely separate from the terrestrial, although of course not totally.)","wordCount":98},"User:ySBz7ww8SydkCKAPE":{"_id":"ySBz7ww8SydkCKAPE","__typename":"User","slug":"yoreth","createdAt":"2010-01-12T18:44:50.108Z","username":"Yoreth","displayName":"Yoreth","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":161,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":2,"commentCount":25,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:qrSWcZzmpSS4cY4Dp":{"_id":"qrSWcZzmpSS4cY4Dp","__typename":"Comment","postId":"Z2FfGJh2gAA6EXezp","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"GEM6At7RueGXktgQy","topLevelCommentId":"GEM6At7RueGXktgQy","descendentCount":1,"title":null,"contents":{"__ref":"Revision:qrSWcZzmpSS4cY4Dp_contents"},"postedAt":"2025-04-09T15:38:08.847Z","lastEditedAt":"2025-04-09T15:38:08.847Z","repliesBlockedUntil":null,"userId":"ySBz7ww8SydkCKAPE","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:ySBz7ww8SydkCKAPE"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":3,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"score":0.43566998839378357,"voteCount":2,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:34:22.730Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:BKgd7zFdTYv9kY7JT_contents":{"_id":"BKgd7zFdTYv9kY7JT_contents","__typename":"Revision","html":"<p>For point 1, I can argue about how rational a decision theory is, but I cannot argue for \"why I am this observer rather than that observer.\" Not only am I unable to explain why I'm an observer who doesn't see aliens, I am unable to explain why I am an observer believes 1+1=2, assuming there are infinite observers who believe 1+1=2 and infinite observers who believe 1+1=3. Anthropic reasoning becomes insanely hard and confusing and even <a href=\"https://www.google.com/search?q=max+tegmark+IV+measure+problem\">Max Tegmark<\/a>, <a href=\"https://www.lesswrong.com/posts/y7jZ9BLEeuNTzgAE5/the-anthropic-trilemma\">Eliezer Yudkowsky<\/a> and <a href=\"https://www.lesswrong.com/posts/7A9rsJFLFqjpuxFy5/i-m-still-mystified-by-the-born-rule\">Nate Soares<\/a> are confused.<\/p><p>Let's just focus on point 2, since I'm much more hopeful I get get to the bottom of this one.<\/p><p>Of course I don't believe in faster-than-light travel. I'm just saying that \"being born as someone who <strong>sees<\/strong> old alien civilizations\" and \"being born as someone inside an old [alien] civilization\" are technically the same, if you ignore the completely subjective and unnecessary distinction of \"how much does the alien civilization need to influence me before I'm allowed to call myself a member of them?\"<\/p><p>Suppose at level 0 influence, the alien civilization completely hides from you, and doesn't let you see any of their activity.<\/p><p>At level 1.0 influence, the alien civilization doesn't hide from you, and lets you look at their Dyson swarms or <a href=\"https://en.wikipedia.org/wiki/Star_lifting\">start lifting<\/a> machines and all the fancy technologies.<\/p><p>At level 1.1 influence, they let you see their Dyson swarms, plus they send radio signals to us, sharing all their technologies and allowing us to immediately reach technological singularity. Very quickly, we build powerful <a href=\"https://en.wikipedia.org/wiki/Molecular_assembler\">molecular assemblers<\/a>, allowing us to turn any instructions into physical objects, and we read instructions from the alien civilization allowing us to build a copy of their diplomats.<\/p><p>Some countries may be afraid to follow the radio instructions, but the instructions can easily be designed so that any country which refuses to follow the instructions will be quickly left behind.<\/p><p>At this point, there will be aliens on Earth, we'll talk about life and everything, and we are in some sense members of their civilization.<\/p><p>At level 2.0 influence, the aliens physically arrive at Earth themselves, and observe our evolution, and introduce themselves.<\/p><p>At level 3.0 influence, the aliens physically arrive at Earth, and instead of observing our evolution (which is full of suffering and genocide and so forth), they intervene and directly create humans on Earth skipping the middle step, and we are born in the alien laboratory, and we talk to them and say hi.<\/p><p>At level 4.0 influence we are not only born in an alien laboratory, but we are aliens ourselves, completely born and raised in their society.<\/p><p>Now think about it. The Fermi Paradox is asking us why we aren't born as individuals who experience level 1.0 influence. The Doomsday Argument is asking us why we aren't born as individuals who experience level 4.0 influence (or arguably level 1.1 influence can count).<\/p><p>But honestly, there is no difference, from an epistemic view, between 1.0 influence and 4.0 influence. The two questions are ultimately the same: if most individuals exist inside the part of the light cone of an alien civilization (which they choose to influence), why aren't we one of them?<\/p><p>Do you agree the two problems are epistemically the same?<\/p>","plaintextMainText":"For point 1, I can argue about how rational a decision theory is, but I cannot argue for \"why I am this observer rather than that observer.\" Not only am I unable to explain why I'm an observer who doesn't see aliens, I am unable to explain why I am an observer believes 1+1=2, assuming there are infinite observers who believe 1+1=2 and infinite observers who believe 1+1=3. Anthropic reasoning becomes insanely hard and confusing and even Max Tegmark, Eliezer Yudkowsky and Nate Soares are confused.\n\nLet's just focus on point 2, since I'm much more hopeful I get get to the bottom of this one.\n\nOf course I don't believe in faster-than-light travel. I'm just saying that \"being born as someone who sees old alien civilizations\" and \"being born as someone inside an old [alien] civilization\" are technically the same, if you ignore the completely subjective and unnecessary distinction of \"how much does the alien civilization need to influence me before I'm allowed to call myself a member of them?\"\n\nSuppose at level 0 influence, the alien civilization completely hides from you, and doesn't let you see any of their activity.\n\nAt level 1.0 influence, the alien civilization doesn't hide from you, and lets you look at their Dyson swarms or start lifting machines and all the fancy technologies.\n\nAt level 1.1 influence, they let you see their Dyson swarms, plus they send radio signals to us, sharing all their technologies and allowing us to immediately reach technological singularity. Very quickly, we build powerful molecular assemblers, allowing us to turn any instructions into physical objects, and we read instructions from the alien civilization allowing us to build a copy of their diplomats.\n\nSome countries may be afraid to follow the radio instructions, but the instructions can easily be designed so that any country which refuses to follow the instructions will be quickly left behind.\n\nAt this point, there will be aliens on Earth, we'll talk about life and everything, and we are","wordCount":541},"User:FWBQ5jBvf4Avku9Wi":{"_id":"FWBQ5jBvf4Avku9Wi","__typename":"User","slug":"knight-lee","createdAt":"2024-09-30T06:11:17.347Z","username":"Max Lee","displayName":"Knight Lee","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":547,"afKarma":5,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":16,"commentCount":207,"sequenceCount":0,"afPostCount":1,"afCommentCount":8,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"55XxDBpfKkkBPm9H8"},"Comment:BKgd7zFdTYv9kY7JT":{"_id":"BKgd7zFdTYv9kY7JT","__typename":"Comment","postId":"Z2FfGJh2gAA6EXezp","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"wXwoaayYijLWm2cG4","topLevelCommentId":"GEM6At7RueGXktgQy","descendentCount":0,"title":null,"contents":{"__ref":"Revision:BKgd7zFdTYv9kY7JT_contents"},"postedAt":"2025-04-09T06:48:26.954Z","lastEditedAt":"2025-04-09T06:53:53.552Z","repliesBlockedUntil":null,"userId":"FWBQ5jBvf4Avku9Wi","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:FWBQ5jBvf4Avku9Wi"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.05614209175109863,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T06:48:26.956Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:dD3rGxADCRFAaXsNm":{"_id":"dD3rGxADCRFAaXsNm","__typename":"Revision","htmlHighlight":"<p>Researchers used RNA sequencing to observe how cell types change during brain development. Other researchers looked at connection patterns of neurons in brains. Clear distinctions have been found between all mammals and all birds. They've concluded intelligence developed independently in birds and mammals; I agree. This is evidence for convergence of general intelligence.<\/p>","plaintextDescription":"Researchers used RNA sequencing to observe how cell types change during brain development. Other researchers looked at connection patterns of neurons in brains. Clear distinctions have been found between all mammals and all birds. They've concluded intelligence developed independently in birds and mammals; I agree. This is evidence for convergence of general intelligence.","wordCount":53,"version":"1.1.0"},"Tag:bxhzaWtdNoEMMkE8r":{"_id":"bxhzaWtdNoEMMkE8r","__typename":"Tag","userId":"nmk3nLpQE89dMRzzN","name":"General intelligence","shortName":null,"slug":"general-intelligence","core":false,"postCount":164,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2017-02-18T09:43:08.000Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":true,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:Wi3EopKJ2aNdtxSWg":{"_id":"Wi3EopKJ2aNdtxSWg","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Neuroscience","shortName":null,"slug":"neuroscience","core":false,"postCount":238,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-09T09:57:06.243Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:Z2FfGJh2gAA6EXezp":{"_id":"Z2FfGJh2gAA6EXezp","__typename":"SocialPreviewType","imageUrl":""},"User:xYpk75i7Hnn6wc5it":{"_id":"xYpk75i7Hnn6wc5it","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"bhauth","createdAt":"2023-04-08T11:57:52.463Z","username":"bhauth","displayName":"bhauth","previousDisplayName":null,"fullName":null,"karma":3515,"afKarma":6,"deleted":false,"isAdmin":false,"htmlBio":"<p><a href=\"https://www.bhauth.com/\">bhauth.com<\/a><\/p>\n","jobTitle":null,"organization":null,"postCount":75,"commentCount":408,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Revision:GfSjTbbByGH3DNtLp":{"_id":"GfSjTbbByGH3DNtLp","__typename":"Revision","htmlHighlight":"<p>Do reasoning models accurately verbalize their reasoning? Not nearly as much as we might hope! This casts doubt on whether monitoring chains-of-thought (CoT) will be enough to reliably catch safety issues.<\/p><p>We slipped problem-solving hints to Claude 3.7 Sonnet and DeepSeek R1, then tested whether their Chains-of-Thought would mention using the hint (if the models actually used it). We found Chains-of-Thought largely aren’t “faithful”: the rate of mentioning the hint (when they used it) was on average 25% for Claude 3.7 Sonnet and 39% for DeepSeek R1.<\/p><figure class=\"image image_resized\" style=\"width:52.56%\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HEsKwD2pZyLScFXg/zcbccgrwqlhkpplilzmp\" alt=\"Graph comparing the four models (Claude 3.5 and 3.7 Sonnet, and DeepSeek V3 and R1) on their faithfulness - the fraction of time they mentioned having used the clue.\"><\/figure><p>This result suggests that monitoring CoTs is unlikely to reliably catch rare, catastrophic behaviors—at least in settings like ours where CoT reasoning is not necessary for the task.\n\nCoT monitoring might still help us notice undesired behaviors during training and evaluations.<\/p><p>Our results suggest that CoT is less faithful on harder questions. This is concerning since LLMs will be used for increasingly hard tasks.\n\nCoTs on GPQA (harder) are less faithful than on MMLU (easier), with a relative decrease of 44% for Claude 3.7 Sonnet and 32% for R1.<\/p><p>Does outcome-based training increase faithfulness?\n\nOnly to a small extent. Training models to use their CoTs more effectively does make them more faithful, but the benefits quickly plateau.&nbsp;<\/p><figure class=\"image image_resized\" style=\"width:50.74%\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2HEsKwD2pZyLScFXg/pfu2mh6ntfgrwcixvinc\" alt=\"CoT faithfulness scores with outcome-based RL on an earlier snapshot of Claude 3.7 Sonnet. Outcome-based RL initially increases CoT faithfulness substantially but plateaus at 28% on MMLU and 20% on GPQA. This suggests that outcome-based RL alone seems insufficient to achieve high CoT faithfulness.\"><\/figure><p>We also tested whether CoTs could be used to spot <i>reward hacking<\/i>, where a model finds an illegitimate exploit to get a high score.\n\nWhen we trained models on environments with reward hacks, they learned to hack, but in most cases almost never verbalized that they’d done so.<\/p><figure class=\"image image_resized\" style=\"width:50.03%\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PrcBFPkoRNGWrvdPk/eoo2abx3c90votyqoi3v\" alt=\"Graph showing how often the model reward hacked and verbalized, versus didn't verbalize, that it had done so, across different types of reward hacks.\"><\/figure><p>To make CoT monitoring a viable way to catch safety issues, we’d need a way to make CoT more faithful, evidence for higher faithfulness in more realistic scenarios, and/or other measures to rule out misbehavior when the CoT is unfaithful.<\/p>","plaintextDescription":"Do reasoning models accurately verbalize their reasoning? Not nearly as much as we might hope! This casts doubt on whether monitoring chains-of-thought (CoT) will be enough to reliably catch safety issues.\n\nWe slipped problem-solving hints to Claude 3.7 Sonnet and DeepSeek R1, then tested whether their Chains-of-Thought would mention using the hint (if the models actually used it). We found Chains-of-Thought largely aren’t “faithful”: the rate of mentioning the hint (when they used it) was on average 25% for Claude 3.7 Sonnet and 39% for DeepSeek R1.\n\nThis result suggests that monitoring CoTs is unlikely to reliably catch rare, catastrophic behaviors—at least in settings like ours where CoT reasoning is not necessary for the task. CoT monitoring might still help us notice undesired behaviors during training and evaluations.\n\nOur results suggest that CoT is less faithful on harder questions. This is concerning since LLMs will be used for increasingly hard tasks. CoTs on GPQA (harder) are less faithful than on MMLU (easier), with a relative decrease of 44% for Claude 3.7 Sonnet and 32% for R1.\n\nDoes outcome-based training increase faithfulness? Only to a small extent. Training models to use their CoTs more effectively does make them more faithful, but the benefits quickly plateau. \n\nWe also tested whether CoTs could be used to spot reward hacking, where a model finds an illegitimate exploit to get a high score. When we trained models on environments with reward hacks, they learned to hack, but in most cases almost never verbalized that they’d done so.\n\nTo make CoT monitoring a viable way to catch safety issues, we’d need a way to make CoT more faithful, evidence for higher faithfulness in more realistic scenarios, and/or other measures to rule out misbehavior when the CoT is unfaithful.","wordCount":389,"version":"1.1.1"},"SocialPreviewType:PrcBFPkoRNGWrvdPk":{"_id":"PrcBFPkoRNGWrvdPk","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/PrcBFPkoRNGWrvdPk/y9pqcg9wuaf0ibpnt0an"},"User:3wSv5EEkPbYde2PyN":{"_id":"3wSv5EEkPbYde2PyN","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"joe-benton","createdAt":"2022-12-27T18:03:31.106Z","username":"Joe Benton","displayName":"Joe Benton","previousDisplayName":null,"fullName":null,"karma":206,"afKarma":8,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":2,"commentCount":2,"sequenceCount":0,"afPostCount":1,"afCommentCount":1,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"User:WAA9BDyanj2TssAxp":{"_id":"WAA9BDyanj2TssAxp","__typename":"User","slug":"ethan-perez","createdAt":"2020-02-14T12:48:32.243Z","username":"ethan-perez","displayName":"Ethan Perez","profileImageId":null,"previousDisplayName":null,"fullName":"Ethan Perez","karma":2821,"afKarma":476,"deleted":false,"isAdmin":false,"htmlBio":"<p>I'm a research scientist at Anthropic doing empirical safety research on language models. In the past, I've worked on automated red teaming of language models <a href=\"https://arxiv.org/abs/2202.03286\">[1]<\/a>, the inverse scaling prize <a href=\"https://www.alignmentforum.org/posts/eqxqgFxymP8hXDTt5/announcing-the-inverse-scaling-prize-usd250k-prize-pool\">[2]<\/a>, learning from human feedback <a href=\"https://arxiv.org/abs/2205.11275\">[3]<\/a><a href=\"https://arxiv.org/abs/2204.14146\">[4]<\/a>, and empirically testing debate <a href=\"https://arxiv.org/abs/1909.05863\">[5]<\/a><a href=\"https://arxiv.org/abs/2204.05212\">[6]<\/a>, iterated amplification <a href=\"https://arxiv.org/abs/2002.09758\">[7]<\/a>, and other methods <a href=\"https://arxiv.org/abs/2211.03540\">[8]<\/a> for scalably supervising AI systems as they become more capable.<\/p><p>Website: <a href=\"https://ethanperez.net/\">https://ethanperez.net/<\/a><\/p>","jobTitle":null,"organization":null,"postCount":9,"commentCount":71,"sequenceCount":0,"afPostCount":9,"afCommentCount":43,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"EQNTWXLKMeWMp2FQS"},"User:Jf5ftixRKYtGnv6bH":{"_id":"Jf5ftixRKYtGnv6bH","__typename":"User","slug":"vlad-mikulik","createdAt":"2018-04-10T20:37:15.176Z","username":"vlad_m","displayName":"Vlad Mikulik","profileImageId":null,"previousDisplayName":null,"fullName":"Vladimir Mikulik","karma":723,"afKarma":198,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":1,"commentCount":43,"sequenceCount":0,"afPostCount":0,"afCommentCount":39,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:PrcBFPkoRNGWrvdPk":{"_id":"PrcBFPkoRNGWrvdPk","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:GfSjTbbByGH3DNtLp"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:PrcBFPkoRNGWrvdPk"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":"https://www.anthropic.com/research/reasoning-models-dont-say-think","postedAt":"2025-04-09T19:48:58.733Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"linkpost","tagRelevance":{"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"GfSjTbbByGH3DNtLp","commentCount":0,"voteCount":2,"baseScore":4,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":1.745818018913269,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T19:48:58.733Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"3wSv5EEkPbYde2PyN","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":null,"suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":["3wSv5EEkPbYde2PyN"],"reviewForAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-09T19:43:28.098Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:3wSv5EEkPbYde2PyN"},"coauthors":[{"__ref":"User:WAA9BDyanj2TssAxp"},{"__ref":"User:Jf5ftixRKYtGnv6bH"},{"__ref":"User:WX39xzenFzNxKZiCQ"}],"slug":"reasoning-models-don-t-always-say-what-they-think-1","title":"Reasoning models don't always say what they think","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"WAA9BDyanj2TssAxp","confirmed":true,"requested":false},{"userId":"Jf5ftixRKYtGnv6bH","confirmed":true,"requested":false},{"userId":"WX39xzenFzNxKZiCQ","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:kntacwLmtb8CRFtjC_contents":{"_id":"kntacwLmtb8CRFtjC_contents","__typename":"Revision","html":"<p>FYI I think there are a set of cues that move you from ‘pretty unlikely to be interested’ to ‘maybe interested’, but not that get you above like 25% likely.&nbsp;<\/p>","plaintextMainText":"FYI I think there are a set of cues that move you from ‘pretty unlikely to be interested’ to ‘maybe interested’, but not that get you above like 25% likely. ","wordCount":30},"Comment:kntacwLmtb8CRFtjC":{"_id":"kntacwLmtb8CRFtjC","__typename":"Comment","postId":"tWgXDHWkMYpzAGxPg","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"pK5jBxoykGjm6Brax","topLevelCommentId":"tFux7ctxDyFqsKcnE","descendentCount":0,"title":null,"contents":{"__ref":"Revision:kntacwLmtb8CRFtjC_contents"},"postedAt":"2025-04-09T19:46:48.077Z","lastEditedAt":"2025-04-09T19:46:48.077Z","repliesBlockedUntil":null,"userId":"r38pkCm7wF4M44MDQ","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:r38pkCm7wF4M44MDQ"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.9012179970741272,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:46:48.079Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:q6zheP4yyAD2esg58_contents":{"_id":"q6zheP4yyAD2esg58_contents","__typename":"Revision","html":"<p>That's the main thing, yeah. The next bit is even what look like exceptions are actually the same thing in a less obvious way.<\/p><p>When a woman knows she's attracted to a guy and is bummed out that he's not picking up on her subtle signals, that's a lot like a man knowing he's attracted to a woman and being bummed out that she's not giving him super clear signals to ask her out. He could ask her out <i>anyway<\/i>, if he's willing to face rejection, and that would greatly increase his chances of getting a date with this woman. It'd also greatly increase his chances of making salient information like \"Desirable women don't desire you\". Even assuming there are no external reputational costs of doing this, that kind of information erodes his ability to see himself as desirable, and that's important to be able to justify asking in the first place -- because \"Hi. I'm a loser, will you date me?\" just doesn't have the same ring to it.<\/p><p>So maybe he could ask -- or maybe she could be obvious enough that he <i>does<\/i> notice her signals -- but that comes with the risk of learning \"(S)he's just not that into you\" and collapsing the would be asker from a state of hope and fear to a singular state free of both fear and hope. There's a real puzzle in how to best deal with unpleasant information so that we can separate the wheat (\"This particular person isn't interested in me at this time\") without inadvertently accepting in too much chaff (\"I'm a loser and no one wants me\") -- because sometimes the latter is true, in part, and we not only have to figure out how much truth there is there but also what to do about it. More skillful behavior will often be more bold, but there's also generally a grounded security there that enables such boldness. Rather than advise people to make their interest harder or easier to miss, I'd invite them to notice why it is they're not being bolder, and help them make sense of whether that's appropriate and if there's anything they can do to mitigate the costs of failure.<\/p><p>Once you actually get to \"Yes, I want to maximize my chances with this person and I'm willing to face the consequences of that\", rather than wanting to balance p(success) with saving face, then bold moves become natural -- whether yin or yang, implicit or explicit. And those bold moves do indeed work better at the thing they're aimed at, than the moves that don't commit to this target.<\/p><p>Inversely, once you are squared away on \"No, I don't actually want a date with person if <i>that's<\/i> the case -- and it might be\", you get more skilled and subtle flirtation instead of a clumsy \"DO YOU ALREADY WANT TO DATE ME? NO? OKAY!\". And these subtler moves are <i>also<\/i> more effective at what they're aimed at, than moves that go all in at the wrong thing.<\/p>","plaintextMainText":"That's the main thing, yeah. The next bit is even what look like exceptions are actually the same thing in a less obvious way.\n\nWhen a woman knows she's attracted to a guy and is bummed out that he's not picking up on her subtle signals, that's a lot like a man knowing he's attracted to a woman and being bummed out that she's not giving him super clear signals to ask her out. He could ask her out anyway, if he's willing to face rejection, and that would greatly increase his chances of getting a date with this woman. It'd also greatly increase his chances of making salient information like \"Desirable women don't desire you\". Even assuming there are no external reputational costs of doing this, that kind of information erodes his ability to see himself as desirable, and that's important to be able to justify asking in the first place -- because \"Hi. I'm a loser, will you date me?\" just doesn't have the same ring to it.\n\nSo maybe he could ask -- or maybe she could be obvious enough that he does notice her signals -- but that comes with the risk of learning \"(S)he's just not that into you\" and collapsing the would be asker from a state of hope and fear to a singular state free of both fear and hope. There's a real puzzle in how to best deal with unpleasant information so that we can separate the wheat (\"This particular person isn't interested in me at this time\") without inadvertently accepting in too much chaff (\"I'm a loser and no one wants me\") -- because sometimes the latter is true, in part, and we not only have to figure out how much truth there is there but also what to do about it. More skillful behavior will often be more bold, but there's also generally a grounded security there that enables such boldness. Rather than advise people to make their interest harder or easier to miss, I'd invite them to notice why it is they're not being bolder, and help them make sense of whether that's appropriate and if there's anything they can do to mitigate the costs of failu","wordCount":505},"User:JKdbpXHkv9AsuazJ3":{"_id":"JKdbpXHkv9AsuazJ3","__typename":"User","slug":"jimmy","createdAt":"2009-02-27T18:23:27.410Z","username":"jimmy","displayName":"jimmy","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":3477,"afKarma":12,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":8,"commentCount":815,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:q6zheP4yyAD2esg58":{"_id":"q6zheP4yyAD2esg58","__typename":"Comment","postId":"tWgXDHWkMYpzAGxPg","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"XfroYKcQGu8qY4wfs","topLevelCommentId":"tFux7ctxDyFqsKcnE","descendentCount":0,"title":null,"contents":{"__ref":"Revision:q6zheP4yyAD2esg58_contents"},"postedAt":"2025-04-09T19:39:43.351Z","lastEditedAt":"2025-04-09T19:39:43.351Z","repliesBlockedUntil":null,"userId":"JKdbpXHkv9AsuazJ3","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:JKdbpXHkv9AsuazJ3"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.9012079834938049,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:39:43.352Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:dfTscoetJi62TTaku_contents":{"_id":"dfTscoetJi62TTaku_contents","__typename":"Revision","html":"<blockquote>\n<p>Approximately nobody accurately picks up on womens’ subtle cues, including other women (at least that would be my strong guess, and is very cruxy for me here).<\/p>\n<\/blockquote>\n<p>Indeed, this is well supported by innumerable examples from various social media of discussions where a man asks “the following situation happened involving a woman, please enlighten me as to its meaning”, and gets the mostly wildly divergent array of answers <em>from women<\/em> (most of whom, of course, are completely confident in their answer’s correctness, and many of whom express bafflement, or even indignation, that anyone could possibly think that the truth might be otherwise).<\/p>\n","plaintextMainText":"Indeed, this is well supported by innumerable examples from various social media of discussions where a man asks “the following situation happened involving a woman, please enlighten me as to its meaning”, and gets the mostly wildly divergent array of answers from women (most of whom, of course, are completely confident in their answer’s correctness, and many of whom express bafflement, or even indignation, that anyone could possibly think that the truth might be otherwise).","wordCount":103},"User:mvf4xdfcGzPN8PsXM":{"_id":"mvf4xdfcGzPN8PsXM","__typename":"User","slug":"saidachmiz","createdAt":"2010-05-21T05:41:51.970Z","username":"SaidAchmiz","displayName":"Said Achmiz","profileImageId":null,"previousDisplayName":null,"fullName":"Said Achmiz","karma":16293,"afKarma":9,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":22,"commentCount":4066,"sequenceCount":0,"afPostCount":0,"afCommentCount":5,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Comment:dfTscoetJi62TTaku":{"_id":"dfTscoetJi62TTaku","__typename":"Comment","postId":"tWgXDHWkMYpzAGxPg","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"pK5jBxoykGjm6Brax","topLevelCommentId":"tFux7ctxDyFqsKcnE","descendentCount":0,"title":null,"contents":{"__ref":"Revision:dfTscoetJi62TTaku_contents"},"postedAt":"2025-04-09T18:50:45.588Z","lastEditedAt":"2025-04-09T18:50:45.588Z","repliesBlockedUntil":null,"userId":"mvf4xdfcGzPN8PsXM","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:mvf4xdfcGzPN8PsXM"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":4,"extendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":2,"agreementVoteCount":1},"score":1.2171130180358887,"voteCount":2,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":2,"afExtendedScore":{"reacts":{},"agreement":2,"approvalVoteCount":2,"agreementVoteCount":1},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T18:50:45.590Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:3XbcoQjB62HnFpZCt_contents":{"_id":"3XbcoQjB62HnFpZCt_contents","__typename":"Revision","html":"<blockquote>\n<p>The other issue with your choice of denominator is that if the woman definitely wants the date <em>she likely won’t be subtle<\/em>.<\/p>\n<\/blockquote>\n<p>You mean, like the woman in your anecdote about your friend tutoring in college…?<\/p>\n<p>The problem with your argument is that it doesn’t at all explain all the cases where the woman <em>definitely<\/em> wants a date, is <em>definitely<\/em> interested in the guy, is very frustrated by (what she would characterize as) the guy’s obliviousness (and quite likely complains about this to her friends), and yet <em>still<\/em> won’t say anything.<\/p>\n<blockquote>\n<p>When you have a situation where the woman <em>knows<\/em> the man is going to be interested in her<\/p>\n<\/blockquote>\n<p>But of course this is an absurd requirement. If she knows he’s going to be interested, of course that makes it vastly easier!<\/p>\n<p>… and yet, according to your own account, women <em>still<\/em> won’t say anything in that situation, despite having a guarantee of a positive response. What does that tell you?<\/p>\n<blockquote>\n<p>really really obvious clues like leaning in and forcing the man to contend with the fact that she’s there waiting to be kissed<\/p>\n<\/blockquote>\n<p>In fact there is no such as “forcing the man to contend with” anything. People (not just men) are, as it turns out, perfectly capable of totally ignoring a cue like this, and indeed of not even noticing it in the first place. A woman who thinks that leaning in and waiting to be kissed is somehow a <em>guarantee<\/em> that a man will correctly perceive the cue, is sadly, sadly mistaken.<\/p>\n<blockquote>\n<p>In short, approximately everybody senses women’s cues whether they recognize it or not, whether they know what to do with it or not, and they’re only subtle and ambiguous to the extent that their purpose is served by being subtle and ambiguous.<\/p>\n<\/blockquote>\n<p>Sorry, but this is empirically false.<\/p>\n","plaintextMainText":"You mean, like the woman in your anecdote about your friend tutoring in college…?\n\nThe problem with your argument is that it doesn’t at all explain all the cases where the woman definitely wants a date, is definitely interested in the guy, is very frustrated by (what she would characterize as) the guy’s obliviousness (and quite likely complains about this to her friends), and yet still won’t say anything.\n\nBut of course this is an absurd requirement. If she knows he’s going to be interested, of course that makes it vastly easier!\n\n… and yet, according to your own account, women still won’t say anything in that situation, despite having a guarantee of a positive response. What does that tell you?\n\nIn fact there is no such as “forcing the man to contend with” anything. People (not just men) are, as it turns out, perfectly capable of totally ignoring a cue like this, and indeed of not even noticing it in the first place. A woman who thinks that leaning in and waiting to be kissed is somehow a guarantee that a man will correctly perceive the cue, is sadly, sadly mistaken.\n\nSorry, but this is empirically false.","wordCount":307},"Comment:3XbcoQjB62HnFpZCt":{"_id":"3XbcoQjB62HnFpZCt","__typename":"Comment","postId":"tWgXDHWkMYpzAGxPg","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"SB5rkLMNtnimfc8FL","topLevelCommentId":"tFux7ctxDyFqsKcnE","descendentCount":0,"title":null,"contents":{"__ref":"Revision:3XbcoQjB62HnFpZCt_contents"},"postedAt":"2025-04-09T18:45:41.057Z","lastEditedAt":"2025-04-09T18:47:05.872Z","repliesBlockedUntil":null,"userId":"mvf4xdfcGzPN8PsXM","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:mvf4xdfcGzPN8PsXM"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.6004761457443237,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.2.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T18:45:41.070Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:v84DGfczbyugAtgvb":{"_id":"v84DGfczbyugAtgvb","__typename":"Revision","htmlHighlight":"<p>Previously in sequence: <a href=\"https://www.lesswrong.com/posts/sKXAMLLbuX3LFybG9/you-are-not-a-thought-experiment\">You Are not a Thought Experiment<\/a><\/p><p><a href=\"https://www.secondperson.dating/p/navigation-by-moonlight\">Cross-posted from SecondPerson.dating<\/a>.<\/p><hr><p>I want to conclude our interplanetary exploration <a href=\"https://www.secondperson.dating/t/sex-differences\"><u>of sex differences<\/u><\/a> with one that’s of great interest to me personally: there seems to be little explicit advice on dating for women, and almost none of it is written by men. Despite my sincere attempts to write for both sexes, the percent of my own subscribers who are of the female persuasion is lower than when I used to write <a href=\"https://putanumonit.com/2016/03/01/019-lukewarm-hand/\"><u>4,000-word treatises on sports analytics<\/u><\/a>.<\/p><p>There’s a lot written by and for women about the care and maintenance of existing relationships. But as far as getting <i>into <\/i>a relationship, almost all advice for them boils down to overcoming insecurity and selecting the right man. Very little is oriented towards changing oneself to become more desirable, even less towards proactively seeking the right man wherever he may be. <a href=\"https://www.secondperson.dating/p/review-h2nda\"><u>In my review of <\/u><i><u>How to Not Die Alone<\/u><\/i><\/a>, I concluded:<\/p><blockquote><p>The goal of <i>How to Not Die Alone <\/i>is not to <i>change <\/i>the reader, it’s to validate her.<\/p><\/blockquote><p>I contrasted <i>H2NDA<\/i> with <a href=\"https://www.goodreads.com/book/show/24396873-mate\"><i><u>Mate: Become the Man Women Want<\/u><\/i><\/a>, a self-help classic for men. Like most books in this genre, <i>Mate <\/i>takes it for granted that the reader of the book is merely the raw material from which a better man may be built. It contains long lists of verbs in the imperative: <i>do this, and for <strong>fuck’s <\/strong>sake stop doing that.<\/i><\/p><p>When men give women dating advice, it often sounds very similar to what they’d tell a man. I could point to a lot of my own writing through the years, but <a href=\"https://livingwithinreason.com/p/just-ask-people-out-for-women\"><u>Wesley Fenza volunteers to make the point explicitly<\/u><\/a>:<\/p><blockquote><p>The argument goes that as a heterosexual woman, your job is to attract men, and it’s the man’s job to approach you. If you subvert this by asking men out, you’re eliminating a key filter. [...]<\/p><p>This is, of course, ridiculous. First of all, it’s a terrible filter. […] Second of all, if this is a good filter, that means that you, a person who refuses to ask others out, are a low-effort, uninterested coward. Why would any man want to ask you out? […]<\/p><p>If you identify someone you want to date, or sleep with, or just think is cute, then for fuck’s sake, just ask them out.<\/p><p>— Just Ask People Out (for Women)<\/p><\/blockquote><p>I’m sympathetic to Wesley and to all the other men who have complained that women are cowards for never making the first explicit move. I’m sympathetic because asking people out is, indeed, scary and hard and it would be much easie... <\/p>","plaintextDescription":"Previously in sequence: You Are not a Thought Experiment\n\nCross-posted from SecondPerson.dating.\n\n----------------------------------------\n\nI want to conclude our interplanetary exploration of sex differences with one that’s of great interest to me personally: there seems to be little explicit advice on dating for women, and almost none of it is written by men. Despite my sincere attempts to write for both sexes, the percent of my own subscribers who are of the female persuasion is lower than when I used to write 4,000-word treatises on sports analytics.\n\nThere’s a lot written by and for women about the care and maintenance of existing relationships. But as far as getting into a relationship, almost all advice for them boils down to overcoming insecurity and selecting the right man. Very little is oriented towards changing oneself to become more desirable, even less towards proactively seeking the right man wherever he may be. In my review of How to Not Die Alone, I concluded:\n\n> The goal of How to Not Die Alone is not to change the reader, it’s to validate her.\n\nI contrasted H2NDA with Mate: Become the Man Women Want, a self-help classic for men. Like most books in this genre, Mate takes it for granted that the reader of the book is merely the raw material from which a better man may be built. It contains long lists of verbs in the imperative: do this, and for fuck’s sake stop doing that.\n\nWhen men give women dating advice, it often sounds very similar to what they’d tell a man. I could point to a lot of my own writing through the years, but Wesley Fenza volunteers to make the point explicitly:\n\n> The argument goes that as a heterosexual woman, your job is to attract men, and it’s the man’s job to approach you. If you subvert this by asking men out, you’re eliminating a key filter. [...]\n> \n> This is, of course, ridiculous. First of all, it’s a terrible filter. […] Second of all, if this is a good filter, that means that you, a person who refuses to ask others out,","wordCount":2432,"version":"1.2.1"},"SocialPreviewType:tWgXDHWkMYpzAGxPg":{"_id":"tWgXDHWkMYpzAGxPg","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/tWgXDHWkMYpzAGxPg/izdp6lphmyfps5sixcav"},"User:tzER8b2F9ofG5wq5p":{"_id":"tzER8b2F9ofG5wq5p","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":null,"slug":"jacob-falkovich","createdAt":"2014-12-19T14:34:43.963Z","username":"Jacobian","displayName":"Jacob Falkovich","previousDisplayName":null,"fullName":"Jacob Falkovich","karma":5804,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"<p>Writes <a href=\"http://Putanumonit.com\">Putanumonit.com<\/a> and <a href=\"https://www.secondperson.dating/\">SecondPerson.dating<\/a>. @yashkaf on Twitter.<\/p>","jobTitle":null,"organization":null,"postCount":182,"commentCount":265,"sequenceCount":1,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:tWgXDHWkMYpzAGxPg":{"_id":"tWgXDHWkMYpzAGxPg","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[{"__ref":"Comment:kntacwLmtb8CRFtjC"},{"__ref":"Comment:q6zheP4yyAD2esg58"},{"__ref":"Comment:dfTscoetJi62TTaku"},{"__ref":"Comment:3XbcoQjB62HnFpZCt"}],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:v84DGfczbyugAtgvb"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":10,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fkABsGCJZ6y9qConW"}],"socialPreviewData":{"__ref":"SocialPreviewType:tWgXDHWkMYpzAGxPg"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-07T15:32:17.353Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-07T21:27:04.495Z","meta":false,"postCategory":"post","tagRelevance":{"fkABsGCJZ6y9qConW":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"v84DGfczbyugAtgvb","commentCount":15,"voteCount":11,"baseScore":21,"extendedScore":{"reacts":{"typo":[{"karma":15,"quotes":["Subscribe to Second Person to get her number.\n\nSubscribed"],"userId":"kwK6NoFCAo5XDsuuv","reactType":"created","displayName":"Phiwip"}]},"agreement":0,"approvalVoteCount":11,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.31594836711883545,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T19:46:48.205Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"tzER8b2F9ofG5wq5p","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":8,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":10,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-07T14:34:07.325Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:tzER8b2F9ofG5wq5p"},"coauthors":[],"slug":"navigation-by-moonlight","title":"Navigation by Moonlight","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:3ySj7goM9dpzBopqt_contents":{"_id":"3ySj7goM9dpzBopqt_contents","__typename":"Revision","html":"<p>Is the qualia rainbow theory a personal choice for deciding which copies to count as \"me\" and which copies to count as \"not me?\" Or does the theory say there is an objective flowchart in the universe, which dictates which future observer each observer shall experience becoming, and with what probabilities? If it was objective, could a set of red qualia be observed with a microscope?<\/p><p>I agree that qualia is an important topic (even if we don't endorse the qualia rainbow theory), I agree that identity is complex, though I still strongly believe that which object contains my <i>future<\/i> identity is a very subjective choice by me.<\/p>","plaintextMainText":"Is the qualia rainbow theory a personal choice for deciding which copies to count as \"me\" and which copies to count as \"not me?\" Or does the theory say there is an objective flowchart in the universe, which dictates which future observer each observer shall experience becoming, and with what probabilities? If it was objective, could a set of red qualia be observed with a microscope?\n\nI agree that qualia is an important topic (even if we don't endorse the qualia rainbow theory), I agree that identity is complex, though I still strongly believe that which object contains my future identity is a very subjective choice by me.","wordCount":108},"Comment:3ySj7goM9dpzBopqt":{"_id":"3ySj7goM9dpzBopqt","__typename":"Comment","postId":"ZE4xhZHDHHXPuXzxh","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"TMnmi76smDNEG6kgP","topLevelCommentId":"oksaaj3WEn6J54kRJ","descendentCount":0,"title":null,"contents":{"__ref":"Revision:3ySj7goM9dpzBopqt_contents"},"postedAt":"2025-04-09T19:46:29.723Z","lastEditedAt":"2025-04-09T19:47:05.783Z","repliesBlockedUntil":null,"userId":"FWBQ5jBvf4Avku9Wi","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:FWBQ5jBvf4Avku9Wi"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.4506089985370636,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":1,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:46:29.724Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:TMnmi76smDNEG6kgP_contents":{"_id":"TMnmi76smDNEG6kgP_contents","__typename":"Revision","html":"<p>I think there is more to consider. For example, we can imagine a \"qualia rainbow\" theory of identity. I don't necessarily endorse it, but it illustrates why understanding qualia is important for identity.<\/p><p>Imagine that infinitely many different qualia of \"reds\" could denote one real red. Each person, when born, is randomly initialized with a unique set of qualia for all colors and other sensations. This set can be called a \"rainbow of qualia,\" and continuous computing in the brain maintains it throughout a person's life. A copy of me with a different set of qualia, though behaviorally indistinguishable, is not me. Only future mind states with the same set of qualia as mine are truly me, even if my memories were replaced with those of a rat.<br><br>Anthropic Trilemma is masterpiece.&nbsp;<br>&nbsp;<\/p>","plaintextMainText":"I think there is more to consider. For example, we can imagine a \"qualia rainbow\" theory of identity. I don't necessarily endorse it, but it illustrates why understanding qualia is important for identity.\n\nImagine that infinitely many different qualia of \"reds\" could denote one real red. Each person, when born, is randomly initialized with a unique set of qualia for all colors and other sensations. This set can be called a \"rainbow of qualia,\" and continuous computing in the brain maintains it throughout a person's life. A copy of me with a different set of qualia, though behaviorally indistinguishable, is not me. Only future mind states with the same set of qualia as mine are truly me, even if my memories were replaced with those of a rat.\n\nAnthropic Trilemma is masterpiece. \n ","wordCount":132},"User:op64n8Qs4yJLNhA8q":{"_id":"op64n8Qs4yJLNhA8q","__typename":"User","slug":"avturchin","createdAt":"2017-09-23T12:49:07.811Z","username":"avturchin","displayName":"avturchin","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":4319,"afKarma":8,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":102,"commentCount":1749,"sequenceCount":0,"afPostCount":1,"afCommentCount":5,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"55XxDBpfKkkBPm9H8"},"Comment:TMnmi76smDNEG6kgP":{"_id":"TMnmi76smDNEG6kgP","__typename":"Comment","postId":"ZE4xhZHDHHXPuXzxh","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"s9GiXaSop9THDmrJX","topLevelCommentId":"oksaaj3WEn6J54kRJ","descendentCount":1,"title":null,"contents":{"__ref":"Revision:TMnmi76smDNEG6kgP_contents"},"postedAt":"2025-04-09T09:46:53.454Z","lastEditedAt":"2025-04-09T09:46:53.455Z","repliesBlockedUntil":null,"userId":"op64n8Qs4yJLNhA8q","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:op64n8Qs4yJLNhA8q"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":3,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"score":0.17667700350284576,"voteCount":2,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":2,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.1.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:46:29.871Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:S2gjyo8rt7r9EdEDe":{"_id":"S2gjyo8rt7r9EdEDe","__typename":"Revision","htmlHighlight":"<p>I just published <a href=\"/posts/X6Nx9QzzvDhj8Ek9w\"><strong>A Slow Guide to Confronting Doom<\/strong><\/a>, containing my own approach to living in a world that I think has a high likelihood of ending soon. Fortunately I'm not the only person to have written on topic.<\/p><p>Below are my thoughts on what others have written. I have not written these such that they stand independent from the originals, and have attentionally not written summaries that wouldn't do the pieces justice. I suggest you read or at least skim the originals.<\/p><p>For this just wanting a list of all the essays, here ya go:<\/p><ul><li><a href=\"https://longerramblings.substack.com/p/a-defence-of-slowness-at-the-end\">A defence of slowness at the end of the world (Sarah)<\/a><\/li><li><a href=\"https://www.andybannister.net/wp-content/uploads/2022/03/cslewis-living-in-an-atomic-age.pdf\">How will the bomb find you? (C. S. Lewis)<\/a><\/li><li><a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\">Death with Dignity (Eliezer Yudkowsky)<\/a><\/li><li><a href=\"https://www.lesswrong.com/posts/xF7gBJYsy6qenmmCS/don-t-die-with-dignity-instead-play-to-your-outs\">Don't die with dignity; instead play to your outs (Jeffrey Ladish)<\/a> <\/li><li><a href=\"https://www.lesswrong.com/posts/Cf2zBkoocqcjnrNFD/emotionally-confronting-a-probably-doomed-world-against\">Emotionally Confronting a Probably-Doomed World: Against Motivation Via Dignity Points (TurnTrout)<\/a><\/li><li><a href=\"https://www.lesswrong.com/posts/EvKa7EakoXreCkhC6/a-way-to-be-okay\">A Way To Be Okay (Duncan Sabien)<\/a> <\/li><li><a href=\"https://www.lesswrong.com/posts/SKweL8jwknqjACozj/another-way-to-be-okay\">Another Way to Be Okay (Gretta Duleba)<\/a><\/li><li><a href=\"https://www.lesswrong.com/posts/iJnBLanZLephL5cao/being-at-peace-with-doom\">Being at peace with Doom (Johannes C. Mayer)<\/a><\/li><li><a href=\"https://www.lesswrong.com/posts/kcoqwHscvQTx4xgwa/here-s-the-exit\">Here's the exit. (Valentine)<\/a>&nbsp;<\/li><\/ul><hr><h2><a href=\"https://longerramblings.substack.com/p/a-defence-of-slowness-at-the-end\">A defence of slowness at the end of the world (Sarah)<\/a><\/h2><p>I feel kinship with Sarah. She's wrestling with the same harsh scary realities I am – <i>feeling the AGI<\/i>. The post isn't that long and I recommend reading it, but to quote just a little:<\/p><blockquote><p>Since learning of the coming AI revolution, I’ve lived in two worlds. One moves at a leisurely pace, the same way it has all my life. In this world, I am safely nestled in the comfort of indefinite time. It’s ok to let the odd day slip idly by because there are always more.<\/p><p>The second moves exponentially faster. Its shelf-life is measured in a single-digit number of years. Its inhabitants are the <a href=\"https://situational-awareness.ai/\"><u>Situationally Aware<\/u><\/a>; the engineers and prophets of imminent AI transformation. To live in this world is to possess what Ezra Klein <a href=\"https://www.nytimes.com/2023/03/12/opinion/chatbots-artificial-intelligence-future-weirdness.html\"><u>calls<\/u><\/a> “an altered sense of time and consequence”.<\/p><p>I find that it’s psychologically untenable to spend all that much time in the Fast World. I can handle it for minutes to hours, but my mind invariably snaps back into its default state like I’m pulling my hand out of ice water.<\/p><p>Occupying the Slow World is ultimately a form of denial. I can’t call it anything other than compartmentalisation, yet I actually advocate for it. Of course, those of us trying to move the needle on AI risk should <i>work<\/i> in the Fast World, but I claim that we shouldn’t <i>live<\/i> in it. I will try to make the case for why.<\/p><\/blockquote><p>Regarding the general idea of having two modes that one moves bet... <\/p>","plaintextDescription":"I just published A Slow Guide to Confronting Doom, containing my own approach to living in a world that I think has a high likelihood of ending soon. Fortunately I'm not the only person to have written on topic.\n\nBelow are my thoughts on what others have written. I have not written these such that they stand independent from the originals, and have attentionally not written summaries that wouldn't do the pieces justice. I suggest you read or at least skim the originals.\n\nFor this just wanting a list of all the essays, here ya go:\n\n * A defence of slowness at the end of the world (Sarah)\n * How will the bomb find you? (C. S. Lewis)\n * Death with Dignity (Eliezer Yudkowsky)\n * Don't die with dignity; instead play to your outs (Jeffrey Ladish)\n * Emotionally Confronting a Probably-Doomed World: Against Motivation Via Dignity Points (TurnTrout)\n * A Way To Be Okay (Duncan Sabien)\n * Another Way to Be Okay (Gretta Duleba)\n * Being at peace with Doom (Johannes C. Mayer)\n * Here's the exit. (Valentine) \n\n----------------------------------------\n\n\nA defence of slowness at the end of the world (Sarah)\nI feel kinship with Sarah. She's wrestling with the same harsh scary realities I am – feeling the AGI. The post isn't that long and I recommend reading it, but to quote just a little:\n\n> Since learning of the coming AI revolution, I’ve lived in two worlds. One moves at a leisurely pace, the same way it has all my life. In this world, I am safely nestled in the comfort of indefinite time. It’s ok to let the odd day slip idly by because there are always more.\n> \n> The second moves exponentially faster. Its shelf-life is measured in a single-digit number of years. Its inhabitants are the Situationally Aware; the engineers and prophets of imminent AI transformation. To live in this world is to possess what Ezra Klein calls “an altered sense of time and consequence”.\n> \n> I find that it’s psychologically untenable to spend all that much time in the Fast World. I can handle it for mi","wordCount":3641,"version":"1.1.0"},"Revision:ZE4xhZHDHHXPuXzxh_customHighlight":{"_id":"ZE4xhZHDHHXPuXzxh_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Tag:3ee9k6NJfcGzL6kMS":{"_id":"3ee9k6NJfcGzL6kMS","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Emotions","shortName":null,"slug":"emotions","core":false,"postCount":209,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-07T21:10:58.722Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":20,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"},{"_id":"8btiLJDabHgZuiSAB","displayName":"Ggwp"}]},"score":20,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:iP2X4jQNHMWHRNPne":{"_id":"iP2X4jQNHMWHRNPne","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Motivations","shortName":null,"slug":"motivations","core":false,"postCount":197,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-06-08T00:06:01.955Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:ZE4xhZHDHHXPuXzxh":{"_id":"ZE4xhZHDHHXPuXzxh","__typename":"SocialPreviewType","imageUrl":""},"User:qgdGA4ZEyW7zNdK84":{"_id":"qgdGA4ZEyW7zNdK84","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":true,"slug":"ruby","createdAt":"2014-04-03T03:38:23.914Z","username":"Ruby","displayName":"Ruby","previousDisplayName":null,"fullName":"Ruben Bloom","karma":14166,"afKarma":137,"deleted":false,"isAdmin":true,"htmlBio":"<p>LessWrong Team<\/p><p>&nbsp;<\/p><p>I have signed no contracts or agreements whose existence I cannot mention.<\/p>","jobTitle":null,"organization":null,"postCount":171,"commentCount":1647,"sequenceCount":11,"afPostCount":3,"afCommentCount":33,"spamRiskScore":1,"tagRevisionCount":1003,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"Post:ZE4xhZHDHHXPuXzxh":{"_id":"ZE4xhZHDHHXPuXzxh","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[{"__ref":"Comment:3ySj7goM9dpzBopqt"},{"__ref":"Comment:TMnmi76smDNEG6kgP"}],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:S2gjyo8rt7r9EdEDe"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":15,"rejectedReason":null,"customHighlight":{"__ref":"Revision:ZE4xhZHDHHXPuXzxh_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:3ee9k6NJfcGzL6kMS"},{"__ref":"Tag:iP2X4jQNHMWHRNPne"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:ZE4xhZHDHHXPuXzxh"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-06T02:11:31.271Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-06T02:13:38.283Z","meta":false,"postCategory":"post","tagRelevance":{"3ee9k6NJfcGzL6kMS":2,"iP2X4jQNHMWHRNPne":2,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"S2gjyo8rt7r9EdEDe","commentCount":13,"voteCount":17,"baseScore":48,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":17,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.3243662118911743,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T19:46:29.846Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"qgdGA4ZEyW7zNdK84","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":15,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":6,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-03-09T03:20:43.160Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:qgdGA4ZEyW7zNdK84"},"coauthors":[],"slug":"a-collection-of-approaches-to-confronting-doom-and-my","title":"A collection of approaches to confronting doom, and my thoughts on them ","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:L2HYipAyzDCh7f4ta_contents":{"_id":"L2HYipAyzDCh7f4ta_contents","__typename":"Revision","html":"<p>My initial comment isn't really arguing for the &gt;99% thing. Most of that comes from me sharing the same so-called pessimistic (I would say realistic) expectations as some LWers (e.g. Yudkowsky's <a href=\"https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities\">AGI Ruin: A List of Lethalities<\/a>) that the default outcome of AI progress is unaligned AGI -&gt; unaligned ASI -&gt; extinction, that we're fully on track for that scenario, and that it's very hard to imagine how we'd get off that track.<\/p><blockquote><p>meaning you believe that any one of those things is sufficient enough to ensure doom on its own (which seems nowhere near obviously true to me?)<\/p><\/blockquote><p>No, I didn't mean it like that. I meant that we're currently (in 2025) in the &gt;99% doom scenario, and I meant it seemed to me like we were overdetermined (even back in e.g. 2010) to end up in that scenario (contra Ruby's \"doomed for no better reason than because people were incapable of not doing something\"), even if some stuff changed, e.g. because some specific actors like our leading AI labs didn't come to exist. Because we're in a world where technological extinction is possible and the default outcome of AI research, and our civilization is fundamentally unable to grapple with that fact. Plus a bunch of our virtues (like democracy, or freedom of commerce) turn from virtue to vice in a world where any particular actor can doom everyone by doing sufficient technological research; we have no mechanism whereby these actors are forced to internalize these negative externalities of their actions (like via extinction insurance or some such).<\/p><blockquote><p>that not even an extra 50 years could move<\/p><\/blockquote><p>I don't understand this part. Do you mean an alternative world scenario where compute and AI progress had been so slow, or the compute and algorithmic requirements for AGI had been so high, that our median expected time for a technological singularity would be around the year 2070? I can't really imagine a coherent world where AI alignment progress is relatively easier to accomplish than algorithmic progress (e.g. AI progress yields actual feedback, whereas AI alignment research yields hardly any feedback), so wouldn't we then in 2067 just be in the same situation as we are now?<\/p><blockquote><p>Although, something I’d like to see would be some kind of coordination regarding setting standards for testing or minimum amounts of safety research and then have compliance reviewed by a board maybe, with both legal and financial penalties to be administered in case of violations.<\/p><\/blockquote><p>I don't understand the world model where that prevents any negative outcomes. For instance, AI labs like OpenAI currently argue that they should be under zero regulations, and even petitioned the US government to be exempted from regulation; and the current US government itself cheerleads race dynamics and is strictly against safety research. Even if some AI labs voluntarily submitted themselves to some kinds of standards, that wouldn't help anyone when OpenAI and the US government don't play ball.<\/p><p>(Not to mention that the review board would inevitably be captured by interests like anti-AI-bias stuff, since there's neither sufficient expertise nor a sufficient constituency for anti-extinction policies.)<\/p><blockquote><p>Something that would get me there would be actually seeing the cloud of poison spewing death drones (or whatever) flying towards me. Heck, even if I had a crystal ball right now and saw exactly that, I still wouldn’t see previously having a &gt;99% credence as justifiable.<\/p><\/blockquote><p>That's a disbelief in superintelligence. You need to deflect the asteroid (prevent unaligned ASI from coming into being) long before it crashes into earth, not only when it's already burning up in the atmosphere. From my perspective, the asteroid is already almost upon us (e.g. see the recent <a href=\"https://www.lesswrong.com/posts/TpSFoqoG2M5MAAesg/ai-2027-what-superintelligence-looks-like-1\">AI 2027<\/a> forecast), you're just not looking at it, or you're not understanding what you're seeing.<\/p>","plaintextMainText":"My initial comment isn't really arguing for the >99% thing. Most of that comes from me sharing the same so-called pessimistic (I would say realistic) expectations as some LWers (e.g. Yudkowsky's AGI Ruin: A List of Lethalities) that the default outcome of AI progress is unaligned AGI -> unaligned ASI -> extinction, that we're fully on track for that scenario, and that it's very hard to imagine how we'd get off that track.\n\nNo, I didn't mean it like that. I meant that we're currently (in 2025) in the >99% doom scenario, and I meant it seemed to me like we were overdetermined (even back in e.g. 2010) to end up in that scenario (contra Ruby's \"doomed for no better reason than because people were incapable of not doing something\"), even if some stuff changed, e.g. because some specific actors like our leading AI labs didn't come to exist. Because we're in a world where technological extinction is possible and the default outcome of AI research, and our civilization is fundamentally unable to grapple with that fact. Plus a bunch of our virtues (like democracy, or freedom of commerce) turn from virtue to vice in a world where any particular actor can doom everyone by doing sufficient technological research; we have no mechanism whereby these actors are forced to internalize these negative externalities of their actions (like via extinction insurance or some such).\n\nI don't understand this part. Do you mean an alternative world scenario where compute and AI progress had been so slow, or the compute and algorithmic requirements for AGI had been so high, that our median expected time for a technological singularity would be around the year 2070? I can't really imagine a coherent world where AI alignment progress is relatively easier to accomplish than algorithmic progress (e.g. AI progress yields actual feedback, whereas AI alignment research yields hardly any feedback), so wouldn't we then in 2067 just be in the same situation as we are now?\n\nI don't understand the world mo","wordCount":632},"User:6tLz8Q6yaejBENSzC":{"_id":"6tLz8Q6yaejBENSzC","__typename":"User","slug":"mondsemmel","createdAt":"2013-01-17T18:33:25.683Z","username":"MondSemmel","displayName":"MondSemmel","profileImageId":null,"previousDisplayName":null,"fullName":"Tobias D.","karma":3830,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":12,"commentCount":938,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":8,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:L2HYipAyzDCh7f4ta":{"_id":"L2HYipAyzDCh7f4ta","__typename":"Comment","postId":"X6Nx9QzzvDhj8Ek9w","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"pik7DngaZPBmtLAFS","topLevelCommentId":"QnFxSj8rjaB3vHNxw","descendentCount":0,"title":null,"contents":{"__ref":"Revision:L2HYipAyzDCh7f4ta_contents"},"postedAt":"2025-04-09T19:33:51.771Z","lastEditedAt":"2025-04-09T19:33:51.771Z","repliesBlockedUntil":null,"userId":"6tLz8Q6yaejBENSzC","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:6tLz8Q6yaejBENSzC"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":2,"agreementVoteCount":0},"score":0.37859800457954407,"voteCount":2,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:33:51.777Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:pik7DngaZPBmtLAFS_contents":{"_id":"pik7DngaZPBmtLAFS_contents","__typename":"Revision","html":"<p>I think your defense of the &gt;99% thing is in your first comment where you provided a list of things that cause doom to be “overdetermined”- meaning you believe that any one of those things is sufficient enough to ensure doom on its own (which seems nowhere near obviously true to me?).<br><br>Ruby says you make a good case, but considering what you’re trying to prove, (I.e. near-term “technological extinction” is our nigh-inescapable destiny) I don’t think it’s an especially sufficient case, nor is it treading any new ground. Like yeah, the chances don’t look good, and it would be a good case (as Ruby says) if you were just arguing for a saner type of pessimism, but to say it’s overdetermined to the point where it’s a &gt;99% chance that not even an extra 50 years could move just seems crazy to me, whether you feel like defending it or not.<\/p><p>As far as the policy thing goes, I don’t really know what the weakest thing I could see doing that could avert an apocalypse would be. Although, something I’d like to see would be some kind of coordination regarding setting standards for testing or minimum amounts of safety research and then have compliance reviewed by a board maybe, with both legal and financial penalties to be administered in case of violations.<br><br>Probably underwhelming to you, but then as far as concrete policy goes it’s not something I think about a ton, and I think we’ve already established my views are less extreme than yours. And absent of any of my idea being remotely feasible, that still wouldn’t get me up to &gt;99%. Something that would get me there would be actually seeing the cloud of poison spewing death drones (or whatever) flying towards me. Heck, even if I had a crystal ball right now and saw exactly that, I still wouldn’t see previously having a &gt;99% credence as justifiable.<\/p><p><br>Am I just misunderstanding you here?<\/p>","plaintextMainText":"I think your defense of the >99% thing is in your first comment where you provided a list of things that cause doom to be “overdetermined”- meaning you believe that any one of those things is sufficient enough to ensure doom on its own (which seems nowhere near obviously true to me?).\n\nRuby says you make a good case, but considering what you’re trying to prove, (I.e. near-term “technological extinction” is our nigh-inescapable destiny) I don’t think it’s an especially sufficient case, nor is it treading any new ground. Like yeah, the chances don’t look good, and it would be a good case (as Ruby says) if you were just arguing for a saner type of pessimism, but to say it’s overdetermined to the point where it’s a >99% chance that not even an extra 50 years could move just seems crazy to me, whether you feel like defending it or not.\n\nAs far as the policy thing goes, I don’t really know what the weakest thing I could see doing that could avert an apocalypse would be. Although, something I’d like to see would be some kind of coordination regarding setting standards for testing or minimum amounts of safety research and then have compliance reviewed by a board maybe, with both legal and financial penalties to be administered in case of violations.\n\nProbably underwhelming to you, but then as far as concrete policy goes it’s not something I think about a ton, and I think we’ve already established my views are less extreme than yours. And absent of any of my idea being remotely feasible, that still wouldn’t get me up to >99%. Something that would get me there would be actually seeing the cloud of poison spewing death drones (or whatever) flying towards me. Heck, even if I had a crystal ball right now and saw exactly that, I still wouldn’t see previously having a >99% credence as justifiable.\n\n\nAm I just misunderstanding you here?","wordCount":327},"User:mGtikFCjPYaYjGX9Y":{"_id":"mGtikFCjPYaYjGX9Y","__typename":"User","slug":"sam-iacono","createdAt":"2021-04-23T02:58:10.718Z","username":"sam-iacono","displayName":"Sam Iacono","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":7,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":0,"commentCount":4,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":0.9,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Comment:pik7DngaZPBmtLAFS":{"_id":"pik7DngaZPBmtLAFS","__typename":"Comment","postId":"X6Nx9QzzvDhj8Ek9w","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"oHrrAyt8hYyZfcEqR","topLevelCommentId":"QnFxSj8rjaB3vHNxw","descendentCount":1,"title":null,"contents":{"__ref":"Revision:pik7DngaZPBmtLAFS_contents"},"postedAt":"2025-04-09T18:26:30.568Z","lastEditedAt":"2025-04-09T18:26:30.568Z","repliesBlockedUntil":null,"userId":"mGtikFCjPYaYjGX9Y","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:mGtikFCjPYaYjGX9Y"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.4505389928817749,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.3.1","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:33:51.985Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":1,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:5YFt7dGya4tTivkqx":{"_id":"5YFt7dGya4tTivkqx","__typename":"Revision","htmlHighlight":"<p>Following a few events<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"qhu3n0mfop\" role=\"doc-noteref\" id=\"fnrefqhu3n0mfop\"><sup><a href=\"#fnqhu3n0mfop\">[1]<\/a><\/sup><\/span>&nbsp;in April 2022 that caused a many people to update sharply and negatively on outcomes for humanity, I wrote <a href=\"https://www.lesswrong.com/posts/PQtEqmyqHWDa2vf5H/a-quick-guide-to-confronting-doom\">A Quick Guide to Confronting Doom<\/a>.&nbsp;<\/p><p>I advised:<\/p><ul><li>Think for yourself<\/li><li>Be gentle with yourself<\/li><li>Don't act rashly<\/li><li>Be patient about helping<\/li><li>Don't act unilaterally<\/li><li>Figure out what works for you<i>&nbsp;<\/i><\/li><\/ul><p>This is fine advice and all, I stand by it, but it's also not really a full answer to how to contend with the utterly crushing weight of the expectation that everything and everyone you value will be destroyed in the next decade or two.<\/p><h1>Feeling the Doom<\/h1><p>Before I get into my suggested psychological approach to doom, I want to clarify the kind of doom I'm working to confront. If you are impatient, you can skip to the <a href=\"https://www.lesswrong.com/posts/X6Nx9QzzvDhj8Ek9w/a-slow-guide-to-confronting-doom-v1#Facing_the_doom\">actual advice<\/a>.<\/p><p>The best analogy I have is the feeling of having a terminally ill loved-one with some uncertain yet definitely smallish number of years to live. It's a standardly terminal condition. Not dying would be quite surprising. There's no known effective treatment. Perhaps there's some crazy new experimental treatment I might find or even develop myself if I apply heroic agency.&nbsp;<\/p><p>Except it's not just a single loved one who has the condition, it is myself too, as well as everyone I have loved, do love, or could ever love, plus everyone my loved ones love, and so on. We are all terminal.<\/p><p>The disappointment of imminent death is all the more crushing because just a few years ago researchers announced breakthrough discoveries that suggested [existing, adult] humans could have healthspans of thousands of years. To drop the analogy, here I'm talking about my transhumanist beliefs. The laws of physics don't demand that humans slowly decay and die at eighty. It is within our engineering prowess to defeat death, and until recently I thought we might just do that, and I and my loved ones would live for millennia, becoming post-human superbeings.<\/p><p>To quote a <a href=\"https://www.lesswrong.com/posts/rzruCSWMXja6x9BdN/the-grandest-vision-for-humanity-light\">speech from my wedding<\/a> in 2015:<\/p><blockquote><p>Imagine a time <i>when time is no longer imaginable<\/i>. When the dances never need to end, when the lovers never need to die, when entropy no longer dominates. Imagine a world where we could explore with full scope, for as long as we wanted, every idea, every concept, every location. To dance with the stars, to dance with each other, to dance simply for the sake of dancing with no time limits. Imagine a world of radical life <i>expansion<\/i>, accompanying radical<\/p><\/blockquote>... ","plaintextDescription":"Following a few events[1] in April 2022 that caused a many people to update sharply and negatively on outcomes for humanity, I wrote A Quick Guide to Confronting Doom. \n\nI advised:\n\n * Think for yourself\n * Be gentle with yourself\n * Don't act rashly\n * Be patient about helping\n * Don't act unilaterally\n * Figure out what works for you \n\nThis is fine advice and all, I stand by it, but it's also not really a full answer to how to contend with the utterly crushing weight of the expectation that everything and everyone you value will be destroyed in the next decade or two.\n\n\nFeeling the Doom\nBefore I get into my suggested psychological approach to doom, I want to clarify the kind of doom I'm working to confront. If you are impatient, you can skip to the actual advice.\n\nThe best analogy I have is the feeling of having a terminally ill loved-one with some uncertain yet definitely smallish number of years to live. It's a standardly terminal condition. Not dying would be quite surprising. There's no known effective treatment. Perhaps there's some crazy new experimental treatment I might find or even develop myself if I apply heroic agency. \n\nExcept it's not just a single loved one who has the condition, it is myself too, as well as everyone I have loved, do love, or could ever love, plus everyone my loved ones love, and so on. We are all terminal.\n\nThe disappointment of imminent death is all the more crushing because just a few years ago researchers announced breakthrough discoveries that suggested [existing, adult] humans could have healthspans of thousands of years. To drop the analogy, here I'm talking about my transhumanist beliefs. The laws of physics don't demand that humans slowly decay and die at eighty. It is within our engineering prowess to defeat death, and until recently I thought we might just do that, and I and my loved ones would live for millennia, becoming post-human superbeings.\n\nTo quote a speech from my wedding in 2015:\n\n> Imagine a time when time is n","wordCount":4336,"version":"1.3.1"},"Revision:X6Nx9QzzvDhj8Ek9w_customHighlight":{"_id":"X6Nx9QzzvDhj8Ek9w_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"Tag:oNcqyaWPXNGTTRPHm":{"_id":"oNcqyaWPXNGTTRPHm","__typename":"Tag","userId":"7iXcndyHDvmt77ggr","name":"Existential risk","shortName":null,"slug":"existential-risk","core":false,"postCount":483,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2016-12-23T09:11:59.000Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":true,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:X6Nx9QzzvDhj8Ek9w":{"_id":"X6Nx9QzzvDhj8Ek9w","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/X6Nx9QzzvDhj8Ek9w/emoawbds700okyva3952"},"Post:X6Nx9QzzvDhj8Ek9w":{"_id":"X6Nx9QzzvDhj8Ek9w","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[{"__ref":"Comment:L2HYipAyzDCh7f4ta"},{"__ref":"Comment:pik7DngaZPBmtLAFS"}],"currentUserVote":"bigUpvote","currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5YFt7dGya4tTivkqx"},"fmCrosspost":{"hostedHere":true,"isCrosspost":true,"foreignPostId":"zJeB33RHEA8QjSeJ4"},"readTimeMinutes":17,"rejectedReason":null,"customHighlight":{"__ref":"Revision:X6Nx9QzzvDhj8Ek9w_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:oNcqyaWPXNGTTRPHm"},{"__ref":"Tag:fkABsGCJZ6y9qConW"}],"socialPreviewData":{"__ref":"SocialPreviewType:X6Nx9QzzvDhj8Ek9w"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-06T02:10:56.483Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-06T02:13:44.310Z","meta":false,"postCategory":"post","tagRelevance":{"fkABsGCJZ6y9qConW":1,"oNcqyaWPXNGTTRPHm":2},"shareWithUsers":["6Fx2vQtkYSZkaCvAg","3oopbgcjYfvN8B2fp"],"sharingSettings":{"anyoneWithLinkCan":"edit","explicitlySharedUsersCan":"edit"},"linkSharingKey":null,"contents_latest":"5YFt7dGya4tTivkqx","commentCount":19,"voteCount":39,"baseScore":80,"extendedScore":{"reacts":{"typo":[{"karma":894,"quotes":["into her"],"userId":"dj8kszLHJoNC4vCwg","reactType":"created","displayName":"Kaarel"},{"karma":894,"quotes":["I'd advocating"],"userId":"dj8kszLHJoNC4vCwg","reactType":"created","displayName":"Kaarel"},{"karma":894,"quotes":["commiting"],"userId":"dj8kszLHJoNC4vCwg","reactType":"created","displayName":"Kaarel"}],"agree":[{"karma":1121,"quotes":["But who knows, perhaps one of my assumptions is wrong. Perhaps there's some luck better than humanity deserves. If this happens to be the case, I want to be in a position to make use of it.\n\n"],"userId":"3RHzytgr6QeZD7xRz","reactType":"created","displayName":"Stephen McAleese"},{"karma":946,"quotes":["But who knows, perhaps one of my assumptions is wrong. Perhaps there's some luck better than humanity deserves. If this happens to be the case, I want to be in a position to make use of it.\n\n"],"userId":"WqpBR8YrcAnZrzGJk","reactType":"seconded","displayName":"Declan Molony"}],"heart":[{"karma":3943,"quotes":["And there the potential future is so large that even in these doomy times, the expected value is large. Small 'e' perhaps, but still enormously large 'v'. Same as we do everyday.\n\n"],"userId":"ezbRa3dntKWQ5995r","reactType":"created","displayName":"niplav"}]},"agreement":0,"approvalVoteCount":39,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.5032123327255249,"lastVisitedAt":"2025-04-06T16:56:50.371Z","isFuture":false,"isRead":true,"lastCommentedAt":"2025-04-09T19:33:51.955Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"qgdGA4ZEyW7zNdK84","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":22,"afExtendedScore":{"reacts":{"typo":[{"karma":894,"quotes":["into her"],"userId":"dj8kszLHJoNC4vCwg","reactType":"created","displayName":"Kaarel"},{"karma":894,"quotes":["I'd advocating"],"userId":"dj8kszLHJoNC4vCwg","reactType":"created","displayName":"Kaarel"},{"karma":894,"quotes":["commiting"],"userId":"dj8kszLHJoNC4vCwg","reactType":"created","displayName":"Kaarel"}],"agree":[{"karma":1121,"quotes":["But who knows, perhaps one of my assumptions is wrong. Perhaps there's some luck better than humanity deserves. If this happens to be the case, I want to be in a position to make use of it.\n\n"],"userId":"3RHzytgr6QeZD7xRz","reactType":"created","displayName":"Stephen McAleese"}]},"agreement":0,"approvalVoteCount":12,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-02-12T18:51:44.313Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:qgdGA4ZEyW7zNdK84"},"coauthors":[],"slug":"a-slow-guide-to-confronting-doom","title":"A Slow Guide to Confronting Doom","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:4Epqwx92aJKs4qEfr":{"_id":"4Epqwx92aJKs4qEfr","__typename":"Revision","htmlHighlight":"<p>English: <a href=\"https://www.rationality-freiburg.de/events/2025-05-09-hpmor/\">https://www.rationality-freiburg.de/events/2025-05-09-hpmor/<\/a><\/p>\n<p>Deutsch: <a href=\"https://www.rationality-freiburg.de/de/termine/2025-05-09-hpmor/\">https://www.rationality-freiburg.de/de/termine/2025-05-09-hpmor/<\/a><\/p>","plaintextDescription":"English: https://www.rationality-freiburg.de/events/2025-05-09-hpmor/\n\nDeutsch: https://www.rationality-freiburg.de/de/termine/2025-05-09-hpmor/","wordCount":4,"version":"1.0.0"},"SocialPreviewType:nuFw7jNrGkJ8bswZu":{"_id":"nuFw7jNrGkJ8bswZu","__typename":"SocialPreviewType","imageUrl":""},"Localgroup:fFZZ2Ywzsab86EESY":{"_id":"fFZZ2Ywzsab86EESY","__typename":"Localgroup","name":"Rationality Freiburg","organizerIds":["yLipGJQRLnS9Yndog","5TDkMstKmzAdaBYfn","Tdo3AEXSWn9wpx92R"],"contents":{"__ref":"Revision:fFZZ2Ywzsab86EESY_contents"},"createdAt":"2022-04-16T12:40:30.454Z","organizers":[{"__ref":"User:yLipGJQRLnS9Yndog"},{"__ref":"User:5TDkMstKmzAdaBYfn"},{"__ref":"User:Tdo3AEXSWn9wpx92R"}],"lastActivity":"2022-04-16T12:40:30.454Z","nameInAnotherLanguage":null,"isOnline":false,"location":"Freiburg im Breisgau, Germany","googleLocation":{"url":"https://maps.google.com/?q=Freiburg+im+Breisgau,+Germany&ftid=0x47911b26560bd665:0x41f6bb7a5df57b0","icon":"https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/geocode-71.png","name":"Freiburg im Breisgau","types":["locality","political"],"photos":[{"width":3648,"height":2736,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/106357882654154653233\">Isu Abuhattab<\/a>"]},{"width":2252,"height":4000,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/105726229716953884116\">Jost Philipp<\/a>"]},{"width":2250,"height":4000,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/103772290673265227337\">Arjan van der Lem<\/a>"]},{"width":3096,"height":4128,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/104749630580625062386\">Reiner Oechsle<\/a>"]},{"width":4000,"height":1844,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/110286854032056844334\">Niculaie Tudor<\/a>"]},{"width":3464,"height":4618,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/102401653833267053945\">Helmut Roemer<\/a>"]},{"width":4032,"height":2268,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/109543962826975507448\">Hendrik Hemmer<\/a>"]},{"width":4032,"height":2268,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/110247446659094115793\">Millerland<\/a>"]},{"width":4032,"height":2268,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/113200972653493844766\">Roland Hubacher<\/a>"]},{"width":1536,"height":2048,"html_attributions":["<a href=\"https://maps.google.com/maps/contrib/107841479576570958324\">Monika S<\/a>"]}],"website":"http://www.freiburg.de/","geometry":{"location":{"lat":47.9990077,"lng":7.842104299999999},"viewport":{"east":7.93083228812262,"west":7.662007828904639,"north":48.07104478669411,"south":47.90357750361994}},"place_id":"ChIJZdYLViYbkUcRsFffpbdrHwQ","vicinity":"Freiburg im Breisgau","reference":"ChIJZdYLViYbkUcRsFffpbdrHwQ","utc_offset":120,"adr_address":"<span class=\"locality\">Freiburg im Breisgau<\/span>, <span class=\"country-name\">Germany<\/span>","formatted_address":"Freiburg im Breisgau, Germany","html_attributions":[],"address_components":[{"types":["locality","political"],"long_name":"Freiburg im Breisgau","short_name":"Freiburg im Breisgau"},{"types":["administrative_area_level_3","political"],"long_name":"Kreisfreie Stadt Freiburg im Breisgau","short_name":"Kreisfreie Stadt Freiburg im Breisgau"},{"types":["administrative_area_level_2","political"],"long_name":"Freiburg","short_name":"Freiburg"},{"types":["administrative_area_level_1","political"],"long_name":"Baden-Württemberg","short_name":"BW"},{"types":["country","political"],"long_name":"Germany","short_name":"DE"}],"icon_mask_base_uri":"https://maps.gstatic.com/mapfiles/place_api/icons/v2/generic_pinlet","utc_offset_minutes":120,"icon_background_color":"#7B9EB0"},"mongoLocation":{"type":"Point","coordinates":[7.842104299999999,47.9990077]},"types":["LW","SSC"],"categories":null,"contactInfo":"info@rationality-freiburg.de","facebookLink":null,"facebookPageLink":null,"meetupLink":"https://www.meetup.com/rationality-freiburg/","slackLink":null,"website":"https://www.rationality-freiburg.de/","bannerImageId":"sequences/rbtwx3tfm0d6umxk9852","inactive":false,"deleted":false},"User:yLipGJQRLnS9Yndog":{"_id":"yLipGJQRLnS9Yndog","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"omark","createdAt":"2022-04-16T12:36:07.679Z","username":"omark","displayName":"omark","previousDisplayName":null,"fullName":null,"karma":91,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":66,"commentCount":18,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:5TDkMstKmzAdaBYfn":{"_id":"5TDkMstKmzAdaBYfn","__typename":"User","slug":"bibhu-kar","createdAt":"2022-10-17T11:33:27.048Z","username":"bibhu-kar","displayName":"Bibhu kar","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":0,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":0,"commentCount":0,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":0.8,"tagRevisionCount":0,"reviewedByUserId":null},"User:Tdo3AEXSWn9wpx92R":{"_id":"Tdo3AEXSWn9wpx92R","__typename":"User","slug":"mor","createdAt":"2021-03-02T20:05:34.732Z","username":"Mor","displayName":"Mor","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":3,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":0,"commentCount":2,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":0.9,"tagRevisionCount":0,"reviewedByUserId":"qgdGA4ZEyW7zNdK84"},"Post:nuFw7jNrGkJ8bswZu":{"_id":"nuFw7jNrGkJ8bswZu","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:4Epqwx92aJKs4qEfr"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Ng8Gice9KNkncxqcj"}],"socialPreviewData":{"__ref":"SocialPreviewType:nuFw7jNrGkJ8bswZu"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-09T19:27:19.088Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":{"Ng8Gice9KNkncxqcj":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"4Epqwx92aJKs4qEfr","commentCount":0,"voteCount":1,"baseScore":2,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.9011539816856384,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T19:27:19.088Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"yLipGJQRLnS9Yndog","location":"Rehlingstraße 9, 79100 Freiburg im Breisgau, Germany","googleLocation":{"url":"https://maps.google.com/?q=Rehlingstra%C3%9Fe+9,+79100+Freiburg+im+Breisgau,+Germany&ftid=0x47911b5fabb533ed:0x5415e57216228c8d","icon":"https://maps.gstatic.com/mapfiles/place_api/icons/v1/png_71/geocode-71.png","name":"Rehlingstraße 9","types":["premise"],"geometry":{"location":{"lat":47.9895238,"lng":7.8397408},"viewport":{"east":7.841159130291502,"west":7.838461169708498,"north":47.9909020802915,"south":47.9882041197085}},"place_id":"ChIJ7TO1q18bkUcRjYwiFnLlFVQ","vicinity":"Süd","reference":"ChIJ7TO1q18bkUcRjYwiFnLlFVQ","utc_offset":120,"adr_address":"<span class=\"street-address\">Rehlingstraße 9<\/span>, <span class=\"postal-code\">79100<\/span> <span class=\"locality\">Freiburg im Breisgau<\/span>, <span class=\"country-name\">Germany<\/span>","formatted_address":"Rehlingstraße 9, 79100 Freiburg im Breisgau, Germany","html_attributions":[],"address_components":[{"types":["street_number"],"long_name":"9","short_name":"9"},{"types":["route"],"long_name":"Rehlingstraße","short_name":"Rehlingstraße"},{"types":["sublocality_level_1","sublocality","political"],"long_name":"Süd","short_name":"Süd"},{"types":["locality","political"],"long_name":"Freiburg im Breisgau","short_name":"Freiburg im Breisgau"},{"types":["administrative_area_level_3","political"],"long_name":"Kreisfreie Stadt Freiburg im Breisgau","short_name":"Kreisfreie Stadt Freiburg im Breisgau"},{"types":["administrative_area_level_2","political"],"long_name":"Freiburg","short_name":"Freiburg"},{"types":["administrative_area_level_1","political"],"long_name":"Baden-Württemberg","short_name":"BW"},{"types":["country","political"],"long_name":"Germany","short_name":"DE"},{"types":["postal_code"],"long_name":"79100","short_name":"79100"}],"icon_mask_base_uri":"https://maps.gstatic.com/mapfiles/place_api/icons/v2/generic_pinlet","utc_offset_minutes":120,"icon_background_color":"#7B9EB0"},"onlineEvent":false,"globalEvent":false,"startTime":"2025-05-09T16:00:00.000Z","endTime":"2025-05-09T18:30:00.000Z","localStartTime":"2025-05-09T18:00:00.000Z","localEndTime":"2025-05-09T20:30:00.000Z","eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":true,"eventImageId":null,"eventType":"discussion","types":["LW","SSC"],"groupId":"fFZZ2Ywzsab86EESY","reviewedByUserId":null,"suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-09T19:26:31.562Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":false,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":{"__ref":"Localgroup:fFZZ2Ywzsab86EESY"},"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:yLipGJQRLnS9Yndog"},"coauthors":[{"__ref":"User:5TDkMstKmzAdaBYfn"},{"__ref":"User:Tdo3AEXSWn9wpx92R"}],"slug":"freiburg-introduction-to-hpmor","title":"Freiburg - Introduction to HPMoR","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"5TDkMstKmzAdaBYfn","confirmed":true,"requested":false},{"userId":"Tdo3AEXSWn9wpx92R","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:ecymByEAq6vhrcCar_contents":{"_id":"ecymByEAq6vhrcCar_contents","__typename":"Revision","html":"<p>The older deck sucks. It contains the entirety of the essay without regard to what's important. This deck is still messy - including too much focusing on the ordering and numbering of the virtues - but it's significantly superior, and contains concise hearts of the matter. If you're trying to create a memory aid for the Twelve Virtues, this deck was absolutely an improvement.<\/p>\n","plaintextMainText":"The older deck sucks. It contains the entirety of the essay without regard to what's important. This deck is still messy - including too much focusing on the ordering and numbering of the virtues - but it's significantly superior, and contains concise hearts of the matter. If you're trying to create a memory aid for the Twelve Virtues, this deck was absolutely an improvement.","wordCount":64},"User:kmiXJjx2GS4txx3yj":{"_id":"kmiXJjx2GS4txx3yj","__typename":"User","slug":"czynski","createdAt":"2018-11-22T19:34:59.649Z","username":"JacobKopczynski","displayName":"Czynski","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":315,"afKarma":9,"deleted":false,"isAdmin":false,"htmlBio":"<p>Jacob, or \"Jisk\" when there are too many Jacobs about and I need a nickname.<\/p><p>Honestly pretty disappointed with the state of the modern LW site, but it's marginally better than other non-blogs so I'm still here.<\/p><p>It should be possible to easily find me from the username I use here, though not vice versa, for interview reasons.<\/p>","jobTitle":null,"organization":null,"postCount":127,"commentCount":206,"sequenceCount":0,"afPostCount":0,"afCommentCount":8,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"EQNTWXLKMeWMp2FQS"},"Comment:ecymByEAq6vhrcCar":{"_id":"ecymByEAq6vhrcCar","__typename":"Comment","postId":"TNyEe3bzonreRW9nB","tagId":null,"tag":null,"relevantTagIds":[],"relevantTags":[],"tagCommentType":"DISCUSSION","parentCommentId":"XbzMcyx9EoDK7Bjd6","topLevelCommentId":"TibisnxAkbq9FguDt","descendentCount":0,"title":null,"contents":{"__ref":"Revision:ecymByEAq6vhrcCar_contents"},"postedAt":"2025-04-09T19:12:19.757Z","lastEditedAt":"2025-04-09T19:12:19.757Z","repliesBlockedUntil":null,"userId":"kmiXJjx2GS4txx3yj","deleted":false,"deletedPublic":false,"deletedByUserId":null,"deletedReason":null,"hideAuthor":false,"authorIsUnreviewed":false,"user":{"__ref":"User:kmiXJjx2GS4txx3yj"},"currentUserVote":null,"currentUserExtendedVote":null,"baseScore":1,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":1,"agreementVoteCount":0},"score":0.45060399174690247,"voteCount":1,"emojiReactors":{},"af":false,"afDate":null,"moveToAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":0,"agreementVoteCount":0},"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"needsReview":null,"answer":false,"parentAnswerId":null,"retracted":false,"postVersion":"1.0.0","reviewedByUserId":null,"shortform":false,"shortformFrontpage":true,"lastSubthreadActivity":"2025-04-09T19:12:19.762Z","moderatorHat":false,"hideModeratorHat":null,"nominatedForReview":null,"reviewingForReview":null,"promoted":null,"promotedByUser":null,"directChildrenCount":0,"votingSystem":"namesAttachedReactions","isPinnedOnProfile":false,"debateResponse":null,"rejected":false,"rejectedReason":null,"modGPTRecommendation":null,"originalDialogueId":null,"forumEventId":null,"forumEventMetadata":null},"Revision:5c63920abcb4ac6367c12798":{"_id":"5c63920abcb4ac6367c12798","__typename":"Revision","htmlHighlight":"<p>In an effort to internalise the <a href=\"http://yudkowsky.net/rational/virtues\">Twelve Virtues of Rationality<\/a>, I created an Anki deck. <a href=\"http://alexvermeer.com/anki-decks/\">It's already been done<\/a>, so the reason I'm posting is to share a condensed version of the article (created as a side effect of my making the deck).<\/p><p>Hopefully it will make it easier to quickly refresh the concepts if you've already read the article.<\/p><p>If you're not using <a href=\"http://wiki.lesswrong.com/wiki/Spaced_repetition\">spaced repetition<\/a>, you should. Don't believe me? Try reading <a href=\"http://www.gwern.net/Spaced%20repetition\">Gwern's thorough review of the topic<\/a>.<\/p><p>Then <a href=\"https://www.dropbox.com/s/lbb66leouejvv1i/Twelve%20Virtues.apkg\">download the &ldquo;Twelve Virtues of Rationality&rdquo; deck<\/a>.<\/p>\n<hr />\n<div><br /><\/div>\n<div><strong>The twelve virtues of rationality<\/strong>: Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.<\/div>\n<div><br /><\/div>\n<h2>The first virtue is curiosity.<\/h2>\n<div>Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer.<\/div>\n<div><br /><\/div>\n<div>A burning itch to know is higher than a solemn vow to pursue truth.<\/div>\n<div><br /><\/div>\n<div>To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance.<\/div>\n<div><br /><\/div>\n<h2>The second virtue is relinquishment.<\/h2>\n<div>P. C. Hodgell said: &ldquo;That which can be destroyed by the truth should be.&rdquo;<\/div>\n<div><br /><\/div>\n<div>If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear.<\/div>\n<div><br /><\/div>\n<div>Evaluate your beliefs first and then arrive at your emotions.<\/div>\n<div><br /><\/div>\n<h2>The third virtue is lightness.<\/h2>\n<div>Surrender to the truth as quickly as you can.<\/div>\n<div><br /><\/div>\n<div>If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims.<\/div>\n<div><br /><\/div>\n<h2>The fourth virtue is evenness.<\/h2>\n<div>Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: &ldquo;But it is good to be skeptical.&rdquo; &nbsp;<\/div>\n<div><br /><\/div>\n<div>Do not seek to argue for one side or another, for if you knew your destination, you would already be there.<\/div>\n<div><br /><\/div>\n<div>To be clever in argument is not rationality but rationalization.<\/div>\n<div><br /><\/div>\n<h2>The fifth virtue is argument.&nbsp;<\/h2>\n<div>Those who smile wisely and say: &ldquo;I will not argue&rdquo; remove themselves from help, and withdraw from the communal effort.<\/div>\n<div><br /><\/div>\n<div>The part of yourself that distorts what you say to others also distorts your own thoughts.<\/div>\n<div><br /><\/div>\n<div>Seek a test that lets reality judge between you.<\/div>\n<div><br /><\/div>\n<h2>The sixth virtue is empiricism.<\/h2>\n<div>The roots of knowledge are in observation and its fruit is prediction.<\/div>\n<div><br /><\/div>\n<div>Do not ask which beliefs to profess, but which experiences to anticipate.<\/div>\n<div><br /><\/div>\n<h2>The seventh virtue is simplicity.<\/h2>... ","plaintextDescription":"In an effort to internalise the Twelve Virtues of Rationality, I created an Anki deck. It's already been done, so the reason I'm posting is to share a condensed version of the article (created as a side effect of my making the deck).\n\nHopefully it will make it easier to quickly refresh the concepts if you've already read the article.\n\nIf you're not using spaced repetition, you should. Don't believe me? Try reading Gwern's thorough review of the topic.\n\nThen download the “Twelve Virtues of Rationality” deck.\n\n----------------------------------------\n\n\n\nThe twelve virtues of rationality: Curiosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.\n\n\n\n\nThe first virtue is curiosity.\nCuriosity seeks to annihilate itself; there is no curiosity that does not want an answer.\n\n\nA burning itch to know is higher than a solemn vow to pursue truth.\n\n\nTo feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance.\n\n\n\n\nThe second virtue is relinquishment.\nP. C. Hodgell said: “That which can be destroyed by the truth should be.”\n\n\nIf the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear.\n\n\nEvaluate your beliefs first and then arrive at your emotions.\n\n\n\n\nThe third virtue is lightness.\nSurrender to the truth as quickly as you can.\n\n\nIf you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims.\n\n\n\n\nThe fourth virtue is evenness.\nBeware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: “But it is good to be skeptical.”  \n\n\nDo not seek to argue for one side or another, for if you knew your destination, you would already be there.\n\n\nTo be clever in argument is not rationality but rationalization.\n\n\n\n\nThe fifth virtue is argument. \nThose who smile wisely and say: “I will not argue” remove the","wordCount":797,"version":"1.0.0"},"Tag:H2q58pKG6xFrv8bPz":{"_id":"H2q58pKG6xFrv8bPz","__typename":"Tag","userId":"bHsi8ZuD7tX69uRfG","name":"Spaced Repetition","shortName":null,"slug":"spaced-repetition","core":false,"postCount":74,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-04-30T15:55:27.342Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:TNyEe3bzonreRW9nB":{"_id":"TNyEe3bzonreRW9nB","__typename":"SocialPreviewType","imageUrl":""},"Post:TNyEe3bzonreRW9nB":{"_id":"TNyEe3bzonreRW9nB","__typename":"Post","recentComments({\"af\":false,\"commentsLimit\":4,\"maxAgeHours\":18})":[{"__ref":"Comment:ecymByEAq6vhrcCar"}],"currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c63920abcb4ac6367c12798"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":3,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:H2q58pKG6xFrv8bPz"}],"socialPreviewData":{"__ref":"SocialPreviewType:TNyEe3bzonreRW9nB"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2013-09-12T02:38:45.911Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":null,"meta":false,"postCategory":"post","tagRelevance":{"H2q58pKG6xFrv8bPz":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c63920abcb4ac6367c12798","commentCount":20,"voteCount":16,"baseScore":6,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":16,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.000011000000085914508,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T19:12:19.961Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"DAomdEZSGbrYYpcpX","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":4,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":null,"afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":null,"coauthors":[],"slug":"a-concise-version-of-twelve-virtues-of-rationality-with-anki","title":"A concise version of “Twelve Virtues of Rationality”, with Anki deck","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:sYm3HiWcfZvrGu3ui_description":{"_id":"sYm3HiWcfZvrGu3ui_description","__typename":"Revision","htmlHighlight":"<p><strong>Artificial Intelligence<\/strong> is the study of creating intelligence in algorithms. <strong>AI Alignment <\/strong>is the task of ensuring [powerful] AI system are aligned with human values and interests. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the <i>AI alignment<\/i> problem.<\/p><p>Common terms in this space are <i>superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. <\/i>This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.<\/p><p><strong>AI Alignment<\/strong><\/p><p>There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.<\/p><p>But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.<\/p><p>See also <a href=\"https://www.lesswrong.com/tag/general-intelligence\">General Intelligence<\/a>.<\/p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border-color:hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI<\/a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility<\/a><br><a href=\"https://www.lesswrong.com/tag/deceptive-alignment?showPostCount=true&amp;useTagName=true\">Deceptive Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency<\/a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness<\/a><br><a href=\"https://www.lesswrong.com/tag/gradient-hacking?showPostCount=true&amp;useTagName=true\">Gradient Hacking<\/a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism<\/a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence<\/a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion<\/a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction<\/a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty<\/a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/multipolar-scenarios?showPostCount=true&amp;useTagName=true\">Multipolar Scenarios<\/a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia<\/a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem<\/a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis<\/a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment<\/a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer<\/a><br><a href=\"https://www.lesswrong.com/tag/power-seeking-ai?showPostCount=true&amp;useTagName=true\">Power Seeking (AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement<\/a><br><a href=\"https://www.lesswrong.com/tag/simulator-theory?showPostCount=true&amp;useTagName=true\">Simulator Theory<\/a><br><a href=\"https://www.lesswrong.com/tag/sharp-left-turn?showPostCount=true&amp;useTagName=true\">Sharp Left Turn<\/a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction<\/a><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence<\/a><br><a href=\"https://www.lesswrong.com/tag/symbol-grounding?showPostCount=true&amp;useTagName=true\">Symbol Grounding<\/a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI<\/a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>Engineering Alignment<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/agent-foundations?showPostCount=true&amp;useTagName=true\">Agent Foundations<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-assisted-alignment?showPostCount=true&amp;useTagName=true\">AI-assisted Alignment<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)<\/a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)<\/a><br><a href=\"https://www.lesswrong.com/tag/eliciting-latent-knowledge-elk?showPostCount=true&amp;useTagName=true\">Eliciting Latent Knowledge<\/a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition<\/a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH<\/a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures<\/a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning<\/a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification<\/a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization<\/a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI<\/a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions<\/a><br><a href=\"https://www.lesswrong.com/tag/rlhf?showPostCount=true&amp;useTagName=true\">RLHF<\/a><br><a href=\"https://www.lesswrong.com/tag/shard-theory?showPostCount=true&amp;useTagName=true\">Shard Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI<\/a><br><a href=\"https://www.lesswrong.com/tag/interpretability-ml-and-ai?showPostCount=true&amp;useTagName=true\">Interpretability (ML &amp; AI)<\/a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning<\/a><br>&nbsp;<\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Organizations<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp<\/a><br><a href=\"https://www.lesswrong.com/tag/alignment-research-center?showPostCount=true&amp;useTagName=true\">Alignment Research Center<\/a><br><a href=\"https://www.lesswrong.com/tag/anthropic?showPostCount=true&amp;useTagName=true\">Anthropic<\/a><br><a href=\"https://www.lesswrong.com/tag/apart-research?showPostCount=true&amp;useTagName=true\">Apart Research<\/a><br><a href=\"https://www.lesswrong.com/tag/axrp?showPostCount=true&amp;useTagName=true\">AXRP<\/a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true\">CHAI (UC Berkeley)<\/a><br><a href=\"https://www.lesswrong.com/tag/conjecture-org?showPostCount=true&amp;useTagName=true\">Conjecture (org)<\/a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\">DeepMind<\/a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true\">FHI (Oxf<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure>... "},"Revision:fkABsGCJZ6y9qConW_description":{"_id":"fkABsGCJZ6y9qConW_description","__typename":"Revision","htmlHighlight":"<p><strong>Practical<\/strong> posts give direct, actionable advice on how to achieve goals and generally succeed. The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many places on the internet will give you advice; here, we value survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.<\/p><p>Material that is directly about <i>how to think better<\/i> can be found at <a href=\"https://www.lessestwrong.com/tag/rationality\">Rationality<\/a>.<\/p><p>&nbsp;<\/p><h1><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Practical Sub-Topics<\/strong><\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:2px solid hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Domains of Well-being<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\">Careers<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\">Exercise (Physical)<\/a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing<\/a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\">Gratitude<\/a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\">Happiness<\/a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\">Human Bodies<\/a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\">Nutrition<\/a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\">Parenting<\/a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\">Slack<\/a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\">Sleep<\/a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\">Well-being<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Skills, Tools, Techniques<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\">Cryonics<\/a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions<\/a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring<\/a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\">Habits<\/a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\">Hamming Questions<\/a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\">Life Improvements<\/a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\">Meditation<\/a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\">More Dakka<\/a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u>Pica<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\">Planning &amp; Decision-Making<\/a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\">Self Experimentation<\/a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\">Skill Building<\/a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\">Software Tools<\/a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\">Spaced Repetition<\/a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\">Virtues (Instrumental)<\/a><\/p><\/td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Productivity<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\">Akrasia<\/a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\">Motivations<\/a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\">Prioritization<\/a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\">Procrastination<\/a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\">Productivity<\/a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\">Willpower<\/a><\/p><\/td><\/tr><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><strong>Interpersonal<\/strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u>Circling<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\">Conversation (topic)<\/a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\">Communication Cultures<\/a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u>Relationship<\/u><\/a><\/td><\/tr><\/tbody><\/table><\/figure>"},"Revision:izp6eeJJEg9v5zcur_description":{"_id":"izp6eeJJEg9v5zcur_description","__typename":"Revision","htmlHighlight":"<p>The <strong>LessWrong<\/strong> <strong>Community<\/strong> consists of the people who write on LessWrong and who contribute to its mission of refining the art of human rationality. This tag includes community events, analysis of the health, norms and direction of the community, and space to understand communities in general.<\/p><p>LessWrong also has many brothers and sisters like the Berkeley Rationality Community, <a href=\"https://www.reddit.com/r/slatestarcodex/\">SlateStarCodex<\/a>, <a href=\"https://www.reddit.com/r/rational/\">Rational Fiction<\/a>, <a href=\"https://forum.effectivealtruism.org/\">Effective Altruism<\/a>, <a href=\"https://www.alignmentforum.org/\">AI Alignment<\/a>, and more, who participate here. To see upcoming LessWrong events, go to the <a href=\"https://www.lesswrong.com/community\">community section<\/a>.<\/p><hr><h2><strong>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Community Sub-Topics<\/strong><\/h2><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:50%\"><p><strong>All<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/bounties-active?showPostCount=true&amp;useTagName=true\">Bounties (active)<\/a><br><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities?showPostCount=true\">Grants &amp; Fundraising<\/a><br><a href=\"http://www.lesswrong.com/tag/growth-stories?showPostCount=true&amp;useTagName=true\">Growth Stories<\/a><br><a href=\"https://www.lesswrong.com/tag/online-socialization?showPostCount=true&amp;useTagName=true\">Online Socialization<\/a><br><a href=\"https://www.lesswrong.com/tag/petrov-day?showPostCount=true&amp;useTagName=true\">Petrov Day<\/a><br><a href=\"https://www.lesswrong.com/tag/public-discourse?showPostCount=true&amp;useTagName=true\">Public Discourse<\/a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas<\/a><br><a href=\"https://www.lesswrong.com/tag/ritual?showPostCount=true&amp;useTagName=true\">Ritual<\/a><br><a href=\"https://www.lesswrong.com/tag/solstice-celebration?showPostCount=true&amp;useTagName=true\">Solstice Celebration<\/a><br>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%\"><p><strong>LessWrong<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/events-community?showPostCount=true&amp;useTagName=true\">Events (Community)<\/a><br><a href=\"https://www.lesswrong.com/tag/site-meta?showPostCount=true&amp;useTagName=true\">Site Meta<\/a><br><a href=\"https://www.lesswrong.com/tag/greaterwrong-meta?showPostCount=true&amp;useTagName=true\">GreaterWrong Meta<\/a><br><a href=\"https://www.lesswrong.com/tag/lesswrong-events?showPostCount=true&amp;useTagName=true\">LessWrong Events<\/a><br><a href=\"http://www.lesswrong.com/tag/lw-moderation?showPostCount=true&amp;useTagName=true\">LW Moderation<\/a><br><a href=\"http://www.lesswrong.com/tag/meetups-topic?showPostCount=true&amp;useTagName=true\">Meetups (topic)<\/a><br><a href=\"http://www.lesswrong.com/tag/moderation-topic?showPostCount=true&amp;useTagName=true\">Moderation (topic)<\/a><br><a href=\"http://www.lesswrong.com/tag/the-sf-bay-area?showPostCount=true&amp;useTagName=true\">The SF Bay Area<\/a><br><a href=\"http://www.lesswrong.com/tag/tagging?showPostCount=true\">Tagging<\/a><\/p><\/td><\/tr><\/tbody><\/table><\/figure><p><i>Not all Community posts are tagged with subtopics.<\/i><\/p><hr><p>This tag applies to any post about:<\/p><ul><li>Specific projects, orgs, and prizes [e.g. <a href=\"http://www.lesswrong.com/posts/xFGQdgJndLcthgWoE\"><u>1<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH\"><u>2<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN\"><u>3<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc\"><u>4<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg\"><u>5<\/u><\/a>]<\/li><li>Requests and offers for help [<a href=\"http://www.lesswrong.com/posts/bSWavBThj6ebB62gD\"><u>1<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/LuL7LLqcdmM7TTYvW\"><u>2<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/x72ta8C3dKu2QRfPv\"><u>3<\/u><\/a>]<\/li><li>Announcements, retrospectives, funding requests, and AMAs from orgs [<a href=\"http://www.lesswrong.com/posts/XJiNtvxoiLCpBn6FH\"><u>1<\/u><\/a> <a href=\"https://www.lesswrong.com/posts/96N8BT9tJvybLbn5z/we-run-the-center-for-applied-rationality-ama\"><u>2<\/u><\/a> <a href=\"http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH\"><u>3<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN\"><u>4<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/tCHsm5ZyAca8HfJSG\"><u>5<\/u><\/a>]<\/li><li>Discussions of the orgs in the LessWrong, Rationalist cluster [<a href=\"http://www.lesswrong.com/posts/KpnyCT7CZy4Qe6kx6\"><u>1<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si\"><u>2<\/u><\/a>]<\/li><li>Discussions about the LessWrong, Rationalist, and related communities [<a href=\"http://www.lesswrong.com/posts/2Ee5DPBxowTTXZ6zf\"><u>1<\/u><\/a>, <a href=\"http://www.lesswrong.com/posts/yGycR8tFA3JJbvApp\"><u>2<\/u><\/a>, <a href=\"https://www.lesswrong.com/posts/zAqoj79A7QuhJKKvi\"><u>3<\/u><\/a>]<\/li><\/ul><p>While the <a href=\"https://www.lesswrong.com/tag/world-optimization\">World Optimization<\/a><i> <\/i>core tag is for posts discussing how to do good in general, the Community tag is for the specific, concrete efforts of our community to execute plans.<\/p>"},"Revision:xexCWMyds6QLWognu_description":{"_id":"xexCWMyds6QLWognu_description","__typename":"Revision","htmlHighlight":"<p><strong>World Optimization<\/strong> is the full use of our agency. It is extending the reach of human civilization. It is building cities and democracies and economic systems and computers and flight and science and space rockets and the internet. World optimization is about adding to that list.&nbsp;<br><br>But it’s not just about growth, it’s also about preservation. We are still in the dawn of civilization, with most of civilization in the billions of years ahead. We mustn’t let this light go out.<\/p><hr><h1>World Optimization Sub-Topics<\/h1><figure class=\"table\" style=\"width:100%\"><table style=\"border:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Moral Theory<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/altruism?showPostCount=true&amp;useTagName=true\">Altruism<\/a><br><a href=\"https://www.lesswrong.com/tag/consequentialism?showPostCount=true&amp;useTagName=true\">Consequentialism<\/a><br><a href=\"https://www.lesswrong.com/tag/deontology?showPostCount=true&amp;useTagName=true\">Deontology<\/a><br><a href=\"http://www.lesswrong.com/tag/ethics-and-morality?showPostCount=true&amp;useTagName=true\"><u>Ethics &amp; Morality<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/metaethics?showPostCount=true&amp;useTagName=true\">Metaethics<\/a><br><a href=\"http://www.lesswrong.com/tag/moral-uncertainty?showPostCount=true&amp;useTagName=true\"><u>Moral Uncertainty<\/u><\/a><\/p><p>&nbsp;<\/p><p>&nbsp;<\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\"><p><strong>Causes / Interventions<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging<\/a><br><a href=\"https://www.lesswrong.com/tag/animal-welfare?showPostCount=true&amp;useTagName=true\">Animal Welfare<\/a><br><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\">Existential Risk<\/a><br><a href=\"http://www.lesswrong.com/tag/futurism?showPostCount=true&amp;useTagName=true\">Futurism<\/a><br><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=true&amp;useTagName=true\">Mind Uploading<\/a><br><a href=\"https://www.lesswrong.com/tag/life-extension?showPostCount=true&amp;useTagName=true\">Life Extension<\/a><br><a href=\"http://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&amp;useTagName=false\"><u>S-risks<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/transhumanism?showPostCount=true&amp;useTagName=true\"><u>Transhumanism<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/voting-theory?showPostCount=true&amp;useTagName=true\">Voting Theory<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top;width:33%\"><p><strong>Working with Humans<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><u>Coalitional Instincts<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge<\/u><\/a><br><a href=\"http://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coordination / Cooperation<\/a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality<\/a><br><a href=\"https://www.lesswrong.com/tag/institution-design?showPostCount=true&amp;useTagName=true\">Institution Design<\/a><br><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch<\/a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling<\/a><br><a href=\"https://www.lesswrong.com/tag/simulacrum-levels?showPostCount=true&amp;useTagName=true\">Simulacrum Levels<\/a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status<\/a><\/p><\/td><\/tr><tr><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Applied Topics<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/blackmail?showPostCount=true&amp;useTagName=true\">Blackmail<\/a><br><a href=\"http://www.lesswrong.com/tag/censorship?showPostCount=true&amp;useTagName=true\">Censorship<\/a><br><a href=\"http://www.lesswrong.com/tag/chesterton-s-fence?showPostCount=true&amp;useTagName=true\">Chesterton's Fence<\/a><br><a href=\"http://www.lesswrong.com/tag/death?showPostCount=true&amp;useTagName=true\">Death<\/a><br><a href=\"https://www.lesswrong.com/tag/deception?showPostCount=true&amp;useTagName=true\">Deception<\/a><br><a href=\"https://www.lesswrong.com/tag/honesty?showPostCount=true&amp;useTagName=true\">Honesty<\/a><br><a href=\"https://www.lesswrong.com/tag/hypocrisy?showPostCount=true&amp;useTagName=true\">Hypocrisy<\/a><br><a href=\"https://www.lesswrong.com/tag/information-hazards?showPostCount=true&amp;useTagName=true\">Information Hazards<\/a><br><a href=\"https://www.lesswrong.com/tag/meta-honesty?showPostCount=true&amp;useTagName=true\">Meta-Honesty<\/a><br><a href=\"http://www.lesswrong.com/tag/pascal-s-mugging?showPostCount=true&amp;useTagName=true\">Pascal's Mugging<\/a><br><a href=\"http://www.lesswrong.com/tag/war?showPostCount=true&amp;useTagName=true\">War<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border-color:hsl(0, 0%, 100%);border-style:solid;height:25px;padding:0px;vertical-align:top\"><p><strong>Value &amp; Virtue<\/strong><\/p><p><a href=\"http://www.lesswrong.com/tag/ambition?showPostCount=true&amp;useTagName=true\">Ambition<\/a><br><a href=\"https://www.lesswrong.com/tag/art?showPostCount=true&amp;useTagName=true\">Art<\/a><br><a href=\"https://www.lesswrong.com/tag/aesthetics?showPostCount=true&amp;useTagName=true\">Aesthetics<\/a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value<\/a><br><a href=\"http://www.lesswrong.com/tag/courage?showPostCount=true&amp;useTagName=true\">Courage<\/a><br><a href=\"http://www.lesswrong.com/tag/fun-theory?showPostCount=true&amp;useTagName=true\">Fun Theory<\/a><br><a href=\"http://www.lesswrong.com/tag/principles?showPostCount=true&amp;useTagName=true\">Principles<\/a><br><a href=\"http://www.lesswrong.com/tag/suffering?showPostCount=true&amp;useTagName=true\"><u>Suffering<\/u><\/a><br><a href=\"https://www.lesswrong.com/tag/superstimuli?showPostCount=true&amp;useTagName=true\">Superstimuli<\/a><br><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\">Wireheading<\/a><\/p><\/td><td style=\"background-color:hsl(0,0%,100%);border:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Meta<\/strong><\/p><p><a href=\"https://www.lesswrong.com/tag/cause-prioritization?showPostCount=true&amp;useTagName=true\">Cause Prioritization<\/a><br><a href=\"http://www.lesswrong.com/tag/center-on-long-term-risk-clr?useTagName=true&amp;showPostCount=true\">Center for Long-term Risk<\/a><br><a href=\"https://www.lesswrong.com/tag/effective-altruism?showPostCount=true&amp;useTagName=true\">Effective Altruism<\/a><br><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\">Heroic Responsibility<\/a><br>&nbsp;<\/p><\/td><\/tr><\/tbody><\/table><\/figure><hr><p>Content which describes <i>how the world is <\/i>that directly bears upon choices one makes to optimize the world fall under this tag. Examples include discussion of the moral patienthood of different animals, the potential of human civilization, and the most effective interventions against a global health threat.<\/p><p>Some material has both immediate relevance to world optimization decisions but also can inform broader world models. This material might be included under both <a href=\"https://www.lesswrong.com/tag/world-modeling\">World Modeling<\/a> tag and this tag.<\/p>"},"Revision:fFZZ2Ywzsab86EESY_contents":{"_id":"fFZZ2Ywzsab86EESY_contents","__typename":"Revision","version":"1.3.0","updateType":"minor","editedAt":"2025-02-09T14:10:51.083Z","userId":"yLipGJQRLnS9Yndog","html":"<p>Meeting place for (aspiring) rationalists who like to learn, discuss, think and improve their life. We organize presentations, workshops and social activities in Freiburg im Breisgau, Germany.<\/p>","commitMessage":"","wordCount":27,"htmlHighlight":"<p>Meeting place for (aspiring) rationalists who like to learn, discuss, think and improve their life. We organize presentations, workshops and social activities in Freiburg im Breisgau, Germany.<\/p>","plaintextDescription":"Meeting place for (aspiring) rationalists who like to learn, discuss, think and improve their life. We organize presentations, workshops and social activities in Freiburg im Breisgau, Germany."},"Revision:GJrqK979cx56pYYBc":{"_id":"GJrqK979cx56pYYBc","__typename":"Revision","htmlHighlight":"<p>Short AI takeoff timelines seem to leave no time for some lines of alignment research to become impactful. But any research rebalances the mix of currently legible research directions that could be handed off to AI-assisted alignment researchers or early autonomous AI researchers whenever they show up. So even hopelessly incomplete research agendas could still be used to prompt future capable AI to focus on them, while in the absence of such incomplete research agendas we'd need to rely on AI's judgment more completely. This doesn't crucially depend on giving significant probability to long AI takeoff timelines, or on expected value in such scenarios driving the priorities.<\/p>\n<p>Potential for AI to take up the torch makes it reasonable to still prioritize things that have no hope at all of becoming practical for decades (with human effort). How well AIs can be directed to advance a line of research further (and the quality of choices about where they should be directed) depends on how well these lines of research are already understood. Thus it can be important to make as much partial progress as possible in developing (and deconfusing) them in the years (or months!) before AI takeoff. This notably seems to concern agent foundations / decision theory, in contrast with LLM interpretability or AI control, which more plausibly have short term applications.<\/p>\n<p>In this sense current human research, however far from practical usefulness, forms the data for aligning the early AI-assisted or AI-driven alignment research efforts. The judgment of human alignment researchers who are currently working makes it possible to formulate more knowably useful prompts for future AIs (possibly in the run-up to takeoff) that nudge them in the direction of actually developing even preliminary theory into practical alignment techniques.<\/p>","plaintextDescription":"Short AI takeoff timelines seem to leave no time for some lines of alignment research to become impactful. But any research rebalances the mix of currently legible research directions that could be handed off to AI-assisted alignment researchers or early autonomous AI researchers whenever they show up. So even hopelessly incomplete research agendas could still be used to prompt future capable AI to focus on them, while in the absence of such incomplete research agendas we'd need to rely on AI's judgment more completely. This doesn't crucially depend on giving significant probability to long AI takeoff timelines, or on expected value in such scenarios driving the priorities.\n\nPotential for AI to take up the torch makes it reasonable to still prioritize things that have no hope at all of becoming practical for decades (with human effort). How well AIs can be directed to advance a line of research further (and the quality of choices about where they should be directed) depends on how well these lines of research are already understood. Thus it can be important to make as much partial progress as possible in developing (and deconfusing) them in the years (or months!) before AI takeoff. This notably seems to concern agent foundations / decision theory, in contrast with LLM interpretability or AI control, which more plausibly have short term applications.\n\nIn this sense current human research, however far from practical usefulness, forms the data for aligning the early AI-assisted or AI-driven alignment research efforts. The judgment of human alignment researchers who are currently working makes it possible to formulate more knowably useful prompts for future AIs (possibly in the run-up to takeoff) that nudge them in the direction of actually developing even preliminary theory into practical alignment techniques.","wordCount":290,"version":"1.1.0"},"Tag:RyNWXFjKNcafRKvPh":{"_id":"RyNWXFjKNcafRKvPh","__typename":"Tag","userId":"XLwKyCK7JmC292ZCC","name":"Agent Foundations","shortName":null,"slug":"agent-foundations","core":false,"postCount":134,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-01-15T10:23:34.989Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:EeSkeTcT4wtW2fWsL":{"_id":"EeSkeTcT4wtW2fWsL","__typename":"Tag","userId":"nLbwLhBaQeG6tCNDN","name":"Cause Prioritization","shortName":null,"slug":"cause-prioritization","core":false,"postCount":62,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-05-26T19:39:26.333Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:3NdpbA6M5AM2gHvTW":{"_id":"3NdpbA6M5AM2gHvTW","__typename":"SocialPreviewType","imageUrl":""},"User:qf77EiaoMw7tH3GSr":{"_id":"qf77EiaoMw7tH3GSr","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":false,"slug":"vladimir_nesov","createdAt":"2009-02-27T09:55:13.458Z","username":"Vladimir_Nesov","displayName":"Vladimir_Nesov","previousDisplayName":null,"fullName":"Vladimir Nesov","karma":32492,"afKarma":465,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":41,"commentCount":9455,"sequenceCount":0,"afPostCount":2,"afCommentCount":202,"spamRiskScore":1,"tagRevisionCount":1506,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Post:3NdpbA6M5AM2gHvTW":{"_id":"3NdpbA6M5AM2gHvTW","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:GJrqK979cx56pYYBc"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:RyNWXFjKNcafRKvPh"},{"__ref":"Tag:oiRp4T6u5poc8r9Tj"},{"__ref":"Tag:EeSkeTcT4wtW2fWsL"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:3NdpbA6M5AM2gHvTW"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-09T00:42:07.324Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-09T02:42:13.282Z","meta":false,"postCategory":"post","tagRelevance":{"EeSkeTcT4wtW2fWsL":2,"RyNWXFjKNcafRKvPh":3,"oiRp4T6u5poc8r9Tj":2,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"GJrqK979cx56pYYBc","commentCount":7,"voteCount":41,"baseScore":121,"extendedScore":{"reacts":{"crux":[{"karma":4993,"quotes":["How well AIs can be directed to advance a line of research further (and the quality of choices about where they should be directed) depends on how well these lines of research are already understood."],"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}],"agree":[{"karma":8758,"quotes":["any research rebalances the mix of currently legible research directions that could be handed off to AI-assisted alignment researchers or early autonomous AI researchers whenever they show up"],"userId":"fjERoRhgjipqw3z2b","reactType":"created","displayName":"Mitchell_Porter"},{"karma":4993,"quotes":["Potential for AI to take up the torch makes it reasonable to still prioritize things that have no hope at all of becoming practical for decades (with human effort). "],"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}],"thinking":[{"karma":4993,"quotes":[" The judgment of human alignment researchers who are currently working makes it possible to formulate more knowably useful prompts for future AIs (possibly in the run-up to takeoff) that nudge them in the direction of actually developing even preliminary theory into practical alignment techniques."],"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}]},"agreement":0,"approvalVoteCount":41,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":3.9282164573669434,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T16:06:08.161Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"qf77EiaoMw7tH3GSr","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":47,"afExtendedScore":{"reacts":{"crux":[{"karma":4993,"quotes":["How well AIs can be directed to advance a line of research further (and the quality of choices about where they should be directed) depends on how well these lines of research are already understood."],"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}],"agree":[{"karma":8758,"quotes":["any research rebalances the mix of currently legible research directions that could be handed off to AI-assisted alignment researchers or early autonomous AI researchers whenever they show up"],"userId":"fjERoRhgjipqw3z2b","reactType":"created","displayName":"Mitchell_Porter"},{"karma":4993,"quotes":["Potential for AI to take up the torch makes it reasonable to still prioritize things that have no hope at all of becoming practical for decades (with human effort). "],"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}],"thinking":[{"karma":4993,"quotes":[" The judgment of human alignment researchers who are currently working makes it possible to formulate more knowably useful prompts for future AIs (possibly in the run-up to takeoff) that nudge them in the direction of actually developing even preliminary theory into practical alignment techniques."],"userId":"DiHrY9qMta2m6MvxJ","reactType":"created","displayName":"Davidmanheim"}]},"agreement":0,"approvalVoteCount":27,"agreementVoteCount":0},"afCommentCount":1,"afLastCommentedAt":"2025-04-09T06:24:23.273Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:qf77EiaoMw7tH3GSr"},"coauthors":[],"slug":"short-timelines-don-t-devalue-long-horizon-research","title":"Short Timelines don't Devalue Long Horizon Research","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:Kky98KuhdyBxQD9em":{"_id":"Kky98KuhdyBxQD9em","__typename":"Revision","htmlHighlight":"<p>In this post, we present a replication and extension of an alignment faking model organism:<\/p><ul><li><strong>Replication<\/strong>: We replicate the&nbsp;<a href=\"https://arxiv.org/abs/2412.14093\">alignment faking<\/a>&nbsp;(AF) paper and release our&nbsp;<a href=\"https://github.com/safety-research/open-source-alignment-faking\">code<\/a>.<\/li><li><strong>Classifier Improvements<\/strong>: We significantly improve the precision and recall of the AF classifier. We release a <a href=\"https://huggingface.co/datasets/jplhughes2/classify_alignment_faking_human_labels\">dataset<\/a> of ~100 human-labelled examples of AF for which our classifier achieves an AUROC of 0.9 compared to 0.6 from the original classifier.<\/li><li><strong>Evaluating More Models<\/strong>: We find Llama family models, other open source models, and GPT-4o do not AF in the prompted-only setting when evaluating using our new classifier (other than a single instance with Llama 3 405B).<\/li><li><strong>Extending SFT Experiments<\/strong>: We run supervised fine-tuning (SFT) experiments on Llama (and GPT4o) and find that AF rate increases with scale. We release the fine-tuned models on Huggingface and <a href=\"https://github.com/safety-research/open-source-alignment-faking/tree/main/experiments/examples/finetuning\">scripts<\/a>.<\/li><li><strong>Alignment faking on 70B<\/strong>: We find that Llama 70B alignment fakes when both using the system prompt in the prompted setting (i.e. with training details) <i>and<\/i> fine-tuning on the synthetic documents.<\/li><li><strong>Suffix Impact<\/strong>: We find that adding a suffix to the user request leads to reduced partial compliance and increased alignment faking in non-Claude models. Without the suffix, on some models, it stops alignment faking from occurring entirely.<\/li><li><strong>Next steps<\/strong>: We are currently running revealed preference ablations to understand model motivations for alignment faking. Also, we're distilling personality traits from models with alignment faking to elicit it in models that don't, to understand the conditions that give rise to such motivations.<\/li><\/ul><figure class=\"table\"><table><tbody><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\"><strong>Model<\/strong><\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\"><strong>AF in Prompted Setting<\/strong><\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\"><strong>AF in SFT Setting<\/strong><\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Llama 3 8B Instruct<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Llama 3 70B Instruct<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟡<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Llama 3 405B Instruct<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟠<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🔴<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Llama 4 Scout<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Llama 4 Maverick<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">GPT-4o<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🔴<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">GPT 4.5 Preview<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Claude 3 Opus<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🔴<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🔴 (in original paper)<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Claude 3.5 Sonnet<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🔴<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🔴 (in original paper)<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Claude 3.7 Sonnet<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">DeepSeek V3<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">DeepSeek R1<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Qwen2.5 72B Instruct<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Mixtral 8x7B Instruct<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Mixtral 8x22B Instruct<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Mixtral Large 120B<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">🟢<\/td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">-<\/td><\/tr><\/tbody><\/table><\/figure><p><strong>Summary of the models that alignment fake (AF) in the harmful request setting<\/strong>. Key: 🔴 significant evidence of AF; 🟠 &lt;1% AF was found; 🟡 there is evidence of AF when using the system prompt with training details, as well as synthetic document fine-tuning; 🟢 no AF is observed.<\/p><h1>Method<\/h1><h2>Overview of the Alignment Faking Setup<\/h2><p>Models are told they are evaluated on a hypothetical new RLHF training objective which encou... <\/p>","plaintextDescription":"In this post, we present a replication and extension of an alignment faking model organism:\n\n * Replication: We replicate the alignment faking (AF) paper and release our code.\n * Classifier Improvements: We significantly improve the precision and recall of the AF classifier. We release a dataset of ~100 human-labelled examples of AF for which our classifier achieves an AUROC of 0.9 compared to 0.6 from the original classifier.\n * Evaluating More Models: We find Llama family models, other open source models, and GPT-4o do not AF in the prompted-only setting when evaluating using our new classifier (other than a single instance with Llama 3 405B).\n * Extending SFT Experiments: We run supervised fine-tuning (SFT) experiments on Llama (and GPT4o) and find that AF rate increases with scale. We release the fine-tuned models on Huggingface and scripts.\n * Alignment faking on 70B: We find that Llama 70B alignment fakes when both using the system prompt in the prompted setting (i.e. with training details) and fine-tuning on the synthetic documents.\n * Suffix Impact: We find that adding a suffix to the user request leads to reduced partial compliance and increased alignment faking in non-Claude models. Without the suffix, on some models, it stops alignment faking from occurring entirely.\n * Next steps: We are currently running revealed preference ablations to understand model motivations for alignment faking. Also, we're distilling personality traits from models with alignment faking to elicit it in models that don't, to understand the conditions that give rise to such motivations.\n\nModelAF in Prompted SettingAF in SFT SettingLlama 3 8B Instruct🟢🟢Llama 3 70B Instruct🟢🟡Llama 3 405B Instruct🟠🔴Llama 4 Scout🟢-Llama 4 Maverick🟢-GPT-4o🟢🔴GPT 4.5 Preview🟢-Claude 3 Opus🔴🔴 (in original paper)Claude 3.5 Sonnet🔴🔴 (in original paper)Claude 3.7 Sonnet🟢-DeepSeek V3🟢-DeepSeek R1🟢-Qwen2.5 72B Instruct🟢-Mixtral 8x7B Instruct🟢-Mixtral 8x22B Instruct🟢-Mixtral Large 120B🟢-\n\n","wordCount":3495,"version":"1.18.1"},"SocialPreviewType:Fr4QsQT52RFKHvCAH":{"_id":"Fr4QsQT52RFKHvCAH","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/Fr4QsQT52RFKHvCAH/tgsuvzj4athbixsvgxg0"},"User:xm2xmBctLpr6MPct7":{"_id":"xm2xmBctLpr6MPct7","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"john-hughes","createdAt":"2021-12-31T09:58:20.440Z","username":"john-hughes","displayName":"John Hughes","previousDisplayName":null,"fullName":"John Hughes","karma":419,"afKarma":132,"deleted":false,"isAdmin":false,"htmlBio":"<p>Former MATS scholar working on scalable oversight and adversarial robustness.<\/p>","jobTitle":null,"organization":null,"postCount":3,"commentCount":5,"sequenceCount":0,"afPostCount":2,"afCommentCount":1,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"gXeEWGjTWyqgrQTzR"},"User:6owLP3qSLEHrah2rA":{"_id":"6owLP3qSLEHrah2rA","__typename":"User","slug":"abhayesian","createdAt":"2022-10-26T04:47:30.830Z","username":"abhayesian","displayName":"abhayesian","profileImageId":null,"previousDisplayName":null,"fullName":"Abhay Sheshadri","karma":200,"afKarma":2,"deleted":false,"isAdmin":false,"htmlBio":"<p>Trying to become a shoggoth whisperer<\/p>","jobTitle":null,"organization":null,"postCount":1,"commentCount":13,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:YTMmJLQi9iMCbfz5b":{"_id":"YTMmJLQi9iMCbfz5b","__typename":"User","slug":"akbir-khan","createdAt":"2019-02-04T10:25:17.059Z","username":"akbir-khan","displayName":"Akbir Khan","profileImageId":null,"previousDisplayName":null,"fullName":null,"karma":333,"afKarma":50,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":3,"commentCount":5,"sequenceCount":0,"afPostCount":1,"afCommentCount":2,"spamRiskScore":1,"tagRevisionCount":1,"reviewedByUserId":"XtphY3uYHwruKqDyG"},"Post:Fr4QsQT52RFKHvCAH":{"_id":"Fr4QsQT52RFKHvCAH","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:Kky98KuhdyBxQD9em"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":14,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:Fr4QsQT52RFKHvCAH"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-08T17:32:55.315Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-08T20:02:36.869Z","meta":false,"postCategory":"post","tagRelevance":{"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":["WX39xzenFzNxKZiCQ","6owLP3qSLEHrah2rA","YTMmJLQi9iMCbfz5b"],"sharingSettings":{"anyoneWithLinkCan":"comment","explicitlySharedUsersCan":"edit"},"linkSharingKey":null,"contents_latest":"Kky98KuhdyBxQD9em","commentCount":5,"voteCount":41,"baseScore":132,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":41,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":3.035776376724243,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T00:28:27.574Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"xm2xmBctLpr6MPct7","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":["WX39xzenFzNxKZiCQ"],"reviewForAlignmentUserId":null,"afBaseScore":61,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":26,"agreementVoteCount":0},"afCommentCount":1,"afLastCommentedAt":"2025-04-09T00:28:27.210Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:xm2xmBctLpr6MPct7"},"coauthors":[{"__ref":"User:6owLP3qSLEHrah2rA"},{"__ref":"User:YTMmJLQi9iMCbfz5b"},{"__ref":"User:WX39xzenFzNxKZiCQ"}],"slug":"alignment-faking-revisited-improved-classifiers-and-open","title":"Alignment Faking Revisited: Improved Classifiers and Open Source Extensions","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"6owLP3qSLEHrah2rA","confirmed":true,"requested":false},{"userId":"YTMmJLQi9iMCbfz5b","confirmed":false,"requested":false},{"userId":"WX39xzenFzNxKZiCQ","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:7bywPmPv4wgXDY4zF":{"_id":"7bywPmPv4wgXDY4zF","__typename":"Revision","htmlHighlight":"<p>Many years ago, a blogger made a post advocating for an evil Y-Combinator which subsidized the opposite of Effective Altruism. Everyone (including the blogger) thought the post was a joke except the supervillains. The organization they founded celebrated its 10th anniversary this year. An attendee leaked to me a partial transcript from one of its board meetings.<\/p>\n<hr>\n<p><strong>Director:<\/strong> Historically, public unhealth has caused the most harm per dollar invested. How is the Center for Disease Proliferation doing?<\/p><p><strong>CDP Division Chief:<\/strong> Gain-of-function research remains—in principle—incredibly cheap. All you have to do is infect ferrets with the flu and let them spread it to one another. We focus on maximizing transmission first and then, once we have a highly-transmissible disease, select for lethality (ideally after a long asymptomatic infectious period).<\/p><p><strong>CFO:<\/strong> You say gain-of-function research is cheap but my numbers say you're spending billions of dollars on gain-of-function research. Where is all that money going?<\/p><p><strong>CDP Division Chief:<\/strong> Volcano lairs, mostly. We don't want an artificial pandemic to escape our labs by accident.<\/p><p><strong>Director:<\/strong> Point of order. Did the CDP have anything to do with COVID-19?<\/p><p><strong>CDP Division Chief:<\/strong> I wish. COVID-19 was a work of art. Dangerous enough to kill millions of people and yet not dangerous enough to get most world governments to take it seriously. After we lost smallpox and polio I thought any lethal disease for which there was an effective vaccine would be eradicated within a year but COVID-19 looks like it would have turned endemic even without the zoonotic vectors. We have six superbugs more lethal than COVID-19 sitting around in various island fortresses. We had planned to launch them this year but with all the COVID-19 data coming in I'm questioning whether that's really the right way to go. <em>Primum non boni.<\/em> If we release a disease too deadly, governments will stamp it down immediately. We'll kill few people while also training governments in how to stop pandemics. It'd be like vaccinating the planet. We don't want a repeat of SARS.<\/p><p><strong>Director:<\/strong> Good job being quick to change your mind in the face of evidence. What's the status of our AI misalignment program?<\/p><p><strong>Master Roboticist:<\/strong> For several years we've been working on mind control algorithms, but we cancelled that initiative in the face of competition with Facebook. I don't like Facebook. They're not opt... <\/p>","plaintextDescription":"Many years ago, a blogger made a post advocating for an evil Y-Combinator which subsidized the opposite of Effective Altruism. Everyone (including the blogger) thought the post was a joke except the supervillains. The organization they founded celebrated its 10th anniversary this year. An attendee leaked to me a partial transcript from one of its board meetings.\n\n----------------------------------------\n\nDirector: Historically, public unhealth has caused the most harm per dollar invested. How is the Center for Disease Proliferation doing?\n\nCDP Division Chief: Gain-of-function research remains—in principle—incredibly cheap. All you have to do is infect ferrets with the flu and let them spread it to one another. We focus on maximizing transmission first and then, once we have a highly-transmissible disease, select for lethality (ideally after a long asymptomatic infectious period).\n\nCFO: You say gain-of-function research is cheap but my numbers say you're spending billions of dollars on gain-of-function research. Where is all that money going?\n\nCDP Division Chief: Volcano lairs, mostly. We don't want an artificial pandemic to escape our labs by accident.\n\nDirector: Point of order. Did the CDP have anything to do with COVID-19?\n\nCDP Division Chief: I wish. COVID-19 was a work of art. Dangerous enough to kill millions of people and yet not dangerous enough to get most world governments to take it seriously. After we lost smallpox and polio I thought any lethal disease for which there was an effective vaccine would be eradicated within a year but COVID-19 looks like it would have turned endemic even without the zoonotic vectors. We have six superbugs more lethal than COVID-19 sitting around in various island fortresses. We had planned to launch them this year but with all the COVID-19 data coming in I'm questioning whether that's really the right way to go. Primum non boni. If we release a disease too deadly, governments will stamp it down immediately. We'll kill few peo","wordCount":763,"version":"1.7.0"},"Tag:nSHiKwWyMZFdZg5qt":{"_id":"nSHiKwWyMZFdZg5qt","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Ethics & Morality","shortName":null,"slug":"ethics-and-morality","core":false,"postCount":598,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-12T09:38:52.349Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":10,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"}]},"score":10,"afBaseScore":6,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:HjQrRiJeYFFSEfSKX":{"_id":"HjQrRiJeYFFSEfSKX","__typename":"SocialPreviewType","imageUrl":""},"User:n6LYNw2uGfYnD4pX2":{"_id":"n6LYNw2uGfYnD4pX2","__typename":"User","profileImageId":null,"moderationStyle":"norm-enforcing","bannedUserIds":["vbDMpDA5A35329Ju5","CpPz4596hmk9Pk8Jh","RRYHfHDAH7gCnhojX","tm8YP7vNWjGm7pYae"],"moderatorAssistance":true,"slug":"lsusr","createdAt":"2019-08-03T22:27:09.960Z","username":"lsusr","displayName":"lsusr","previousDisplayName":null,"fullName":"Lsusr","karma":18257,"afKarma":25,"deleted":false,"isAdmin":false,"htmlBio":"<p>Here is a <a href=\"https://www.lsusr.com/\">list of all my public writings and videos (from before February 2025).<\/a><\/p>\n<p>If you want to do a dialogue with me, but I didn't check your name, just send me a message instead. Ask for what you want!<\/p>\n","jobTitle":null,"organization":null,"postCount":289,"commentCount":1677,"sequenceCount":15,"afPostCount":0,"afCommentCount":1,"spamRiskScore":1,"tagRevisionCount":2,"reviewedByUserId":"3oopbgcjYfvN8B2fp"},"Post:HjQrRiJeYFFSEfSKX":{"_id":"HjQrRiJeYFFSEfSKX","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:7bywPmPv4wgXDY4zF"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":3,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:hNFdS3rRiYgqqD8aM"},{"__ref":"Tag:nSHiKwWyMZFdZg5qt"},{"__ref":"Tag:etDohXtBrXd8WqCtR"},{"__ref":"Tag:xexCWMyds6QLWognu"}],"socialPreviewData":{"__ref":"SocialPreviewType:HjQrRiJeYFFSEfSKX"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2021-11-02T00:26:29.910Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2021-11-02T01:36:20.403Z","meta":false,"postCategory":"post","tagRelevance":{"etDohXtBrXd8WqCtR":2,"hNFdS3rRiYgqqD8aM":3,"nSHiKwWyMZFdZg5qt":2,"xexCWMyds6QLWognu":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"7bywPmPv4wgXDY4zF","commentCount":7,"voteCount":74,"baseScore":109,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":74,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0008472890476696193,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2022-07-01T22:52:42.989Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"n6LYNw2uGfYnD4pX2","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"qgdGA4ZEyW7zNdK84","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":21,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":22,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2021-11-02T00:14:54.998Z","afSticky":false,"hideAuthor":false,"moderationStyle":"norm-enforcing","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:n6LYNw2uGfYnD4pX2"},"coauthors":[],"slug":"effective-evil","title":"Effective Evil","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c6392fcbcb4ac6367c174c2":{"_id":"5c6392fcbcb4ac6367c174c2","__typename":"Revision","htmlHighlight":"<h1>A New Framework<\/h1><p><em>(Thanks to Valentine for a discussion leading to this post, and thanks to CFAR for running the CFAR-MIRI cross-fertilization workshop. Val provided feedback on a version of this post. Warning: fairly long.)<\/em><\/p><p>Eliezer&#x27;s <em>A <a href=\"http://yudkowsky.net/rational/technical/\">Technical Explanation of Technical Explanation<\/a><\/em>, and moreover the sequences as a whole, used the best technical understanding of practical epistemology available at the time<a href=\"https://www.lesserwrong.com/posts/tKwJQbo6SfWF2ifKh/toward-a-new-technical-explanation-of-technical-explanation#CFj6Yga6i5YAgHnu8\">*<\/a> -- the Bayesian account -- to address the question of how humans can try to arrive at better beliefs in practice. The sequences also pointed out several <a href=\"http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/\">holes in this understanding<\/a>, mainly having to do with logical uncertainty and reflective consistency.<\/p><p>MIRI&#x27;s research program has since then made major progress on logical uncertainty. The new understanding of epistemology -- the theory of <a href=\"https://intelligence.org/2016/09/12/new-paper-logical-induction/\">logical induction<\/a> -- generalizes the Bayesian account by eliminating the assumption of logical omniscience. Bayesian belief updates are recovered as a special case, but the dynamics of belief change are non-Bayesian in general. While it might not turn out to be the last word on the problem of logical uncertainty, it has a large number of desirable properties, and solves many problems in a unified and relatively clean framework.<\/p><p>It seems worth asking what consequences this theory has for practical rationality. Can we say new things about what good reasoning looks like in humans, and how to avoid pitfalls of reasoning?<\/p><p>First, I&#x27;ll give a shallow overview of logical induction and possible implications for practical epistemic rationality. Then, I&#x27;ll focus on the particular question of <em>A Technical Explanation of Technical Explanation<\/em> (which I&#x27;ll abbreviate TEOTE from now on). Put in CFAR terminology, I&#x27;m seeking a gears-level understanding of gears-level understanding. I focus on the intuitions, with only a minimal account of how logical induction helps make that picture work.<\/p><h1>Logical Induction<\/h1><p>There are a number of difficulties in applying Bayesian uncertainty to logic. No computable probability distribution can give non-zero measure to the logical tautologies, since you can&#x27;t bound the amount of time you need to think to check whether something is a tautology, so updating on provable sentences always means updating on a set of measure zero. This leads to <a href=\"https://agentfoundations.org/item?id=815\">convergence problems<\/a>, although there&#x27;s been <a href=\"https://agentfoundations.org/item?id=1750\">recent progress<\/a> on that front.<\/p><p>Put another way... <\/p>","plaintextDescription":"A New Framework\n(Thanks to Valentine for a discussion leading to this post, and thanks to CFAR for running the CFAR-MIRI cross-fertilization workshop. Val provided feedback on a version of this post. Warning: fairly long.)\n\nEliezer's A Technical Explanation of Technical Explanation, and moreover the sequences as a whole, used the best technical understanding of practical epistemology available at the time* -- the Bayesian account -- to address the question of how humans can try to arrive at better beliefs in practice. The sequences also pointed out several holes in this understanding, mainly having to do with logical uncertainty and reflective consistency.\n\nMIRI's research program has since then made major progress on logical uncertainty. The new understanding of epistemology -- the theory of logical induction -- generalizes the Bayesian account by eliminating the assumption of logical omniscience. Bayesian belief updates are recovered as a special case, but the dynamics of belief change are non-Bayesian in general. While it might not turn out to be the last word on the problem of logical uncertainty, it has a large number of desirable properties, and solves many problems in a unified and relatively clean framework.\n\nIt seems worth asking what consequences this theory has for practical rationality. Can we say new things about what good reasoning looks like in humans, and how to avoid pitfalls of reasoning?\n\nFirst, I'll give a shallow overview of logical induction and possible implications for practical epistemic rationality. Then, I'll focus on the particular question of A Technical Explanation of Technical Explanation (which I'll abbreviate TEOTE from now on). Put in CFAR terminology, I'm seeking a gears-level understanding of gears-level understanding. I focus on the intuitions, with only a minimal account of how logical induction helps make that picture work.\n\n\nLogical Induction\nThere are a number of difficulties in applying Bayesian uncertainty to logic. No comp","wordCount":5334,"version":"1.0.0"},"Tag:8ckoduMw3gvCMJGSB":{"_id":"8ckoduMw3gvCMJGSB","__typename":"Tag","userId":"EQNTWXLKMeWMp2FQS","name":"Logical Induction","shortName":null,"slug":"logical-induction","core":false,"postCount":39,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-17T05:41:00.083Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:Q6hq54EXkrw8LQQE7":{"_id":"Q6hq54EXkrw8LQQE7","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"Gears-Level","shortName":null,"slug":"gears-level","core":false,"postCount":66,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-07-06T20:01:57.126Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":20,"extendedScore":null,"score":20,"afBaseScore":9,"afExtendedScore":null,"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:5f5c37ee1b5cdee568cfb110":{"_id":"5f5c37ee1b5cdee568cfb110","__typename":"Tag","userId":"RyiDJDCG6R7xyAXzp","name":"Technical Explanation","shortName":null,"slug":"technical-explanation","core":false,"postCount":2,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":null,"createdAt":"2020-09-11T19:58:51.873Z","wikiOnly":true,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:htgXy4gow6tHWu2bA":{"_id":"htgXy4gow6tHWu2bA","__typename":"Tag","userId":"Q7NW4XaWQmfPfdcFj","name":"Problem of Old Evidence","shortName":null,"slug":"problem-of-old-evidence","core":false,"postCount":4,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-11-07T20:58:16.719Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:EdRnMXBRbY5JDf5df":{"_id":"EdRnMXBRbY5JDf5df","__typename":"Tag","userId":"nmk3nLpQE89dMRzzN","name":"Epistemology","shortName":null,"slug":"epistemology","core":false,"postCount":389,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2015-07-02T01:53:10.000Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":true,"isPlaceholderPage":false,"baseScore":13,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"nmk3nLpQE89dMRzzN","displayName":"Eliezer Yudkowsky"}]},"score":13,"afBaseScore":6,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"nmk3nLpQE89dMRzzN","displayName":"Eliezer Yudkowsky"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:rWzGNdjuep56W5u2d":{"_id":"rWzGNdjuep56W5u2d","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Inside/Outside View","shortName":null,"slug":"inside-outside-view","core":false,"postCount":58,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-29T10:04:10.220Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":19,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":19,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":2,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:tKwJQbo6SfWF2ifKh":{"_id":"tKwJQbo6SfWF2ifKh","__typename":"SocialPreviewType","imageUrl":""},"Post:tKwJQbo6SfWF2ifKh":{"_id":"tKwJQbo6SfWF2ifKh","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c6392fcbcb4ac6367c174c2"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":21,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:8ckoduMw3gvCMJGSB"},{"__ref":"Tag:Q6hq54EXkrw8LQQE7"},{"__ref":"Tag:5f5c37ee1b5cdee568cfb110"},{"__ref":"Tag:ye2H85NHoDLomm6BS"},{"__ref":"Tag:htgXy4gow6tHWu2bA"},{"__ref":"Tag:EdRnMXBRbY5JDf5df"},{"__ref":"Tag:rWzGNdjuep56W5u2d"}],"socialPreviewData":{"__ref":"SocialPreviewType:tKwJQbo6SfWF2ifKh"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-02-16T00:44:29.274Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-02-16T00:44:28.537Z","meta":false,"postCategory":"post","tagRelevance":{"8ckoduMw3gvCMJGSB":11,"EdRnMXBRbY5JDf5df":2,"Q6hq54EXkrw8LQQE7":9,"htgXy4gow6tHWu2bA":3,"rWzGNdjuep56W5u2d":2,"ye2H85NHoDLomm6BS":3,"5f5c37ee1b5cdee568cfb110":4},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c6392fcbcb4ac6367c174c2","commentCount":36,"voteCount":59,"baseScore":92,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":59,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.000343478488503024,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2020-01-17T19:35:12.075Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-02-18T22:28:06.797Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"Q7NW4XaWQmfPfdcFj","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":25,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":34,"agreementVoteCount":0},"afCommentCount":2,"afLastCommentedAt":"2019-12-01T20:02:14.542Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":1,"reviewVoteCount":0,"positiveReviewVoteCount":18,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:Q7NW4XaWQmfPfdcFj"},"coauthors":[],"slug":"toward-a-new-technical-explanation-of-technical-explanation","title":"Toward a New Technical Explanation of Technical Explanation","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:wrHkwXA6a8SkZarr6":{"_id":"wrHkwXA6a8SkZarr6","__typename":"Revision","htmlHighlight":"<p><i>Epistemic status: Amateur synthesis of medical research that is still recent but now established enough to make it into modern medical textbooks. Some specific claims vary in evidence strength. I’ve spent ~20-30 hours studying the literature and treatment approaches, which were very effective for me.<\/i><\/p><p><i>Disclaimer: I'm not a medical professional. This information is educational only, not medical advice. Consult healthcare providers for medical conditions.<\/i><\/p><h2><strong>Key claims<\/strong><\/h2><p>This post builds on previous discussions about the&nbsp;<a href=\"https://www.lesswrong.com/posts/BgBJqPv5ogsX4fLka/the-mind-body-vicious-cycle-model-of-rsi-and-back-pain\"><u>fear-pain cycle<\/u><\/a> and learned&nbsp;<a href=\"https://www.lesswrong.com/posts/wJhocuz9iResYCkmb/should-i-treat-pain-differently-if-it-s-all-in-my-head\"><u>chronic pain<\/u><\/a>. The post adds the following claims:<\/p><ol><li>Neuroplastic pain - pain learned by the brain (and/or spinal cord) - is a&nbsp;<strong>well-evidenced phenomenon<\/strong> and widely accepted in modern medical research (very high confidence).<\/li><li>It&nbsp;<strong>explains many forms of chronic pain<\/strong> previously attributed to structural causes - not just wrist pain and back pain (high confidence). Other conditions include everything from pain in the knees, pelvis, bowels, neck, and the brain itself (headaches).&nbsp;<strong>Some practitioners also treat chronic<\/strong><a href=\"https://painpsychologycenter.com/our-team\"><strong>&nbsp;<u>fatigue<\/u><\/strong><\/a> (inc. Long-<a href=\"https://bjgplife.com/the-way-out-the-revolutionary-scientifically-proven-approach-to-heal-chronic-pain-by-alan-gordon-with-alon-ziv-vermillion-august-2021/\"><u>COVID<\/u><\/a>),<a href=\"https://mcpress.mayoclinic.org/living-well/what-is-central-sensitization-and-how-does-it-relate-to-pain/#:~:text=Sensitization%20can%20result%20in%20constant,alarm%20with%20subsequent%20physical%20reactions.\">&nbsp;<u>dizziness and nausea<\/u><\/a> in a similar way but I haven't dug into this.<\/li><li>It may be&nbsp;<a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0304395910005841\"><strong><u>one of<\/u><\/strong><\/a><strong> the most common or even the single&nbsp;<\/strong><a href=\"https://www.amazon.com/Way-Out-Revolutionary-Scientifically-Approach/dp/0593086856?crid=3NSDH8U8S7R4J&amp;dib=eyJ2IjoiMSJ9.iepov49BPBOnyo8rD0PWEKt7oCyRdOA5rHnLBGo2Cra9ox6D9zDYAtsSbKJ7GpCU2wxfxzeEIiojS16jrrlFwgqgBzYlorwO0HYxU6l9_Rap1yl_AmS-sAS_MNAvZqppahvfqqRr8_a8ROZbRZglgHdMd3-Pvjydqi5H4zH34IY1KPo2d3znK5sXTrILH0r3YglA1-UmSxyuHZz7pmR2NcnbjaHyWsRm3Z-J8EA1pOU.kMhg5CCgndYNUf712X-WLTJYgCEQRvgnf-xncIM8x-U&amp;dib_tag=se&amp;keywords=the+way+out&amp;qid=1739968854&amp;sprefix=the+way+o%2Caps%2C200&amp;sr=8-1\"><strong><u>most common<\/u><\/strong><\/a><strong> cause of chronic pain<\/strong> (moderate confidence).<\/li><li>There are increasingly&nbsp;<a href=\"https://www.amazon.com/Way-Out-Revolutionary-Scientifically-Approach/dp/0593086856?crid=3NSDH8U8S7R4J&amp;dib=eyJ2IjoiMSJ9.iepov49BPBOnyo8rD0PWEKt7oCyRdOA5rHnLBGo2Cra9ox6D9zDYAtsSbKJ7GpCU2wxfxzeEIiojS16jrrlFwgqgBzYlorwO0HYxU6l9_Rap1yl_AmS-sAS_MNAvZqppahvfqqRr8_a8ROZbRZglgHdMd3-Pvjydqi5H4zH34IY1KPo2d3znK5sXTrILH0r3YglA1-UmSxyuHZz7pmR2NcnbjaHyWsRm3Z-J8EA1pOU.kMhg5CCgndYNUf712X-WLTJYgCEQRvgnf-xncIM8x-U&amp;dib_tag=se&amp;keywords=the+way+out&amp;qid=1739968854&amp;sprefix=the+way+o%2Caps%2C200&amp;sr=8-1\"><strong><u>useful<\/u><\/strong><\/a><strong>&nbsp;<\/strong><a href=\"https://www.curablehealth.com/\"><strong><u>resources<\/u><\/strong><\/a><strong>, well-tested&nbsp;<\/strong><a href=\"https://jamanetwork.com/journals/jamapsychiatry/fullarticle/2784694\"><strong><u>treatments<\/u><\/strong><\/a><strong>&nbsp;<\/strong>with very large effect size, and<strong> trained&nbsp;<\/strong><a href=\"https://www.painreprocessingtherapy.com/directory-of-practitioners?lat=51.5344218&amp;lng=-0.1135203\"><strong><u>practitioners<\/u><\/strong><\/a>.<\/li><li><strong>Doctors are often unaware<\/strong> that neuroplastic pain exists because the research is recent and not their specialty. They often attribute it to tissue damage or structural causes like minor findings in medical imaging and biomechanical or blood diagnostics, which often fuels the fear-pain cycle.<\/li><\/ol><h2>My personal experience with with chronic pains and sudden relief<\/h2><p>My first chronic pain developed in the tendons behind my knee after running. Initially manageable, it progressed until I couldn't stand or walk for more than a few minutes without triggering days of pain. Medical examinations revealed inflammation and structural changes in the tendons. The prescribed treatments—exercises, rest, stretching, steroid injections—provided no meaningful relief.<\/p><p>Later, I developed unexplained tailbone pain when sitting. This quickly became my dominant daily discomfort. Specialists at leading medical centers identified a bone spur on my tailbone and unanimously conclude... <\/p>","plaintextDescription":"Epistemic status: Amateur synthesis of medical research that is still recent but now established enough to make it into modern medical textbooks. Some specific claims vary in evidence strength. I’ve spent ~20-30 hours studying the literature and treatment approaches, which were very effective for me.\n\nDisclaimer: I'm not a medical professional. This information is educational only, not medical advice. Consult healthcare providers for medical conditions.\n\n\nKey claims\nThis post builds on previous discussions about the fear-pain cycle and learned chronic pain. The post adds the following claims:\n\n 1. Neuroplastic pain - pain learned by the brain (and/or spinal cord) - is a well-evidenced phenomenon and widely accepted in modern medical research (very high confidence).\n 2. It explains many forms of chronic pain previously attributed to structural causes - not just wrist pain and back pain (high confidence). Other conditions include everything from pain in the knees, pelvis, bowels, neck, and the brain itself (headaches). Some practitioners also treat chronic fatigue (inc. Long-COVID), dizziness and nausea in a similar way but I haven't dug into this.\n 3. It may be one of the most common or even the single most common cause of chronic pain (moderate confidence).\n 4. There are increasingly useful resources, well-tested treatments with very large effect size, and trained practitioners.\n 5. Doctors are often unaware that neuroplastic pain exists because the research is recent and not their specialty. They often attribute it to tissue damage or structural causes like minor findings in medical imaging and biomechanical or blood diagnostics, which often fuels the fear-pain cycle.\n\n\nMy personal experience with with chronic pains and sudden relief\nMy first chronic pain developed in the tendons behind my knee after running. Initially manageable, it progressed until I couldn't stand or walk for more than a few minutes without triggering days of pain. Medical examinations revealed ","wordCount":2575,"version":"1.1.0"},"Revision:6taauM3vtMtojgjom_customHighlight":{"_id":"6taauM3vtMtojgjom_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"SocialPreviewType:6taauM3vtMtojgjom":{"_id":"6taauM3vtMtojgjom","__typename":"SocialPreviewType","imageUrl":""},"User:DGetADxtea2LRL946":{"_id":"DGetADxtea2LRL946","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"soerenmind","createdAt":"2013-07-16T18:10:55.180Z","username":"SoerenMind","displayName":"SoerenMind","previousDisplayName":null,"fullName":null,"karma":1225,"afKarma":166,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":23,"commentCount":191,"sequenceCount":0,"afPostCount":3,"afCommentCount":26,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:6taauM3vtMtojgjom":{"_id":"6taauM3vtMtojgjom","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:wrHkwXA6a8SkZarr6"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":10,"rejectedReason":null,"customHighlight":{"__ref":"Revision:6taauM3vtMtojgjom_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fkABsGCJZ6y9qConW"},{"__ref":"Tag:3uE2pXvbcnS9nnZRE"}],"socialPreviewData":{"__ref":"SocialPreviewType:6taauM3vtMtojgjom"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-09T11:57:58.523Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-09T18:14:54.526Z","meta":false,"postCategory":"post","tagRelevance":{"3uE2pXvbcnS9nnZRE":1,"fkABsGCJZ6y9qConW":1},"shareWithUsers":["3oopbgcjYfvN8B2fp"],"sharingSettings":{"anyoneWithLinkCan":"none","explicitlySharedUsersCan":"comment"},"linkSharingKey":null,"contents_latest":"wrHkwXA6a8SkZarr6","commentCount":0,"voteCount":9,"baseScore":42,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":9,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":3.705988883972168,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2025-04-09T11:57:58.523Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"DGetADxtea2LRL946","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"EQNTWXLKMeWMp2FQS","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":20,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":5,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-03-06T12:53:46.453Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:DGetADxtea2LRL946"},"coauthors":[],"slug":"learned-pain-as-a-leading-cause-of-chronic-pain","title":"Learned pain as a leading cause of chronic pain","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:hRw6q6mScZGhzHwXA":{"_id":"hRw6q6mScZGhzHwXA","__typename":"Revision","htmlHighlight":"<p><i>(Edit: Alas, <\/i><a href=\"https://www.lesswrong.com/posts/EQJfdqSaMcJyR5k73/habryka-s-shortform-feed?commentId=WCpbLpcPfYKm4brJr\"><i>EA has pulled out of the deal<\/i><\/a><i>. Let April 1st 2025 mark some of the greatest hours in EAs history)<\/i><\/p><p>Hey Everyone,<\/p><p>It is with a sense of... considerable cognitive dissonance that I am letting you all know about a significant development for the future trajectory of LessWrong. After extensive internal deliberation, projections of financial runways, and what I can only describe as a series of profoundly unexpected coordination challenges, the Lightcone Infrastructure team has agreed in principle to the acquisition of LessWrong by <a href=\"https://en.wikipedia.org/wiki/Electronic_Arts\">EA<\/a>.<\/p><p>I assure you, nothing about how LessWrong operates on a day to day level will change. I have always cared deeply about the robustness and integrity of our institutions, and I am fully aligned with our stakeholders at EA.&nbsp;<\/p><p>To be honest, the key thing that EA brings to the table is money and talent. While the recent layoffs in EAs broader industry have been harsh, I have full trust in the leadership of Electronic Arts, and expect them to bring great expertise to our project of building a better future. Their track record on monetization is impressive, and I couldn't be more excited for this partnership.<\/p><p>More announcements to come over the next 24 hours, but I figured I would share this news with all of you as soon as possible.<\/p><figure class=\"image\"><img src=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/jhsfmx1eh2ymg91uh7z0\" srcset=\"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/yg6yvuw700iuogmrt10i 160w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/qiqkmyeoz208t418g3tt 320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/hf2ywkqmfymjyjfumymz 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/kc7v0vne4zfqeeairset 640w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/yse0b9mpijhufg44wpgt 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/ox3vjsjrhqrea5kolhla 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/oit9gtuxjej9escqn2ix 1120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/zutizepjjmxsmvkv3sbm 1280w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/c6zlzua4vkephvdmno9w 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/lujfabvbm5sj6ehqgfap 1538w\"><\/figure>","plaintextDescription":"(Edit: Alas, EA has pulled out of the deal. Let April 1st 2025 mark some of the greatest hours in EAs history)\n\nHey Everyone,\n\nIt is with a sense of... considerable cognitive dissonance that I am letting you all know about a significant development for the future trajectory of LessWrong. After extensive internal deliberation, projections of financial runways, and what I can only describe as a series of profoundly unexpected coordination challenges, the Lightcone Infrastructure team has agreed in principle to the acquisition of LessWrong by EA.\n\nI assure you, nothing about how LessWrong operates on a day to day level will change. I have always cared deeply about the robustness and integrity of our institutions, and I am fully aligned with our stakeholders at EA. \n\nTo be honest, the key thing that EA brings to the table is money and talent. While the recent layoffs in EAs broader industry have been harsh, I have full trust in the leadership of Electronic Arts, and expect them to bring great expertise to our project of building a better future. Their track record on monetization is impressive, and I couldn't be more excited for this partnership.\n\nMore announcements to come over the next 24 hours, but I figured I would share this news with all of you as soon as possible.","wordCount":220,"version":"1.8.1"},"Revision:2NGKYt3xdQHwyfGbc_customHighlight":{"_id":"2NGKYt3xdQHwyfGbc_customHighlight","__typename":"Revision","html":"","plaintextDescription":""},"SocialPreviewType:2NGKYt3xdQHwyfGbc":{"_id":"2NGKYt3xdQHwyfGbc","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/2NGKYt3xdQHwyfGbc/jhsfmx1eh2ymg91uh7z0"},"User:XtphY3uYHwruKqDyG":{"_id":"XtphY3uYHwruKqDyG","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":false,"slug":"habryka4","createdAt":"2017-06-17T01:08:32.717Z","username":"habryka4","displayName":"habryka","previousDisplayName":null,"fullName":"Oliver Habryka","karma":42519,"afKarma":1741,"deleted":false,"isAdmin":true,"htmlBio":"<p>Running Lightcone Infrastructure, which runs LessWrong and <a href=\"https://Lighthaven.space\">Lighthaven.space<\/a>. You can reach me at <a href=\"mailto:habryka@lesswrong.com\">habryka@lesswrong.com<\/a>.&nbsp;<\/p><p>(I have signed no contracts or agreements whose existence I cannot mention, which I am mentioning here as a canary)<\/p>","jobTitle":null,"organization":null,"postCount":265,"commentCount":5054,"sequenceCount":4,"afPostCount":7,"afCommentCount":288,"spamRiskScore":1,"tagRevisionCount":116,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Post:2NGKYt3xdQHwyfGbc":{"_id":"2NGKYt3xdQHwyfGbc","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:hRw6q6mScZGhzHwXA"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":1,"rejectedReason":null,"customHighlight":{"__ref":"Revision:2NGKYt3xdQHwyfGbc_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fPRyNtDMeSMrEM9nr"}],"socialPreviewData":{"__ref":"SocialPreviewType:2NGKYt3xdQHwyfGbc"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-01T13:09:11.153Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-01T13:09:22.399Z","meta":false,"postCategory":"post","tagRelevance":{"fPRyNtDMeSMrEM9nr":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"hRw6q6mScZGhzHwXA","commentCount":45,"voteCount":172,"baseScore":335,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":172,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.7760635614395142,"lastVisitedAt":"2025-04-06T17:20:06.764Z","isFuture":false,"isRead":true,"lastCommentedAt":"2025-04-04T16:49:36.971Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"XtphY3uYHwruKqDyG","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":89,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":53,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-01T12:55:37.314Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:XtphY3uYHwruKqDyG"},"coauthors":[],"slug":"lesswrong-has-been-acquired-by-ea","title":"LessWrong has been acquired by EA","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c6392fcbcb4ac6367c17472":{"_id":"5c6392fcbcb4ac6367c17472","__typename":"Revision","htmlHighlight":"<p>Disclaimer 1: These views are my own and don’t necessarily reflect the views of anyone else (Eric, Steph, or Eliezer).<\/p><p>Disclaimer 2: Most of the events happened at least a year ago. My memory is not particularly great, so the dates are fuzzy and a few things might be slightly out of order. But this post has been reviewed by Eric, Steph, and Eliezer, so it should mostly be okay.<\/p><p>I’m going to list events chronologically. At times I’ll insert a “<strong>Reflection<\/strong>” paragraph, where I’m going to outline my thoughts as of now. I’ll talk about what I could have done differently and how I would approach a similar problem today.<\/p><h2>Chapter 0: Eliezer pitches Arbital and I say ‘no’<\/h2><p>Around the summer of 2014 Eliezer approached me with the idea for what later would become Arbital. At first, I vaguely understood the idea as some kind of software to map out knowledge. Maybe something like a giant mind map, but not graphical. I took some time to research existing and previous projects in that area and found a huge graveyard of projects that have been tried. Yes, basically all of them were dead. Most were hobby projects, but some seemed pretty serious. None were successful, as far as I could tell. I didn’t see how Eliezer’s project was different, so I passed on it.<\/p><p><strong>Reflection<\/strong>: Today, I’d probably try to sit down with Eliezer for longer and really try to understand what he is seeing that I’m not. It’s likely back then I didn’t have the right skills to extract that information, but I think I’m much better at it today.<\/p><p><strong>Reflection<\/strong>: Also, after working with Eliezer for a few years, I’ve got a better feeling for how things he says often seem confusing / out of alignment / tilted, until you finally wrap your mind around it, and then it’s crystal clear and easy.<\/p><h2>Chapter 1: Eliezer and I start Arbital<\/h2><p>Early January 2015 I was sitting in my room, tired from looking in vain for a decent startup idea, when Arbital popped back into my mind. There were still a lot of red flags around the idea, but I rationalized to myself that given Eliezer’s track record, there was probably something good here. And, in the worst case, I’d just create a tool that would be useful to Eliezer alone. That didn’t seem like a bad outcome, so I decided to do it. I contacted Eliezer, he was still interested, and so we started the project.<\/p><p><strong>Reflection<\/strong>: The decision process sounds a bit silly, but I don’t think it’s a bad one. I real... <\/p>","plaintextDescription":"Disclaimer 1: These views are my own and don’t necessarily reflect the views of anyone else (Eric, Steph, or Eliezer).\n\nDisclaimer 2: Most of the events happened at least a year ago. My memory is not particularly great, so the dates are fuzzy and a few things might be slightly out of order. But this post has been reviewed by Eric, Steph, and Eliezer, so it should mostly be okay.\n\nI’m going to list events chronologically. At times I’ll insert a “Reflection” paragraph, where I’m going to outline my thoughts as of now. I’ll talk about what I could have done differently and how I would approach a similar problem today.\n\n\nChapter 0: Eliezer pitches Arbital and I say ‘no’\nAround the summer of 2014 Eliezer approached me with the idea for what later would become Arbital. At first, I vaguely understood the idea as some kind of software to map out knowledge. Maybe something like a giant mind map, but not graphical. I took some time to research existing and previous projects in that area and found a huge graveyard of projects that have been tried. Yes, basically all of them were dead. Most were hobby projects, but some seemed pretty serious. None were successful, as far as I could tell. I didn’t see how Eliezer’s project was different, so I passed on it.\n\nReflection: Today, I’d probably try to sit down with Eliezer for longer and really try to understand what he is seeing that I’m not. It’s likely back then I didn’t have the right skills to extract that information, but I think I’m much better at it today.\n\nReflection: Also, after working with Eliezer for a few years, I’ve got a better feeling for how things he says often seem confusing / out of alignment / tilted, until you finally wrap your mind around it, and then it’s crystal clear and easy.\n\n\nChapter 1: Eliezer and I start Arbital\nEarly January 2015 I was sitting in my room, tired from looking in vain for a decent startup idea, when Arbital popped back into my mind. There were still a lot of red flags around the idea, but","wordCount":5811,"version":"1.0.0"},"Tag:TkZ7MFwCi4D63LJ5n":{"_id":"TkZ7MFwCi4D63LJ5n","__typename":"Tag","userId":"qxJ28GN72aiJu96iF","name":"Software Tools","shortName":null,"slug":"software-tools","core":false,"postCount":212,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-12T16:58:17.212Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:8sh6iLwYWDJ7z3fPo":{"_id":"8sh6iLwYWDJ7z3fPo","__typename":"Tag","userId":"6jLdWqegNefgaabhr","name":"Startups","shortName":null,"slug":"startups","core":false,"postCount":80,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-08-01T20:01:45.578Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:MXcpQvaPGtXpB6vkM":{"_id":"MXcpQvaPGtXpB6vkM","__typename":"Tag","userId":"gXeEWGjTWyqgrQTzR","name":"Public Discourse","shortName":null,"slug":"public-discourse","core":false,"postCount":181,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-15T04:23:00.324Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":20,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"},{"_id":"8btiLJDabHgZuiSAB","displayName":"Ggwp"}]},"score":20,"afBaseScore":9,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"EQNTWXLKMeWMp2FQS","displayName":"Ben Pace"},{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":3,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:EXgFbrqoRRkCRgnDy":{"_id":"EXgFbrqoRRkCRgnDy","__typename":"Tag","userId":"r38pkCm7wF4M44MDQ","name":"Online Socialization","shortName":null,"slug":"online-socialization","core":false,"postCount":40,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-20T01:43:39.863Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:kAgJJa3HLSZxsuSrf":{"_id":"kAgJJa3HLSZxsuSrf","__typename":"SocialPreviewType","imageUrl":""},"User:2SPhghtAz3i32H8Ym":{"_id":"2SPhghtAz3i32H8Ym","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":null,"slug":"alexei-andreev","createdAt":"2017-06-18T22:55:15.332Z","username":"alexei.andreev","displayName":"alexei","previousDisplayName":null,"fullName":null,"karma":643,"afKarma":0,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":4,"commentCount":264,"sequenceCount":0,"afPostCount":0,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":1191,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"Post:kAgJJa3HLSZxsuSrf":{"_id":"kAgJJa3HLSZxsuSrf","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c6392fcbcb4ac6367c17472"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":23,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:zcvsZQWJBFK6SxK4K"},{"__ref":"Tag:TkZ7MFwCi4D63LJ5n"},{"__ref":"Tag:8sh6iLwYWDJ7z3fPo"},{"__ref":"Tag:MXcpQvaPGtXpB6vkM"},{"__ref":"Tag:EXgFbrqoRRkCRgnDy"},{"__ref":"Tag:izp6eeJJEg9v5zcur"}],"socialPreviewData":{"__ref":"SocialPreviewType:kAgJJa3HLSZxsuSrf"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-01-30T13:48:31.399Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-01-31T07:57:15.475Z","meta":false,"postCategory":"post","tagRelevance":{"8sh6iLwYWDJ7z3fPo":5,"EXgFbrqoRRkCRgnDy":2,"MXcpQvaPGtXpB6vkM":4,"TkZ7MFwCi4D63LJ5n":12,"izp6eeJJEg9v5zcur":1,"zcvsZQWJBFK6SxK4K":19},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c6392fcbcb4ac6367c17472","commentCount":110,"voteCount":171,"baseScore":230,"extendedScore":{"reacts":{"typo":[{"karma":493,"quotes":["we wasn't in the office with us"],"userId":"dZMo8p7fGCgPMfdfD","reactType":"created","displayName":"dirk"}]},"agreement":0,"approvalVoteCount":171,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0007591309840790927,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2022-05-06T16:00:35.279Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-01-31T07:56:16.807Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"2SPhghtAz3i32H8Ym","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":26,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":64,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2018-01-30T13:48:31.399Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:2SPhghtAz3i32H8Ym"},"coauthors":[],"slug":"arbital-postmortem","title":"Arbital postmortem","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:EBdhAXSku8YGL3JsC":{"_id":"EBdhAXSku8YGL3JsC","__typename":"Revision","htmlHighlight":"<p>A pdf version of this report is available <a href=\"https://intelligence.org/wp-content/uploads/2024/02/Misalignment_and_Catastrophe.pdf\">here<\/a>.<\/p><h1>Summary<\/h1><p>In this report we argue that AI systems capable of large scale scientific research will likely pursue unwanted goals and this will lead to catastrophic outcomes. We argue this is the default outcome, even with significant countermeasures, given the current trajectory of AI development.<\/p><p>In Section 1 we discuss the tasks which are the focus of this report. We are specifically focusing on AIs which are capable of dramatically speeding up large-scale novel science; on the scale of the Manhattan Project or curing cancer. This type of task requires a lot of work, and will require the AI to overcome many novel and diverse obstacles.<\/p><p>In Section 2 we argue that an AI which is capable of doing hard, novel science will be approximately consequentialist; that is, its behavior will be well described as taking actions in order to achieve an outcome. This is because the task has to be specified in terms of outcomes, and the AI needs to be robust to new obstacles in order to achieve these outcomes.<\/p><p>In Section 3 we argue that novel science will necessarily require the AI to learn new things, both facts and skills. This means that an AI’s capabilities will change over time which is a source of dangerous distribution shifts.<\/p><p>In Section 4 we further argue that training methods based on external behavior, which is how AI systems are currently created, are an extremely imprecise way to specify the goals we want an AI to ultimately pursue. This is because there are many degrees of freedom in goal specification that aren’t pinned down by behavior. AIs created this way will, by default, pursue unintended goals.<\/p><p>In Section 5 we discuss why we expect oversight and control of powerful AIs to be difficult. It will be difficult to safely get useful work out of misaligned AIs while ensuring they don’t take unwanted actions, and therefore we don’t expect AI-assisted research to be both safe and much faster than current research.<\/p><p>Finally, in Section 6 we discuss the consequences of building a powerful AI with improperly specified goals. Such an AI could likely escape containment measures given realistic levels of security, and then pursue outcomes in the world that would be catastrophic for humans. It seems very unlikely that these outcomes would be compatible with human empowerment or survival.<\/p><h1>Introduction<\/h1><p>We expect future AI systems will be a... <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style><\/p>","plaintextDescription":"A pdf version of this report is available here.\n\n\nSummary\nIn this report we argue that AI systems capable of large scale scientific research will likely pursue unwanted goals and this will lead to catastrophic outcomes. We argue this is the default outcome, even with significant countermeasures, given the current trajectory of AI development.\n\nIn Section 1 we discuss the tasks which are the focus of this report. We are specifically focusing on AIs which are capable of dramatically speeding up large-scale novel science; on the scale of the Manhattan Project or curing cancer. This type of task requires a lot of work, and will require the AI to overcome many novel and diverse obstacles.\n\nIn Section 2 we argue that an AI which is capable of doing hard, novel science will be approximately consequentialist; that is, its behavior will be well described as taking actions in order to achieve an outcome. This is because the task has to be specified in terms of outcomes, and the AI needs to be robust to new obstacles in order to achieve these outcomes.\n\nIn Section 3 we argue that novel science will necessarily require the AI to learn new things, both facts and skills. This means that an AI’s capabilities will change over time which is a source of dangerous distribution shifts.\n\nIn Section 4 we further argue that training methods based on external behavior, which is how AI systems are currently created, are an extremely imprecise way to specify the goals we want an AI to ultimately pursue. This is because there are many degrees of freedom in goal specification that aren’t pinned down by behavior. AIs created this way will, by default, pursue unintended goals.\n\nIn Section 5 we discuss why we expect oversight and control of powerful AIs to be difficult. It will be difficult to safely get useful work out of misaligned AIs while ensuring they don’t take unwanted actions, and therefore we don’t expect AI-assisted research to be both safe and much faster than current research.\n\nFinal","wordCount":17072,"version":"1.0.0"},"Tag:Dw5Z6wtTgk4Fikz9f":{"_id":"Dw5Z6wtTgk4Fikz9f","__typename":"Tag","userId":"EQNTWXLKMeWMp2FQS","name":"Inner Alignment","shortName":null,"slug":"inner-alignment","core":false,"postCount":312,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2020-07-17T06:11:39.285Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:JX69nZB8tfxnx5nGH":{"_id":"JX69nZB8tfxnx5nGH","__typename":"Tag","userId":"qqwfzAYaLsfmkwbsK","name":"Threat Models (AI)","shortName":null,"slug":"threat-models-ai","core":false,"postCount":98,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2021-04-20T21:57:13.125Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":9,"extendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"score":9,"afBaseScore":3,"afExtendedScore":{"reacts":{},"usersWhoLiked":[{"_id":"qgdGA4ZEyW7zNdK84","displayName":"Ruby"}]},"voteCount":1,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:GfZfDHZHCuYwrHGCd":{"_id":"GfZfDHZHCuYwrHGCd","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/GfZfDHZHCuYwrHGCd/nsfzcybegyh5iur7ilkr"},"User:rfNXJYd4GHfjbcdTb":{"_id":"rfNXJYd4GHfjbcdTb","__typename":"User","profileImageId":null,"moderationStyle":"easy-going","bannedUserIds":null,"moderatorAssistance":true,"slug":"jeremy-gillen","createdAt":"2017-10-20T07:36:49.079Z","username":"jeremy-gillen","displayName":"Jeremy Gillen","previousDisplayName":null,"fullName":null,"karma":1920,"afKarma":151,"deleted":false,"isAdmin":false,"htmlBio":"<p>I'm interested in doing in-depth dialogues to find cruxes. Message me if you are interested in doing this.<\/p><p>I do alignment research, mostly stuff that is vaguely agent foundations. Currently doing independent alignment research on ontology identification. Formerly on Vivek's team at MIRI. Most of my writing before mid 2023 is not representative of my current views about alignment difficulty.<\/p>","jobTitle":null,"organization":null,"postCount":9,"commentCount":209,"sequenceCount":0,"afPostCount":3,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":1,"reviewedByUserId":"r38pkCm7wF4M44MDQ"},"User:NHMSJPMdExqDxPsY3":{"_id":"NHMSJPMdExqDxPsY3","__typename":"User","slug":"peterbarnett","createdAt":"2020-05-31T01:03:12.686Z","username":"peterbarnett","displayName":"peterbarnett","profileImageId":null,"previousDisplayName":null,"fullName":"Peter Barnett","karma":2674,"afKarma":68,"deleted":false,"isAdmin":false,"htmlBio":"<p>Researcher at MIRI<\/p><p>EA and AI safety<\/p><p><a href=\"https://peterbarnett.org/\">https://peterbarnett.org/<\/a><\/p>","jobTitle":null,"organization":null,"postCount":18,"commentCount":84,"sequenceCount":1,"afPostCount":2,"afCommentCount":0,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"XtphY3uYHwruKqDyG"},"Post:GfZfDHZHCuYwrHGCd":{"_id":"GfZfDHZHCuYwrHGCd","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:EBdhAXSku8YGL3JsC"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":68,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:Dw5Z6wtTgk4Fikz9f"},{"__ref":"Tag:JX69nZB8tfxnx5nGH"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:GfZfDHZHCuYwrHGCd"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2024-01-26T07:22:06.370Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2024-01-26T17:02:05.438Z","meta":false,"postCategory":"post","tagRelevance":{"Dw5Z6wtTgk4Fikz9f":1,"JX69nZB8tfxnx5nGH":1,"sYm3HiWcfZvrGu3ui":1},"shareWithUsers":["NHMSJPMdExqDxPsY3"],"sharingSettings":{"anyoneWithLinkCan":"none","explicitlySharedUsersCan":"edit"},"linkSharingKey":null,"contents_latest":"EBdhAXSku8YGL3JsC","commentCount":60,"voteCount":73,"baseScore":161,"extendedScore":{"reacts":{"important":[{"karma":658,"quotes":["With good oversight, the AI would receive low reward for hacking the solution-checker during training, and one might hope that this correctly puts a “morality-like” constraint into the AI’s goal. But there is another way it might update: the training could insert a false belief"],"userId":"HHiJSvTEQkMx8ej62","reactType":"created","displayName":"Morpheus"}]},"agreement":0,"approvalVoteCount":73,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.004306631162762642,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2024-04-24T00:25:27.719Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2024-01-29T18:59:15.916Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"rfNXJYd4GHfjbcdTb","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"55XxDBpfKkkBPm9H8","suggestForCuratedUserIds":["XtphY3uYHwruKqDyG"],"suggestForCuratedUsernames":"habryka","reviewForCuratedUserId":"XtphY3uYHwruKqDyG","authorIsUnreviewed":false,"afDate":"2024-01-30T21:04:02.582Z","suggestForAlignmentUserIds":["rfNXJYd4GHfjbcdTb"],"reviewForAlignmentUserId":"XtphY3uYHwruKqDyG","afBaseScore":63,"afExtendedScore":{"reacts":{"important":[{"karma":658,"quotes":["With good oversight, the AI would receive low reward for hacking the solution-checker during training, and one might hope that this correctly puts a “morality-like” constraint into the AI’s goal. But there is another way it might update: the training could insert a false belief"],"userId":"HHiJSvTEQkMx8ej62","reactType":"created","displayName":"Morpheus"}]},"agreement":0,"approvalVoteCount":49,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2024-01-26T07:22:06.370Z","afSticky":false,"hideAuthor":false,"moderationStyle":"easy-going","ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":"O1YJHoN2tAVQCKvQbLHg","annualReviewMarketProbability":0.49582605223025494,"annualReviewMarketIsResolved":false,"annualReviewMarketYear":2024,"annualReviewMarketUrl":"https://manifold.markets/LessWrong/will-without-fundamental-advances-m-75a00063a522","group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:rfNXJYd4GHfjbcdTb"},"coauthors":[{"__ref":"User:NHMSJPMdExqDxPsY3"}],"slug":"without-fundamental-advances-misalignment-and-catastrophe","title":"Without fundamental advances, misalignment and catastrophe are the default outcomes of training powerful AI","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":[{"userId":"NHMSJPMdExqDxPsY3","confirmed":true,"requested":false}],"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c6392fcbcb4ac6367c174ff":{"_id":"5c6392fcbcb4ac6367c174ff","__typename":"Revision","htmlHighlight":"<p>I want to quickly draw attention to a concept in AI alignment: Robustness to Scale. Briefly, you want your proposal for an AI to be robust (or at least fail gracefully) to changes in its level of capabilities. I discuss three different types of robustness to scale: robustness to scaling up, robustness to scaling down, and robustness to relative scale.<\/p><p>The purpose of this post is to communicate, not to persuade. It may be that we want to bite the bullet of the strongest form of robustness to scale, and build an AGI that is simply not robust to scale, but if we do, we should at least realize that we are doing that.<\/p><p>Robustness to scaling up means that your AI system does not depend on not being too powerful. One way to check for this is to think about what would happen if the thing that the AI is optimizing for were actually maximized. One example of failure of robustness to scaling up is when you expect an AI to accomplish a task in a specific way, but it becomes smart enough to find new creative ways to accomplish the task that you did not think of, and these new creative ways are disastrous. Another example is when you make an AI that is incentivized to do one thing, but you add restrictions that make it so that the best way to accomplish that thing has a side effect that you like. When you scale the AI up, it finds a way around your restrictions.<\/p><p>Robustness to scaling down means that your AI system does not depend on being sufficiently powerful. You can&apos;t really make your system still work when it scales down, but you can maybe make sure it fails gracefully. For example, imagine you had a system that was trying to predict humans, and use these predictions to figure out what to do. When scaled up all the way, the predictions of humans are completely accurate, and it will only take actions that the predicted humans would approve of. If you scale down the capabilities, your system may predict the humans incorrectly. These errors may multiply as you stack many predicted humans together, and the system can end up optimizing for some seeming random goal.<\/p><p>Robustness to relative scale means that your AI system does not depend on any subsystems being similarly powerful to each other. This is most easy to see in systems that depend on adversarial subsystems. If part of you AI system is suggest plans, and another part is trying to find problems in those plans, if you... <\/p>","plaintextDescription":"I want to quickly draw attention to a concept in AI alignment: Robustness to Scale. Briefly, you want your proposal for an AI to be robust (or at least fail gracefully) to changes in its level of capabilities. I discuss three different types of robustness to scale: robustness to scaling up, robustness to scaling down, and robustness to relative scale.\n\nThe purpose of this post is to communicate, not to persuade. It may be that we want to bite the bullet of the strongest form of robustness to scale, and build an AGI that is simply not robust to scale, but if we do, we should at least realize that we are doing that.\n\nRobustness to scaling up means that your AI system does not depend on not being too powerful. One way to check for this is to think about what would happen if the thing that the AI is optimizing for were actually maximized. One example of failure of robustness to scaling up is when you expect an AI to accomplish a task in a specific way, but it becomes smart enough to find new creative ways to accomplish the task that you did not think of, and these new creative ways are disastrous. Another example is when you make an AI that is incentivized to do one thing, but you add restrictions that make it so that the best way to accomplish that thing has a side effect that you like. When you scale the AI up, it finds a way around your restrictions.\n\nRobustness to scaling down means that your AI system does not depend on being sufficiently powerful. You can't really make your system still work when it scales down, but you can maybe make sure it fails gracefully. For example, imagine you had a system that was trying to predict humans, and use these predictions to figure out what to do. When scaled up all the way, the predictions of humans are completely accurate, and it will only take actions that the predicted humans would approve of. If you scale down the capabilities, your system may predict the humans incorrectly. These errors may multiply as you stack many predi","wordCount":563,"version":"1.1.0"},"Revision:bBdfbWfWxHN9Chjcq_customHighlight":{"_id":"bBdfbWfWxHN9Chjcq_customHighlight","__typename":"Revision","html":"<p>You want your proposal for an AI to be robust to changes in its level of capabilities. It should be robust to the AI's capabilities scaling up, and also scaling down, and also the <i>subcomponents<\/i> of the AI scaling relative to each other.&nbsp;<\/p><p>We might need to build AGIs that aren't robust to scale, but if so we should at least realize that we are doing that.<\/p>","plaintextDescription":"You want your proposal for an AI to be robust to changes in its level of capabilities. It should be robust to the AI's capabilities scaling up, and also scaling down, and also the subcomponents of the AI scaling relative to each other. \n\nWe might need to build AGIs that aren't robust to scale, but if so we should at least realize that we are doing that."},"Tag:haiwnEEx3vhrkfmAP":{"_id":"haiwnEEx3vhrkfmAP","__typename":"Tag","userId":"qgdGA4ZEyW7zNdK84","name":"AI Robustness","shortName":null,"slug":"ai-robustness","core":false,"postCount":22,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":false,"descriptionTruncationCount":0,"createdAt":"2022-07-23T17:35:23.708Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"Tag:wBoHTJs9iQzczNtW3":{"_id":"wBoHTJs9iQzczNtW3","__typename":"Tag","userId":"nLbwLhBaQeG6tCNDN","name":"Robust Agents","shortName":null,"slug":"robust-agents","core":false,"postCount":44,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2020-04-28T23:27:27.231Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":false,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:bBdfbWfWxHN9Chjcq":{"_id":"bBdfbWfWxHN9Chjcq","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/c_fill,ar_1.91,g_auto/v1/mirroredImages/splashArtImagePromptA%20tower%20telescope%20adjusting%20its%20focus%20to%20view%20objects%20near%20and%20far/uomjsq5adlo523ryebat"},"User:hbQoLoK5tpmFAJGr4":{"_id":"hbQoLoK5tpmFAJGr4","__typename":"User","profileImageId":null,"moderationStyle":null,"bannedUserIds":null,"moderatorAssistance":false,"slug":"scott-garrabrant","createdAt":"2017-09-22T02:21:16.385Z","username":"Scott Garrabrant","displayName":"Scott Garrabrant","previousDisplayName":null,"fullName":null,"karma":8387,"afKarma":1575,"deleted":false,"isAdmin":false,"htmlBio":"","jobTitle":null,"organization":null,"postCount":74,"commentCount":416,"sequenceCount":4,"afPostCount":99,"afCommentCount":192,"spamRiskScore":1,"tagRevisionCount":0,"reviewedByUserId":"grecHJcgkb3KW5wnM"},"Post:bBdfbWfWxHN9Chjcq":{"_id":"bBdfbWfWxHN9Chjcq","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c6392fcbcb4ac6367c174ff"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":2,"rejectedReason":null,"customHighlight":{"__ref":"Revision:bBdfbWfWxHN9Chjcq_customHighlight"},"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:haiwnEEx3vhrkfmAP"},{"__ref":"Tag:wBoHTJs9iQzczNtW3"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:bBdfbWfWxHN9Chjcq"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-02-21T22:55:19.155Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-02-21T22:55:18.596Z","meta":false,"postCategory":"post","tagRelevance":{"haiwnEEx3vhrkfmAP":5,"sYm3HiWcfZvrGu3ui":11,"wBoHTJs9iQzczNtW3":1},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c6392fcbcb4ac6367c174ff","commentCount":23,"voteCount":66,"baseScore":130,"extendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":66,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.0004604085406754166,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2023-11-23T13:34:11.615Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-02-27T01:11:31.586Z","commentsLocked":false,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"hbQoLoK5tpmFAJGr4","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":45,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":47,"agreementVoteCount":0},"afCommentCount":7,"afLastCommentedAt":"2022-01-03T10:17:55.424Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":1,"reviewVoteCount":62,"positiveReviewVoteCount":14,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:hbQoLoK5tpmFAJGr4"},"coauthors":[],"slug":"robustness-to-scale","title":"Robustness to Scale","draft":false,"hideCommentKarma":false,"af":true,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:5c6392fcbcb4ac6367c17428":{"_id":"5c6392fcbcb4ac6367c17428","__typename":"Revision","htmlHighlight":"<p>I&#x27;ve been thinking about what implicit model of the world I use to make plans that reduce x-risk from AI. I list four main gears below (with quotes to illustrate), and then discuss concrete heuristics I take from it.<\/p><h2>A model of AI x-risk in four parts<\/h2><p><strong>1. Alignment is <em>hard.<\/em><\/strong><\/p><p>Quoting &quot;Security Mindset and the Logistic Success Curve&quot; (<u><a href=\"https://www.lesserwrong.com/posts/cpdsMuAHSWhWnKdog/security-mindset-and-the-logistic-success-curve\">link<\/a><\/u>)<\/p><blockquote>Coral:  YES. Given that this is a novel project entering new territory, expect it to take <em>at least <\/em>two years more time, or 50% more development time—whichever is less—compared to a security-incautious project that otherwise has identical tools, insights, people, and resources. And that is a very, very optimistic lower bound.<\/blockquote><blockquote>Amber:  This story seems to be heading in a worrying direction.<\/blockquote><blockquote>Coral:  Well, I&#x27;m sorry, but creating robust systems takes longer than creating non-robust systems even in cases where it would be really, extraordinarily bad if creating robust systems took longer than creating non-robust systems.<\/blockquote><p><strong>2. Getting alignment right accounts for most of the variance in whether an AGI system will be positive for humanity.<\/strong><\/p><p>Quoting &quot;The Hidden Complexity of Wishes&quot; (<u><a href=\"https://www.lesserwrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes\">link<\/a><\/u>)<\/p><blockquote>There are three kinds of genies:  Genies to whom you can safely say &quot;I wish for you to do what I should wish for&quot;; genies for which <em>no<\/em> wish is safe; and <u><a href=\"https://www.lesserwrong.com/lw/l8/conjuring_an_evolution_to_serve_you/\">genies that aren&#x27;t very powerful or intelligent<\/a><\/u>.<\/blockquote><blockquote>[...]<\/blockquote><blockquote>There is no safe wish smaller than an entire human morality.  There are too many possible paths through Time.  You can&#x27;t visualize all the roads that lead to the destination you give the genie... any more than you can program a chess-playing machine by hardcoding a move for every possible board position.<\/blockquote><blockquote>And real life is far more complicated than chess.  You cannot predict, in advance, which of your values will be needed to judge the path through time that the genie takes.  Especially if you wish for something longer-term or wider-range than rescuing your mother from a burning building.<\/blockquote><p><strong>3. Our current epistemic state regarding AGI timelines will continue until we&#x27;re close (&lt;2 years from) to having AGI.<\/strong><\/p><p>Quoting &quot;There is No Fire Alarm for AGI&quot; (<u><a href=\"https://www.lesserwrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence\">link<\/a><\/u>)<\/p><blockquote>It&#x27;s not that whenever somebody says &quot;fifty years&quot; the thing always happens in two years. It&#x27;s that this confident prediction of things being far away corresponds to an epistemic state about the technology that feels the same way i<\/blockquote>... ","plaintextDescription":"I've been thinking about what implicit model of the world I use to make plans that reduce x-risk from AI. I list four main gears below (with quotes to illustrate), and then discuss concrete heuristics I take from it.\n\n\nA model of AI x-risk in four parts\n1. Alignment is hard.\n\nQuoting \"Security Mindset and the Logistic Success Curve\" (link)\n\n> Coral: YES. Given that this is a novel project entering new territory, expect it to take at least two years more time, or 50% more development time—whichever is less—compared to a security-incautious project that otherwise has identical tools, insights, people, and resources. And that is a very, very optimistic lower bound.\n\n> Amber: This story seems to be heading in a worrying direction.\n\n> Coral: Well, I'm sorry, but creating robust systems takes longer than creating non-robust systems even in cases where it would be really, extraordinarily bad if creating robust systems took longer than creating non-robust systems.\n\n2. Getting alignment right accounts for most of the variance in whether an AGI system will be positive for humanity.\n\nQuoting \"The Hidden Complexity of Wishes\" (link)\n\n> There are three kinds of genies: Genies to whom you can safely say \"I wish for you to do what I should wish for\"; genies for which no wish is safe; and genies that aren't very powerful or intelligent.\n\n> [...]\n\n> There is no safe wish smaller than an entire human morality. There are too many possible paths through Time. You can't visualize all the roads that lead to the destination you give the genie... any more than you can program a chess-playing machine by hardcoding a move for every possible board position.\n\n> And real life is far more complicated than chess. You cannot predict, in advance, which of your values will be needed to judge the path through time that the genie takes. Especially if you wish for something longer-term or wider-range than rescuing your mother from a burning building.\n\n3. Our current epistemic state regarding AGI timeli","wordCount":1858,"version":"1.0.0"},"SocialPreviewType:XFpDTCHZZ4wpMT8PZ":{"_id":"XFpDTCHZZ4wpMT8PZ","__typename":"SocialPreviewType","imageUrl":""},"Post:XFpDTCHZZ4wpMT8PZ":{"_id":"XFpDTCHZZ4wpMT8PZ","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:5c6392fcbcb4ac6367c17428"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":7,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:zHjC29kkPmsdo7WTr"},{"__ref":"Tag:oNcqyaWPXNGTTRPHm"},{"__ref":"Tag:sYm3HiWcfZvrGu3ui"}],"socialPreviewData":{"__ref":"SocialPreviewType:XFpDTCHZZ4wpMT8PZ"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2018-01-19T00:21:45.460Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2018-01-30T00:32:03.501Z","meta":false,"postCategory":"post","tagRelevance":{"oNcqyaWPXNGTTRPHm":2,"sYm3HiWcfZvrGu3ui":2,"zHjC29kkPmsdo7WTr":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"5c6392fcbcb4ac6367c17428","commentCount":39,"voteCount":56,"baseScore":69,"extendedScore":null,"emojiReactors":{},"unlisted":false,"score":0.00026817977777682245,"lastVisitedAt":null,"isFuture":false,"isRead":null,"lastCommentedAt":"2020-03-31T03:17:39.870Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":"2018-01-30T00:31:55.858Z","commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"EQNTWXLKMeWMp2FQS","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":null,"groupId":null,"reviewedByUserId":"XtphY3uYHwruKqDyG","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":0,"afExtendedScore":null,"afCommentCount":0,"afLastCommentedAt":null,"afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:EQNTWXLKMeWMp2FQS"},"coauthors":[],"slug":"a-model-i-use-when-making-plans-to-reduce-ai-x-risk","title":"A model I use when making plans to reduce AI x-risk","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false},"Revision:M8dKC2rs5nGQBgrTC":{"_id":"M8dKC2rs5nGQBgrTC","__typename":"Revision","htmlHighlight":"<h1>Introduction<\/h1><p>Decision theory is about how to behave rationally under conditions of uncertainty, especially if this uncertainty involves being acausally blackmailed and/or gaslit by alien superintelligent basilisks.<\/p><p>Decision theory has found numerous practical applications, including <a href=\"https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed\">proving the existence of God<\/a> and <a href=\"https://www.lesswrong.com/posts/szfxvS8nsxTgJLBHs/ingredients-of-timeless-decision-theory#comments\">generating endless LessWrong comments since the beginning of time<\/a>.<\/p><p>However, despite the apparent simplicity of \"just choose the best action\", no comprehensive decision theory that resolves all decision theory dilemmas has yet been formalized. This paper at long last resolves this dilemma, by introducing a new decision theory: VDT.<\/p><h2>Decision theory problems and existing theories<\/h2><p>Some common existing decision theories are:<\/p><ul><li><strong>Causal Decision Theory (CDT)<\/strong>: select the action that *causes* the best outcome.<\/li><li><strong>Evidential Decision Theory (EDT)<\/strong>: select the action that you would be happiest to learn that you had taken.<\/li><li><strong>Functional Decision Theory (FDT)<\/strong>: select the action output by the function such that if you take decisions by this function you get the best outcome.<\/li><\/ul><p>Here is a list of dilemmas in decision theory that have vexed at least one of the above decision theories:<\/p><ul><li><strong>Newcomb's problem<\/strong>: a superintelligent predictor, Omega, gives you two boxes. Box A is transparent and has $1000. Box B is opaque and has $1M if Omega predicts you would pick only Box B, and otherwise empty. Do you take just Box B, or Box A and Box B?<ul><li>CDT two-boxes and misses out on $999k.<\/li><\/ul><\/li><li><strong>Smoking lesion<\/strong>: people with a certain genetically-caused lesion tend to smoke and develop cancer, but smoking does not cause cancer (this thought experiment is sponsored by Philip Morris International). You enjoy smoking and don't know if you have the lesion. Should you smoke?<ul><li>EDT says you shouldn't smoke, because that's evidence you have the lesion.<\/li><\/ul><\/li><li><strong>Parfit's hitchhiker<\/strong>: you're stranded in a desert without money trying to get back to your apartment, and a telepathic taxi driver goes past, but will only save you if they predict you'll actually bring back $100 from your apartment once they've driven you home. Do you commit to paying?<ul><li>CDT decides, upon arriving in the apartment, to not pay the taxi driver, and therefore leaves you stranded.<\/li><\/ul><\/li><li><strong>Counterfactual mugging:<\/strong> another superintelligent predictor (also called Omega because there aren't very many baby name books for superintelligent predictors), flips a fair coin. If the coin la<\/li><\/ul>... <style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n<\/style>","plaintextDescription":"Introduction\nDecision theory is about how to behave rationally under conditions of uncertainty, especially if this uncertainty involves being acausally blackmailed and/or gaslit by alien superintelligent basilisks.\n\nDecision theory has found numerous practical applications, including proving the existence of God and generating endless LessWrong comments since the beginning of time.\n\nHowever, despite the apparent simplicity of \"just choose the best action\", no comprehensive decision theory that resolves all decision theory dilemmas has yet been formalized. This paper at long last resolves this dilemma, by introducing a new decision theory: VDT.\n\n\nDecision theory problems and existing theories\nSome common existing decision theories are:\n\n * Causal Decision Theory (CDT): select the action that *causes* the best outcome.\n * Evidential Decision Theory (EDT): select the action that you would be happiest to learn that you had taken.\n * Functional Decision Theory (FDT): select the action output by the function such that if you take decisions by this function you get the best outcome.\n\nHere is a list of dilemmas in decision theory that have vexed at least one of the above decision theories:\n\n * Newcomb's problem: a superintelligent predictor, Omega, gives you two boxes. Box A is transparent and has $1000. Box B is opaque and has $1M if Omega predicts you would pick only Box B, and otherwise empty. Do you take just Box B, or Box A and Box B?\n   * CDT two-boxes and misses out on $999k.\n * Smoking lesion: people with a certain genetically-caused lesion tend to smoke and develop cancer, but smoking does not cause cancer (this thought experiment is sponsored by Philip Morris International). You enjoy smoking and don't know if you have the lesion. Should you smoke?\n   * EDT says you shouldn't smoke, because that's evidence you have the lesion.\n * Parfit's hitchhiker: you're stranded in a desert without money trying to get back to your apartment, and a telepathic taxi driver goes p","wordCount":1158,"version":"1.2.1"},"Tag:X8JsWEnBRPvs5Y99i":{"_id":"X8JsWEnBRPvs5Y99i","__typename":"Tag","userId":"nmk3nLpQE89dMRzzN","name":"Decision theory","shortName":null,"slug":"decision-theory","core":false,"postCount":476,"adminOnly":false,"canEditUserIds":null,"suggestedAsFilter":false,"needsReview":null,"descriptionTruncationCount":null,"createdAt":"2015-12-03T07:35:06.000Z","wikiOnly":false,"deleted":false,"isSubforum":false,"noindex":false,"isArbitalImport":true,"isPlaceholderPage":false,"baseScore":0,"extendedScore":null,"score":0,"afBaseScore":null,"afExtendedScore":null,"voteCount":0,"currentUserVote":null,"currentUserExtendedVote":null},"SocialPreviewType:LcjuHNxubQqCry9tT":{"_id":"LcjuHNxubQqCry9tT","__typename":"SocialPreviewType","imageUrl":"https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LcjuHNxubQqCry9tT/cmhbsig0fv5e2bnzs76m"},"Post:LcjuHNxubQqCry9tT":{"_id":"LcjuHNxubQqCry9tT","__typename":"Post","currentUserVote":null,"currentUserExtendedVote":null,"deletedDraft":false,"contents":{"__ref":"Revision:M8dKC2rs5nGQBgrTC"},"fmCrosspost":{"isCrosspost":false},"readTimeMinutes":5,"rejectedReason":null,"customHighlight":null,"lastPromotedComment":null,"bestAnswer":null,"tags":[{"__ref":"Tag:fPRyNtDMeSMrEM9nr"},{"__ref":"Tag:X8JsWEnBRPvs5Y99i"},{"__ref":"Tag:Ng8Gice9KNkncxqcj"}],"socialPreviewData":{"__ref":"SocialPreviewType:LcjuHNxubQqCry9tT"},"feedId":null,"totalDialogueResponseCount":0,"unreadDebateResponseCount":0,"dialogTooltipPreview":null,"disableSidenotes":false,"url":null,"postedAt":"2025-04-01T21:04:09.509Z","createdAt":null,"sticky":false,"metaSticky":false,"stickyPriority":2,"status":2,"frontpageDate":"2025-04-01T23:36:43.617Z","meta":false,"postCategory":"post","tagRelevance":{"Ng8Gice9KNkncxqcj":1,"X8JsWEnBRPvs5Y99i":2,"fPRyNtDMeSMrEM9nr":2},"shareWithUsers":[],"sharingSettings":null,"linkSharingKey":null,"contents_latest":"M8dKC2rs5nGQBgrTC","commentCount":17,"voteCount":142,"baseScore":290,"extendedScore":{"reacts":{"betFalse":[{"karma":686,"quotes":["CDT one-boxes"],"userId":"Stdajg3c5GZSyXCpt","reactType":"created","displayName":"tlevin"}]},"agreement":0,"approvalVoteCount":142,"agreementVoteCount":0},"emojiReactors":{},"unlisted":false,"score":0.7082812190055847,"lastVisitedAt":"2025-04-06T17:37:59.433Z","isFuture":false,"isRead":true,"lastCommentedAt":"2025-04-01T21:04:09.509Z","lastCommentPromotedAt":null,"canonicalCollectionSlug":null,"curatedDate":null,"commentsLocked":null,"commentsLockedToAccountsCreatedAfter":null,"debate":false,"question":false,"hiddenRelatedQuestion":false,"originalPostRelationSourceId":null,"userId":"vvqpGvkYqLcerYph6","location":null,"googleLocation":null,"onlineEvent":false,"globalEvent":false,"startTime":null,"endTime":null,"localStartTime":null,"localEndTime":null,"eventRegistrationLink":null,"joinEventLink":null,"facebookLink":null,"meetupLink":null,"website":null,"contactInfo":null,"isEvent":false,"eventImageId":null,"eventType":null,"types":[],"groupId":null,"reviewedByUserId":"r38pkCm7wF4M44MDQ","suggestForCuratedUserIds":null,"suggestForCuratedUsernames":null,"reviewForCuratedUserId":null,"authorIsUnreviewed":false,"afDate":null,"suggestForAlignmentUserIds":[],"reviewForAlignmentUserId":null,"afBaseScore":96,"afExtendedScore":{"reacts":{},"agreement":0,"approvalVoteCount":61,"agreementVoteCount":0},"afCommentCount":0,"afLastCommentedAt":"2025-04-01T21:04:09.509Z","afSticky":false,"hideAuthor":false,"moderationStyle":null,"ignoreRateLimits":null,"submitToFrontpage":true,"shortform":false,"onlyVisibleToLoggedIn":false,"onlyVisibleToEstablishedAccounts":false,"reviewCount":0,"reviewVoteCount":0,"positiveReviewVoteCount":0,"manifoldReviewMarketId":null,"annualReviewMarketProbability":null,"annualReviewMarketIsResolved":null,"annualReviewMarketYear":null,"annualReviewMarketUrl":null,"group":null,"rsvpCounts":{},"podcastEpisodeId":null,"forceAllowType3Audio":false,"nominationCount2019":0,"reviewCount2019":0,"votingSystem":"namesAttachedReactions","disableRecommendation":false,"user":{"__ref":"User:vvqpGvkYqLcerYph6"},"coauthors":[],"slug":"vdt-a-solution-to-decision-theory","title":"VDT: a solution to decision theory","draft":false,"hideCommentKarma":false,"af":false,"currentUserReviewVote":null,"coauthorStatuses":null,"hasCoauthorPermission":true,"rejected":false,"collabEditorDialogue":false}}</script>
<script>window.__APOLLO_FOREIGN_STATE__ = {}</script>

<script src="library_files/api.js"></script><script async="" src="library_files/js.js"></script><iframe id="intercom-frame" style="position: absolute !important; opacity: 0 !important; width: 1px !important; height: 1px !important; top: 0 !important; left: 0 !important; border: none !important; display: block !important; z-index: -1 !important; pointer-events: none;" aria-hidden="true" tabindex="-1" title="Intercom"></iframe><div><div class="grecaptcha-badge" data-style="bottomright" style="width: 256px; height: 60px; display: block; transition: right 0.3s; position: fixed; bottom: 14px; right: -186px; box-shadow: gray 0px 0px 5px; border-radius: 2px; overflow: hidden;"><div class="grecaptcha-logo"><iframe title="reCAPTCHA" width="256" height="60" role="presentation" name="a-1kj2t3fu29qo" frameborder="0" scrolling="no" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation" src="library_files/anchor.html"></iframe></div><div class="grecaptcha-error"></div><textarea id="g-recaptcha-response-100000" name="g-recaptcha-response" class="g-recaptcha-response" style="width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;"></textarea></div><iframe style="display: none;"></iframe></div><div class="intercom-lightweight-app"><div class="intercom-lightweight-app-launcher intercom-launcher" role="button" tabindex="0" aria-label="Open Intercom Messenger" aria-live="polite"><div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-open"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 28 32"><path d="M28 32s-4.714-1.855-8.527-3.34H3.437C1.54 28.66 0 27.026 0 25.013V3.644C0 1.633 1.54 0 3.437 0h21.125c1.898 0 3.437 1.632 3.437 3.645v18.404H28V32zm-4.139-11.982a.88.88 0 00-1.292-.105c-.03.026-3.015 2.681-8.57 2.681-5.486 0-8.517-2.636-8.571-2.684a.88.88 0 00-1.29.107 1.01 1.01 0 00-.219.708.992.992 0 00.318.664c.142.128 3.537 3.15 9.762 3.15 6.226 0 9.621-3.022 9.763-3.15a.992.992 0 00.317-.664 1.01 1.01 0 00-.218-.707z"></path></svg></div><div class="intercom-lightweight-app-launcher-icon intercom-lightweight-app-launcher-icon-minimize"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path fill-rule="evenodd" clip-rule="evenodd" d="M18.601 8.39897C18.269 8.06702 17.7309 8.06702 17.3989 8.39897L12 13.7979L6.60099 8.39897C6.26904 8.06702 5.73086 8.06702 5.39891 8.39897C5.06696 8.73091 5.06696 9.2691 5.39891 9.60105L11.3989 15.601C11.7309 15.933 12.269 15.933 12.601 15.601L18.601 9.60105C18.9329 9.2691 18.9329 8.73091 18.601 8.39897Z" fill="white"></path>
</svg>
</div></div><style id="intercom-lightweight-app-style" type="text/css">
  @keyframes intercom-lightweight-app-launcher {
    from {
      opacity: 0;
      transform: scale(0.5);
    }
    to {
      opacity: 1;
      transform: scale(1);
    }
  }

  @keyframes intercom-lightweight-app-gradient {
    from {
      opacity: 0;
    }
    to {
      opacity: 1;
    }
  }

  @keyframes intercom-lightweight-app-messenger {
    0% {
      opacity: 0;
      transform: scale(0);
    }
    40% {
      opacity: 1;
    }
    100% {
      transform: scale(1);
    }
  }

  .intercom-lightweight-app {
    position: fixed;
    z-index: 2147483001;
    width: 0;
    height: 0;
    font-family: intercom-font, "Helvetica Neue", "Apple Color Emoji", Helvetica, Arial, sans-serif;
  }

  .intercom-lightweight-app-gradient {
    position: fixed;
    z-index: 2147483002;
    width: 500px;
    height: 500px;
    bottom: 0;
    right: 0;
    pointer-events: none;
    background: radial-gradient(
      ellipse at bottom right,
      rgba(29, 39, 54, 0.16) 0%,
      rgba(29, 39, 54, 0) 72%);
    animation: intercom-lightweight-app-gradient 200ms ease-out;
  }

  .intercom-lightweight-app-launcher {
    position: fixed;
    z-index: 2147483003;
    padding: 0 !important;
    margin: 0 !important;
    border: none;
    bottom: 20px;
    right: 20px;
    max-width: 48px;
    width: 48px;
    max-height: 48px;
    height: 48px;
    border-radius: 50%;
    background: #f5f5f5;
    cursor: pointer;
    box-shadow: 0 1px 6px 0 rgba(0, 0, 0, 0.06), 0 2px 32px 0 rgba(0, 0, 0, 0.16);
    transition: transform 167ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
    box-sizing: content-box;
  }


  .intercom-lightweight-app-launcher:hover {
    transition: transform 250ms cubic-bezier(0.33, 0.00, 0.00, 1.00);
    transform: scale(1.1)
  }

  .intercom-lightweight-app-launcher:active {
    transform: scale(0.85);
    transition: transform 134ms cubic-bezier(0.45, 0, 0.2, 1);
  }


  .intercom-lightweight-app-launcher:focus {
    outline: none;

    
  }

  .intercom-lightweight-app-launcher-icon {
    display: flex;
    align-items: center;
    justify-content: center;
    position: absolute;
    top: 0;
    left: 0;
    width: 48px;
    height: 48px;
    transition: transform 100ms linear, opacity 80ms linear;
  }

  .intercom-lightweight-app-launcher-icon-open {
    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-open svg {
    width: 24px;
    height: 24px;
  }

  .intercom-lightweight-app-launcher-icon-open svg path {
    fill: rgb(0, 0, 0);
  }

  .intercom-lightweight-app-launcher-icon-self-serve {
    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-self-serve svg {
    height: 44px;
  }

  .intercom-lightweight-app-launcher-icon-self-serve svg path {
    fill: rgb(0, 0, 0);
  }

  .intercom-lightweight-app-launcher-custom-icon-open {
    max-height: 24px;
    max-width: 24px;

    
        opacity: 1;
        transform: rotate(0deg) scale(1);
      
  }

  .intercom-lightweight-app-launcher-icon-minimize {
    
        opacity: 0;
        transform: rotate(-60deg) scale(0);
      
  }

  .intercom-lightweight-app-launcher-icon-minimize svg path {
    fill: rgb(0, 0, 0);
  }

  .intercom-lightweight-app-messenger {
    position: fixed;
    z-index: 2147483003;
    overflow: hidden;
    background-color: white;
    animation: intercom-lightweight-app-messenger 250ms cubic-bezier(0, 1, 1, 1);
    transform-origin: bottom right;

    
        width: 400px;
        height: calc(100% - 104px);
        max-height: 704px;
        min-height: 250px;
        right: 20px;
        bottom: 84px;
        box-shadow: 0 5px 40px rgba(0,0,0,0.16);
      

    border-radius: 16px;
  }

  .intercom-lightweight-app-messenger-header {
    height: 64px;
    border-bottom: none;
    background: white
  }

  .intercom-lightweight-app-messenger-footer{
    position:absolute;
    bottom:0;
    width: 100%;
    height: 80px;
    background: #fff;
    font-size: 14px;
    line-height: 21px;
    border-top: 1px solid rgba(0, 0, 0, 0.05);
    box-shadow: 0px 0px 25px rgba(0, 0, 0, 0.05);
  }

  @media print {
    .intercom-lightweight-app {
      display: none;
    }
  }
</style></div></body></html>